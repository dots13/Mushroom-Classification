 2/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
 2/2: train = pd.read_csv('products_sentiment_train.tsv', sep='\t', header=None, names=['text', 'label'])
 2/3: train
 2/4: train[0]
 2/5: train
 2/6: train['0']
 2/7: train.dtypes
 2/8: train
 2/9: train[train.label == 1]
2/10: train.iloc[1, :]
2/11: train.iloc[2, :]
2/12: train.iloc[3, :]
2/13: st = train.iloc[3, :]
2/14: st
2/15: st.text
2/16: train.iloc[3, :].text
2/17: st.text
2/18: train.iloc[5, :].text
2/19: train.iloc[8, :].text
2/20: train.iloc[10, :].text
2/21: train.iloc[19, :].text
2/22: train.iloc[44, :].text
2/23: train.iloc[66, :].text
2/24: train.iloc[65, :].text
2/25: train.iloc[64, :].text
2/26: train.iloc[63, :].text
2/27: train.iloc[62, :].text
2/28: train.iloc[6612, :].text
2/29: train.iloc[61, :].text
2/30: train.iloc[67, :].text
2/31: train.iloc[68, :].text
2/32: train.iloc[69, :].text
2/33: train.iloc[70, :].text
2/34: train.iloc[71, :].text
2/35: train.iloc[72, :].text
2/36: train.iloc[73, :].text
2/37: train.iloc[74, :].text
2/38: train.iloc[75, :].text
2/39: train.iloc[88, :].text
 3/1:
import plotly.graph_objects as go
fig = go.Figure(data=go.Bar(y=[2, 3, 1]))
fig.show()
 3/2:
import plotly.graph_objects as go
fig = go.Figure(data=go.Bar(y=[2, 3, 1]))
 3/3: fig.show()
 3/4:
import plotly.graph_objects as go
fig = go.Figure(data=go.Bar(y=[2, 3, 1]))
fig.write_html('first_figure.html', auto_open=True)
 4/1:
import plotly.graph_objects as go
fig = go.Figure(data=go.Bar(y=[2, 3, 1]))
fig.show()
 4/2:
import plotly.graph_objects as go
fig = go.FigureWidget(data=go.Bar(y=[2, 3, 1]))
fig
 4/3: fig
 4/4:
import plotly.graph_objects as go
fig = go.Figure(data=go.Bar(y=[2, 3, 1]))
fig.show()
 4/5:
# Import the necessaries libraries
import plotly.offline as pyo
import plotly.graph_objs as go
# Set notebook mode to work in offline
pyo.init_notebook_mode()
# Create traces
trace0 = go.Scatter(
    x=[1, 2, 3, 4],
    y=[10, 15, 13, 17]
)
trace1 = go.Scatter(
    x=[1, 2, 3, 4],
    y=[16, 5, 11, 9]
)
# Fill out data with our traces
data = [trace0, trace1]
# Plot it and save as basic-line.html
pyo.iplot(data, filename = 'basic-line')
 5/1:
import plotly.graph_objects as go
fig = go.FigureWidget(data=go.Bar(y=[2, 3, 1]))
fig
 5/2: fig
 5/3:
# Import the necessaries libraries
import plotly.offline as pyo
import plotly.graph_objs as go
# Set notebook mode to work in offline
pyo.init_notebook_mode()
# Create traces
trace0 = go.Scatter(
    x=[1, 2, 3, 4],
    y=[10, 15, 13, 17]
)
trace1 = go.Scatter(
    x=[1, 2, 3, 4],
    y=[16, 5, 11, 9]
)
# Fill out data with our traces
data = [trace0, trace1]
# Plot it and save as basic-line.html
pyo.iplot(data, filename = 'basic-line')
 5/4: !pip install dash==1.12.0
 5/5:
import dash
import dash_core_components as dcc
import dash_html_components as html

external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']

app = dash.Dash(__name__, external_stylesheets=external_stylesheets)

app.layout = html.Div(children=[
    html.H1(children='Hello Dash'),

    html.Div(children='''
        Dash: A web application framework for Python.
    '''),

    dcc.Graph(
        id='example-graph',
        figure={
            'data': [
                {'x': [1, 2, 3], 'y': [4, 1, 2], 'type': 'bar', 'name': 'SF'},
                {'x': [1, 2, 3], 'y': [2, 4, 5], 'type': 'bar', 'name': u'Montr√©al'},
            ],
            'layout': {
                'title': 'Dash Data Visualization'
            }
        }
    )
])

if __name__ == '__main__':
    app.run_server(debug=True)
 5/6: ! python app.py
 5/7: !python app.py
 6/1: !pwd
 6/2: !pwt
 6/3: !pwd
 6/4: %pwd
 6/5: install pandas as pd
 6/6: instal pandas as pd
 6/7: import pandas as pd
 6/8: df = pd.read_csv('phasmophobia.csv')
 6/9: df
6/10: df.columns
6/11: 'df.head()'
6/12: df.head()
6/13: df[['key', 'en', 'rus']]
6/14: df[['key', 'en', 'ru']]
6/15: pandas.set_option('display.max_rows', df.shape[0]+1)
6/16: pd.set_option('display.max_rows', df.shape[0]+1)
6/17: df[['key', 'en', 'ru']]
6/18: df[['key', 'en', 'ru']].iloc[:, 348:]
6/19: df[['key', 'en', 'ru']].iloc[348:, :]
6/20: df[['key', 'en', 'ru']].iloc[347:, :]
6/21: df[['key', 'en', 'ru']].iloc[348:, :]
6/22: df[['key', 'en', 'ru']].iloc[348:, :].to_exel('phasmophobia_rus.xlsx')
6/23: df[['key', 'en', 'ru']].iloc[348:, :].to_excel('phasmophobia_rus.xlsx')
 7/1:
import gspread
from oauth2client.service_account import ServiceAccountCredentials
import pandas as pd
 7/2:
import pip
pip.main(["install", "gspread"])
 7/3:
import pip
%pip.main(["install", "gspread"])
 7/4:
import pip
pip.main(["install", "gspread"])
 7/5:
from __future__ import print_function
from apiclient.discovery import build
from httplib2 import Http
from oauth2client import file, client, tools
import pandas as pd
 7/6:
from __future__ import print_function
from apiclient.discovery import build
from httplib2 import Http
from oauth2client import file, client, tools
import pandas as pd
 7/7:
SPREADSHEET_ID = # <Your spreadsheet ID>
RANGE_NAME = # <Your worksheet name>
 7/8:
def get_google_sheet(spreadsheet_id, range_name):
    """ Retrieve sheet data using OAuth credentials and Google Python API. """
    scopes = 'https://www.googleapis.com/auth/spreadsheets.readonly'
    # Setup the Sheets API
    store = file.Storage('credentials.json')
    creds = store.get()
    if not creds or creds.invalid:
        flow = client.flow_from_clientsecrets('client_secret.json', scopes)
        creds = tools.run_flow(flow, store)
    service = build('sheets', 'v4', http=creds.authorize(Http()))

    # Call the Sheets API
    gsheet = service.spreadsheets().values().get(spreadsheetId=spreadsheet_id, range=range_name).execute()
    return gsheet


def gsheet2df(gsheet):
    """ Converts Google sheet data to a Pandas DataFrame.
    Note: This script assumes that your data contains a header file on the first row!
    Also note that the Google API returns 'none' from empty cells - in order for the code
    below to work, you'll need to make sure your sheet doesn't contain empty cells,
    or update the code to account for such instances.
    """
    header = gsheet.get('values', [])[0]   # Assumes first line is header!
    values = gsheet.get('values', [])[1:]  # Everything else is data.
    if not values:
        print('No data found.')
    else:
        all_data = []
        for col_id, col_name in enumerate(header):
            column_data = []
            for row in values:
                column_data.append(row[col_id])
            ds = pd.Series(data=column_data, name=col_name)
            all_data.append(ds)
        df = pd.concat(all_data, axis=1)
        return df
 7/9:
SPREADSHEET_ID = '13XNFMKz0L0VRGvHhJt7rz-WBICyHgSwX_gTCqEH2J_4'
RANGE_NAME = "USA"
7/10:
def get_google_sheet(spreadsheet_id, range_name):
    """ Retrieve sheet data using OAuth credentials and Google Python API. """
    scopes = 'https://www.googleapis.com/auth/spreadsheets.readonly'
    # Setup the Sheets API
    store = file.Storage('credentials.json')
    creds = store.get()
    if not creds or creds.invalid:
        flow = client.flow_from_clientsecrets('client_secret.json', scopes)
        creds = tools.run_flow(flow, store)
    service = build('sheets', 'v4', http=creds.authorize(Http()))

    # Call the Sheets API
    gsheet = service.spreadsheets().values().get(spreadsheetId=spreadsheet_id, range=range_name).execute()
    return gsheet


def gsheet2df(gsheet):
    """ Converts Google sheet data to a Pandas DataFrame.
    Note: This script assumes that your data contains a header file on the first row!
    Also note that the Google API returns 'none' from empty cells - in order for the code
    below to work, you'll need to make sure your sheet doesn't contain empty cells,
    or update the code to account for such instances.
    """
    header = gsheet.get('values', [])[0]   # Assumes first line is header!
    values = gsheet.get('values', [])[1:]  # Everything else is data.
    if not values:
        print('No data found.')
    else:
        all_data = []
        for col_id, col_name in enumerate(header):
            column_data = []
            for row in values:
                column_data.append(row[col_id])
            ds = pd.Series(data=column_data, name=col_name)
            all_data.append(ds)
        df = pd.concat(all_data, axis=1)
        return df
7/11:
gsheet = get_google_sheet(SPREADSHEET_ID, RANGE_NAME)
df = gsheet2df(gsheet)
print('Dataframe size = ', df.shape)
7/12:
from io import BytesIO

import requests
7/13:
r = requests.get('https://docs.google.com/spreadsheets/d/13XNFMKz0L0VRGvHhJt7rz-WBICyHgSwX_gTCqEH2J_4/edit#gid=1710185683=csv')
data = r.content
7/14: df = pd.read_csv(BytesIO(data), index_col=0,parse_dates=['Quradate'])
7/15: df = pd.read_csv(BytesIO(data), index_col=0,parse_dates=['USA'])
7/16:
r = requests.get('https://docs.google.com/spreadsheets/d/13XNFMKz0L0VRGvHhJt7rz-WBICyHgSwX_gTCqEH2J_4//export?format=csv&gid=1710185683=csv')
data = r.content
7/17: df = pd.read_csv(BytesIO(data), index_col=0,parse_dates=['USA'])
7/18: df = pd.read_csv(BytesIO(data), index_col=0,parse_dates=['Quradate'])
7/19: df = pd.read_csv(BytesIO(data), index_col=0,parse_dates=['ID'])
7/20: df = pd.read_csv(BytesIO(data), index_col=0)
7/21: import pandas as pd
7/22:
xls = pd.ExcelFile('Data Scientist Test.xlsx')
df1 = pd.read_excel(xls, 'USA')
7/23: df_1
7/24:
xls = pd.ExcelFile('Data Scientist Test.xlsx')
df_1 = pd.read_excel(xls, 'USA')
7/25: df_1
7/26: df_1.reset_index()
7/27: df_1.columns
7/28: df_1
7/29: df_1.reset_index(0).reset_index(drop=True)
7/30: df_1.reset_index(0).reset_index(drop=True).reset_index(0).reset_index(drop=True)
7/31: df_1.reset_index(0)
7/32: df_1.reset_index(1)
7/33: df_1.columns.droplevel(0)
7/34:
xls = pd.ExcelFile('Data Scientist Test.xlsx')
df_1 = pd.read_excel(xls, 'USA')
7/35: df_1.columns.droplevel(0)
7/36: df_1
7/37:
xls = pd.ExcelFile('Data Scientist Test.xlsx', header=2)
df_1 = pd.read_excel(xls, 'USA')
7/38:
xls = pd.ExcelFile('Data Scientist Test.xls')
df_1 = pd.read_excel(xls, 'USA', header=2)
7/39:
xls = pd.ExcelFile('Data Scientist Test.xlsx')
df_1 = pd.read_excel(xls, 'USA', header=2)
7/40: df_1
7/41:
xls = pd.ExcelFile('Data Scientist Test.xlsx')
df_1 = pd.read_excel(xls, 'USA', header=2, index_col=1)
7/42: df_1
7/43:
xls = pd.ExcelFile('Data Scientist Test.xlsx')
df_1 = pd.read_excel(xls, 'USA', header=2, index_col=0)
7/44: df_1
7/45: df_1.describe()
7/46: df_1.isnull().mean()
7/47: df_1
7/48: df_1
7/49: df_1[df_1['Username (can be anything inoffensive)'].notnull()]
7/50:
xls = pd.ExcelFile('Data Scientist Test.xlsx')
df_1 = pd.read_excel(xls, 'USA', header=2, index_col=0)
df_2 = pd.read_excel(xls, 'UK', header=2, index_col=0)
df_3 = pd.read_excel(xls, 'Canada', header=2, index_col=0)
df_4 = pd.read_excel(xls, 'Asia Pacific & Europe (excl UK)', header=2, index_col=0)
7/51: df_2
7/52: df_2[df_2['Username (can be anything inoffensive)'].notnull()]
7/53:
df_1 = df_1[df_1['Username (can be anything inoffensive)'].notnull()]
print(df_1.shape)
7/54:
df_2 = df_2[df_2['Username (can be anything inoffensive)'].notnull()]
print(df_2.shape)
7/55:
df_3 = df_3[df_3['Username (can be anything inoffensive)'].notnull()]
print(df_3.shape)
7/56:
df_4 = df_4[df_4['Username (can be anything inoffensive)'].notnull()]
print(df_4.shape)
7/57: df = pd.concat([df_1, df_2, df_3, df_4], ignore_index=True)
7/58: df = pd.concat([df_1, df_2, df_3, df_4], ignore_index=True, sort=False)
7/59: df
7/60: df_1
7/61: df.head(1)
7/62: df_1.columns
7/63: df_1[['Typical range after correction if range mode was off.1']]
7/64: df_4.head(1)
7/65:
import pandas as pd
pd.set_option('display.max_columns', None)
7/66: df = pd.concat([df_1, df_2, df_3, df_4], ignore_index=True, sort=False)
7/67: df.head(1)
7/68: df_4.head(1)
7/69: df.isnull().mean()
7/70: df.isnull()
7/71: df.isnull().sum()
7/72:
import pandas as pd
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
7/73: df.isnull().sum()
7/74: df.isnull().mean()
7/75: df.columns[df.isnull().all()]
7/76: df_1.columns[df_1.isnull().all()]
7/77: df_2.columns[df_2.isnull().all()]
7/78: df_1.head(1)
7/79: df_1.head(10)
7/80: df_4
7/81: df_4[df_4['Did you have a battery replacement?' == 'Yes']]
7/82: df_4[df_4['Did you have a battery replacement?'] == 'Yes']
7/83: df_4[df_4['Did you have a battery replacement?'] == 'Yes'][['Battery age (days) after correction if battery was replaced', 'Vehicle age (days)']]
7/84: df_4
7/85: df_4['day_before_repl'] = df_4['Vehicle age (days)'] - df_4['Battery age (days) after correction if battery was replaced']
7/86: df_4.day_before_repl.describecribe()
7/87: df_4.day_before_repl
7/88: df_4.day_before_repl.describe()
7/89: df_4[df_4.day_before_repl > 0].day_before_repl.describe()
7/90:
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode, iplot
init_notebook_mode(connected = True)
7/91:
import os
import matplotlib.pyplot as plt#visualization
from PIL import  Image
%matplotlib inline
warnings.filterwarnings("ignore")
import io
import plotly.offline as py#visualization
py.init_notebook_mode(connected=True)#visualization
import plotly.graph_objs as go#visualization
import plotly.tools as tls#visualization
import plotly.figure_factory as ff
7/92:
import os
import matplotlib.pyplot as plt#visualization
from PIL import  Image
%matplotlib inline
import io
import plotly.offline as py#visualization
py.init_notebook_mode(connected=True)#visualization
import plotly.graph_objs as go#visualization
import plotly.tools as tls#visualization
import plotly.figure_factory as ff
7/93:
Anastasia, [22.11.20 18:34]
def histogram2data(data, column1, name1, data2, column2, name2, bound) :
    trace1 = go.Histogram(x  = data[column1],
                          histnorm= "percent",
                          name = name1,
                          xbins=dict(start=np.min(data[column1]), size=bound, end=np.max(data[column1])),
                          marker = dict(line = dict(width = .5,
                                                    color = "black"
                                                    )
                                        ),
                         opacity = .5 
                         ) 
    trace2 = go.Histogram(x  = data2[column2],
                          histnorm= "percent",
                          name = name2,
                          xbins=dict(start=np.min(data2[column2]), size=bound, end=np.max(data2[column2])),
                          marker = dict(line = dict(width = .5,
                                                    color = "black"
                                                    )
                                        ),
                         opacity = .5 
                         ) 
    datadata = [trace1, trace2]
    layout = go.Layout(dict(title =column1 + " distribution ",
                            plot_bgcolor  = "rgba(243,243,243, 0.5)",
                            paper_bgcolor = "rgba(243,243,243, 0.5)",
                            barmode='overlay',
                            
                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',
                                             title = column1,
#                                             zerolinewidth=1,
                                             ticklen=5,
                                             gridwidth=2
                                            ),
                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',
                                             title = "percent",
                                             zerolinewidth=1,
                                             ticklen=5,
                                             gridwidth=2
                                            ),
                           )
                      )
    fig  = go.Figure(data=datadata, layout=layout)
    py.iplot(fig)

Anastasia, [22.11.20 18:35]
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode, iplot
init_notebook_mode(connected = True)
7/94:
def histogram2data(data, column1, name1, data2, column2, name2, bound) :
    trace1 = go.Histogram(x  = data[column1],
                          histnorm= "percent",
                          name = name1,
                          xbins=dict(start=np.min(data[column1]), size=bound, end=np.max(data[column1])),
                          marker = dict(line = dict(width = .5,
                                                    color = "black"
                                                    )
                                        ),
                         opacity = .5 
                         ) 
    trace2 = go.Histogram(x  = data2[column2],
                          histnorm= "percent",
                          name = name2,
                          xbins=dict(start=np.min(data2[column2]), size=bound, end=np.max(data2[column2])),
                          marker = dict(line = dict(width = .5,
                                                    color = "black"
                                                    )
                                        ),
                         opacity = .5 
                         ) 
    datadata = [trace1, trace2]
    layout = go.Layout(dict(title =column1 + " distribution ",
                            plot_bgcolor  = "rgba(243,243,243, 0.5)",
                            paper_bgcolor = "rgba(243,243,243, 0.5)",
                            barmode='overlay',
                            
                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',
                                             title = column1,
#                                             zerolinewidth=1,
                                             ticklen=5,
                                             gridwidth=2
                                            ),
                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',
                                             title = "percent",
                                             zerolinewidth=1,
                                             ticklen=5,
                                             gridwidth=2
                                            ),
                           )
                      )
    fig  = go.Figure(data=datadata, layout=layout)
    py.iplot(fig)

Anastasia, [22.11.20 18:35]
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode, iplot
init_notebook_mode(connected = True)
7/95:
def histogram2data(data, column1, name1, data2, column2, name2, bound) :
    trace1 = go.Histogram(x  = data[column1],
                          histnorm= "percent",
                          name = name1,
                          xbins=dict(start=np.min(data[column1]), size=bound, end=np.max(data[column1])),
                          marker = dict(line = dict(width = .5,
                                                    color = "black"
                                                    )
                                        ),
                         opacity = .5 
                         ) 
    trace2 = go.Histogram(x  = data2[column2],
                          histnorm= "percent",
                          name = name2,
                          xbins=dict(start=np.min(data2[column2]), size=bound, end=np.max(data2[column2])),
                          marker = dict(line = dict(width = .5,
                                                    color = "black"
                                                    )
                                        ),
                         opacity = .5 
                         ) 
    datadata = [trace1, trace2]
    layout = go.Layout(dict(title =column1 + " distribution ",
                            plot_bgcolor  = "rgba(243,243,243, 0.5)",
                            paper_bgcolor = "rgba(243,243,243, 0.5)",
                            barmode='overlay',
                            
                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',
                                             title = column1,
#                                             zerolinewidth=1,
                                             ticklen=5,
                                             gridwidth=2
                                            ),
                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',
                                             title = "percent",
                                             zerolinewidth=1,
                                             ticklen=5,
                                             gridwidth=2
                                            ),
                           )
                      )
    fig  = go.Figure(data=datadata, layout=layout)
    py.iplot(fig)
7/96: df_4[df_4['Did you have a battery replacement?'] == 'Yes'][['Battery age (days) after correction if battery was replaced', 'Vehicle age (days)', day_before_repl]]
7/97: df_4[df_4['Did you have a battery replacement?'] == 'Yes'][['Battery age (days) after correction if battery was replaced', 'Vehicle age (days)', day_before_repl]]
7/98: df_4[df_4['Did you have a battery replacement?'] == 'Yes'][['Battery age (days) after correction if battery was replaced', 'Vehicle age (days)', 'day_before_repl']]
7/99: df_4[df_4['Did you have a battery replacement?'] == 'Yes'][['Battery age (days) after correction if battery was replaced', 'Vehicle age (days)', 'day_before_repl']].head(10)
7/100: histogram2data(df_4[df_4.day_before_repl > 0], 'Vehicle age (days)', 1, df_4[df_4.day_before_repl == 0], 'Vehicle age (days)', 1)
7/101: histogram2data(df_4[df_4.day_before_repl > 0], 'Vehicle age (days)', '>0', df_4[df_4.day_before_repl == 0], 'Vehicle age (days)', '0', 1)
7/102:
import pandas as pd
import numpy as np
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
7/103: histogram2data(df_4[df_4.day_before_repl > 0], 'Vehicle age (days)', '>0', df_4[df_4.day_before_repl == 0], 'Vehicle age (days)', '0', 1)
7/104:
import os
import matplotlib.pyplot as plt#visualization
from PIL import  Image
%matplotlib inline
import pandas as pd
import seaborn as sns#visualization
import itertools
import warnings
warnings.filterwarnings("ignore")
import io
import plotly.offline as py#visualization
py.init_notebook_mode(connected=True)#visualization
import plotly.graph_objs as go#visualization
import plotly.tools as tls#visualization
import plotly.figure_factory as ff
7/105: histogram2data(df_4[df_4.day_before_repl > 0], 'Vehicle age (days)', '>0', df_4[df_4.day_before_repl == 0], 'Vehicle age (days)', '0', 1)
7/106:
import pandas as pd
import numpy as np
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
7/107:
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode, iplot
init_notebook_mode(connected = True)
7/108:
import os
import matplotlib.pyplot as plt#visualization
from PIL import  Image
%matplotlib inline
import pandas as pd
import seaborn as sns#visualization
import itertools
import warnings
warnings.filterwarnings("ignore")
import io
import plotly.offline as py#visualization
py.init_notebook_mode(connected=True)#visualization
import plotly.graph_objs as go#visualization
import plotly.tools as tls#visualization
import plotly.figure_factory as ff
7/109:
def histogram2data(data, column1, name1, data2, column2, name2, bound) :
    trace1 = go.Histogram(x  = data[column1],
                          histnorm= "percent",
                          name = name1,
                          xbins=dict(start=np.min(data[column1]), size=bound, end=np.max(data[column1])),
                          marker = dict(line = dict(width = .5,
                                                    color = "black"
                                                    )
                                        ),
                         opacity = .5 
                         ) 
    trace2 = go.Histogram(x  = data2[column2],
                          histnorm= "percent",
                          name = name2,
                          xbins=dict(start=np.min(data2[column2]), size=bound, end=np.max(data2[column2])),
                          marker = dict(line = dict(width = .5,
                                                    color = "black"
                                                    )dat
                                        
                                        ),
                         opacity = .5 
                         ) 
    datadata = [trace1, trace2]
    layout = go.Layout(dict(title =column1 + " distribution ",
                            plot_bgcolor  = "rgba(243,243,243, 0.5)",
                            paper_bgcolor = "rgba(243,243,243, 0.5)",
                            barmode='overlay',
                            
                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',
                                             title = column1,
#                                             zerolinewidth=1,
                                             ticklen=5,
                                             gridwidth=2
                                            ),
                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',
                                             title = "percent",
                                             zerolinewidth=1,
                                             ticklen=5,
                                             gridwidth=2
                                            ),
                           )
                      )
    fig  = go.Figure(data=datadata, layout=layout)
    py.iplot(fig)
7/110:
def histogram2data(data, column1, name1, data2, column2, name2, bound) :
    trace1 = go.Histogram(x  = data[column1],
                          histnorm= "percent",
                          name = name1,
                          xbins=dict(start=np.min(data[column1]), size=bound, end=np.max(data[column1])),
                          marker = dict(line = dict(width = .5,
                                                    color = "black"
                                                    )
                                        ),
                         opacity = .5 
                         ) 
    trace2 = go.Histogram(x  = data2[column2],
                          histnorm= "percent",
                          name = name2,
                          xbins=dict(start=np.min(data2[column2]), size=bound, end=np.max(data2[column2])),
                          marker = dict(line = dict(width = .5,
                                                    color = "black"
                                                    )
                                        ),
                         opacity = .5 
                         ) 
    datadata = [trace1, trace2]
    layout = go.Layout(dict(title =column1 + " distribution ",
                            plot_bgcolor  = "rgba(243,243,243, 0.5)",
                            paper_bgcolor = "rgba(243,243,243, 0.5)",
                            barmode='overlay',
                            
                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',
                                             title = column1,
#                                             zerolinewidth=1,
                                             ticklen=5,
                                             gridwidth=2
                                            ),
                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',
                                             title = "percent",
                                             zerolinewidth=1,
                                             ticklen=5,
                                             gridwidth=2
                                            ),
                           )
                      )
    fig  = go.Figure(data=datadata, layout=layout)
    py.iplot(fig)
7/111:
xls = pd.ExcelFile('Data Scientist Test.xlsx')
df_1 = pd.read_excel(xls, 'USA', header=2, index_col=0)
df_2 = pd.read_excel(xls, 'UK', header=2, index_col=0)
df_3 = pd.read_excel(xls, 'Canada', header=2, index_col=0)
df_4 = pd.read_excel(xls, 'Asia Pacific & Europe (excl UK)', header=2, index_col=0)
7/112:
df_1 = df_1[df_1['Username (can be anything inoffensive)'].notnull()]
print(df_1.shape)
7/113:
df_2 = df_2[df_2['Username (can be anything inoffensive)'].notnull()]
print(df_2.shape)
7/114:
df_3 = df_3[df_3['Username (can be anything inoffensive)'].notnull()]
print(df_3.shape)
7/115:
df_4 = df_4[df_4['Username (can be anything inoffensive)'].notnull()]
print(df_4.shape)
7/116: df = pd.concat([df_1, df_2, df_3, df_4], ignore_index=True, sort=False)
7/117: df_1.head(10)
7/118: df_4
7/119: df_4['day_before_repl'] = df_4['Vehicle age (days)'] - df_4['Battery age (days) after correction if battery was replaced']
7/120: df_4[df_4.day_before_repl > 0].day_before_repl.describe()
7/121: df_4[df_4['Did you have a battery replacement?'] == 'Yes'][['Battery age (days) after correction if battery was replaced', 'Vehicle age (days)', 'day_before_repl']].head(10)
7/122: histogram2data(df_4[df_4.day_before_repl > 0], 'Vehicle age (days)', '>0', df_4[df_4.day_before_repl == 0], 'Vehicle age (days)', '0', 1)
7/123: df.isnull().mean().
7/124: df_4.head(10)
7/125:
def histogram2data(data, column1, name1, data2, column2, name2, bound) :
    trace1 = go.Histogram(x  = data[column1],
                          histnorm= "percent",
                          name = name1,
                          xbins=dict(start=np.min(data[column1]), size=bound, end=np.max(data[column1])),
                          marker = dict(line = dict(width = .5,
                                                    color = "black"
                                                    )
                                        ),
                         opacity = .5 
                         ) 
    trace2 = go.Histogram(x  = data2[column2],
                          histnorm= "percent",
                          name = name2,
                          xbins=dict(start=np.min(data2[column2]), size=bound, end=np.max(data2[column2])),
                          marker = dict(line = dict(width = .5,
                                                    color = "black"
                                                    )
                                        ),
                         opacity = .5 
                         ) 
    datadata = [trace1, trace2]
    layout = go.Layout(dict(title =column1 + " distribution ",
                            plot_bgcolor  = "rgba(243,243,243, 0.5)",
                            paper_bgcolor = "rgba(243,243,243, 0.5)",
                            barmode='overlay',
                            
                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',
                                             title = column1,
#                                             zerolinewidth=1,
                                             ticklen=5,
                                             gridwidth=2
                                            ),
                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',
                                             title = "percent",
                                             zerolinewidth=1,
                                             ticklen=5,
                                             gridwidth=2
                                            ),
                           )
                      )
    fig  = go.Figure(data=datadata, layout=layout)
    %matplotlib inline
    iplot(fig)
7/126: histogram2data(df_4[df_4.day_before_repl > 0], 'Vehicle age (days)', '>0', df_4[df_4.day_before_repl == 0], 'Vehicle age (days)', '0', 1)
7/127:
def histogram2data(data, column1, name1, data2, column2, name2, bound) :
    trace1 = go.Histogram(x  = data[column1],
                          histnorm= "percent",
                          name = name1,
                          xbins=dict(start=np.min(data[column1]), size=bound, end=np.max(data[column1])),
                          marker = dict(line = dict(width = .5,
                                                    color = "black"
                                                    )
                                        ),
                         opacity = .5 
                         ) 
    trace2 = go.Histogram(x  = data2[column2],
                          histnorm= "percent",
                          name = name2,
                          xbins=dict(start=np.min(data2[column2]), size=bound, end=np.max(data2[column2])),
                          marker = dict(line = dict(width = .5,
                                                    color = "black"
                                                    )
                                        ),
                         opacity = .5 
                         ) 
    datadata = [trace1, trace2]
    layout = go.Layout(dict(title =column1 + " distribution ",
                            plot_bgcolor  = "rgba(243,243,243, 0.5)",
                            paper_bgcolor = "rgba(243,243,243, 0.5)",
                            barmode='overlay',
                            
                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',
                                             title = column1,
#                                             zerolinewidth=1,
                                             ticklen=5,
                                             gridwidth=2
                                            ),
                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',
                                             title = "percent",
                                             zerolinewidth=1,
                                             ticklen=5,
                                             gridwidth=2
                                            ),
                           )
                      )
    fig  = go.Figure(data=datadata, layout=layout)
    return fig
7/128: fig = histogram2data(df_4[df_4.day_before_repl > 0], 'Vehicle age (days)', '>0', df_4[df_4.day_before_repl == 0], 'Vehicle age (days)', '0', 1)
7/129: fig
7/130: fig.info
7/131: fig.info()
7/132:
%matplotlib inline
iplot(fig)
 9/1:
import pandas as pd
import numpy as np
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
 9/2:
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode, iplot
init_notebook_mode(connected = True)
 9/3:
import os
import matplotlib.pyplot as plt#visualization
from PIL import  Image
%matplotlib inline
import pandas as pd
import seaborn as sns#visualization
import itertools
import warnings
warnings.filterwarnings("ignore")
import io
import plotly.offline as py#visualization
py.init_notebook_mode(connected=True)#visualization
import plotly.graph_objs as go#visualization
import plotly.tools as tls#visualization
import plotly.figure_factory as ff
 9/4:
def histogram2data(data, column1, name1, data2, column2, name2, bound) :
    trace1 = go.Histogram(x  = data[column1],
                          histnorm= "percent",
                          name = name1,
                          xbins=dict(start=np.min(data[column1]), size=bound, end=np.max(data[column1])),
                          marker = dict(line = dict(width = .5,
                                                    color = "black"
                                                    )
                                        ),
                         opacity = .5 
                         ) 
    trace2 = go.Histogram(x  = data2[column2],
                          histnorm= "percent",
                          name = name2,
                          xbins=dict(start=np.min(data2[column2]), size=bound, end=np.max(data2[column2])),
                          marker = dict(line = dict(width = .5,
                                                    color = "black"
                                                    )
                                        ),
                         opacity = .5 
                         ) 
    datadata = [trace1, trace2]
    layout = go.Layout(dict(title =column1 + " distribution ",
                            plot_bgcolor  = "rgba(243,243,243, 0.5)",
                            paper_bgcolor = "rgba(243,243,243, 0.5)",
                            barmode='overlay',
                            
                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',
                                             title = column1,
#                                             zerolinewidth=1,
                                             ticklen=5,
                                             gridwidth=2
                                            ),
                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',
                                             title = "percent",
                                             zerolinewidth=1,
                                             ticklen=5,
                                             gridwidth=2
                                            ),
                           )
                      )
    fig  = go.Figure(data=datadata, layout=layout)
    py.iplot(fig)
 9/5:
xls = pd.ExcelFile('Data Scientist Test.xlsx')
df_1 = pd.read_excel(xls, 'USA', header=2, index_col=0)
df_2 = pd.read_excel(xls, 'UK', header=2, index_col=0)
df_3 = pd.read_excel(xls, 'Canada', header=2, index_col=0)
df_4 = pd.read_excel(xls, 'Asia Pacific & Europe (excl UK)', header=2, index_col=0)
 9/6:
df_1 = df_1[df_1['Username (can be anything inoffensive)'].notnull()]
print(df_1.shape)
 9/7:
df_2 = df_2[df_2['Username (can be anything inoffensive)'].notnull()]
print(df_2.shape)
 9/8:
df_3 = df_3[df_3['Username (can be anything inoffensive)'].notnull()]
print(df_3.shape)
 9/9:
df_4 = df_4[df_4['Username (can be anything inoffensive)'].notnull()]
print(df_4.shape)
9/10: df = pd.concat([df_1, df_2, df_3, df_4], ignore_index=True, sort=False)
9/11: df_1.head(10)
9/12: df_4.head(10)
9/13: df_4['day_before_repl'] = df_4['Vehicle age (days)'] - df_4['Battery age (days) after correction if battery was replaced']
9/14: df_4[df_4.day_before_repl > 0].day_before_repl.describe()
9/15: df_4[df_4['Did you have a battery replacement?'] == 'Yes'][['Battery age (days) after correction if battery was replaced', 'Vehicle age (days)', 'day_before_repl']].head(10)
9/16: histogram2data(df_4[df_4.day_before_repl > 0], 'Vehicle age (days)', '>0', df_4[df_4.day_before_repl == 0], 'Vehicle age (days)', '0', 1)
9/17: df.isnull().mean().
9/18: df_1.columns[df_1.isnull().all()]
9/19: df_2.columns[df_2.isnull().all()]
10/1:
import pandas as pd
import numpy as np
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
10/2:
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode, iplot
init_notebook_mode(connected = True)
10/3:
import os
import matplotlib.pyplot as plt#visualization
from PIL import  Image
%matplotlib inline
import pandas as pd
import seaborn as sns#visualization
import itertools
import warnings
warnings.filterwarnings("ignore")
import io
import plotly.offline as py#visualization
py.init_notebook_mode(connected=True)#visualization
import plotly.graph_objs as go#visualization
import plotly.tools as tls#visualization
import plotly.figure_factory as ff
10/4:
def histogram2data(data, column1, name1, data2, column2, name2, bound) :
    trace1 = go.Histogram(x  = data[column1],
                          histnorm= "percent",
                          name = name1,
                          xbins=dict(start=np.min(data[column1]), size=bound, end=np.max(data[column1])),
                          marker = dict(line = dict(width = .5,
                                                    color = "black"
                                                    )
                                        ),
                         opacity = .5 
                         ) 
    trace2 = go.Histogram(x  = data2[column2],
                          histnorm= "percent",
                          name = name2,
                          xbins=dict(start=np.min(data2[column2]), size=bound, end=np.max(data2[column2])),
                          marker = dict(line = dict(width = .5,
                                                    color = "black"
                                                    )
                                        ),
                         opacity = .5 
                         ) 
    datadata = [trace1, trace2]
    layout = go.Layout(dict(title =column1 + " distribution ",
                            plot_bgcolor  = "rgba(243,243,243, 0.5)",
                            paper_bgcolor = "rgba(243,243,243, 0.5)",
                            barmode='overlay',
                            
                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',
                                             title = column1,
#                                             zerolinewidth=1,
                                             ticklen=5,
                                             gridwidth=2
                                            ),
                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',
                                             title = "percent",
                                             zerolinewidth=1,
                                             ticklen=5,
                                             gridwidth=2
                                            ),
                           )
                      )
    fig  = go.Figure(data=datadata, layout=layout)
    py.iplot(fig)
10/5:
xls = pd.ExcelFile('Data Scientist Test.xlsx')
df_1 = pd.read_excel(xls, 'USA', header=2, index_col=0)
df_2 = pd.read_excel(xls, 'UK', header=2, index_col=0)
df_3 = pd.read_excel(xls, 'Canada', header=2, index_col=0)
df_4 = pd.read_excel(xls, 'Asia Pacific & Europe (excl UK)', header=2, index_col=0)
10/6:
df_1 = df_1[df_1['Username (can be anything inoffensive)'].notnull()]
print(df_1.shape)
10/7:
df_2 = df_2[df_2['Username (can be anything inoffensive)'].notnull()]
print(df_2.shape)
10/8:
df_3 = df_3[df_3['Username (can be anything inoffensive)'].notnull()]
print(df_3.shape)
10/9:
df_4 = df_4[df_4['Username (can be anything inoffensive)'].notnull()]
print(df_4.shape)
10/10: df = pd.concat([df_1, df_2, df_3, df_4], ignore_index=True, sort=False)
10/11: df_1.head(10)
10/12: df_4.head(10)
10/13: df_4['day_before_repl'] = df_4['Vehicle age (days)'] - df_4['Battery age (days) after correction if battery was replaced']
10/14: df_4[df_4.day_before_repl > 0].day_before_repl.describe()
10/15: df_4[df_4['Did you have a battery replacement?'] == 'Yes'][['Battery age (days) after correction if battery was replaced', 'Vehicle age (days)', 'day_before_repl']].head(10)
10/16: histogram2data(df_4[df_4.day_before_repl > 0], 'Vehicle age (days)', '>0', df_4[df_4.day_before_repl == 0], 'Vehicle age (days)', '0', 1)
10/17:
# Import the necessaries libraries
import plotly.offline as pyo
import plotly.graph_objs as go
# Set notebook mode to work in offline
pyo.init_notebook_mode()
# Create traces
trace0 = go.Scatter(
    x=[1, 2, 3, 4],
    y=[10, 15, 13, 17]
)
trace1 = go.Scatter(
    x=[1, 2, 3, 4],
    y=[16, 5, 11, 9]
)
# Fill out data with our traces
data = [trace0, trace1]
# Plot it and save as basic-line.html
pyo.iplot(data, filename = 'basic-line')
10/18: df_2.columns[df_2.isnull().all()]
10/19:
# Import the necessaries libraries
import plotly.offline as pyo
import plotly.graph_objs as go
# Set notebook mode to work in offline
pyo.init_notebook_mode()
# Create traces
trace0 = go.Scatter(
    x=[1, 2, 3, 4],
    y=[10, 15, 13, 17]
)
trace1 = go.Scatter(
    x=[1, 2, 3, 4],
    y=[16, 5, 11, 9]
)
# Fill out data with our traces
data = [trace0, trace1]
# Plot it and save as basic-line.html
pyo.iplot(data, filename = 'basic-line')
11/1:
import pandas as pd
import numpy as np
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
11/2:
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode, iplot
init_notebook_mode(connected = True)
11/3:
import os
import matplotlib.pyplot as plt#visualization
from PIL import  Image
%matplotlib inline
import pandas as pd
import seaborn as sns#visualization
import itertools
import warnings
warnings.filterwarnings("ignore")
import io
import plotly.offline as py#visualization
py.init_notebook_mode(connected=True)#visualization
import plotly.graph_objs as go#visualization
import plotly.tools as tls#visualization
import plotly.figure_factory as ff
11/4:
def histogram2data(data, column1, name1, data2, column2, name2, bound) :
    trace1 = go.Histogram(x  = data[column1],
                          histnorm= "percent",
                          name = name1,
                          xbins=dict(start=np.min(data[column1]), size=bound, end=np.max(data[column1])),
                          marker = dict(line = dict(width = .5,
                                                    color = "black"
                                                    )
                                        ),
                         opacity = .5 
                         ) 
    trace2 = go.Histogram(x  = data2[column2],
                          histnorm= "percent",
                          name = name2,
                          xbins=dict(start=np.min(data2[column2]), size=bound, end=np.max(data2[column2])),
                          marker = dict(line = dict(width = .5,
                                                    color = "black"
                                                    )
                                        ),
                         opacity = .5 
                         ) 
    datadata = [trace1, trace2]
    layout = go.Layout(dict(title =column1 + " distribution ",
                            plot_bgcolor  = "rgba(243,243,243, 0.5)",
                            paper_bgcolor = "rgba(243,243,243, 0.5)",
                            barmode='overlay',
                            
                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',
                                             title = column1,
#                                             zerolinewidth=1,
                                             ticklen=5,
                                             gridwidth=2
                                            ),
                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',
                                             title = "percent",
                                             zerolinewidth=1,
                                             ticklen=5,
                                             gridwidth=2
                                            ),
                           )
                      )
    fig  = go.Figure(data=datadata, layout=layout)
    py.iplot(fig)
11/5:
xls = pd.ExcelFile('Data Scientist Test.xlsx')
df_1 = pd.read_excel(xls, 'USA', header=2, index_col=0)
df_2 = pd.read_excel(xls, 'UK', header=2, index_col=0)
df_3 = pd.read_excel(xls, 'Canada', header=2, index_col=0)
df_4 = pd.read_excel(xls, 'Asia Pacific & Europe (excl UK)', header=2, index_col=0)
11/6:
df_1 = df_1[df_1['Username (can be anything inoffensive)'].notnull()]
print(df_1.shape)
11/7:
df_2 = df_2[df_2['Username (can be anything inoffensive)'].notnull()]
print(df_2.shape)
11/8:
df_3 = df_3[df_3['Username (can be anything inoffensive)'].notnull()]
print(df_3.shape)
11/9:
df_4 = df_4[df_4['Username (can be anything inoffensive)'].notnull()]
print(df_4.shape)
11/10: df = pd.concat([df_1, df_2, df_3, df_4], ignore_index=True, sort=False)
11/11: df_1.head(10)
11/12: df_4.head(10)
11/13: df_4['day_before_repl'] = df_4['Vehicle age (days)'] - df_4['Battery age (days) after correction if battery was replaced']
11/14: df_4[df_4.day_before_repl > 0].day_before_repl.describe()
11/15: df_4[df_4['Did you have a battery replacement?'] == 'Yes'][['Battery age (days) after correction if battery was replaced', 'Vehicle age (days)', 'day_before_repl']].head(10)
11/16: histogram2data(df_4[df_4.day_before_repl > 0], 'Vehicle age (days)', '>0', df_4[df_4.day_before_repl == 0], 'Vehicle age (days)', '0', 1)
11/17: histogram2data(df_4[df_4.day_before_repl > 0], 'Vehicle age (days)', '>0', df_4[df_4.day_before_repl == 0], 'Vehicle age (days)', '0', 10)
11/18: histogram2data(df_4[df_4.day_before_repl > 0], 'Vehicle age (days)', '>0', df_4[df_4.day_before_repl == 0], 'Vehicle age (days)', '0', 100)
12/1: import nltk
12/2: nltk.download()
13/1:
import joblib

matrix_train = joblib.load("data/matrix_train.jbl")
matrix_test = joblib.load("data/matrix_test.jbl")
13/2:
import requests

from matplotlib import pyplot as plt

%matplotlib inline


with open("data/lenna.png", "wb") as img_f:
    img_f.write(requests.get("https://i.stack.imgur.com/nL4u3.png").content)

im = plt.imread("lenna.png")

def plti(im, h=6, **kwargs):
    """
    Helper function to plot an image.
    """
    y = im.shape[0]
    x = im.shape[1]
    w = (y/x) * h
    plt.figure(figsize=(w,h))
    plt.imshow(im, interpolation="nearest", **kwargs)
    plt.axis('off')

plti(im)
13/3: import numpy as np
13/4: random_matrix = np.random.randint(min_val,max_val,(<num_rows>,<num_cols>))
13/5: random_matrix = np.random.randint(-10,100,(100,10))
13/6: random_matrix
13/7: random_matrix = np.random.randint(-100,100,(100,10))
13/8:
import numpy as np
from sklearn.model_selection import train_test_split
13/9: X_train, X_test, y_train, y_test = train_test_split(X, test_size=0.2)
13/10: X_train, X_test, y_train, y_test = train_test_split(random_matrix, test_size=0.2)
13/11: X_train, X_test = train_test_split(random_matrix, test_size=0.2)
13/12: X_train
13/13: X_train.describe()
13/14:
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
13/15:
df_describe = pd.DataFrame(X_train)
df_describe.describe()
13/16:
df_describe = pd.DataFrame(X_test)
df_describe.describe()
13/17:
random_matrix_1 = np.random.randint(-100,100,(50,3))
random_matrix_2 = np.random.randint(0,5,(50,3))
13/18: np.concatenate((random_matrix_1, random_matrix_2), axis=1)
13/19:
random_matrix_1 = np.random.randint(-100,100,(50,2))
random_matrix_2 = np.random.randint(0,5,(50,4))
13/20: np.concatenate((random_matrix_1, random_matrix_2), axis=1)
13/21: random_matrix = np.concatenate((random_matrix_1, random_matrix_2), axis=1)
13/22: X_train, X_test = train_test_split(random_matrix, test_size=0.2)
13/23:
df_describe = pd.DataFrame(X_train)
df_describe.describe()
13/24:
df_describe = pd.DataFrame(X_test)
df_describe.describe()
13/25: X_test
13/26: X_test[1]
13/27: X_test
13/28: X_test[:1]
13/29: X_test[[1]]
13/30: X_test[:,0]
13/31: X_test[:,5]
13/32: X_test[:,4]
13/33: X_test[:,4] == 0
13/34: X_test[X_test[:,4] == 0]
13/35: X_test[:,4]
13/36: X_test[X_test[:,2] == 0]
13/37: X_test[:,2]
13/38: X_test[X_test[:,2] == 0, 2]
13/39: X_test[X_test[:,2] == 0, 2] = -5
13/40: X_test[:,2]
13/41: X_test
14/1:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        pass
14/2: CalWeek('2018-51')
14/3:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
14/4: CalWeek('2018-51')
14/5: {%G-%V}".format('2018-51')
14/6:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
14/7: CalWeek('2018-51')
14/8: '%-10.5s' % ('2018-51')
14/9: '%G-%V' % ('2018-51')
14/10: '%G-%V' % (2018-51)
14/11: '%G-%V' % (2018,51)
14/12: '%G-%V' % (201851)
14/13: str('w')
14/14:  str(CalWeek('2018-51'))
14/15:  str(CalWeek('2018-51'))
14/16:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
14/17: CalWeek('2018-51')
14/18: q = CalWeek('2018-51')
14/19: q
14/20: {%G-%V}".format('2018-51')
14/21: '%G-%V' % ('2018-51')
14/22: '%G-%V' % (2018)
14/23: '%G-%V' % (2018 3)
14/24: '%G-%V' % (2018, 3)
14/25: '%G-%V' % (2018, 32)
14/26: '%G-%V' % (2018, 04)
14/27: '%G-%V' % (2018, 4)
14/28: '%G-%V' % (2018, 10)
14/29: from datetime import datetime
14/30:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = datetime.strptime(year_week, format)
    def __str__(self):
        return "x: {0}, y: {1}, z: {2}".format(self.x, self.y, self.z)
14/31: q = CalWeek('2018-51')
14/32: q = CalWeek(2018-51)
14/33: datetime.strptime(2018-51, '%G-%V')
14/34: datetime.strptime(2018-51, '%G-%V')
14/35: datetime.strptime("2018-51", '%G-%V')
14/36: datetime.strptime("2018-W51", '%G-%V')
14/37: datetime.strptime("2018-W51", '%G-W%V')
14/38:
datestr = "2020-W02"
datetime.datetime.strptime(datestr, "%G-W%V")
14/39:
datestr = "2020-W02"
datetime.strptime(datestr, "%G-W%V")
14/40: !pip python upgrate
14/41: !conda python upgrate
14/42:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = datetime
        self.format = format
    def __str__(self):
        return "x: {0}, y: {1}, z: {2}".format(self.x, self.y, self.z)
14/43: datetime.strptime("2018-W51", '%G-W%V')
14/44:  datetime.strptime('2011 22 1', '%G %V %u')
14/45:  datetime.strptime('2011 22', '%G %V')
14/46: datetime.strptime('2011 22 1', '%G %V %u')
14/47: datetime.strptime('2011 11 1', '%G %V %u')
14/48: datetime.strptime('2011 1 1', '%G %V %u')
14/49: datetime.strptime('2021 1 1', '%G %V %u')
14/50: datetime.strptime('2021 10 1', '%G %V %u')
14/51:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = datetime
        self.format = format
        
    def __str__(self):
        return "{0}".format(self.year_week)
14/52:  str(CalWeek('2018-51'))
14/53:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
        self.format = format
        
    def __str__(self):
        return "{0}".format(self.year_week)
14/54:  str(CalWeek('2018-51'))
14/55: CalWeek('2018-51')
14/56: CalWeek('2018-51')
14/57:  str(CalWeek('2018-51'))
14/58:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
        self.format = format
        
    def __repr__ (self):
        return "{0}".format(self.year_week)
14/59:  str(CalWeek('2018-51'))
14/60: CalWeek('2018-51')
14/61: str(CalWeek('2018-51'))
15/1:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
        self.format = format
        
    def __repr__ (self):
        return "{0}".format(self.year_week)
15/2: CalWeek('2018-51')
15/3: str(CalWeek('2018-51'))
15/4: 'Hello, %G %V' % '2020-02'
15/5: 'Hello, %G %V' % (2020)
15/6: 'Hello, %G %V' % (2020, 9)
15/7: 'Hello, %G-%V' % (2020-09)
15/8: 'Hello, %G-%V' % (''2020-09'')
15/9: 'Hello, %G-%V' % ('2020-09')
15/10: 'Hello, %G-%V' % (2020,9)
15/11:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
        self.format = format
        self.year = 2999
        
    def __repr__ (self):
        return "{0}".format(self.year_week)
15/12: CalWeek('2018-51')
15/13: str(CalWeek('2018-51'))
15/14: CalWeek('2018-50')
15/15: str(CalWeek('2018-50'))
15/16: CalWeek.yaer
15/17: CalWeek.year
15/18:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
        self.format = format
        self.year = 2999
        
    def __repr__ (self):
        return "{0}".format(self.year)
15/19: str(CalWeek('2018-50'))
15/20: '2018-50'[:3]
15/21: '2018-50'[:4]
15/22: '2018-50'[-2:]
15/23: '%G-%–ü' % (2020,9)
15/24: '%G-%G' % (2020,9)
15/25: '%G-%02d"' % (2020,9)
15/26:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
        self.format = format
        self.year = year_week[:4]
        self.month = year_week[-2:]
        
    def __repr__ (self):
        return "{0}".format(self.year_week)
    
    def __add__(self, other):
        try:
            b = int(other)
            return b
        except ValueError:
            print "I/O error({0}): {1}".format(e.errno, e.strerror)
        except ValueError:
            print "Could not convert data to an integer."
15/27:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
        self.format = format
        self.year = year_week[:4]
        self.month = year_week[-2:]
        
    def __repr__ (self):
        return "{0}".format(self.year_week)
    
    def __add__(self, other):
        try:
            b = int(other)
            return b
        except ValueError:
            print("I/O error({0}): {1}".format(e.errno, e.strerror))
        except ValueError:
            print("Could not convert data to an integer.")
15/28: a = CalWeek('2018-50')
15/29: a + 4
15/30: a + 4
15/31: a + 4d
15/32: a + d
15/33: a + 'd'
15/34:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
        self.format = format
        self.year = year_week[:4]
        self.month = year_week[-2:]
        
    def __repr__ (self):
        return "{0}".format(self.year_week)
    
    def __add__(self, other):
        try:
            b = int(other)
            return b
        except ValueError:
            print('Could not convert data to an integer.')
        except:
            print('something is wrong')
15/35: a + 'd'
15/36:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
        self.format = format
        self.year = year_week[:4]
        self.month = year_week[-2:]
        
    def __repr__ (self):
        return "{0}".format(self.year_week)
    
    def __add__(self, other):
        try:
            b = int(other)
            return b
        except ValueError:
            print('Could not convert data to an integer.')
        except:
            print('something is wrong')
15/37: a + 'd'
15/38:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
        self.format = format
        self.year = year_week[:4]
        self.month = year_week[-2:]
        
    def __repr__ (self):
        return "{0}".format(self.year_week)
    
    def __add__(self, other):
        try:
            b = int(other)
            return b
        except ValueError:
            print('Could not convert data to an integer.')
        except:
            print('something is wrong')
15/39: a = CalWeek('2018-50')
15/40: str(CalWeek('2018-50'))
15/41: a + 'd'
15/42:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
        self.format = format
        self.year = year_week[:4]
        self.month = year_week[-2:]
        
    def __repr__ (self):
        return "{0}".format(self.year_week)
    
    def __add__(self, other):
        try:
            b = int(other)
            return b
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/43:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
        self.format = format
        self.year = year_week[:4]
        self.month = year_week[-2:]
        
    def __repr__ (self):
        return "{0}".format(self.year_week)
    
    def __add__(self, other):
        try:
            b = int(other)
            return '%G-%02d"' % (2020,9)
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/44: a = CalWeek('2018-50')
15/45: str(CalWeek('2018-50'))
15/46: CalWeek.year
15/47: a + 'd'
15/48: a + 2
15/49: '%G-%02d' % (2020,9)
15/50: '%G-%02d' % (2020,9)
15/51: a + 2
15/52:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
        self.format = format
        self.year = year_week[:4]
        self.month = year_week[-2:]
        
    def __repr__ (self):
        return "{0}".format(self.year_week)
    
    def __add__(self, other):
        try:
            b = int(other)
            return '%G-%02d' % (2020,9)
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/53: a = CalWeek('2018-50')
15/54: str(CalWeek('2018-50'))
15/55: a + 2
15/56: a
15/57: a.type
15/58: b = a + 2
15/59: b
15/60:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
        self.format = format
        self.year = year_week[:4]
        self.month = year_week[-2:]
        
    def __repr__ (self):
        return "{0}-{1}".format(self.year, self.month)
    
    def __add__(self, other):
        try:
            b = int(other)
            return '%G-%02d' % (2020,9)
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/61: q = CalWeek(2018-51)
15/62: q = CalWeek('2018-51')
15/63: q
15/64: a = CalWeek('2018-50')
15/65: str(CalWeek('2018-50'))
15/66: a
15/67: b = a + 2
15/68: b
15/69:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
        self.format = format
        self.year = year_week[:4]
        self.month = year_week[-2:]
        
    def __repr__ (self):
        return "{0}-{1}".format(self.year, self.month)
    
    def __add__(self, other):
        try:
            b = int(other)
            return "{0}-{1}".format(self.year, self.month + b)
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/70: a = CalWeek('2018-50')
15/71: a
15/72: str(CalWeek('2018-50'))
15/73: b = a + 2
15/74:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
        self.format = format
        self.year = year_week[:4]
        self.month = year_week[-2:]
        
    def __repr__ (self):
        return "{0}-{1}".format(self.year, self.month)
    
    def __add__(self, other):
        try:
            b = int(other)
            return "{0}-{1}".format(self.year, int(self.month) + b)
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/75: a = CalWeek('2018-50')
15/76: a
15/77: str(CalWeek('2018-50'))
15/78: b = a + 2
15/79: b
15/80:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
        self.format = format
        self.year = year_week[:4]
        self.month = year_week[-2:]
        
    def __repr__ (self):
        return "{0}-{1}".format(self.year, self.month)
    
    def __iadd__(self, other):
        try:
            b = int(other)
            return "{0}-{1}".format(self.year, int(self.month) + b)
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/81: a = CalWeek('2018-50')
15/82: a
15/83: str(CalWeek('2018-50'))
15/84: b = a + 2
15/85: a + 2
15/86:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
        self.format = format
        self.year = year_week[:4]
        self.month = year_week[-2:]
        
    def __repr__ (self):
        return "{0}-{1}".format(self.year, self.month)
    
    def __iadd__(self, other):
        try:
            b = int(other)
            return "{0}-{1}".format(self.year, int(self.month) + b)
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/87: a = CalWeek('2018-50')
15/88: a
15/89: str(CalWeek('2018-50'))
15/90: a + 2
15/91: a += 2
15/92:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
        self.format = format
        self.year = year_week[:4]
        self.month = year_week[-2:]
        
    def __repr__ (self):
        return "{0}-{1}".format(self.year, self.month)
    
    def __iadd__(self, other):
        try:
            b = int(other)
            return "{0}-{1}".format(self.year, int(self.month) + b)
            print("{0}-{1}".format(self.year, int(self.month) + b))
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/93:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
        self.format = format
        self.year = year_week[:4]
        self.month = year_week[-2:]
        
    def __repr__ (self):
        return "{0}-{1}".format(self.year, self.month)
    
    def __add__(self, other):
        try:
            b = int(other)
            return "{0}-{1}".format(self.year, int(self.month) + b)
            print("{0}-{1}".format(self.year, int(self.month) + b))
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/94: a = CalWeek('2018-50')
15/95: a
15/96: str(CalWeek('2018-50'))
15/97: a += 2
15/98: a + 2
15/99:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
        self.format = format
        self.year = year_week[:4]
        self.month = year_week[-2:]
        
    def __repr__ (self):
        return "{0}-{1}".format(self.year, self.month)
    
    def __add__(self, other):
        try:
            b = int(other)
            return "{0}-{1}".format(self.year, int(self.month) + b)
            print("{0}-{1}".format(self.year, int(self.month) + b))
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/100: a = CalWeek('2018-50')
15/101: a
15/102: str(CalWeek('2018-50'))
15/103: a + 2
15/104: q = a + 2
15/105: 1
15/106: q
15/107: str(q)
15/108: q
15/109: a
15/110:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
        self.format = format
        self.year = year_week[:4]
        self.month = year_week[-2:]
        
    def __repr__ (self):
        return "{0}-{1}".format(self.year, self.month)
    
    def __add__(self, other):
        try:
            b = int(other)
            self.year_week = "{0}-{1}".format(self.year, int(self.month) + b)
            return self.year_week
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/111: q = CalWeek('2018-51')
15/112: q
15/113: a = CalWeek('2018-50')
15/114: a
15/115: str(CalWeek('2018-50'))
15/116: q = a + 2
15/117: str(q)
15/118: q
15/119: a + 2
15/120: a + 4
15/121: a + 11
15/122: a
15/123:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
        self.format = format
        self.year = year_week[:4]
        self.month = year_week[-2:]
        
    def __repr__ (self):
        return "{0}-{1}".format(self.year, self.month)
    
    def __iadd__(self, other):
        try:
            b = int(other)
            self.year_week = "{0}-{1}".format(self.year, int(self.month) + b)
            return self.year_week
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/124: a = CalWeek('2018-50')
15/125: a
15/126: str(CalWeek('2018-50'))
15/127: a + 11
15/128: a += 11
15/129:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
        self.format = format
        self.year = year_week[:4]
        self.month = year_week[-2:]
        
    def __repr__ (self):
        return "{0}-{1}".format(self.year, self.month)
    
    def __add__(self, other):
        try:
            b = int(other)
            self.year_week = "{0}-{1}".format(self.year, int(self.month) + b)
            return self.year_week
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/130: a = CalWeek('2018-51')
15/131: a
15/132: CalWeek('2018-51') - 3
15/133: CalWeek('2018-51') + 3
15/134: CalWeek('2018-51') + 3
15/135: q = CalWeek('2018-51') + 3
15/136: q
15/137: q.type
15/138: a.type
15/139:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
        self.format = format
        self.year = year_week[:4]
        self.month = year_week[-2:]
        
    def __repr__ (self):
        return "{0}-{1}".format(self.year, self.month)
    
    def __add__(self, other):
        try:
            b = int(other)
            self.year_week = "{0}-{1}".format(self.year, int(self.month) + b)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/140: a = CalWeek('2018-51')
15/141: a
15/142: q = CalWeek('2018-51') + 3
15/143: a.type
15/144: q.type
15/145: CalWeek('2018-51') + 3
15/146: CalWeek('2018-51') + 3
15/147:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
        self.format = format
        self.year = year_week[:4]
        self.month = year_week[-2:]
        
    def __repr__ (self):
        return "{0}-{1}".format(self.year, self.month)
    
    def __add__(self, other):
        try:
            b = int(other)
            self.year_week = "{0}-{1}".format(int(self.year) + b // 52, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/148: a = CalWeek('2018-51')
15/149: a
15/150: CalWeek('2018-51') + 3
15/151:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
        self.format = format
        self.year = year_week[:4]
        self.month = year_week[-2:]
        
    def __repr__ (self):
        return "{0}-{1}".format(self.year, self.month)
    
    def __add__(self, other):
        try:
            b = int(other)
            print(b)
            self.year_week = "{0}-{1}".format(int(self.year) + b // 52, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/152: a = CalWeek('2018-51')
15/153: a
15/154: CalWeek('2018-51') + 3
15/155:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
        self.format = format
        self.year = year_week[:4]
        self.month = year_week[-2:]
        
    def __repr__ (self):
        return "{0}-{1}".format(self.year, self.month)
    
    def __add__(self, other):
        try:
            b = int(other)
            print(b % 52)
            self.year_week = "{0}-{1}".format(int(self.year) + b // 52, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/156: a = CalWeek('2018-51')
15/157: a
15/158: CalWeek('2018-51') + 3
15/159:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        self.year_week = year_week
        self.format = format
        self.year = year_week[:4]
        self.month = year_week[-2:]
        
    def __repr__ (self):
        return "{0}-{1}".format(self.year, self.month)
    
    def __add__(self, other):
        try:
            b = int(other)
            new_y = int(self.year) + b // 52
            print(new_y)
            self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/160: a = CalWeek('2018-51')
15/161: a
15/162: CalWeek('2018-51') + 3
15/163: CalWeek('2018-51') + 300
15/164:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = year_week[:4]
        self.month = year_week[-2:]
        
    def __repr__ (self):
        return "{0}-{1}".format(self.year, self.month)
    
    def __add__(self, other):
        try:
            b = int(other)
            self.year = int(self.year) + b // 52
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/165: a = CalWeek('2018-51')
15/166: a
15/167: CalWeek('2018-51') + 300
15/168:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = year_week[:4]
        self.month = year_week[-2:]
        
    def __repr__ (self):
        return "{0}-{1}".format(self.year, self.month)
    
    def __add__(self, other):
        try:
            b = int(other)
            self.year = int(self.year) + b // 52
            self.year = int(self.month) + b % 52
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/169: a = CalWeek('2018-51')
15/170: a
15/171: CalWeek('2018-51') + 300
15/172:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = year_week[:4]
        self.month = year_week[-2:]
        
    def __repr__ (self):
        return "{0}-{1}".format(self.year, self.month)
    
    def __add__(self, other):
        try:
            b = int(other)
            self.year = int(self.year) + b // 52
            self.month = int(self.month) + b % 52
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/173:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = year_week[:4]
        self.month = year_week[-2:]
        
    def __repr__ (self):
        return "{0}-{1:0>2d}".format(self.year, self.month)
    
    def __add__(self, other):
        try:
            b = int(other)
            self.year = int(self.year) + b // 52
            self.month = int(self.month) + b % 52
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/174: a = CalWeek('2018-51')
15/175: a
15/176:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = year_week[:4]
        self.month = year_week[-2:]
        
    def __repr__ (self):
        return "{0}-{1:02d}".format(self.year, self.month)
    
    def __add__(self, other):
        try:
            b = int(other)
            self.year = int(self.year) + b // 52
            self.month = int(self.month) + b % 52
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/177: a = CalWeek('2018-51')
15/178: a
15/179:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.month = int(year_week[-2:])
        
    def __repr__ (self):
        return "{0}-{1:0>2d}".format(self.year, self.month)
    
    def __add__(self, other):
        try:
            b = int(other)
            self.year = int(self.year) + b // 52
            self.month = int(self.month) + b % 52
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/180: a = CalWeek('2018-51')
15/181: a
15/182:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.month = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.month)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            self.year = self.year + b // 52
            self.month = self.month + b % 52
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/183: q = CalWeek('2018-51')
15/184: q
15/185: CalWeek('2018-51') + 300
15/186: a = CalWeek('2018-51')
15/187: a
15/188:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.month = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.month)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            self.year = self.year + b // 52
            self.month = self.month + b % 52
            self.year_week = self.year_week
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/189: a = CalWeek('2018-51')
15/190: a
15/191: CalWeek('2018-51') + 300
15/192:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.month = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.month)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            self.year = self.year + b // 52
            self.month = self.month + b % 52
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.month)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/193: a = CalWeek('2018-51')
15/194: a
15/195: CalWeek('2018-51') + 300
15/196: divmod(55, 52)
15/197: year month = divmod(55, 52)
15/198: year, month = divmod(55, 52)
15/199: year, month
15/200: divmod(12, month)
15/201: divmod(12, 12 + month)
15/202: divmod(12 + month, 12)
15/203: divmod(51 + month, 52)
15/204: year, week = divmod(300, 52)
15/205: year, month
15/206: year, week = divmod(300, 52)
15/207: year, month
15/208: year, week
15/209: new_year, week = (50 + week, 52)
15/210: new_year, week
15/211: year, week = divmod(300, 52)
15/212: year, week
15/213: new_year, week = divmod(50 + week, 52)
15/214: new_year, week
15/215:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = self.week + week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/216: a = CalWeek('2018-51')
15/217: a
15/218: CalWeek('2018-51') + 300
15/219:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/220: a = CalWeek('2018-51')
15/221: a
15/222: CalWeek('2018-51') + 300
15/223: CalWeek('2018-51') + 100
15/224: a = CalWeek('2021-01')
15/225: a
15/226: CalWeek('2018-51') + 100
15/227: CalWeek('2021-01') + 100
15/228: q = CalWeek('2021-01') + 100
15/229: q
15/230:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
            
    def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week + 52, 52)
            
            self.year = self.year - year - new_year - 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/231: a = CalWeek('2021-01')
15/232: a
15/233: a - 10
15/234:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
            
    def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            print(week)
            new_year, week = divmod(self.week + week + 52, 52)
            
            self.year = self.year - year - new_year - 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/235: a = CalWeek('2021-01')
15/236: a
15/237: a - 10
15/238:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
            
    def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            print(week)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year - 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/239: a = CalWeek('2021-01')
15/240: a
15/241: a - 10
15/242: CalWeek('2018-51') - 3
15/243:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
            
    def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            print(week)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/244: CalWeek('2018-51') - 3
15/245: CalWeek('2018-51') + 2
15/246: CalWeek('2018-51') - 300
15/247:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
            
    def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/248: CalWeek('2018-51') - 300
15/249: CalWeek('2018-51') + 2
15/250: q
15/251: q.type
15/252: q.year
15/253:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
            
    def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    
    def __sub__(self, other):
        try:
            self.year >= other.year
            self.week >= other.week
        except:
            print('right date is bigger')
15/254: CalWeek('2019-04') - CalWeek('2018-51')
15/255: CalWeek('2014-04') - CalWeek('2018-51')
15/256:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
            
    def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    
    def __sub__(self, other):
        try:
            self.year >= other.year and self.week >= other.week
        except:
            print('right date is bigger')
15/257: CalWeek('2014-04') - CalWeek('2018-51')
15/258:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
            
    def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    
    def __sub__(self, other):
        try:
            if(self.year >= other.year and self.week >= other.week) print('e')
        except:
            print('right date is bigger')
15/259: CalWeek('2014-04') - CalWeek('2018-51')
15/260:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
            
    def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    
    def __sub__(self, other):
        try:
            if(self.year >= other.year and self.week >= other.week): print('e')
        except:
            print('right date is bigger')
15/261: CalWeek('2014-04') - CalWeek('2018-51')
15/262: CalWeek('2019-04') - CalWeek('2018-51')
15/263:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
            
    def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    
    def __sub__(self, other):
        print('1')
        try:
            if(self.year >= other.year and self.week >= other.week): print('e')
        except:
            print('right date is bigger')
15/264: CalWeek('2019-04') - CalWeek('2018-51')
15/265:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
            
    def __sub__(self, other):
        print('0')
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    
    def __sub__(self, other):
        print('1')
        try:
            if(self.year >= other.year and self.week >= other.week): print('e')
        except:
            print('right date is bigger')
15/266: CalWeek('2019-04') - CalWeek('2018-51')
15/267:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
            
    def __sub__(self, other):
        print('0')
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    
    def __sub__(self, other):
        print('1')
        try:
            self.year >= other.year and self.week >= other.week
        except:
            print('right date is bigger')
15/268: CalWeek('2019-04') - CalWeek('2018-51')
15/269: CalWeek('2014-04') - CalWeek('2018-51')
15/270:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
            
    def __sub__(self, other):
        print('0')
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    
    def __sub__(self, other):
        print('1')
        try:
            delta = (self.year * 52 + self.week) - (other.year * 52 + other.week)
            if (delta < 0): raise SuperException()
        except (SuperException, CatchableExceptions):
            print('right date is bigger')
15/271: CalWeek('2014-04') - CalWeek('2018-51')
15/272:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
            
    def __sub__(self, other):
        print('0')
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    
    def __sub__(self, other):
        print('1')
        try:
            delta = (self.year * 52 + self.week) - (other.year * 52 + other.week)
            if (delta < 0): raise ValueError()
        except ValueError:
            print('right date is bigger')
15/273: CalWeek('2014-04') - CalWeek('2018-51')
15/274:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
            
    def __sub__(self, other):
        print('0')
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    
    def __sub__(self, other):
        print('1')
        try:
            delta = (self.year * 52 + self.week) - (other.year * 52 + other.week)
            if (delta < 0): 
                raise ValueError()
            else:
                print(delta)                
        except ValueError:
            print('right date is bigger')
15/275: CalWeek('2014-04') - CalWeek('2018-51')
15/276: CalWeek('2019-04') - CalWeek('2018-51')
15/277: CalWeek('2021-04') - CalWeek('2018-51')
15/278:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
            
    def __sub__(self, other):
        print('0')
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    
    def __sub__(self, other):
        try:
            delta = (self.year * 52 + self.week) - (other.year * 52 + other.week)
            if (delta < 0): 
                raise ValueError()
            else:
                print(delta)                
        except ValueError:
            print('right date is bigger')
15/279: CalWeek('2021-04') - CalWeek('2018-51')
15/280: w = CalWeek('2021-04') - CalWeek('2018-51')
15/281: w
15/282:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
            
    def __sub__(self, other):
        print('0')
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    
    def __sub__(self, other):
        try:
            delta = (self.year * 52 + self.week) - (other.year * 52 + other.week)
            if (delta < 0): 
                raise ValueError()
            else:
                print(delta)
                return(delta)
        except ValueError:
            print('right date is bigger')
15/283: w = CalWeek('2021-04') - CalWeek('2018-51')
15/284: w
15/285: w + 4
15/286: CalWeek('2018-51') - 3
15/287:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
            
    def __sub__(self, other):
        print('0')
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    
    def __sub__(self, other):
        try:
            delta = (self.year * 52 + self.week) - (other.year * 52 + other.week)
            if (delta < 0): 
                raise ValueError()
            else:
                print(delta)
                return(delta)
        except ValueError:
            print('right date is bigger')
15/288: CalWeek('2018-51') - 3
15/289: CalWeek('2018-51') - "3"
15/290:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
            
    def __sub__(self, other):
        print('0')
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    
    def __sub__(self, other):
        try:
            delta = (self.year * 52 + self.week) - (other.year * 52 + other.week)
            if (delta < 0): 
                raise ValueError()
            else:
                print(delta)
                return(delta)
        except ValueError:
            print('right date is bigger')
15/291: CalWeek('2018-51') - "3"
15/292:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
            
    @dispatch(object, int)            
    def __sub__(self, other):
        print('0')
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    
    @dispatch(object, object)
    def __sub__(self, other):
        try:
            delta = (self.year * 52 + self.week) - (other.year * 52 + other.week)
            if (delta < 0): 
                raise ValueError()
            else:
                print(delta)
                return(delta)
        except ValueError:
            print('right date is bigger')
15/293: CalWeek('2018-51') - "3"
15/294: CalWeek('2018-51') - 3
15/295: from multipledispatch import dispatch
15/296:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
            
    @dispatch(object, int)            
    def __sub__(self, other):
        print('0')
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    
    @dispatch(object, object)
    def __sub__(self, other):
        try:
            delta = (self.year * 52 + self.week) - (other.year * 52 + other.week)
            if (delta < 0): 
                raise ValueError()
            else:
                print(delta)
                return(delta)
        except ValueError:
            print('right date is bigger')
15/297: CalWeek('2018-51') - 3
15/298: CalWeek('2018-51') - '3'
15/299: CalWeek('2018-51') - '3'
15/300:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
            
    @dispatch(CalWeek, int)            
    def __sub__(self, other):
        print('0')
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    
    @dispatch(object, object)
    def __sub__(self, other):
        try:
            delta = (self.year * 52 + self.week) - (other.year * 52 + other.week)
            if (delta < 0): 
                raise ValueError()
            else:
                print(delta)
                return(delta)
        except ValueError:
            print('right date is bigger')
15/301: CalWeek('2018-51') - '3'
15/302: CalWeek('2018-51') - 3
15/303:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
            
    @dispatch(object, int)            
    def __sub__(self, other):
        print('0')
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    
    @dispatch(object, object)
    def __sub__(self, other):
        try:
            delta = (self.year * 52 + self.week) - (other.year * 52 + other.week)
            if (delta < 0): 
                raise ValueError()
            else:
                print(delta)
                return(delta)
        except ValueError:
            print('right date is bigger')
15/304: CalWeek('2018-51') - 3
15/305:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
            
    @Overload
    @signature('√≠nt')            
    def __sub__(self, other):
        print('0')
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    
    @dispatch(object, object)
    def __sub__(self, other):
        try:
            delta = (self.year * 52 + self.week) - (other.year * 52 + other.week)
            if (delta < 0): 
                raise ValueError()
            else:
                print(delta)
                return(delta)
        except ValueError:
            print('right date is bigger')
15/306: CalWeek('2018-51') - 3
15/307: from pythonlangutil.overload import Overload, signature
15/308: !conda install pythonlangutil
15/309:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
            
    @Overload
    @signature('√≠nt')            
    def __sub__(self, other):
        print('0')
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    
    @dispatch(object, object)
    def __sub__(self, other):
        try:
            delta = (self.year * 52 + self.week) - (other.year * 52 + other.week)
            if (delta < 0): 
                raise ValueError()
            else:
                print(delta)
                return(delta)
        except ValueError:
            print('right date is bigger')
15/310: CalWeek('2018-51') - 3
15/311:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
                        
    def __sub__(self, other):
        print('0')
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
15/312: CalWeek('2018-51') - 3
15/313: q = CalWeek('2018-51') - 3
15/314: q
15/315: isinstance(q, CalWeek)
15/316:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
                
    def __sub__(self, other):
        try:
            if (b = int(other)):
                year, week = divmod(b, 52)
                new_year, week = divmod(self.week + week, 52)
                
                self.year = self.year + year + new_year
                self.week = week
                self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
                return self
            elseif:
                isinstance(other, CalWeek):
                    print('2')
            else:
                raise ValueError()
        except ValueError:
            print('right date is bigger')
15/317:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
                
    def __sub__(self, other):
        try:
            if (b == int(other)):
                year, week = divmod(b, 52)
                new_year, week = divmod(self.week + week, 52)
                
                self.year = self.year + year + new_year
                self.week = week
                self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
                return self
            elseif:
                isinstance(other, CalWeek):
                    print('2')
            else:
                raise ValueError()
        except ValueError:
            print('right date is bigger')
15/318:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
                
    def __sub__(self, other):
        try:
            if (b == int(other)):
                year, week = divmod(b, 52)
                new_year, week = divmod(self.week + week, 52)
                
                self.year = self.year + year + new_year
                self.week = week
                self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
                return self
            elif:
                isinstance(other, CalWeek):
                    print('2')
            else:
                raise ValueError()
        except ValueError:
            print('right date is bigger')
15/319:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
                
    def __sub__(self, other):
        try:
            if (b == int(other)):
                year, week = divmod(b, 52)
                new_year, week = divmod(self.week + week, 52)
                
                self.year = self.year + year + new_year
                self.week = week
                self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
                return self
            elif isinstance(other, CalWeek):
                print('2')
            else:
                raise ValueError()
        except ValueError:
            print('right date is bigger')
15/320: q = CalWeek('2018-51') - 3
15/321:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
                
    def __sub__(self, other):
        try:
            if (b == int(other)):
                print('1')
                year, week = divmod(b, 52)
                new_year, week = divmod(self.week + week, 52)
                
                self.year = self.year + year + new_year
                self.week = week
                self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
                return self
            elif isinstance(other, CalWeek):
                print('2')
            else:
                raise ValueError()
        except ValueError:
            print('right date is bigger')
15/322: q = CalWeek('2018-51') - 3
15/323:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
                
    def __sub__(self, other):
        print(other)
        try:
            if (b == int(other)):
                print('1')
                year, week = divmod(b, 52)
                new_year, week = divmod(self.week + week, 52)
                
                self.year = self.year + year + new_year
                self.week = week
                self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
                return self
            elif isinstance(other, CalWeek):
                print('2')
            else:
                raise ValueError()
        except ValueError:
            print('right date is bigger')
15/324: q = CalWeek('2018-51') - 3
15/325:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
                
    def __sub__(self, other):
        print(other.type)
        try:
            if (b == int(other)):
                print('1')
                year, week = divmod(b, 52)
                new_year, week = divmod(self.week + week, 52)
                
                self.year = self.year + year + new_year
                self.week = week
                self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
                return self
            elif isinstance(other, CalWeek):
                print('2')
            else:
                raise ValueError()
        except ValueError:
            print('right date is bigger')
15/326: q = CalWeek('2018-51') - 3
15/327: int(3)
15/328: int('3')
15/329: isinstance(3, int)
15/330:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
                
    def __sub__(self, other):
        print(other.type)
        try:
            if isinstance(other, int):
                print('1')
                year, week = divmod(b, 52)
                new_year, week = divmod(self.week + week, 52)
                
                self.year = self.year + year + new_year
                self.week = week
                self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
                return self
            elif isinstance(other, CalWeek):
                print('2')
            else:
                raise ValueError()
        except ValueError:
            print('right date is bigger')
15/331: isinstance(3, int)
15/332: q = CalWeek('2018-51') - 3
15/333:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
                
    def __sub__(self, other):
        try:
            if isinstance(other, int):
                print('1')
                year, week = divmod(b, 52)
                new_year, week = divmod(self.week + week, 52)
                
                self.year = self.year + year + new_year
                self.week = week
                self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
                return self
            elif isinstance(other, CalWeek):
                print('2')
            else:
                raise ValueError()
        except ValueError:
            print('right date is bigger')
15/334: isinstance(3, int)
15/335: q = CalWeek('2018-51') - 3
15/336:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
                
    def __sub__(self, other):
        try:
            if isinstance(other, int):
                print('1')
                year, week = divmod(other, 52)
                new_year, week = divmod(self.week + week, 52)
                
                self.year = self.year + year + new_year
                self.week = week
                self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
                return self
            elif isinstance(other, CalWeek):
                print('2')
            else:
                raise ValueError()
        except ValueError:
            print('right date is bigger')
15/337: isinstance(3, int)
15/338: q = CalWeek('2018-51') - 3
15/339: CalWeek('2018-51') - 3
15/340: CalWeek('2018-51') - 3
15/341: CalWeek('2018-51') - 3
15/342:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
                
    def __sub__(self, other):
        try:
            if isinstance(other, int):
                print('1')
                year, week = divmod(other, 52)
                new_year, week = divmod(self.week - week + 52, 52)
                
                self.year = self.year + year + new_year
                self.week = week
                self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
                return self
            elif isinstance(other, CalWeek):
                print('2')
            else:
                raise ValueError()
        except ValueError:
            print('right date is bigger')
15/343: isinstance(3, int)
15/344: CalWeek('2018-51') - 3
15/345:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
                
    def __sub__(self, other):
        try:
            if isinstance(other, int):
                print('1')
                year, week = divmod(other, 52)
                new_year, week = divmod(self.week - week + 52, 52)
                
                self.year = self.year - year - new_year + 1
                self.week = week
                self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
                return self
            elif isinstance(other, CalWeek):
                print('2')
            else:
                raise ValueError()
        except ValueError:
            print('right date is bigger')
15/346: isinstance(3, int)
15/347: CalWeek('2018-51') - 3
15/348: isinstance(q, CalWeek)
15/349: CalWeek('2019-04') - CalWeek('2018-51')
15/350:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
                
    def __sub__(self, other):
        try:
            if isinstance(other, int):
                print('1')
                year, week = divmod(other, 52)
                new_year, week = divmod(self.week - week + 52, 52)
                
                self.year = self.year - year - new_year + 1
                self.week = week
                self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
                return self
            elif isinstance(other, CalWeek):
                delta = (self.year * 52 + self.week) - (other.year * 52 + other.week)
                if (delta < 0): 
                    raise ValueError()
                else:
                    print(delta)
                    return(delta)
            else:
                raise TypeError()
        except (ValueError, TypeError):
            print('right date is bigger')
15/351: isinstance(3, int)
15/352: CalWeek('2018-51') - 3
15/353: CalWeek('2019-04') - CalWeek('2018-51')
15/354:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
                
    def __sub__(self, other):
        try:
            if isinstance(other, int):
                print('1')
                year, week = divmod(other, 52)
                new_year, week = divmod(self.week - week + 52, 52)
                
                self.year = self.year - year - new_year + 1
                self.week = week
                self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
                return self
            elif isinstance(other, CalWeek):
                delta = (self.year * 52 + self.week) - (other.year * 52 + other.week)
                if (delta < 0): 
                    raise ValueError()
                else:
                    print(delta)
                    return(delta)
            else:
                raise TypeError()
        except ValueError:
            print('right date is bigger')
        except TypeError:
            print('Type problem')
15/355: 'right date is bigger'
15/356: isinstance(3, int)
15/357: CalWeek('2018-51') - 3
15/358: CalWeek('2019-04') - CalWeek('2018-51')
15/359: CalWeek('2015-04') - CalWeek('2018-51')
15/360: CalWeek('2015-04')- 'dd'
15/361: CalWeek('2018-51')
15/362: str(CalWeek('2018-51'))
15/363: CalWeek('2018-51') - 3
15/364:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
                
    def __sub__(self, other):
        try:
            if isinstance(other, int):
                year, week = divmod(other, 52)
                new_year, week = divmod(self.week - week + 52, 52)
                
                self.year = self.year - year - new_year + 1
                self.week = week
                self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
                return self
            elif isinstance(other, CalWeek):
                delta = (self.year * 52 + self.week) - (other.year * 52 + other.week)
                if (delta < 0): 
                    raise ValueError()
                else:
                    print(delta)
                    return(delta)
            else:
                raise TypeError()
        except ValueError:
            print('right date is bigger')
        except TypeError:
            print('Type problem')
15/365:
class CalWeek:
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
                
    def __sub__(self, other):
        try:
            if isinstance(other, int):
                year, week = divmod(other, 52)
                new_year, week = divmod(self.week - week + 52, 52)
                
                self.year = self.year - year - new_year + 1
                self.week = week
                self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
                return self
            elif isinstance(other, CalWeek):
                delta = (self.year * 52 + self.week) - (other.year * 52 + other.week)
                if (delta < 0): 
                    raise ValueError()
                else:
                    print(delta)
                    return(delta)
            else:
                raise TypeError()
        except ValueError:
            print('right date is bigger')
        except TypeError:
            print('Type problem')
15/366: CalWeek('2018-51') - 3
15/367: CalWeek('2018-51') + 2
15/368: CalWeek('2019-04') - CalWeek('2018-51')
15/369:
class CalWeek:
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
                
    def __sub__(self, other):
        try:
            if isinstance(other, int):
                year, week = divmod(other, 52)
                new_year, week = divmod(self.week - week + 52, 52)
                
                self.year = self.year - year - new_year + 1
                self.week = week
                self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
                return self
            elif isinstance(other, CalWeek):
                delta = (self.year * 52 + self.week) - (other.year * 52 + other.week)
                if (delta < 0): 
                    raise ValueError()
                else:
                    return(delta)
            else:
                raise TypeError()
        except ValueError:
            print('right date is bigger')
        except TypeError:
            print('Type problem')
15/370: CalWeek('2018-51')
15/371: str(CalWeek('2018-51'))
15/372: CalWeek('2018-51') - 3
15/373: CalWeek('2018-51') + 2
15/374: CalWeek('2019-04') - CalWeek('2018-51')
16/1:
class CalWeek:
    """
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> def __sub__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
    5
    """
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
                        
    def __sub__(self, other):
        print('0')
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week - week + 52, 52)
            
            self.year = self.year - year - new_year + 1
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            #print(new_y)
            #self.year_week = "{0}-{1}".format(new_y, int(self.month) + b % 52)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
17/1:
class CalWeek:
    def __init__(self, year_week, format='%G-%V'):
        #self.year_week = year_week
        #self.format = format
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
                
    def __sub__(self, other):
        try:
            if isinstance(other, int):
                year, week = divmod(other, 52)
                new_year, week = divmod(self.week - week + 52, 52)
                
                self.year = self.year - year - new_year + 1
                self.week = week
                self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
                return self
            elif isinstance(other, CalWeek):
                delta = (self.year * 52 + self.week) - (other.year * 52 + other.week)
                if (delta < 0): 
                    raise ValueError()
                else:
                    return(delta)
            else:
                raise TypeError()
        except ValueError:
            print('right date is bigger')
        except TypeError:
            print('Type problem')
17/2: CalWeek('2018-51')
17/3: str(CalWeek('2018-51'))
17/4: CalWeek('2018-51') - 3
17/5: CalWeek('2018-51') + 2
17/6: CalWeek('2019-04') - CalWeek('2018-51')
18/1:
# –ú—ã —Ö–æ—Ç–∏–º –Ω–∞–ø–∏—Å–∞—Ç—å –æ–±–µ—Ä—Ç–∫—É –¥–ª—è LGBMRegressor 
# –≤ –∫–æ—Ç–æ—Ä—É—é –º—ã –±—ã —É–º–µ–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞, 
# –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏. 

# –û—Å–Ω–æ–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å - –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —É–º–µ—Ç—å –ø—Ä–∏—Ç–≤–æ—Ä—è—Ç—å—Å—è 
# —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º sklearn —ç—Å—Ç–∏–º–∞—Ç–æ—Ä–æ–º.
# –¢–æ –µ—Å—Ç—å —Ä–∞–±–æ–∞—Ç—å –±–µ–∑ –ø—Ä–æ–±–ª–µ–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, c cross_validate.

# –ü—Ä–æ–¥—É–º–∞–π –∫–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –∫–ª–∞—Å—Å–µ,
# –∫–∞–∫ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LGBMRegressor —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ —Ç–¥.

# hint:
# 1) –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç sklearn.base.clone 
# 2) –ú–æ–∂–µ—Ç –ª–∏ –ø–æ–º–æ—á—å –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤?


from lightgbm import LGBMRegressor

class CustomModel():
    """LGBMRegressor wrapper"""
    pass
18/2: !pip install lightgbm
18/3: !conda install lightgbm
19/1:
# –ú—ã —Ö–æ—Ç–∏–º –Ω–∞–ø–∏—Å–∞—Ç—å –æ–±–µ—Ä—Ç–∫—É –¥–ª—è LGBMRegressor 
# –≤ –∫–æ—Ç–æ—Ä—É—é –º—ã –±—ã —É–º–µ–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞, 
# –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏. 

# –û—Å–Ω–æ–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å - –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —É–º–µ—Ç—å –ø—Ä–∏—Ç–≤–æ—Ä—è—Ç—å—Å—è 
# —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º sklearn —ç—Å—Ç–∏–º–∞—Ç–æ—Ä–æ–º.
# –¢–æ –µ—Å—Ç—å —Ä–∞–±–æ–∞—Ç—å –±–µ–∑ –ø—Ä–æ–±–ª–µ–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, c cross_validate.

# –ü—Ä–æ–¥—É–º–∞–π –∫–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –∫–ª–∞—Å—Å–µ,
# –∫–∞–∫ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LGBMRegressor —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ —Ç–¥.

# hint:
# 1) –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç sklearn.base.clone 
# 2) –ú–æ–∂–µ—Ç –ª–∏ –ø–æ–º–æ—á—å –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤?


from lightgbm import LGBMRegressor
19/2:
# –ú—ã —Ö–æ—Ç–∏–º –Ω–∞–ø–∏—Å–∞—Ç—å –æ–±–µ—Ä—Ç–∫—É –¥–ª—è LGBMRegressor 
# –≤ –∫–æ—Ç–æ—Ä—É—é –º—ã –±—ã —É–º–µ–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞, 
# –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏. 

# –û—Å–Ω–æ–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å - –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —É–º–µ—Ç—å –ø—Ä–∏—Ç–≤–æ—Ä—è—Ç—å—Å—è 
# —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º sklearn —ç—Å—Ç–∏–º–∞—Ç–æ—Ä–æ–º.
# –¢–æ –µ—Å—Ç—å —Ä–∞–±–æ–∞—Ç—å –±–µ–∑ –ø—Ä–æ–±–ª–µ–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, c cross_validate.

# –ü—Ä–æ–¥—É–º–∞–π –∫–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –∫–ª–∞—Å—Å–µ,
# –∫–∞–∫ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LGBMRegressor —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ —Ç–¥.

# hint:
# 1) –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç sklearn.base.clone 
# 2) –ú–æ–∂–µ—Ç –ª–∏ –ø–æ–º–æ—á—å –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤?


from lightgbm import LGBMRegressor
19/3: scores = cross_validate(clf, X, y,scoring='precision_macro', cv=5)
19/4: from sklearn.model_selection import cross_validate
19/5: scores = cross_validate(clf, X, y,scoring='precision_macro', cv=5)
19/6: model = LGBMRegressor()
19/7: scores = cross_validate(model, X, y,scoring='precision_macro', cv=5)
19/8: model
19/9:
X=np.array([[1],[2],[3]])
y=[5,10,15]
19/10: import numpy as np
19/11:
X=np.array([[1],[2],[3]])
y=[5,10,15]
19/12: model = LGBMRegressor()
19/13: model.fit(X)
19/14: model.fit(X, y)
19/15: model.coef_
19/16: model.get_params()
19/17:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    pass
19/18: CustomModel
19/19: CustomModel.fit()
19/20: CustomModel.fit(X, y)
19/21:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def fit(self, X, y = None):
        return self
19/22: CustomModel.fit(X, y)
19/23: –ß
19/24: X
19/25: model.fit(X, y)
19/26: CustomModel..get_params()
19/27: CustomModel.get_params()
19/28: CustomModel.get_params(CustomModel)
19/29:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def fit(self, X, y = None):
        return self
19/30: CustomModel.fit(X, y)
19/31: CustomModel.get_params()
19/32: CustomModel.get_params(X)
19/33: q = CustomModel()
19/34: CustomModel.fit(X, y)
19/35: q.get_params()
19/36:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        lgbmr_est=LGBMRegressor()
        super().__init__(base_estimator=lgbmr_est)
        
    def fit(self, X, y = None):
        return self
19/37: q = CustomModel()
19/38: q = CustomModel(['a'])
19/39: q.get_params()
19/40:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
  #      lgbmr_est=LGBMRegressor()
        super().__init__
        
    def fit(self, X, y = None):
        return self
19/41: q = CustomModel(['a'])
19/42: q.get_params()
19/43:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        lgbmr_est=LGBMRegressor()
        super().__init__(base_estimator=lgbmr_est)
        
    def fit(self, X, y = None):
        return self
19/44: q.get_params()
19/45: q = CustomModel(['a'])
19/46: q.get_params()
19/47:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        lgbmr_est=LGBMRegressor()
        super().__init__()
        
    def fit(self, X, y = None):
        return self
19/48: q = CustomModel(['a'])
19/49: q.get_params()
19/50:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        lgbmr_est=LGBMRegressor()
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y = None):
        return self
19/51: q = CustomModel(['a'])
19/52:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        lgbmr_est=LGBMRegressor()
        super().__init__(base_estimator=lgbmr_est)
        
    def fit(self, X, y = None):
        return self
19/53: q = CustomModel(['a'])
19/54: q.get_params()
19/55: g.base_estimator
19/56: q.base_estimator
19/57:
class CustomModel(BaseEstimator):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        lgbmr_est=LGBMRegressor()
        super().__init__(base_estimator=lgbmr_est)
        
    def fit(self, X, y = None):
        return self
19/58:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        lgbmr_est=LGBMRegressor()
        super().__init__(base_estimator=lgbmr_est)
        
    def fit(self, X, y = None):
        return self
19/59: q = CustomModel(['a'])
19/60: q.get_params()
19/61: CustomModel.fit(X, y)
19/62: model.fit(X, y)
19/63: q.get_params()
19/64: model.get_params()
19/65:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        lgbmr_est=LGBMRegressor()
        super().__init__(LGBMRegressor())
        
    def fit(self, X, y = None):
        return self
19/66: q = CustomModel(['a'])
19/67: q.get_params()
19/68:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        lgbmr_est=LGBMRegressor()
        super().__init__(base_estimator=LGBMRegressor())
        
    def fit(self, X, y = None):
        return self
19/69: q = CustomModel(['a'])
19/70: q.get_params()
19/71: CustomModel.fit(X, y)
19/72:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(base_estimator=LGBMRegressor())
        
    def fit(self, X, y = None):
        self.estimator.fit(X, y)
        return self
19/73: q = CustomModel(['a'])
19/74: q.get_params()
19/75: CustomModel.fit(X, y)
19/76: CustomModel.dtype
19/77:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(base_estimator=LGBMRegressor())
        
    def fit(self, X, y = None):
        print(self.dtype)
        self.estimator.fit(X, y)
        return self
19/78: q = CustomModel(['a'])
19/79: q.get_params()
19/80: CustomModel.fit(X, y)
19/81:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(base_estimator=LGBMRegressor())
        
    def fit(self, X, y = None):
        print(self.dtype)
        print(self)
        self.estimator.fit(X, y)
        return self
19/82: q = CustomModel(['a'])
19/83: q.get_params()
19/84: CustomModel.fit(X, y)
19/85:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(base_estimator=LGBMRegressor())
        
    def fit(self, X, y = None):
        print(self.dtype)
        print(self)
        self.base_estimator.fit(X, y)
        return self
19/86: q = CustomModel(['a'])
19/87: q.get_params()
19/88: CustomModel.fit(X, y)
19/89:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(base_estimator=LGBMRegressor())
        
    def fit(self, X, y = None):
        print(self.dtype)
        print(self)
        self.base_estimator.fit(X, y)
        model = self._fit(X,y)

        return self
19/90: q = CustomModel(['a'])
19/91: q.get_params()
19/92: CustomModel.fit(X, y)
19/93: q.fit(X, y)
19/94:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(base_estimator=LGBMRegressor())
        
    def fit(self, X, y = None):
        print(self)
        self.base_estimator.fit(X, y)
        model = self._fit(X,y)

        return self
19/95: q = CustomModel(['a'])
19/96: q.get_params()
19/97: q.fit(X, y)
19/98:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(base_estimator=LGBMRegressor())
        
    def fit(self, X, y = None):
        print(self)
        self.estimator.fit(X, y)
        return self
19/99: q = CustomModel(['a'])
19/100: q.get_params()
19/101: q.fit(X, y)
19/102:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(base_estimator=LGBMRegressor())
        
    def fit(self, X, y = None):
        print(self)
        self.fit(X, y)
        return self
19/103: q = CustomModel(['a'])
19/104: q.get_params()
19/105: q.fit(X, y)
19/106:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(base_estimator=LGBMRegressor())
        
    def fit(self, X, y = None):
        print(self)
       # self.fit(X, y)
        model = self._fit(X, y)

        return self
19/107: q = CustomModel(['a'])
19/108: q.get_params()
19/109: q.fit(X, y)
19/110:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(base_estimator=LGBMRegressor())
        
    def fit(self, X, y = None):
        print(self)
       # self.fit(X, y)
        model = self.fit(X, y)

        return self
19/111: q = CustomModel(['a'])
19/112: q.get_params()
19/113: q.fit(X, y)
19/114: q.get_params()
19/115:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(base_estimator=LGBMRegressor())
        
    def fit(self, X, y = None):
        print(self)
       # self.fit(X, y)
        model = self.fit(X, y)
        return model
19/116: q = CustomModel(['a'])
19/117: q.get_params()
19/118: q.fit(X, y)
19/119:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        self.model = LGBMRegressor()
        
    def fit(self, X, y = None):
        print(self)
       # self.fit(X, y)
        model = self.fit(X, y)
        return model
19/120: q = CustomModel(['a'])
19/121: q.get_params()
19/122:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        print(self)
       # self.fit(X, y)
        model = self.fit(X, y)
        return model
19/123: q = CustomModel(['a'])
19/124: q.get_params()
19/125: q.fit(X, y)
19/126:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        print(self)
       # self.fit(X, y)
        self.model.fit(X, y)
        return self
19/127: q = CustomModel(['a'])
19/128: q.get_params()
19/129: q.fit(X, y)
19/130: q = CustomModel(['a'], LGBMRegressor())
19/131: q.get_params()
19/132:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols,model):
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        print(self)
       # self.fit(X, y)
        self.model.fit(X, y)
        return self
19/133: q = CustomModel(['a'], LGBMRegressor())
19/134: q.get_params()
19/135: q.fit(X, y)
19/136:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols,model):
        self.cols = cols
        super().__init__(model=model)
        
    def fit(self, X, y = None):
        print(self)
       # self.fit(X, y)
        self.model.fit(X, y)
        return self
19/137: q = CustomModel(['a'], LGBMRegressor())
19/138: q.get_params()
19/139:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols,model):
        self.cols = cols
        super().__init__(model=model)
        
    def fit(self, X, y = None):
       # self.fit(X, y)
        self.model.fit(X, y)
        return self
19/140: q = CustomModel(['a'], LGBMRegressor())
19/141: q.get_params()
19/142: q.fit(X, y)
19/143: X
19/144:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        print(self)
       # self.fit(X, y)
        self.model.fit(X, y)
        return model
19/145: q = CustomModel(['a'], LGBMRegressor())
19/146: q.get_params()
19/147:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(base_estimator=LGBMRegressor())
        
    def fit(self, X, y = None):
        print(self)
       # self.fit(X, y)
        model = self.fit(X, y)
        return model
19/148: q = CustomModel(['a'], LGBMRegressor())
19/149: q.get_params()
19/150:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(base_estimator=LGBMRegressor())
        
    def fit(self, X, y = None):
        print(self)
       # self.fit(X, y)
        model = self.fit(X, y)
        return self
19/151: q = CustomModel(['a'], LGBMRegressor())
19/152: q = CustomModel(['a'])
19/153:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(base_estimator=LGBMRegressor())
        
    def fit(self, X, y = None):
        print(self)
       # self.fit(X, y)
        model = self._fit(X,y)

        return self
19/154: q = CustomModel(['a'])
19/155: q.get_params()
19/156: X
19/157: q.fit(X, y)
19/158:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(base_estimator=LGBMRegressor())
        
    def fit(self, X, y = None):
        print(self)
       # self.fit(X, y)
        self.model.fit(X,y)

        return self
19/159: q = CustomModel(['a'])
19/160: q.get_params()
19/161: X
19/162: q.fit(X, y)
19/163:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(base_estimator=LGBMRegressor())
        
    def fit(self, X, y = None):
        print(self)
       # self.fit(X, y)
        self.fit(X,y)

        return self
19/164: q = CustomModel(['a'])
19/165: q.get_params()
19/166: X
19/167: q.fit(X, y)
19/168:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        print(self)
       # self.fit(X, y)
        self.fit(X,y)

        return self
19/169: q = CustomModel(['a'])
19/170: q.get_params()
19/171:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        print(self)
       # self.fit(X, y)
        self.model.fit(X,y)

        return self
19/172: q = CustomModel(['a'])
19/173: q.get_params()
19/174: X
19/175: q.fit(X, y)
19/176: df = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), columns=['a', 'b', 'c'])
19/177:
import numpy as np
import pandas as pd
19/178: df = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), columns=['a', 'b', 'c'])
19/179: df
19/180: y
19/181: !conda install lightgbm
20/1:
# –ú—ã —Ö–æ—Ç–∏–º –Ω–∞–ø–∏—Å–∞—Ç—å –æ–±–µ—Ä—Ç–∫—É –¥–ª—è LGBMRegressor 
# –≤ –∫–æ—Ç–æ—Ä—É—é –º—ã –±—ã —É–º–µ–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞, 
# –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏. 

# –û—Å–Ω–æ–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å - –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —É–º–µ—Ç—å –ø—Ä–∏—Ç–≤–æ—Ä—è—Ç—å—Å—è 
# —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º sklearn —ç—Å—Ç–∏–º–∞—Ç–æ—Ä–æ–º.
# –¢–æ –µ—Å—Ç—å —Ä–∞–±–æ–∞—Ç—å –±–µ–∑ –ø—Ä–æ–±–ª–µ–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, c cross_validate.

# –ü—Ä–æ–¥—É–º–∞–π –∫–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –∫–ª–∞—Å—Å–µ,
# –∫–∞–∫ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LGBMRegressor —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ —Ç–¥.

# hint:
# 1) –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç sklearn.base.clone 
# 2) –ú–æ–∂–µ—Ç –ª–∏ –ø–æ–º–æ—á—å –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤?


from lightgbm import LGBMRegressor
20/2:
import numpy as np
import pandas as pd
20/3:
# –ú—ã —Ö–æ—Ç–∏–º –Ω–∞–ø–∏—Å–∞—Ç—å –æ–±–µ—Ä—Ç–∫—É –¥–ª—è LGBMRegressor 
# –≤ –∫–æ—Ç–æ—Ä—É—é –º—ã –±—ã —É–º–µ–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞, 
# –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏. 

# –û—Å–Ω–æ–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å - –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —É–º–µ—Ç—å –ø—Ä–∏—Ç–≤–æ—Ä—è—Ç—å—Å—è 
# —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º sklearn —ç—Å—Ç–∏–º–∞—Ç–æ—Ä–æ–º.
# –¢–æ –µ—Å—Ç—å —Ä–∞–±–æ–∞—Ç—å –±–µ–∑ –ø—Ä–æ–±–ª–µ–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, c cross_validate.

# –ü—Ä–æ–¥—É–º–∞–π –∫–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –∫–ª–∞—Å—Å–µ,
# –∫–∞–∫ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LGBMRegressor —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ —Ç–¥.

# hint:
# 1) –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç sklearn.base.clone 
# 2) –ú–æ–∂–µ—Ç –ª–∏ –ø–æ–º–æ—á—å –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤?


from lightgbm import LGBMRegressor
20/4:
import numpy as np
import pandas as pd
20/5: df = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), columns=['a', 'b', 'c'])
20/6: df
20/7:
#.fit()
#.predict()
#.score()
#.set_params()
#.get_params()
20/8: from sklearn.model_selection import cross_validate
20/9:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        print(self)
       # self.fit(X, y)
        self.model.fit(X,y)

        return self
20/10: q = CustomModel(['a'])
20/11: q.get_params()
20/12: X
20/13: q.fit(X, y)
20/14:
X=np.array([[1],[2],[3]])
y=[5,10,15]
20/15: q.fit(df, y)
20/16: y=[0,1,0]
20/17:
#.fit()
#.predict()
#.score()
#.set_params()
#.get_params()
20/18: from sklearn.model_selection import cross_validate
20/19:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        print(self)
       # self.fit(X, y)
        self.model.fit(X,y)

        return self
20/20: q = CustomModel(['a'])
20/21: q.get_params()
20/22: q.fit(df, y)
20/23: q.predict(df)
20/24: q.model.predict(df)
20/25: q.model.fit(df, y)
20/26: q.model.predict(df)
20/27: df = pd.DataFrame(np.array([[1, 22, 3], [4, 5, 26], [7, 8, 9]]), columns=['a', 'b', 'c'])
20/28: y=[0,1,0]
20/29:
#.fit()
#.predict()
#.score()
#.set_params()
#.get_params()
20/30: from sklearn.model_selection import cross_validate
20/31:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        print(self)
       # self.fit(X, y)
        self.model.fit(X,y)

        return self
20/32: q = CustomModel(['a'])
20/33: q.get_params()
20/34: q.model.fit(df, y)
20/35: q.model.predict(df)
20/36:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        self.model = LGBMRegressor()
        #super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        print(self)
       # self.fit(X, y)
        self.model.fit(X,y)

        return self
20/37: q = CustomModel(['a'])
20/38: q.get_params()
20/39:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        #super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        print(self)
       # self.fit(X, y)
        self.model.fit(X,y)

        return self
20/40: q = CustomModel(['a'])
20/41: q.get_params()
20/42:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        #super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        print(self)
       # self.fit(X, y)
        self.model.fit(X,y)

        return self
20/43: q = CustomModel(['a'])
20/44: q.get_params()
20/45:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        print(self)
       # self.fit(X, y)
        self.model.fit(X,y)

        return self
20/46: q = CustomModel(['a'])
20/47: q.get_params()
20/48: from sklearn import datasets
20/49:
from sklearn import datasets
iris = datasets.load_iris()
20/50:
X = iris.data[:, :2]  # we only take the first two features.
y = iris.target
20/51: X
20/52: iris
20/53:
X = iris.data  # we only take the first two features.
y = iris.target
20/54: X
20/55: X.type
20/56: df = pd.DataFrame(X, columns = ['a','b','c','d'])
20/57: df
20/58:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        print(self)
       # self.fit(X, y)
        self.model.fit(X,y)

        return self
20/59: q = CustomModel(['a'])
20/60: q.get_params()
20/61: q.model.fit(df, y)
20/62: q.model.predict(df)
20/63:
my_r = LGBMRegressor()
my_r.fit(df, y)
20/64:
my_r = LGBMRegressor()
my_r.fit(df, y)
my_r.predict(df)
20/65:
real_model = LGBMRegressor()
real_model.fit(df, y)
y_real = real_model.predict(df)
20/66: y_my = q.model.predict(df)
20/67: np.array_equal(y_real, y_my)
20/68: X
20/69: Xdf
20/70: df
20/71: df
20/72: df.drop(['a'], axis=1)
20/73: list = ['a']
20/74: list_to_drop = ['a']
20/75: df.drop(list_to_drop, axis=1)
20/76:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model.fit(X_copy,y)

        return self
20/77: list_to_drop = ['a']
20/78: df.drop(list_to_drop, axis=1)
20/79: q = CustomModel(['a'])
20/80: q.get_params()
20/81: q.model.fit(df, y)
20/82: y_my = q.model.predict(df)
20/83: np.array_equal(y_real, y_my)
20/84:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy)
        self.model.fit(X_copy,y)

        return self
20/85: list_to_drop = ['a']
20/86: df.drop(list_to_drop, axis=1)
20/87: q = CustomModel(['a'])
20/88: q.get_params()
20/89: q.model.fit(df, y)
20/90:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        print('Hi')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy)
        self.model.fit(X_copy,y)

        return self
20/91: list_to_drop = ['a']
20/92: df.drop(list_to_drop, axis=1)
20/93: q = CustomModel(['a'])
20/94: q.get_params()
20/95: q.model.fit(df, y)
20/96:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None, cols = self.cols):
        print('Hi')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(cols, axis=1)
        print(X_copy)
        self.model.fit(X_copy,y)

        return self
20/97: list_to_drop = ['a']
20/98:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None, cols = cols):
        print('Hi')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(cols, axis=1)
        print(X_copy)
        self.model.fit(X_copy,y)

        return self
20/99: list_to_drop = ['a']
20/100:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        print('Hi')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy)
        self.model.fit(X_copy,y)

        return self
20/101: list_to_drop = ['a']
20/102: df.drop(list_to_drop, axis=1)
20/103: q = CustomModel(['a'])
20/104: q = CustomModel(['b'])
20/105: q.get_params()
20/106: q.model.fit(df, y)
20/107: y_my = q.model.predict(df)
20/108: np.array_equal(y_real, y_my)
20/109:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def n_fit(self, X, y = None):
        print('Hi')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy)
        self.model.fit(X_copy,y)

        return self
20/110: list_to_drop = ['a']
20/111: df.drop(list_to_drop, axis=1)
20/112: q = CustomModel(['b'])
20/113: q.get_params()
20/114: q.model.n_fit(df, y)
20/115: q.n_fit(df, y)
20/116:
# –ú—ã —Ö–æ—Ç–∏–º –Ω–∞–ø–∏—Å–∞—Ç—å –æ–±–µ—Ä—Ç–∫—É –¥–ª—è LGBMRegressor 
# –≤ –∫–æ—Ç–æ—Ä—É—é –º—ã –±—ã —É–º–µ–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞, 
# –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏. 

# –û—Å–Ω–æ–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å - –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —É–º–µ—Ç—å –ø—Ä–∏—Ç–≤–æ—Ä—è—Ç—å—Å—è 
# —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º sklearn —ç—Å—Ç–∏–º–∞—Ç–æ—Ä–æ–º.
# –¢–æ –µ—Å—Ç—å —Ä–∞–±–æ–∞—Ç—å –±–µ–∑ –ø—Ä–æ–±–ª–µ–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, c cross_validate.

# –ü—Ä–æ–¥—É–º–∞–π –∫–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –∫–ª–∞—Å—Å–µ,
# –∫–∞–∫ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LGBMRegressor —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ —Ç–¥.

# hint:
# 1) –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç sklearn.base.clone 
# 2) –ú–æ–∂–µ—Ç –ª–∏ –ø–æ–º–æ—á—å –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤?


from lightgbm import LGBMRegressor
import copy
20/117: q.n_fit(df, y)
20/118: y_my = q.model.predict(df)
20/119:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def n_fit(self, X, y = None):
        print('Hi')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy)
        self.model.fit(X_copy,y)

        return self
    
     def n_predict(self, X):
        print('Hi')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy)
        self.model.predict(X_copy,)
20/121:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def n_fit(self, X, y = None):
        print('Hi')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy)
        self.model.fit(X_copy,y)

        return self
    
    def n_predict(self, X):
        print('Hi')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy)
        self.model.predict(X_copy,)
20/122: list_to_drop = ['a']
20/123: q = CustomModel(['b'])
20/124: q.get_params()
20/125: q.n_fit(df, y)
20/126: y_my = q.n_predict(df)
20/127:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def n_fit(self, X, y = None):
        print('Hi')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy)
        self.model.fit(X_copy,y)

        return self
    
    def n_predict(self, X):
        print('Hi')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy)
        y = self.model.predict(X_copy,)
        print(y)
20/128: q = CustomModel(['b'])
20/129: q.get_params()
20/130: q.n_fit(df, y)
20/131: y_my = q.n_predict(df)
20/132:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def n_fit(self, X, y = None):
        print('Hi')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy)
        self.model.fit(X_copy,y)

        return self
    
    def n_predict(self, X):
        print('Hi')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy)
20/133:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def n_fit(self, X, y = None):
        print('Hi')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy)
        self.model.fit(X_copy,y)

        return self
    
    def n_predict(self, X):
        print('Hi')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
20/134: q = CustomModel(['b'])
20/135: q.get_params()
20/136: q.n_fit(df, y)
20/137: y_my = q.n_predict(df)
20/138: np.array_equal(y_real, y_my)
20/139: y_my
20/140: scores = cross_validate(q, df, y,scoring='precision_macro', cv=5)
20/141: scores = cross_validate(q.model, df, y,scoring='precision_macro', cv=5)
20/142: y
20/143: scores = cross_validate(q.model, df, df['b'],scoring='precision_macro', cv=5)
20/144: scores = cross_validate(q.model, df, df['b'], cv=5)
20/145: scores
20/146: cross_validate(LGBMRegressor(), df.drop('b', axis=1), df['b'], cv=5)
20/147: scores = cross_validate(q.model, df, df['b'], cv=5)
20/148: scores
20/149: cross_validate(LGBMRegressor(), df.drop('b', axis=1), df['b'], cv=5)
20/150: scores = cross_validate(q.model, df, df['b'], cv=5,scoring='mean_squared_error')
20/151: scores = cross_validate(q.model, df, df['b'], cv=5)
20/152: cross_validate(real_model, df.drop('b', axis=1), df['b'], cv=5)
20/153:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def n_fit(self, X, y = None):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model.fit(X_copy,y)

        return self
    
    def n_predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
20/154: q = CustomModel(['b'])
20/155: q.get_params()
20/156: q.n_fit(df, y)
20/157: y_my = q.n_predict(df)
20/158: np.array_equal(y_real, y_my)
20/159:
real_model = LGBMRegressor()
real_model.fit(df, y)
y_real = real_model.predict(df)
20/160: scores = cross_validate(q.model, df, df['b'], cv=5)
20/161: scores
20/162: cross_validate(real_model, df.drop('b', axis=1), df['b'], cv=5)
20/163:
real_model = LGBMRegressor()
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
20/164: cross_validate(real_model, df.drop('b', axis=1), df['b'], cv=5)
20/165: scores = cross_validate(real_model, df.drop('b', axis=1), df['b'], cv=5)
20/166: scores
20/167: real_model.get_params
20/168: q.model.get_params
20/169: q.model.get_params == real_model.get_params
20/170: q.model.get_params
20/171: q.model == real_model
20/172: q.model.get_params
20/173: real_model.set_params(q.model.get_params)
20/174: df
20/175: df
20/176: q.model.predict(df)
21/1:
# –ú—ã —Ö–æ—Ç–∏–º –Ω–∞–ø–∏—Å–∞—Ç—å –æ–±–µ—Ä—Ç–∫—É –¥–ª—è LGBMRegressor 
# –≤ –∫–æ—Ç–æ—Ä—É—é –º—ã –±—ã —É–º–µ–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞, 
# –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏. 

# –û—Å–Ω–æ–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å - –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —É–º–µ—Ç—å –ø—Ä–∏—Ç–≤–æ—Ä—è—Ç—å—Å—è 
# —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º sklearn —ç—Å—Ç–∏–º–∞—Ç–æ—Ä–æ–º.
# –¢–æ –µ—Å—Ç—å —Ä–∞–±–æ–∞—Ç—å –±–µ–∑ –ø—Ä–æ–±–ª–µ–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, c cross_validate.

# –ü—Ä–æ–¥—É–º–∞–π –∫–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –∫–ª–∞—Å—Å–µ,
# –∫–∞–∫ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LGBMRegressor —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ —Ç–¥.

# hint:
# 1) –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç sklearn.base.clone 
# 2) –ú–æ–∂–µ—Ç –ª–∏ –ø–æ–º–æ—á—å –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤?


from lightgbm import LGBMRegressor
import copy
21/2:
import numpy as np
import pandas as pd
21/3:
from sklearn import datasets
iris = datasets.load_iris()
21/4:
X = iris.data  # we only take the first two features.
y = iris.target
21/5: df = pd.DataFrame(X, columns = ['a','b','c','d'])
21/6: #df = pd.DataFrame(np.array([[1, 22, 3], [4, 5, 26], [7, 8, 9]]), columns=['a', 'b', 'c'])
21/7: #y=[0,1,0]
21/8:
#.fit()
#.predict()
#.score()
#.set_params()
#.get_params()
21/9: from sklearn.model_selection import cross_validate
21/10:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def n_fit(self, X, y = None):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model.fit(X_copy,y)

        return self
    
    def n_predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
21/11: q = CustomModel(['b'])
21/12: q.get_params()
21/13: q.n_fit(df, y)
21/14: y_my = q.n_predict(df)
21/15: np.array_equal(y_real, y_my)
21/16:
real_model = LGBMRegressor()
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
21/17: real_model.get_params
21/18: q.model.get_params
21/19: scores = cross_validate(q.model, df, df['b'], cv=5)
21/20: scores
21/21:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def n_fit(self, X, y = None):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model.fit(X_copy,y)

        return self
    
    def n_predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
21/22: scores = cross_validate(q.model, df, df['b'], cv=5)
21/23: scores
21/24:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def n_fit(self, X, y = None):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model.fit(X_copy,y)

        return self
    
    def n_predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
21/25: q = CustomModel(['b'])
21/26: q.get_params()
21/27: q.n_fit(df, y)
21/28: y_my = q.n_predict(df)
21/29:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def n_fit(self, X, y = None):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model.fit(X_copy,y)

        return self
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
21/30: q = CustomModel(['b'])
21/31: q.get_params()
21/32: q.n_fit(df, y)
21/33: y_my = q.predict(df)
21/34:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model.fit(X_copy,y)

        return self
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
21/35: q = CustomModel(['b'])
21/36: q.get_params()
21/37: q.n_fit(df, y)
21/38: q.fit(df, y)
21/39: q = CustomModel(['b'])
21/40: q.get_params()
21/41: q.fit(df, y)
21/42: y_my = q.predict(df)
21/43:
real_model = LGBMRegressor()
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
21/44: real_model.get_params
21/45: q.model.get_params
21/46: scores = cross_validate(q.model, df, df['b'], cv=5)
21/47: scores
22/1:
# –ú—ã —Ö–æ—Ç–∏–º –Ω–∞–ø–∏—Å–∞—Ç—å –æ–±–µ—Ä—Ç–∫—É –¥–ª—è LGBMRegressor 
# –≤ –∫–æ—Ç–æ—Ä—É—é –º—ã –±—ã —É–º–µ–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞, 
# –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏. 

# –û—Å–Ω–æ–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å - –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —É–º–µ—Ç—å –ø—Ä–∏—Ç–≤–æ—Ä—è—Ç—å—Å—è 
# —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º sklearn —ç—Å—Ç–∏–º–∞—Ç–æ—Ä–æ–º.
# –¢–æ –µ—Å—Ç—å —Ä–∞–±–æ–∞—Ç—å –±–µ–∑ –ø—Ä–æ–±–ª–µ–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, c cross_validate.

# –ü—Ä–æ–¥—É–º–∞–π –∫–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –∫–ª–∞—Å—Å–µ,
# –∫–∞–∫ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LGBMRegressor —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ —Ç–¥.

# hint:
# 1) –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç sklearn.base.clone 
# 2) –ú–æ–∂–µ—Ç –ª–∏ –ø–æ–º–æ—á—å –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤?


from lightgbm import LGBMRegressor
import copy
22/2:
import numpy as np
import pandas as pd
22/3:
from sklearn import datasets
iris = datasets.load_iris()
22/4:
X = iris.data  # we only take the first two features.
y = iris.target
22/5: df = pd.DataFrame(X, columns = ['a','b','c','d'])
22/6: #df = pd.DataFrame(np.array([[1, 22, 3], [4, 5, 26], [7, 8, 9]]), columns=['a', 'b', 'c'])
22/7: #y=[0,1,0]
22/8:
#.fit()
#.predict()
#.score()
#.set_params()
#.get_params()
22/9: from sklearn.model_selection import cross_validate
22/10:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model.fit(X_copy,y)

        return self
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
22/11: q = CustomModel(['b'])
22/12: q.get_params()
22/13: q.fit(df, y)
22/14: y_my = q.predict(df)
22/15:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model = self.model.fit(X_copy,y)

        return self
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
22/16: q = CustomModel(['b'])
22/17: q.fit(df, y)
22/18: y_my = q.predict(df)
22/19:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model = self.model.fit(X_copy,y)

        return self
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
22/20: q = CustomModel(['b'])
22/21: q.fit(df, y)
22/22: from sklearn.base import clone
22/23:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        model = clone(self.model)
        print(model)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model = model.fit(X_copy,y)
        print(self.model)

        return self
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
22/24: q = CustomModel(['b'])
22/25: q.fit(df, y)
22/26:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        model = clone(self.model)
        model = self.model
        print(model)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model = model.fit(X_copy,y)
        print(self.model)

        return self
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
22/27: q = CustomModel(['b'])
22/28: q.fit(df, y)
22/29:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        model = clone(self.model)
        print(model = self.model)
        print(model)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model = model.fit(X_copy,y)
        print(self.model)

        return self
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
22/30: q = CustomModel(['b'])
22/31: q.fit(df, y)
22/32:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        model = clone(self.model)
        print(model == self.model)
        print(model)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model = model.fit(X_copy,y)
        print(self.model)

        return self
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
22/33: q = CustomModel(['b'])
22/34: q.fit(df, y)
22/35:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        model = clone(self.model)
        model == self.model
        print(model)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model = model.fit(X_copy,y)
        print(self.model)

        return self
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
22/36: q = CustomModel(['b'])
22/37: q.fit(df, y)
22/38:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        model = clone(self.model)
        model == self.model
        print('you fit me')
        print(model)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model = model.fit(X_copy,y)
        print(self.model)

        return self
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
22/39: q = CustomModel(['b'])
22/40: q.fit(df, y)
22/41:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        model = clone(self.model)
        model == self.model
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model = model.fit(X_copy,y)
        print(self.model)

        return self
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
22/42: q = CustomModel(['b'])
22/43: q.fit(df, y)
22/44:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        model = clone(self.model)
        print(model == self.model)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model = model.fit(X_copy,y)
        print(self.model)

        return self
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
22/45: q = CustomModel(['b'])
22/46: q.fit(df, y)
22/47: real_model = LGBMRegressor()
22/48: real_model
22/49: real_model.get_params
22/50:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
22/51: real_model.get_params
22/52: real_model.get_params == real_model.get_params
22/53: real_model
22/54: real_model2 = LGBMRegressor()
22/55: real_model.get_params
22/56:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
22/57: real_model == real_model2
22/58: real_model = LGBMRegressor()
22/59: real_model2 = LGBMRegressor()
22/60: real_model == real_model2
22/61: real_model.get_params == real_model2.get_params
22/62: from sklearn import tree
22/63:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(model=DecisionTreeClassifier())
        
    def fit(self, X, y = None):
        model = clone(self.model)
        print(model == self.model)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model = model.fit(X_copy,y)
        print(self.model)

        return self
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
22/64:
class CustomModel(DecisionTreeClassifier):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(model=DecisionTreeClassifier())
        
    def fit(self, X, y = None):
        model = clone(self.model)
        print(model == self.model)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model = model.fit(X_copy,y)
        print(self.model)

        return self
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
22/65:
class CustomModel(tree):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(model=DecisionTreeClassifier())
        
    def fit(self, X, y = None):
        model = clone(self.model)
        print(model == self.model)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model = model.fit(X_copy,y)
        print(self.model)

        return self
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
22/66:
class CustomModel(tree.DecisionTreeClassifier()):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(model=DecisionTreeClassifier())
        
    def fit(self, X, y = None):
        model = clone(self.model)
        print(model == self.model)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model = model.fit(X_copy,y)
        print(self.model)

        return self
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
22/67: q = CustomModel(['b'])
22/68:
class CustomModel(tree.DecisionTreeClassifier:
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(model=DecisionTreeClassifier())
        
    def fit(self, X, y = None):
        model = clone(self.model)
        print(model == self.model)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model = model.fit(X_copy,y)
        print(self.model)

        return self
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
22/69:
class CustomModel (tree.DecisionTreeClassifier):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(model=DecisionTreeClassifier())
        
    def fit(self, X, y = None):
        model = clone(self.model)
        print(model == self.model)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model = model.fit(X_copy,y)
        print(self.model)

        return self
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
22/70: q = CustomModel(['b'])
22/71:
class CustomModel (tree.DecisionTreeClassifier):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(model=tree.DecisionTreeClassifier())
        
    def fit(self, X, y = None):
        model = clone(self.model)
        print(model == self.model)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model = model.fit(X_copy,y)
        print(self.model)

        return self
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
22/72: q = CustomModel(['b'])
22/73:
class CustomModel (tree.DecisionTreeClassifier):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(base_estimator=tree.DecisionTreeClassifier())
        
    def fit(self, X, y = None):
        model = clone(self.model)
        print(model == self.model)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model = model.fit(X_copy,y)
        print(self.model)

        return self
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
22/74: q = CustomModel(['b'])
22/75:
from sklearn import tree
from tree import DecisionTreeClassifier()
22/76:
from sklearn import tree
from tree import DecisionTreeClassifier
22/77:
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
22/78:
class CustomModel (tree.DecisionTreeClassifier):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(base_estimator=tree.DecisionTreeClassifier())
        
    def fit(self, X, y = None):
        model = clone(self.model)
        print(model == self.model)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model = model.fit(X_copy,y)
        print(self.model)

        return self
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
22/79: q = CustomModel(['b'])
22/80:
class CustomModel (DecisionTreeClassifier):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(model=tree.DecisionTreeClassifier())
        
    def fit(self, X, y = None):
        model = clone(self.model)
        print(model == self.model)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model = model.fit(X_copy,y)
        print(self.model)

        return self
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
22/81: q = CustomModel(['b'])
22/82:
class CustomModel (DecisionTreeClassifier):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(classifier=tree.DecisionTreeClassifier())
        
    def fit(self, X, y = None):
        model = clone(self.model)
        print(model == self.model)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model = model.fit(X_copy,y)
        print(self.model)

        return self
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
22/83: q = CustomModel(['b'])
22/84:
class CustomModel (DecisionTreeClassifier):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(classifier=tree.DecisionTreeClassifier())
        
    def fit(self, X, y = None):
        model = clone(self.model)
        print(model == self.model)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model = model.fit(X_copy,y)
        print(self.model)

        return self
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
22/85: q = CustomModel(['b'])
23/1:
# –ú—ã —Ö–æ—Ç–∏–º –Ω–∞–ø–∏—Å–∞—Ç—å –æ–±–µ—Ä—Ç–∫—É –¥–ª—è LGBMRegressor 
# –≤ –∫–æ—Ç–æ—Ä—É—é –º—ã –±—ã —É–º–µ–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞, 
# –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏. 

# –û—Å–Ω–æ–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å - –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —É–º–µ—Ç—å –ø—Ä–∏—Ç–≤–æ—Ä—è—Ç—å—Å—è 
# —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º sklearn —ç—Å—Ç–∏–º–∞—Ç–æ—Ä–æ–º.
# –¢–æ –µ—Å—Ç—å —Ä–∞–±–æ–∞—Ç—å –±–µ–∑ –ø—Ä–æ–±–ª–µ–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, c cross_validate.

# –ü—Ä–æ–¥—É–º–∞–π –∫–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –∫–ª–∞—Å—Å–µ,
# –∫–∞–∫ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LGBMRegressor —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ —Ç–¥.

# hint:
# 1) –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç sklearn.base.clone 
# 2) –ú–æ–∂–µ—Ç –ª–∏ –ø–æ–º–æ—á—å –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤?


from lightgbm import LGBMRegressor
import copy
23/2: from sklearn.base import clone
23/3:
import numpy as np
import pandas as pd
23/4:
from sklearn import datasets
iris = datasets.load_iris()
23/5:
X = iris.data  # we only take the first two features.
y = iris.target
23/6: df = pd.DataFrame(X, columns = ['a','b','c','d'])
23/7: #df = pd.DataFrame(np.array([[1, 22, 3], [4, 5, 26], [7, 8, 9]]), columns=['a', 'b', 'c'])
23/8: #y=[0,1,0]
23/9:
#.fit()
#.predict()
#.score()
#.set_params()
#.get_params()
23/10: from sklearn.model_selection import cross_validate
23/11:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(model=LGBMRegressor())
        
    def fit(self, X, y = None):
        model = clone(self.model)
        print(model == self.model)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model = model.fit(X_copy,y)
        print(self.model)

        return self
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
23/12: q = CustomModel(['b'])
23/13: q
23/14:
model = LGBMRegressor(boosting_type='gbdt', objective='multiclass',num_class=9,
                      early_stopping = 50,num_iteration=10000,num_leaves=31,is_enable_sparse='true',
                      tree_learner='data',min_data_in_leaf=600,max_depth=4, learning_rate=0.1, 
                      n_estimators=675, max_bin=255, subsample_for_bin=50000, min_split_gain=5, 
                      min_child_weight=5, min_child_samples=10, subsample=0.995, subsample_freq=1, 
                      colsample_bytree=1, reg_alpha=0, reg_lambda=0, seed=0, nthread=-1, silent=True)
23/15: model.fit(X, y)
23/16: model.fit(df, y)
23/17:
X = iris.data  # we only take the first two features.
y = iris.target
23/18: df = pd.DataFrame(X, columns = ['a','b','c','d'])
23/19: model.fit(df, y)
23/20:
lgb_estimator = lgb.LGBMClassifier(boosting_type = 'gbdt', 
                                   n_estimators=500, 
                                   learning_rate =  0.05, num_leaves =  64,  
                                   eval_metric  = 'logloss',
                                   verbose_eval=20, 
                                   early_stopping_rounds=10)
23/21: model.fit(df, y)
23/22:
model = LGBMRegressor(boosting_type='gbdt', objective='multiclass',num_class=9,
                      early_stopping = 50,num_iteration=10000,num_leaves=31,is_enable_sparse='true',
                      tree_learner='data',min_data_in_leaf=600,max_depth=4, learning_rate=0.1, 
                      n_estimators=675, max_bin=255, subsample_for_bin=50000, min_split_gain=5, 
                      min_child_weight=5, min_child_samples=10, subsample=0.995, subsample_freq=1, 
                      colsample_bytree=1, reg_alpha=0, reg_lambda=0, seed=0, nthread=-1, silent=True)

lgb_estimator = LGBMClassifier(boosting_type = 'gbdt', 
                                   n_estimators=500, 
                                   learning_rate =  0.05, num_leaves =  64,  
                                   eval_metric  = 'logloss',
                                   verbose_eval=20, 
                                   early_stopping_rounds=10)
23/23:
# –ú—ã —Ö–æ—Ç–∏–º –Ω–∞–ø–∏—Å–∞—Ç—å –æ–±–µ—Ä—Ç–∫—É –¥–ª—è LGBMRegressor 
# –≤ –∫–æ—Ç–æ—Ä—É—é –º—ã –±—ã —É–º–µ–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞, 
# –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏. 

# –û—Å–Ω–æ–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å - –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —É–º–µ—Ç—å –ø—Ä–∏—Ç–≤–æ—Ä—è—Ç—å—Å—è 
# —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º sklearn —ç—Å—Ç–∏–º–∞—Ç–æ—Ä–æ–º.
# –¢–æ –µ—Å—Ç—å —Ä–∞–±–æ–∞—Ç—å –±–µ–∑ –ø—Ä–æ–±–ª–µ–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, c cross_validate.

# –ü—Ä–æ–¥—É–º–∞–π –∫–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –∫–ª–∞—Å—Å–µ,
# –∫–∞–∫ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LGBMRegressor —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ —Ç–¥.

# hint:
# 1) –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç sklearn.base.clone 
# 2) –ú–æ–∂–µ—Ç –ª–∏ –ø–æ–º–æ—á—å –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤?


from lightgbm import LGBMRegressor
from lightgbm import LGBMClassifier
import copy
23/24:
lgb_estimator = LGBMClassifier(boosting_type = 'gbdt', 
                                   n_estimators=500, 
                                   learning_rate =  0.05, num_leaves =  64,  
                                   eval_metric  = 'logloss',
                                   verbose_eval=20, 
                                   early_stopping_rounds=10)
23/25: model.fit(df, y)
23/26: y
23/27: y[y==2]
23/28: y[y==2]=0
23/29:
lgb_estimator = LGBMClassifier(boosting_type = 'gbdt', 
                                   n_estimators=500, 
                                   learning_rate =  0.05, num_leaves =  64,  
                                   eval_metric  = 'logloss',
                                   verbose_eval=20, 
                                   early_stopping_rounds=10)
23/30: model.fit(df, y)
23/31:
model = LGBMClassifier(boosting_type='gbdt', objective='binary', num_leaves=50,
                                learning_rate=0.1, n_estimators=modelcount, max_depth=censhu,
                                bagging_fraction=0.9, feature_fraction=0.9, reg_lambda=0.2)
23/32: model.fit(df, y)
23/33:
model = LGBMClassifier(boosting_type='gbdt', objective='binary', num_leaves=50,
                                learning_rate=0.1, n_estimators=5, max_depth=censhu,
                                bagging_fraction=0.9, feature_fraction=0.9, reg_lambda=0.2)
23/34: model.fit(df, y)
23/35:
model = LGBMClassifier(boosting_type='gbdt', objective='binary', num_leaves=50,
                                learning_rate=0.1, n_estimators=5, max_depth=10,
                                bagging_fraction=0.9, feature_fraction=0.9, reg_lambda=0.2)
23/36: model.fit(df, y)
23/37:
model = LGBMClassifier(boosting_type='gbdt', objective='binary', num_leaves=50,
                                learning_rate=0.1, n_estimators=5, max_depth=2,
                                bagging_fraction=0.9, feature_fraction=0.9, reg_lambda=0.2)
23/38: model
23/39: model.predict(df)
23/40: model.fit(df, y)
23/41: model.predict(df)
23/42: cross_validate(model, df, y, cv=5)
23/43: q.fit(df, y)
23/44: y_my = q.predict(df)
23/45: scores = cross_validate(q, df, df['b'], cv=5)
23/46: q
23/47:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(base_estimator=LGBMRegressor())
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print(model == self.model)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model = model.fit(X_copy,y)
        print(self.model)

        return self
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
23/48: q = CustomModel(['b'])
23/49: q
23/50: q.fit(df, y)
23/51:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(base_estimator=LGBMRegressor())
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print(model == self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model = model.fit(X_copy,y)
        print(self.model)

        return self
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
23/52: q = CustomModel(['b'])
23/53: q
23/54: q.fit(df, y)
23/55:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(base_estimator=LGBMRegressor())
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print(model == self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        model.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
23/56: q = CustomModel(['b'])
23/57: q
23/58: q.fit(df, y)
23/59:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(base_estimator=LGBMRegressor())
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        model.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
23/60: q = CustomModel(['b'])
23/61: q
23/62: q.fit(df, y)
23/63: y_my = q.predict(df)
23/64:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(base_estimator=LGBMRegressor())
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        model.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
23/65: q = CustomModel(['b'])
23/66: q
23/67: q.fit(df, y)
23/68: y_my = q.predict(df)
23/69:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(base_estimator=LGBMRegressor())
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        model.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(model.predict(X_copy))
23/70: q = CustomModel(['b'])
23/71: q
23/72: q.fit(df, y)
23/73: y_my = q.predict(df)
23/74:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(base_estimator=LGBMRegressor())
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator = model.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(model.predict(X_copy))
23/75: q = CustomModel(['b'])
23/76: q
23/77: q.fit(df, y)
23/78: y_my = q.predict(df)
23/79:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(base_estimator=LGBMRegressor())
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator = model.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
23/80: q = CustomModel(['b'])
23/81: q
23/82: q.fit(df, y)
23/83: y_my = q.predict(df)
23/84: q
23/85: scores = cross_validate(q, df, df['b'], cv=5)
23/86: q
23/87: scores = cross_validate(q, df, df['b'], cv=5)
23/88: cross_validate(q, df, df['b'], cv=5)
24/1:
# –ú—ã —Ö–æ—Ç–∏–º –Ω–∞–ø–∏—Å–∞—Ç—å –æ–±–µ—Ä—Ç–∫—É –¥–ª—è LGBMRegressor 
# –≤ –∫–æ—Ç–æ—Ä—É—é –º—ã –±—ã —É–º–µ–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞, 
# –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏. 

# –û—Å–Ω–æ–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å - –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —É–º–µ—Ç—å –ø—Ä–∏—Ç–≤–æ—Ä—è—Ç—å—Å—è 
# —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º sklearn —ç—Å—Ç–∏–º–∞—Ç–æ—Ä–æ–º.
# –¢–æ –µ—Å—Ç—å —Ä–∞–±–æ–∞—Ç—å –±–µ–∑ –ø—Ä–æ–±–ª–µ–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, c cross_validate.

# –ü—Ä–æ–¥—É–º–∞–π –∫–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –∫–ª–∞—Å—Å–µ,
# –∫–∞–∫ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LGBMRegressor —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ —Ç–¥.

# hint:
# 1) –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç sklearn.base.clone 
# 2) –ú–æ–∂–µ—Ç –ª–∏ –ø–æ–º–æ—á—å –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤?


from lightgbm import LGBMRegressor
from lightgbm import LGBMClassifier
import copy
24/2: from sklearn.base import clone
24/3:
import numpy as np
import pandas as pd
24/4:
from sklearn import datasets
iris = datasets.load_iris()
24/5:
X = iris.data  # we only take the first two features.
y = iris.target
24/6: df = pd.DataFrame(X, columns = ['a','b','c','d'])
24/7: #df = pd.DataFrame(np.array([[1, 22, 3], [4, 5, 26], [7, 8, 9]]), columns=['a', 'b', 'c'])
24/8: #y=[0,1,0]
24/9:
#.fit()
#.predict()
#.score()
#.set_params()
#.get_params()
24/10: from sklearn.model_selection import cross_validate
24/11:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(base_estimator=LGBMRegressor())
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator = model.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/12: q = CustomModel(['b'])
24/13: q
24/14: q.fit(df, y)
24/15: y_my = q.predict(df)
24/16: real_model = LGBMRegressor()
24/17: q.fit(df, y)
24/18: y_my = q.predict(df)
24/19: q = CustomModel(['a'])
24/20: cross_validate(q, df, df['b'], cv=5)
24/21:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        base_estimator=LGBMRegressor()
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator = model.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/22: q = CustomModel(['a'])
24/23: q
24/24: q.fit(df, y)
24/25: y_my = q.predict(df)
24/26: real_model = LGBMRegressor()
24/27: real_model2 = LGBMRegressor()
24/28: real_model.get_params
24/29:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
24/30: real_model.get_params == real_model2.get_params
24/31: real_model.get_params
24/32: q
24/33: cross_validate(q, df, df['b'], cv=5)
24/34:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, **params):
        print('You create me')
        self.cols = cols
        #base_estimator=LGBMRegressor()
        super().__init__(**lgbm_params)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator = model.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/35: q = CustomModel(['a'])
24/36:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, **params):
        print('You create me')
        self.cols = cols
        #base_estimator=LGBMRegressor()
        lgbm_params = {}
        for key, value in params.items():
            if key not in LRegressor.lg_params:
                lgbm_params[key] = value
                
        super().__init__(**lgbm_params)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator = model.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/37: q = CustomModel(['a'])
24/38: q
24/39: q.fit(df, y)
24/40:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor()):
        print('You create me')
        self.cols = cols
        self.base_estimator=base_estimator
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator = model.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/41: q = CustomModel(['a'])
24/42: q
24/43: q.fit(df, y)
24/44: y_my = q.predict(df)
24/45: cross_validate(q, df, df['b'], cv=5)
24/46:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor()):
        print('You create me')
        self.cols = cols
        self.base_estimator=base_estimator
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        #self.base_estimator = model.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/47: q = CustomModel(['a'])
24/48: q
24/49: q.fit(df, y)
24/50: y_my = q.predict(df)
24/51:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor()):
        print('You create me')
        self.cols = cols
        self.base_estimator=base_estimator
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/52: q = CustomModel(['a'])
24/53: q
24/54: q.fit(df, y)
24/55: y_my = q.predict(df)
24/56: real_model = LGBMRegressor()
24/57: real_model2 = LGBMRegressor()
24/58: real_model.get_params
24/59:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
24/60: real_model.get_params == real_model2.get_params
24/61: real_model.get_params
24/62: q
24/63: cross_validate(q, df, df['b'], cv=5)
24/64: q = CustomModel(min_child_samples = 10, ['a'])
24/65: q = CustomModel(min_child_samples = 10, cols = ['a'])
24/66:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        base_estimator=base_estimator
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/67: q = CustomModel(min_child_samples = 10, cols = ['a'])
24/68: q = CustomModel(cols = ['a'])
24/69:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        base_estimator=LGBMRegressor()
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/70: q = CustomModel(cols = ['a'])
24/71: q = CustomModel(min_child_samples = 10, cols = ['a'])
24/72:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols,est_class=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.est_class = est_class

        # kwargs depend on the model used, so assign them whatever they are
        for key, value in kwargs.items():
            setattr(self, key, value)

        self._param_names = ['est_class'] + list(kwargs.keys())
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/73: q = CustomModel(min_child_samples = 10, cols = ['a'])
24/74:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols,est_class=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.est_class = est_class

        # kwargs depend on the model used, so assign them whatever they are
        for key, value in kwargs.items():
            setattr(self, key, value)

        self._param_names = ['est_class'] + list(kwargs.keys())
        super().__init__(base_estimator=est_class)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/75: q = CustomModel(min_child_samples = 10, cols = ['a'])
24/76: q
24/77: q.params()
24/78: q.get_params()
24/79: q = CustomModel(min_child_samples = 10, cols = ['a'])
24/80: q.get_params()
24/81:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols,**kwargs):
        print('You create me')
        self.cols = cols
        base_estimator = LGBMRegressor()

        # kwargs depend on the model used, so assign them whatever they are
        for key, value in kwargs.items():
            setattr(self, key, value)

        self._param_names = ['est_class'] + list(kwargs.keys())
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/82: q = CustomModel(min_child_samples = 10, cols = ['a'])
24/83: q.get_params()
24/84:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        base_estimator=LGBMRegressor()
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/85: q = CustomModel(min_child_samples = 10, cols = ['a'])
24/86: q.get_params()
24/87:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor()
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/88: q = CustomModel(min_child_samples = 10, cols = ['a'])
24/89: q = CustomModel(cols = ['a'])
24/90:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols,base_estimator):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor()
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/91: q = CustomModel(cols = ['a'])
24/92:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols,base_estimator=LGBMRegressor()):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor()
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/93: q = CustomModel(cols = ['a'])
24/94: q.get_params()
24/95:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols,base_estimator=LGBMRegressor()):
        print('You create me')
        self.cols = cols
        #self.base_estimator=LGBMRegressor()
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/96: q = CustomModel(cols = ['a'])
24/97: q.get_params()
24/98:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols,base_estimator=LGBMRegressor()):
        print('You create me')
        self.cols = cols
        #self.base_estimator=LGBMRegressor()
        super().__init__(base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/99: q = CustomModel(cols = ['a'])
24/100: q.get_params()
24/101: q.fit(df, y)
24/102:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols,base_estimator=LGBMRegressor()):
        print('You create me')
        self.cols = cols
        #self.base_estimator=LGBMRegressor()
        super().__init__(LGBMRegressor())
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/103: q = CustomModel(cols = ['a'])
24/104: q.get_params()
24/105: q.fit(df, y)
24/106:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols,base_estimator=LGBMRegressor()):
        print('You create me')
        self.cols = cols
        #self.base_estimator=LGBMRegressor()
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/107: q = CustomModel(cols = ['a'])
24/108: q.get_params()
24/109: q = CustomModel(cols = ['a'], max_depth = 2)
24/110:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        #self.base_estimator=LGBMRegressor()
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/111: q = CustomModel(cols = ['a'], max_depth = 2)
24/112: q = CustomModel(cols = ['a'])
24/113:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        base_estimator=LGBMRegressor()
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/114: q = CustomModel(cols = ['a'])
24/115: q.get_params()
24/116:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor()
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/117: q = CustomModel(cols = ['a'])
24/118: q.get_params()
24/119:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self, cols, base_estimator):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor()
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/120: q = CustomModel(cols = ['a'])
24/121:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self, cols, estim_params={}):
        print('You create me')
        self.cols = cols
        self.estim_params = estim_params
        #self.base_estimator=LGBMRegressor()
        super().__init__(LGBMRegressor())
        self.estimator = self.estimator.set_params(**self.estim_params)
        
        
        #super().__init__(LGBMRegressor())
        # the below sets params correctly, but doesn't work for grid search:
        # self.estimator = self.estimator.set_params(**{'learning_rate': 12345})
        # the below works with sklearn estimators, but not here:
        #self.estimator = self.estimator.set_params(**self.estim_params)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/122: q = CustomModel(cols = ['a'])
24/123:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self, cols, estim_params={}):
        print('You create me')
        self.cols = cols
        self.estim_params = estim_params
        #self.base_estimator=LGBMRegressor()
        super().__init__(LGBMRegressor())
        #self.estimator = self.estimator.set_params(**self.estim_params)
        
        
        #super().__init__(LGBMRegressor())
        # the below sets params correctly, but doesn't work for grid search:
        # self.estimator = self.estimator.set_params(**{'learning_rate': 12345})
        # the below works with sklearn estimators, but not here:
        #self.estimator = self.estimator.set_params(**self.estim_params)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/124: q = CustomModel(cols = ['a'])
24/125: q.get_params()
24/126: q.fit(df, y)
24/127:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self, cols, estim_params={}):
        print('You create me')
        self.cols = cols
        self.estim_params = estim_params
        #self.base_estimator=LGBMRegressor()
        super().__init__(base_estimator = LGBMRegressor())
        #self.estimator = self.estimator.set_params(**self.estim_params)
        
        
        #super().__init__(LGBMRegressor())
        # the below sets params correctly, but doesn't work for grid search:
        # self.estimator = self.estimator.set_params(**{'learning_rate': 12345})
        # the below works with sklearn estimators, but not here:
        #self.estimator = self.estimator.set_params(**self.estim_params)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/128: q = CustomModel(cols = ['a'])
24/129: q.get_params()
24/130: q.fit(df, y)
24/131: q = CustomModel(max_depth = 3, cols = ['a'])
24/132: q = CustomModel(cols = ['a'], max_depth = 3)
24/133:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self, cols, estim_params={}):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor()
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/134: q = CustomModel(cols = ['a'], max_depth = 3)
24/135:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols,base_estimator=LGBMRegressor()):
        print('You create me')
        self.cols = cols
        #self.base_estimator=LGBMRegressor()
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/136: q = CustomModel(cols = ['a'], max_depth = 3)
24/137:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols,base_estimator=LGBMRegressor()):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor()
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/138: q = CustomModel(cols = ['a'], max_depth = 3)
24/139:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols,base_estimator):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor()
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/140: q = CustomModel(cols = ['a'], max_depth = 3)
24/141:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols,base_estimator=base_estimator):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor()
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/142: q = CustomModel(cols = ['a'], max_depth = 3)
24/143:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,colsbase_estimator=base_estimator):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor()
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/144:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor()
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/145: q = CustomModel(cols = ['a'], max_depth = 3)
24/146:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor()):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor()
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/147: q = CustomModel(cols = ['a'], max_depth = 3)
24/148: q = CustomModel(cols = ['a'])
24/149: q.get_params()
24/150:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor()):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor()
        super().__init__(base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/151: q = CustomModel(cols = ['a'])
24/152: q.get_params()
24/153:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor()):
        print('You create me')
        self.cols = cols
        self.base_estimator=base_estimator
        super().__init__(base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/154: q = CustomModel(cols = ['a'])
24/155: q.get_params()
24/156:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor()):
        print('You create me')
        self.cols = cols
        base_estimator=base_estimator
        super().__init__(base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/157: q = CustomModel(cols = ['a'])
24/158: q.get_params()
24/159: q.fit(df, y)
24/160:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor()
        super().__init__(base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/161: q = CustomModel(cols = ['a'])
24/162: q.get_params()
24/163: q.fit(df, y)
24/164:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor()):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor()
        super().__init__(base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/165: q = CustomModel(cols = ['a'])
24/166: q.get_params()
24/167: LGBMRegressor()
24/168: LGBMRegressor(n_estimators = 4)
24/169:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor()
        super().__init__(base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/170: q = CustomModel(cols = ['a'])
24/171: q.get_params()
24/172:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(n_estimators = 4)):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor()
        super().__init__(base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/173: q = CustomModel(cols = ['a'])
24/174: q.get_params()
24/175:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor()):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(n_estimators = 4)
        super().__init__(base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/176: q = CustomModel(cols = ['a'])
24/177: q.get_params()
24/178:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(**kwargs)
        super().__init__(base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/179: q = CustomModel(cols = ['a'])
24/180: q.get_params()
24/181: q = CustomModel(cols = ['a'], n_estimators=3)
24/182: q.get_params()
24/183: q.fit(df, y)
24/184: y_my = q.predict(df)
24/185: real_model = LGBMRegressor()
24/186: real_model2 = LGBMRegressor()
24/187: real_model.get_params
24/188:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
24/189: real_model.get_params == real_model2.get_params
24/190: real_model.get_params
24/191: q
24/192: cross_validate(q, df, df['b'], cv=5)
24/193:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(**kwargs)
        self.set_params(**kwargs)
        super().__init__(base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/194: q = CustomModel(cols = ['a'], n_estimators=3)
24/195:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(**kwargs)
        super().__init__(base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/196: q = CustomModel(cols = ['a'], n_estimators=3)
24/197:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(**kwargs), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(**kwargs)
        super().__init__(base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/198: q = CustomModel(cols = ['a'], n_estimators=3)
24/199:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator, **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(**kwargs)
        super().__init__(base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/200: q = CustomModel(cols = ['a'], n_estimators=3)
24/201:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, *base_estimator, **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(**kwargs)
        super().__init__(base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/202: q = CustomModel(cols = ['a'], n_estimators=3)
24/203: q.get_params()
24/204:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator, **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(**kwargs)
        super().__init__(base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/205: q = CustomModel(cols = ['a'], n_estimators=3)
24/206:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(**kwargs), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(**kwargs)
        super().__init__(base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/207:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(**kwargs)):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(**kwargs)
        super().__init__(base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/208:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(**kwargs)
        super().__init__(base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/209: q = CustomModel(cols = ['a'], n_estimators=3)
24/210:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(**kwargs)
        super().__init__(base_estimator = base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/211: q = CustomModel(cols = ['a'], n_estimators=3)
24/212: q.get_params()
24/213: q.fit(df, y)
24/214: y_my = q.predict(df)
24/215: real_model = LGBMRegressor()
24/216: real_model2 = LGBMRegressor()
24/217: real_model.get_params
24/218:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
24/219: real_model.get_params == real_model2.get_params
24/220: real_model.get_params
24/221: q
24/222: cross_validate(q, df, df['b'], cv=5)
24/223:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(**kwargs)
        super().__init__(base_estimator = base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/224:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(**kwargs)
        super().__init__(base_estimator = base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.model)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/225: q = CustomModel(cols = ['a'], n_estimators=3)
24/226: q.get_params()
24/227: q.fit(df, y)
24/228:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(**kwargs)
        super().__init__(base_estimator = base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
24/229: q = CustomModel(cols = ['a'], n_estimators=3)
24/230: q.get_params()
24/231: q.fit(df, y)
24/232: y_my = q.predict(df)
24/233: real_model = LGBMRegressor()
24/234: real_model2 = LGBMRegressor()
24/235: real_model.get_params
24/236:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
24/237: real_model.get_params == real_model2.get_params
24/238: real_model.get_params
24/239: q
24/240: cross_validate(q, df, df['b'], cv=5)
24/241: q = CustomModel(cols = ['a'], n_estimators=3, min_child_samples=10)
24/242: q.get_params()
25/1:
# –ú—ã —Ö–æ—Ç–∏–º –Ω–∞–ø–∏—Å–∞—Ç—å –æ–±–µ—Ä—Ç–∫—É –¥–ª—è LGBMRegressor 
# –≤ –∫–æ—Ç–æ—Ä—É—é –º—ã –±—ã —É–º–µ–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞, 
# –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏. 

# –û—Å–Ω–æ–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å - –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —É–º–µ—Ç—å –ø—Ä–∏—Ç–≤–æ—Ä—è—Ç—å—Å—è 
# —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º sklearn —ç—Å—Ç–∏–º–∞—Ç–æ—Ä–æ–º.
# –¢–æ –µ—Å—Ç—å —Ä–∞–±–æ–∞—Ç—å –±–µ–∑ –ø—Ä–æ–±–ª–µ–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, c cross_validate.

# –ü—Ä–æ–¥—É–º–∞–π –∫–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –∫–ª–∞—Å—Å–µ,
# –∫–∞–∫ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LGBMRegressor —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ —Ç–¥.

# hint:
# 1) –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç sklearn.base.clone 
# 2) –ú–æ–∂–µ—Ç –ª–∏ –ø–æ–º–æ—á—å –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤?


from lightgbm import LGBMRegressor
from lightgbm import LGBMClassifier
import copy
25/2: from sklearn.base import clone
25/3:
import numpy as np
import pandas as pd
25/4:
from sklearn import datasets
iris = datasets.load_iris()
25/5:
X = iris.data  # we only take the first two features.
y = iris.target
25/6: df = pd.DataFrame(X, columns = ['a','b','c','d'])
25/7: #df = pd.DataFrame(np.array([[1, 22, 3], [4, 5, 26], [7, 8, 9]]), columns=['a', 'b', 'c'])
25/8: #y=[0,1,0]
25/9:
#.fit()
#.predict()
#.score()
#.set_params()
#.get_params()
25/10: from sklearn.model_selection import cross_validate
25/11: LGBMRegressor(n_estimators = 4)
25/12:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(**kwargs)
        super().__init__(base_estimator = base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/13: q = CustomModel(cols = ['a'], n_estimators=3, min_child_samples=10)
25/14: q.get_params()
25/15:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(**kwargs)
        super().__init__(base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/16: q = CustomModel(cols = ['a'], n_estimators=3, min_child_samples=10)
25/17: q.get_params()
25/18:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(**kwargs)
        super().__init__(base_estimator=self.base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/19: q = CustomModel(cols = ['a'], n_estimators=3, min_child_samples=10)
25/20: q.get_params()
25/21: q.fit(df, y)
25/22: y_my = q.predict(df)
25/23: real_model = LGBMRegressor()
25/24: real_model2 = LGBMRegressor()
25/25: real_model.get_params
25/26:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
25/27: real_model.get_params == real_model2.get_params
25/28: real_model.get_params
25/29: q
25/30: cross_validate(q, df, df['b'], cv=5)
25/31:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=base_estimator
        super().__init__(base_estimator=LGBMRegressor(**kwargs))
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/32: q = CustomModel(cols = ['a'], n_estimators=3, min_child_samples=10)
25/33: q.get_params()
25/34: q.fit(df, y)
25/35: y_my = q.predict(df)
25/36: real_model = LGBMRegressor()
25/37: real_model2 = LGBMRegressor()
25/38: real_model.get_params
25/39:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
25/40: real_model.get_params == real_model2.get_params
25/41: real_model.get_params
25/42: q
25/43: cross_validate(q, df, df['b'], cv=5)
25/44:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=base_estimator
        for key, value in kwargs.items():
            setattr(self, key, value)

        self._param_names = ['est_class'] + list(kwargs.keys())
        super().__init__(base_estimator=LGBMRegressor(**kwargs))
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/45: q = CustomModel(cols = ['a'], n_estimators=3, min_child_samples=10)
25/46: q.get_params()
25/47: q.fit(df, y)
25/48: y_my = q.predict(df)
25/49: real_model = LGBMRegressor()
25/50: real_model2 = LGBMRegressor()
25/51: real_model.get_params
25/52:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
25/53: real_model.get_params == real_model2.get_params
25/54: real_model.get_params
25/55: q
25/56: cross_validate(q, df, df['b'], cv=5)
25/57:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(**kwargs)
        for key, value in kwargs.items():
            setattr(self, key, value)

        #self._param_names = ['est_class'] + list(kwargs.keys())
        super().__init__(base_estimator=LGBMRegressor(**kwargs))
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/58: q = CustomModel(cols = ['a'], n_estimators=3, min_child_samples=10)
25/59: q.get_params()
25/60: q.fit(df, y)
25/61: y_my = q.predict(df)
25/62: real_model = LGBMRegressor()
25/63: real_model2 = LGBMRegressor()
25/64: real_model.get_params
25/65:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
25/66: real_model.get_params == real_model2.get_params
25/67: real_model.get_params
25/68: q
25/69: cross_validate(q, df, df['b'], cv=5)
25/70:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(**kwargs)
        for key, value in kwargs.items():
            setattr(self, key, value)

        self._param_names = ['est_class'] + list(kwargs.keys())
        print(self._param_names)
        super().__init__(base_estimator=LGBMRegressor(**kwargs))
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/71: q = CustomModel(cols = ['a'], n_estimators=3, min_child_samples=10)
25/72:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(**kwargs)
        for key, value in kwargs.items():
            setattr(self, key, value)

        self._param_names = ['est_class'] + list(kwargs.keys())
        print(['est_class'] + list(kwargs.keys()))
        super().__init__(base_estimator=LGBMRegressor(**kwargs))
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/73: q = CustomModel(cols = ['a'], n_estimators=3, min_child_samples=10)
25/74: q.get_params()
25/75:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(**kwargs)
        for key, value in kwargs.items():
            setattr(self, key, value)

        self._param_names = ['est_class'] + list(kwargs.keys())
        print(['est_class'] + list(kwargs.keys()))
        super().__init__(base_estimator=LGBMRegressor(n_estimators=3))
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/76: q = CustomModel(cols = ['a'], n_estimators=3)
25/77: q.get_params()
25/78: q.fit(df, y)
25/79: y_my = q.predict(df)
25/80: real_model = LGBMRegressor()
25/81: real_model2 = LGBMRegressor()
25/82: real_model.get_params
25/83:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
25/84: real_model.get_params == real_model2.get_params
25/85: real_model.get_params
25/86: q
25/87: cross_validate(q, df, df['b'], cv=5)
25/88:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(n_estimators=3)
        for key, value in kwargs.items():
            setattr(self, key, value)

        self._param_names = ['est_class'] + list(kwargs.keys())
        print(['est_class'] + list(kwargs.keys()))
        super().__init__(base_estimator=LGBMRegressor(n_estimators=3))
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/89: q = CustomModel(cols = ['a'], n_estimators=3)
25/90: q.get_params()
25/91: q.fit(df, y)
25/92: y_my = q.predict(df)
25/93: real_model = LGBMRegressor()
25/94: real_model2 = LGBMRegressor()
25/95: real_model.get_params
25/96:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
25/97: real_model.get_params == real_model2.get_params
25/98: real_model.get_params
25/99: q
25/100: cross_validate(q, df, df['b'], cv=5)
25/101:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(n_estimators=3)
        for key, value in kwargs.items():
            setattr(self, key, value)

        self._param_names = ['est_class'] + list(kwargs.keys())
        print(['est_class'] + list(kwargs.keys()))
        super().__init__(base_estimator=LGBMRegressor())
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/102: q = CustomModel(cols = ['a'], n_estimators=3)
25/103: q.get_params()
25/104: q.fit(df, y)
25/105: y_my = q.predict(df)
25/106: real_model = LGBMRegressor()
25/107: real_model2 = LGBMRegressor()
25/108: real_model.get_params
25/109:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
25/110: real_model.get_params == real_model2.get_params
25/111: real_model.get_params
25/112: q
25/113: cross_validate(q, df, df['b'], cv=5)
25/114:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=base_estimator
        for key, value in kwargs.items():
            setattr(self, key, value)

        self._param_names = ['est_class'] + list(kwargs.keys())
        print(['est_class'] + list(kwargs.keys()))
        super().__init__(base_estimator=LGBMRegressor(n_estimators=3))
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/115: q = CustomModel(cols = ['a'], n_estimators=3)
25/116: q.get_params()
25/117: q.fit(df, y)
25/118: y_my = q.predict(df)
25/119: real_model = LGBMRegressor()
25/120: real_model2 = LGBMRegressor()
25/121: real_model.get_params
25/122:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
25/123: real_model.get_params == real_model2.get_params
25/124: real_model.get_params
25/125: q
25/126: cross_validate(q, df, df['b'], cv=5)
25/127:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        #self.base_estimator=base_estimator
        for key, value in kwargs.items():
            setattr(self, key, value)

        self._param_names = ['est_class'] + list(kwargs.keys())
        print(['est_class'] + list(kwargs.keys()))
        super().__init__(base_estimator=LGBMRegressor(n_estimators=3))
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/128: q = CustomModel(cols = ['a'], n_estimators=3)
25/129: q.get_params()
25/130: q.fit(df, y)
25/131: y_my = q.predict(df)
25/132: real_model = LGBMRegressor()
25/133: real_model2 = LGBMRegressor()
25/134: real_model.get_params
25/135:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
25/136: real_model.get_params == real_model2.get_params
25/137: real_model.get_params
25/138: q
25/139: cross_validate(q, df, df['b'], cv=5)
25/140: scores
25/141:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        #self.base_estimator=base_estimator
        for key, value in kwargs.items():
            setattr(self, key, value)

        self._param_names = ['est_class'] + list(kwargs.keys())
        print(['est_class'] + list(kwargs.keys()))
        super().__init__(base_estimator=LGBMRegressor())
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/142: q = CustomModel(cols = ['a'], n_estimators=3)
25/143: q.get_params()
25/144: q.fit(df, y)
25/145: y_my = q.predict(df)
25/146: real_model = LGBMRegressor()
25/147: real_model2 = LGBMRegressor()
25/148: real_model.get_params
25/149:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
25/150: real_model.get_params == real_model2.get_params
25/151: real_model.get_params
25/152: q
25/153: cross_validate(q, df, df['b'], cv=5)
25/154:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        #self.base_estimator=base_estimator
        for key, value in kwargs.items():
            setattr(self, key, value)

        self._param_names = ['est_class'] + list(kwargs.keys())
        print(['est_class'] + list(kwargs.keys()))
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/155: q = CustomModel(cols = ['a'], n_estimators=3)
25/156: q.get_params()
25/157: q.fit(df, y)
25/158: y_my = q.predict(df)
25/159: real_model = LGBMRegressor()
25/160: real_model2 = LGBMRegressor()
25/161: real_model.get_params
25/162:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
25/163: real_model.get_params == real_model2.get_params
25/164: real_model.get_params
25/165: q
25/166: cross_validate(q, df, df['b'], cv=5)
25/167:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        #self.base_estimator=base_estimator
        for key, value in kwargs.items():
            setattr(self, key, value)

        self._param_names = ['est_class'] + list(kwargs.keys())
        print(['est_class'] + list(kwargs.keys()))
        super().__init__(base_estimator=base_estimator(n_estimators=4))
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/168: q = CustomModel(cols = ['a'], n_estimators=3)
25/169:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        #self.base_estimator=base_estimator
        for key, value in kwargs.items():
            setattr(self, key, value)

        self._param_names = ['est_class'] + list(kwargs.keys())
        print(['est_class'] + list(kwargs.keys()))
        super().__init__(base_estimator=LGBMRegressor(n_estimators=4))
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/170: q = CustomModel(cols = ['a'], n_estimators=3)
25/171: q.get_params()
25/172: q.fit(df, y)
25/173: y_my = q.predict(df)
25/174: real_model = LGBMRegressor()
25/175: real_model2 = LGBMRegressor()
25/176: real_model.get_params
25/177:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
25/178: real_model.get_params == real_model2.get_params
25/179: real_model.get_params
25/180: q
25/181: cross_validate(q, df, df['b'], cv=5)
25/182:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor()
        for key, value in kwargs.items():
            setattr(self, key, value)

        self._param_names = ['est_class'] + list(kwargs.keys())
        print(['est_class'] + list(kwargs.keys()))
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/183: q = CustomModel(cols = ['a'], n_estimators=3)
25/184: q.get_params()
25/185: q.fit(df, y)
25/186: y_my = q.predict(df)
25/187: real_model = LGBMRegressor()
25/188: real_model2 = LGBMRegressor()
25/189: real_model.get_params
25/190:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
25/191: real_model.get_params == real_model2.get_params
25/192: real_model.get_params
25/193: q
25/194: cross_validate(q, df, df['b'], cv=5)
25/195:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(**kwargs)
        for key, value in kwargs.items():
            setattr(self, key, value)

        self._param_names = ['est_class'] + list(kwargs.keys())
        print(['est_class'] + list(kwargs.keys()))
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/196: q = CustomModel(cols = ['a'], n_estimators=3)
25/197: q.get_params()
25/198: q.fit(df, y)
25/199: y_my = q.predict(df)
25/200: real_model = LGBMRegressor()
25/201: real_model2 = LGBMRegressor()
25/202: real_model.get_params
25/203:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
25/204: real_model.get_params == real_model2.get_params
25/205: real_model.get_params
25/206: q
25/207: cross_validate(q, df, df['b'], cv=5)
25/208:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(n_estimators=3)
        for key, value in kwargs.items():
            setattr(self, key, value)

        self._param_names = ['est_class'] + list(kwargs.keys())
        print(['est_class'] + list(kwargs.keys()))
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/209: q = CustomModel(cols = ['a'], n_estimators=3)
25/210: q.get_params()
25/211: base_estimator=LGBMRegressor()
25/212: base_estimator
25/213: base_estimatorn_estimators=3)
25/214: base_estimator(n_estimators=3)
25/215:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(n_estimators=3)
        for key, value in kwargs.items():
            setattr(self, key, value)

        self._param_names = ['est_class'] + list(kwargs.keys())
        print(['est_class'] + list(kwargs.keys()))
        super().__init__(base_estimator=clone(base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/216:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(n_estimators=3)
        for key, value in kwargs.items():
            setattr(self, key, value)

        self._param_names = ['est_class'] + list(kwargs.keys())
        print(['est_class'] + list(kwargs.keys()))
        super().__init__(base_estimator=clone(base_estimator))
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/217: q = CustomModel(cols = ['a'], n_estimators=3)
25/218: q.get_params()
25/219: q.fit(df, y)
25/220:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(**kwargs)
        for key, value in kwargs.items():
            setattr(self, key, value)

        super().__init__(base_estimator=clone(base_estimator))
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/221: q = CustomModel(cols = ['a'], n_estimators=3)
25/222: q.get_params()
25/223:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(**kwargs)
        for key, value in kwargs.items():
            setattr(self, key, value)

        super().__init__(base_estimator=clone(LGBMRegressor(**kwargs)))
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/224: q = CustomModel(cols = ['a'], n_estimators=3)
25/225: q.get_params()
25/226:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator=LGBMRegressor(**kwargs)
        for key, value in kwargs.items():
            setattr(self, key, value)

        super().__init__(LGBMRegressor)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/227: q = CustomModel(cols = ['a'], n_estimators=3)
25/228: q.get_params()
25/229: q.fit(df, y)
25/230: y_my = q.predict(df)
25/231: real_model = LGBMRegressor()
25/232: real_model2 = LGBMRegressor()
25/233: real_model.get_params
25/234:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
25/235: real_model.get_params == real_model2.get_params
25/236: real_model.get_params
25/237: q
25/238: cross_validate(q, df, df['b'], cv=5)
25/239:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        estimator = LGBMRegressor(**kwargs)
        self.base_estimator=copy(estimator)
        for key, value in kwargs.items():
            setattr(self, key, value)

        super().__init__(LGBMRegressor)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/240: q = CustomModel(cols = ['a'], n_estimators=3)
25/241:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        estimator = LGBMRegressor(n_estimators=3)
        self.base_estimator=copy(estimator)
        for key, value in kwargs.items():
            setattr(self, key, value)

        super().__init__(LGBMRegressor)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/242: q = CustomModel(cols = ['a'], n_estimators=3)
25/243:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        estimator = LGBMRegressor(n_estimators=3)
        print(estimator)
        self.base_estimator=copy(estimator)
        for key, value in kwargs.items():
            setattr(self, key, value)

        super().__init__(LGBMRegressor)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/244: q = CustomModel(cols = ['a'], n_estimators=3)
25/245:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        estimator = LGBMRegressor(n_estimators=3)
        print(estimator)
        self.base_estimator = estimator
        for key, value in kwargs.items():
            setattr(self, key, value)

        super().__init__(LGBMRegressor)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/246: q = CustomModel(cols = ['a'], n_estimators=3)
25/247: q.get_params()
25/248: q.fit(df, y)
25/249: y_my = q.predict(df)
25/250: real_model = LGBMRegressor()
25/251: real_model2 = LGBMRegressor()
25/252: real_model.get_params
25/253:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
25/254: real_model.get_params == real_model2.get_params
25/255: real_model.get_params
25/256: q
25/257: cross_validate(q, df, df['b'], cv=5)
25/258: scores
25/259:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        estimator = LGBMRegressor(**kwargs)
        print(estimator)
        self.base_estimator = estimator
        for key, value in kwargs.items():
            setattr(self, key, value)

        super().__init__(LGBMRegressor)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/260: q = CustomModel(cols = ['a'], n_estimators=3)
25/261: q.get_params()
25/262: q.fit(df, y)
25/263: y_my = q.predict(df)
25/264: real_model = LGBMRegressor()
25/265: real_model2 = LGBMRegressor()
25/266: real_model.get_params
25/267:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
25/268: real_model.get_params == real_model2.get_params
25/269: real_model.get_params
25/270: q
25/271: cross_validate(q, df, df['b'], cv=5)
25/272:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        estimator = LGBMRegressor(**kwargs)
        print(estimator)
        self.base_estimator = estimator
        for key, value in kwargs.items():
            setattr(self, key, value)

        super().__init__(LGBMRegressor(**kwargs))
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/273: q = CustomModel(cols = ['a'], n_estimators=3)
25/274: q.get_params()
25/275: q.fit(df, y)
25/276: y_my = q.predict(df)
25/277: real_model = LGBMRegressor()
25/278: real_model2 = LGBMRegressor()
25/279: real_model.get_params
25/280:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
25/281: real_model.get_params == real_model2.get_params
25/282: real_model.get_params
25/283: q
25/284: cross_validate(q, df, df['b'], cv=5)
25/285:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        estimator = LGBMRegressor(**kwargs)
        print(estimator)
        self.base_estimator = estimator
        for key, value in kwargs.items():
            setattr(self, key, value)

        super().__init__(self.base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
25/286: q = CustomModel(cols = ['a'], n_estimators=3)
25/287: q.get_params()
25/288: q.fit(df, y)
25/289: y_my = q.predict(df)
25/290: real_model = LGBMRegressor()
25/291: real_model2 = LGBMRegressor()
25/292: real_model.get_params
25/293:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
25/294: real_model.get_params == real_model2.get_params
25/295: real_model.get_params
25/296: q
25/297: cross_validate(q, df, df['b'], cv=5)
25/298: cross_validate(CustomModel(cols = ['a'], n_estimators=3), df, df['b'], cv=5)
25/299:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        estimator = LGBMRegressor(**kwargs)
        print(estimator)
        self.base_estimator = estimator
        for key, value in kwargs.items():
            setattr(self, key, value)

        super().__init__(self.base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __enter__(self)
25/300: q = CustomModel(cols = ['a'], n_estimators=3)
25/301: q.get_params()
25/302:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        estimator = LGBMRegressor(**kwargs)
        print(estimator)
        self.base_estimator = estimator
        for key, value in kwargs.items():
            setattr(self, key, value)

        super().__init__(self.base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self):
        print("die")
25/303: q = CustomModel(cols = ['a'], n_estimators=3)
25/304: q.get_params()
25/305: q.fit(df, y)
25/306: y_my = q.predict(df)
25/307: real_model = LGBMRegressor()
25/308: real_model2 = LGBMRegressor()
25/309: real_model.get_params
25/310:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
25/311: real_model.get_params == real_model2.get_params
25/312: real_model.get_params
25/313: q
25/314: cross_validate(CustomModel(cols = ['a'], n_estimators=3), df, df['b'], cv=5)
25/315:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        estimator = LGBMRegressor(**kwargs)
        print(estimator)
        self.base_estimator = estimator
        for key, value in kwargs.items():
            setattr(self, key, value)

        super().__init__(copy(self.base_estimator))
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self):
        print("die")
25/316: q = CustomModel(cols = ['a'], n_estimators=3)
25/317:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        estimator = LGBMRegressor(**kwargs)
        print(estimator)
        self.base_estimator = estimator
        for key, value in kwargs.items():
            setattr(self, key, value)

        super().__init__(self.base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self):
        print("die")
25/318: q = CustomModel(cols = ['a'], n_estimators=3)
25/319:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        estimator = copy(LGBMRegressor(**kwargs))
        print(estimator)
        self.base_estimator = estimator
        for key, value in kwargs.items():
            setattr(self, key, value)

        super().__init__(self.base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self):
        print("die")
25/320: q = CustomModel(cols = ['a'], n_estimators=3)
25/321:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        estimator = clone(LGBMRegressor(**kwargs))
        print(estimator)
        self.base_estimator = estimator
        for key, value in kwargs.items():
            setattr(self, key, value)

        super().__init__(self.base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self):
        print("die")
25/322: q = CustomModel(cols = ['a'], n_estimators=3)
25/323:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        estimator = LGBMRegressor(**kwargs)
        print(estimator)
        self.base_estimator = clone(estimator)
        for key, value in kwargs.items():
            setattr(self, key, value)

        super().__init__(self.base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self):
        print("die")
25/324: q = CustomModel(cols = ['a'], n_estimators=3)
25/325: q.get_params()
25/326: q.fit(df, y)
25/327: y_my = q.predict(df)
25/328: real_model = LGBMRegressor()
25/329: real_model2 = LGBMRegressor()
25/330: real_model.get_params
25/331:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
25/332: real_model.get_params == real_model2.get_params
25/333: real_model.get_params
25/334: q
25/335: cross_validate(CustomModel(cols = ['a'], n_estimators=3), df, df['b'], cv=5)
26/1:
# –ú—ã —Ö–æ—Ç–∏–º –Ω–∞–ø–∏—Å–∞—Ç—å –æ–±–µ—Ä—Ç–∫—É –¥–ª—è LGBMRegressor 
# –≤ –∫–æ—Ç–æ—Ä—É—é –º—ã –±—ã —É–º–µ–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞, 
# –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏. 

# –û—Å–Ω–æ–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å - –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —É–º–µ—Ç—å –ø—Ä–∏—Ç–≤–æ—Ä—è—Ç—å—Å—è 
# —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º sklearn —ç—Å—Ç–∏–º–∞—Ç–æ—Ä–æ–º.
# –¢–æ –µ—Å—Ç—å —Ä–∞–±–æ–∞—Ç—å –±–µ–∑ –ø—Ä–æ–±–ª–µ–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, c cross_validate.

# –ü—Ä–æ–¥—É–º–∞–π –∫–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –∫–ª–∞—Å—Å–µ,
# –∫–∞–∫ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LGBMRegressor —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ —Ç–¥.

# hint:
# 1) –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç sklearn.base.clone 
# 2) –ú–æ–∂–µ—Ç –ª–∏ –ø–æ–º–æ—á—å –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤?


from lightgbm import LGBMRegressor
from lightgbm import LGBMClassifier
import copy
26/2: from sklearn.base import clone
26/3:
import numpy as np
import pandas as pd
26/4:
from sklearn import datasets
iris = datasets.load_iris()
26/5:
X = iris.data  # we only take the first two features.
y = iris.target
26/6: df = pd.DataFrame(X, columns = ['a','b','c','d'])
26/7: #df = pd.DataFrame(np.array([[1, 22, 3], [4, 5, 26], [7, 8, 9]]), columns=['a', 'b', 'c'])
26/8: #y=[0,1,0]
26/9:
#.fit()
#.predict()
#.score()
#.set_params()
#.get_params()
26/10: from sklearn.model_selection import cross_validate
26/11: LGBMRegressor(n_estimators = 4)
26/12: base_estimator=LGBMRegressor()
26/13: base_estimator(n_estimators=3)
26/14:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        estimator = LGBMRegressor(**kwargs)
        print(estimator)
        self.base_estimator = clone(estimator)
        for key, value in kwargs.items():
            setattr(self, key, value)

        super().__init__(self.base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self):
        print("die")
26/15: q = CustomModel(cols = ['a'], n_estimators=3)
26/16: q.get_params()
26/17: q.fit(df, y)
26/18: y_my = q.predict(df)
26/19: real_model = LGBMRegressor()
26/20: real_model2 = LGBMRegressor()
26/21: real_model.get_params
26/22:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
26/23: real_model.get_params == real_model2.get_params
26/24: real_model.get_params
26/25: q
26/26: cross_validate(CustomModel(cols = ['a'], n_estimators=3), df, df['b'], cv=5)
26/27: scores
26/28:
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
26/29:
from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier()
26/30:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        estimator = LGBMRegressor(**kwargs)
        print(estimator)
        self.base_estimator = clone(estimator)
        for key, value in kwargs.items():
            setattr(self, key, value)

        super().__init__(clone(self.base_estimator))
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self):
        print("die")
26/31: q = CustomModel(cols = ['a'], n_estimators=3)
26/32: q.get_params()
26/33: q.fit(df, y)
26/34: y_my = q.predict(df)
26/35: real_model = LGBMRegressor()
26/36: real_model2 = LGBMRegressor()
26/37: real_model.get_params
26/38:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
26/39: real_model.get_params == real_model2.get_params
26/40: real_model.get_params
26/41: q
26/42: cross_validate(CustomModel(cols = ['a'], n_estimators=3), df, df['b'], cv=5)
26/43:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        estimator = LGBMRegressor(**kwargs)
        print(estimator)
        self.base_estimator = clone(LGBMRegressor(**kwargs))
        for key, value in kwargs.items():
            setattr(self, key, value)

        super().__init__(self.base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self):
        print("die")
26/44: q = CustomModel(cols = ['a'], n_estimators=3)
26/45: q.get_params()
26/46: q.fit(df, y)
26/47: y_my = q.predict(df)
26/48: real_model = LGBMRegressor()
26/49: real_model2 = LGBMRegressor()
26/50: real_model.get_params
26/51:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
26/52: real_model.get_params == real_model2.get_params
26/53: real_model.get_params
26/54: q
26/55: cross_validate(CustomModel(cols = ['a'], n_estimators=3), df, df['b'], cv=5)
26/56:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator = clone(LGBMRegressor(**kwargs))
        super().__init__(self.base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self):
        print("die")
26/57: q = CustomModel(cols = ['a'], n_estimators=3)
26/58: q.get_params()
26/59: q.fit(df, y)
26/60: y_my = q.predict(df)
26/61: real_model = LGBMRegressor()
26/62: real_model2 = LGBMRegressor()
26/63: real_model.get_params
26/64:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
26/65: real_model.get_params == real_model2.get_params
26/66: real_model.get_params
26/67: q
26/68: cross_validate(CustomModel(cols = ['a'], n_estimators=3), df, df['b'], cv=5)
26/69:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator = clone(LGBMRegressor(**kwargs))
        super().__init__(base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self):
        print("die")
26/70: q = CustomModel(cols = ['a'], n_estimators=3)
26/71: q.get_params()
26/72: q.fit(df, y)
26/73: y_my = q.predict(df)
26/74: real_model = LGBMRegressor()
26/75: real_model2 = LGBMRegressor()
26/76: real_model.get_params
26/77:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
26/78: real_model.get_params == real_model2.get_params
26/79: real_model.get_params
26/80: q
26/81: cross_validate(CustomModel(cols = ['a'], n_estimators=3), df, df['b'], cv=5)
26/82: cross_validate(CustomModel(cols = ['a'], n_estimators=3), df, df['b'], cv=2)
26/83: cross_validate(CustomModel(cols = ['a'], n_estimators=3), df, df['b'], cv=1)
26/84:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator = clone(LGBMRegressor(**kwargs))
        super().__init__(base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self, base_estimator):
        print("die")
26/85: q = CustomModel(cols = ['a'], n_estimators=3)
26/86: q.get_params()
26/87: q = CustomModel(cols = ['a'], n_estimators=3)
26/88: q = CustomModel(cols = ['a'], n_estimators=3)
26/89:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator = clone(LGBMRegressor(**kwargs))
        super().__init__(base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self, base_estimator):
        print("die")
26/90: q = CustomModel(cols = ['a'], n_estimators=3)
26/91: q.get_params()
26/92: q.fit(df, y)
26/93: y_my = q.predict(df)
26/94: real_model = LGBMRegressor()
26/95: real_model2 = LGBMRegressor()
26/96: real_model.get_params
26/97:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
26/98: real_model.get_params == real_model2.get_params
26/99: real_model.get_params
26/100: q
26/101: cross_validate(CustomModel(cols = ['a'], n_estimators=3), df, df['b'], cv=1)
26/102: cross_validate(CustomModel(cols = ['a'], n_estimators=3), df, df['b'], cv=5)
26/103: cross_validate(CustomModel(cols = ['a'], n_estimators=3), df, df['b'], cv=5)
26/104:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, **kwargs):
        print('You create me')
        self.cols = cols
        #self.base_estimator = clone(LGBMRegressor(**kwargs))
        super().__init__(self,cols, **kwargs)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self, base_estimator):
        print("die")
26/105: q = CustomModel(cols = ['a'], n_estimators=3)
26/106: q.get_params()
26/107:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, **kwargs):
        print('You create me')
        self.cols = cols
        #self.base_estimator = clone(LGBMRegressor(**kwargs))
        super().__init__(cols, **kwargs)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self, base_estimator):
        print("die")
26/108: q = CustomModel(cols = ['a'], n_estimators=3)
26/109: q.get_params()
26/110:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, **kwargs):
        print('You create me')
        self.cols = cols
        #self.base_estimator = clone(LGBMRegressor(**kwargs))
        self.set_params() = **kwargs
        super().__init__(cols, **kwargs)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self, base_estimator):
        print("die")
26/111:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, **kwargs):
        print('You create me')
        self.cols = cols
        #self.base_estimator = clone(LGBMRegressor(**kwargs))
        self.set_params(**kwargs)
        super().__init__(cols, **kwargs)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self, base_estimator):
        print("die")
26/112: q = CustomModel(cols = ['a'], n_estimators=3)
26/113:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, **kwargs):
        print('You create me')
        self.cols = cols
        #self.base_estimator = clone(LGBMRegressor(**kwargs))
        self.set_params(**kwargs)
        super().__init__(**kwargs)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self, base_estimator):
        print("die")
26/114: q = CustomModel(cols = ['a'], n_estimators=3)
26/115:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, **kwargs):
        print('You create me')
        self.cols = cols
        #self.base_estimator = clone(LGBMRegressor(**kwargs))
        self.set_params(clone(LGBMRegressor(**kwargs)).get_params())
        super().__init__(cols, **kwargs)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self, base_estimator):
        print("die")
26/116: q = CustomModel(cols = ['a'], n_estimators=3)
26/117:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, **kwargs):
        print('You create me')
        self.cols = cols
        #self.base_estimator = clone(LGBMRegressor(**kwargs))
        print(LGBMRegressor(**kwargs)).get_params())
        self.set_params(clone(LGBMRegressor(**kwargs)).get_params())
        super().__init__(cols, **kwargs)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self, base_estimator):
        print("die")
26/118: q = CustomModel(cols = ['a'], n_estimators=3)
26/119:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, **kwargs):
        print('You create me')
        self.cols = cols
        #self.base_estimator = clone(LGBMRegressor(**kwargs))
        print(LGBMRegressor(**kwargs)).get_params())
        self.set_params(clone(LGBMRegressor(**kwargs)).get_params())
        super().__init__(cols, **kwargs)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self, base_estimator):
        print("die")
26/120:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, **kwargs):
        print('You create me')
        self.cols = cols
        #self.base_estimator = clone(LGBMRegressor(**kwargs))
        print(LGBMRegressor(**kwargs)).get_params()
        self.set_params(clone(LGBMRegressor(**kwargs)).get_params())
        super().__init__(cols, **kwargs)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self, base_estimator):
        print("die")
26/121: q = CustomModel(cols = ['a'], n_estimators=3)
26/122:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, **kwargs):
        print('You create me')
        self.cols = cols
        #self.base_estimator = clone(LGBMRegressor(**kwargs))
        print(LGBMRegressor(**kwargs)).get_params()
        self.set_params(LGBMRegressor(**kwargs).get_params())
        super().__init__(cols, **kwargs)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self, base_estimator):
        print("die")
26/123: q = CustomModel(cols = ['a'], n_estimators=3)
26/124:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, **kwargs):
        print('You create me')
        self.cols = cols
        #self.base_estimator = clone(LGBMRegressor(**kwargs))
        print(LGBMRegressor(**kwargs).get_params()
        self.set_params(LGBMRegressor(**kwargs).get_params())
        super().__init__(cols, **kwargs)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self, base_estimator):
        print("die")
26/125: q = CustomModel(cols = ['a'], n_estimators=3)
26/126:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, **kwargs):
        print('You create me')
        self.cols = cols
        #self.base_estimator = clone(LGBMRegressor(**kwargs))
        print(LGBMRegressor(**kwargs).get_params())
        self.set_params(LGBMRegressor(**kwargs).get_params())
        super().__init__(cols, **kwargs)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self, base_estimator):
        print("die")
26/127: q = CustomModel(cols = ['a'], n_estimators=3)
26/128:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, **kwargs):
        print('You create me')
        self.cols = cols
        #self.base_estimator = clone(LGBMRegressor(**kwargs))
        print(LGBMRegressor(**kwargs).get_params())
        self = LGBMRegressor(**kwargs)
        super().__init__(cols, **kwargs)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self, base_estimator):
        print("die")
26/129: q = CustomModel(cols = ['a'], n_estimators=3)
26/130:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, **kwargs):
        print('You create me')
        self.cols = cols
        #self.base_estimator = clone(LGBMRegressor(**kwargs))
        print(LGBMRegressor(**kwargs).get_params())
        self = LGBMRegressor(**kwargs)
        super().__init__(self)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self, base_estimator):
        print("die")
26/131: q = CustomModel(cols = ['a'], n_estimators=3)
26/132:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, **kwargs):
        print('You create me')
        self.cols = cols
        #self.base_estimator = clone(LGBMRegressor(**kwargs))
        print(LGBMRegressor(**kwargs).get_params())
        self.params = LGBMRegressor(**kwargs)
        super().__init__(cols, **kwargs)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self, base_estimator):
        print("die")
26/133: q = CustomModel(cols = ['a'], n_estimators=3)
26/134: q.get_params()
26/135: q
26/136: q
26/137: q = CustomModel(cols = ['a'], n_estimators=3)
26/138: q.
26/139: q.get_params()
26/140: q.fit(df, y)
26/141:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, **kwargs):
        print('You create me')
        self.cols = cols
        #self.base_estimator = clone(LGBMRegressor(**kwargs))
        print(LGBMRegressor(**kwargs).get_params())
        self.params = LGBMRegressor(**kwargs)
        super().__init__(cols, **kwargs)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.base_estimator)

        self.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self, base_estimator):
        print("die")
26/142: q = CustomModel(cols = ['a'], n_estimators=3)
26/143: q.get_params()
26/144: q.fit(df, y)
26/145:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, **kwargs):
        print('You create me')
        self.cols = cols
        #self.base_estimator = clone(LGBMRegressor(**kwargs))
        print(LGBMRegressor(**kwargs).get_params())
        self.params = LGBMRegressor(**kwargs)
        super().__init__(cols, **kwargs)
        
    def fit(self, X, y):
        #model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.base_estimator)

        self.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self, base_estimator):
        print("die")
26/146: q = CustomModel(cols = ['a'], n_estimators=3)
26/147: q.get_params()
26/148: q.fit(df, y)
26/149:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, **kwargs):
        print('You create me')
        self.cols = cols
        #self.base_estimator = clone(LGBMRegressor(**kwargs))
        self.params = LGBMRegressor(**kwargs)
        super().__init__()
        
    def fit(self, X, y):
        #model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        #print(self.base_estimator)

        self.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self, base_estimator):
        print("die")
26/150: q = CustomModel(cols = ['a'], n_estimators=3)
26/151: q.get_params()
26/152: q.fit(df, y)
26/153: q
26/154:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, **kwargs):
        print('You create me')
        self.cols = cols
        #self.base_estimator = clone(LGBMRegressor(**kwargs))
        
        self.set_params(clone(LGBMRegressor(**kwargs)).get_params())
        super().__init__(cols, **kwargs)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self, base_estimator):
        print("die")
26/155: q = CustomModel(cols = ['a'], n_estimators=3)
26/156:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, **kwargs):
        print('You create me')
        self.cols = cols
        #self.base_estimator = clone(LGBMRegressor(**kwargs))
        self.set_params(clone(LGBMRegressor(**kwargs)))
        super().__init__(cols, **kwargs)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self, base_estimator):
        print("die")
26/157: q = CustomModel(cols = ['a'], n_estimators=3)
26/158:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator = clone(LGBMRegressor(**kwargs))
        super().__init__(base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self):
        print("die")
26/159: q = CustomModel(cols = ['a'], n_estimators=3)
26/160: q = CustomModel(cols = ['a'], n_estimators=3)
26/161: q.get_params()
26/162:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        self.base_estimator = clone(LGBMRegressor(**kwargs))
        super().__init__()
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self):
        print("die")
26/163: q = CustomModel(cols = ['a'], n_estimators=3)
26/164: q.get_params()
26/165: q
26/166:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols, base_estimator=LGBMRegressor(), **kwargs):
        print('You create me')
        self.cols = cols
        #self.base_estimator = clone(LGBMRegressor(**kwargs))
        super().__init__(base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self):
        print("die")
26/167: q = CustomModel(cols = ['a'], n_estimators=3)
26/168: q.get_params()
26/169: q
26/170:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols,**kwargs):
        print('You create me')
        self.cols = cols
        base_estimator = LGBMRegressor()
        super().__init__(base_estimator=base_estimator)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self):
        print("die")
26/171: q = CustomModel(cols = ['a'], n_estimators=3)
26/172: q.get_params()
26/173:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols,**kwargs):
        print('You create me')
        self.cols = cols
        #base_estimator = LGBMRegressor()
        super().__init__(cols,**kwargs)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self):
        print("die")
26/174: q = CustomModel(cols = ['a'], n_estimators=3)
26/175: q.get_params()
26/176:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):

        self.est_class = est_class
        self.cols = cols
        for key, value in kwargs.items():
            setattr(self, key, value)

        self._param_names = ['est_class'] + list(kwargs.keys())

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)

        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        check_is_fitted(self)

        # we use the fitted model and log if logging is enabled
        y_pred = self.model_.predict(X)

        if self._logging_enabled:
            self._logger.info('Logging predicted values: %s', y_pred)

        return y_pred

    # requiring a score method is not documented but throws an
    # error if not implemented
    def score(self, X, y, **kwargs):
        return self.model_.score(X, y, **kwargs)

    # some models implement custom methods. Anything that is not implemented here
    # will be delegated to the underlying model. There is one condition we have
    # to cover: if the underlying estimator has class attributes they won't
    # be accessible until we fit the model (since we instantiate the model there)
    # to fix it, we try to look it up attributes in the instance, if there
    # is no instance, we look up the class. More info here:
    # https://scikit-learn.org/stable/developers/develop.html#estimator-types
    def __getattr__(self, key):
        if key != 'model_':
            if hasattr(self, 'model_'):
                return getattr(self.model_, key)
            else:
                return getattr(self.est_class, key)
        else:
            raise AttributeError(
                "'{}' object has no attribute 'model_'".format(type(self).__name__))

    # these two control logging

    def enable_logging(self):
        self._logging_enabled = True

    def disable_logging(self):
        self._logging_enabled = False

    # ignore the following two for now, more info in the Appendix

    def __getstate__(self):
        state = self.__dict__.copy()
        del state['_logger']
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._logger = logging.getLogger(__name__)
26/177: from sklearn.base import BaseEstimator
26/178:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):

        self.est_class = est_class
        self.cols = cols
        for key, value in kwargs.items():
            setattr(self, key, value)

        self._param_names = ['est_class'] + list(kwargs.keys())

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)

        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        check_is_fitted(self)

        # we use the fitted model and log if logging is enabled
        y_pred = self.model_.predict(X)

        if self._logging_enabled:
            self._logger.info('Logging predicted values: %s', y_pred)

        return y_pred

    # requiring a score method is not documented but throws an
    # error if not implemented
    def score(self, X, y, **kwargs):
        return self.model_.score(X, y, **kwargs)

    # some models implement custom methods. Anything that is not implemented here
    # will be delegated to the underlying model. There is one condition we have
    # to cover: if the underlying estimator has class attributes they won't
    # be accessible until we fit the model (since we instantiate the model there)
    # to fix it, we try to look it up attributes in the instance, if there
    # is no instance, we look up the class. More info here:
    # https://scikit-learn.org/stable/developers/develop.html#estimator-types
    def __getattr__(self, key):
        if key != 'model_':
            if hasattr(self, 'model_'):
                return getattr(self.model_, key)
            else:
                return getattr(self.est_class, key)
        else:
            raise AttributeError(
                "'{}' object has no attribute 'model_'".format(type(self).__name__))

    # these two control logging

    def enable_logging(self):
        self._logging_enabled = True

    def disable_logging(self):
        self._logging_enabled = False

    # ignore the following two for now, more info in the Appendix

    def __getstate__(self):
        state = self.__dict__.copy()
        del state['_logger']
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._logger = logging.getLogger(__name__)
26/179: q = CustomModel(cols = ['a'], n_estimators=3)
26/180: q.get_params()
26/181:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):

        self.est_class = est_class
        self.cols = cols
        for key, value in kwargs.items():
            setattr(self, key, value)

        self._param_names = ['est_class'] + list(kwargs.keys())
        print(self._param_names)

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)

        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        check_is_fitted(self)

        # we use the fitted model and log if logging is enabled
        y_pred = self.model_.predict(X)

        if self._logging_enabled:
            self._logger.info('Logging predicted values: %s', y_pred)

        return y_pred

    # requiring a score method is not documented but throws an
    # error if not implemented
    def score(self, X, y, **kwargs):
        return self.model_.score(X, y, **kwargs)

    # some models implement custom methods. Anything that is not implemented here
    # will be delegated to the underlying model. There is one condition we have
    # to cover: if the underlying estimator has class attributes they won't
    # be accessible until we fit the model (since we instantiate the model there)
    # to fix it, we try to look it up attributes in the instance, if there
    # is no instance, we look up the class. More info here:
    # https://scikit-learn.org/stable/developers/develop.html#estimator-types
    def __getattr__(self, key):
        if key != 'model_':
            if hasattr(self, 'model_'):
                return getattr(self.model_, key)
            else:
                return getattr(self.est_class, key)
        else:
            raise AttributeError(
                "'{}' object has no attribute 'model_'".format(type(self).__name__))

    # these two control logging

    def enable_logging(self):
        self._logging_enabled = True

    def disable_logging(self):
        self._logging_enabled = False

    # ignore the following two for now, more info in the Appendix

    def __getstate__(self):
        state = self.__dict__.copy()
        del state['_logger']
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._logger = logging.getLogger(__name__)
26/182: q = CustomModel(cols = ['a'], n_estimators=3)
26/183:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):

        self.est_class = est_class
        self.cols = cols
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, √ßols, 3)

        self._param_names = ['est_class'] + list(kwargs.keys())
        print(self._param_names)

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)

        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        check_is_fitted(self)

        # we use the fitted model and log if logging is enabled
        y_pred = self.model_.predict(X)

        if self._logging_enabled:
            self._logger.info('Logging predicted values: %s', y_pred)

        return y_pred

    # requiring a score method is not documented but throws an
    # error if not implemented
    def score(self, X, y, **kwargs):
        return self.model_.score(X, y, **kwargs)

    # some models implement custom methods. Anything that is not implemented here
    # will be delegated to the underlying model. There is one condition we have
    # to cover: if the underlying estimator has class attributes they won't
    # be accessible until we fit the model (since we instantiate the model there)
    # to fix it, we try to look it up attributes in the instance, if there
    # is no instance, we look up the class. More info here:
    # https://scikit-learn.org/stable/developers/develop.html#estimator-types
    def __getattr__(self, key):
        if key != 'model_':
            if hasattr(self, 'model_'):
                return getattr(self.model_, key)
            else:
                return getattr(self.est_class, key)
        else:
            raise AttributeError(
                "'{}' object has no attribute 'model_'".format(type(self).__name__))

    # these two control logging

    def enable_logging(self):
        self._logging_enabled = True

    def disable_logging(self):
        self._logging_enabled = False

    # ignore the following two for now, more info in the Appendix

    def __getstate__(self):
        state = self.__dict__.copy()
        del state['_logger']
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._logger = logging.getLogger(__name__)
26/184: q = CustomModel(cols = ['a'], n_estimators=3)
26/185:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):

        self.est_class = est_class
        self.cols = cols
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'col', 3)

        self._param_names = ['est_class'] + list(kwargs.keys())
        print(self._param_names)

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)

        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        check_is_fitted(self)

        # we use the fitted model and log if logging is enabled
        y_pred = self.model_.predict(X)

        if self._logging_enabled:
            self._logger.info('Logging predicted values: %s', y_pred)

        return y_pred

    # requiring a score method is not documented but throws an
    # error if not implemented
    def score(self, X, y, **kwargs):
        return self.model_.score(X, y, **kwargs)

    # some models implement custom methods. Anything that is not implemented here
    # will be delegated to the underlying model. There is one condition we have
    # to cover: if the underlying estimator has class attributes they won't
    # be accessible until we fit the model (since we instantiate the model there)
    # to fix it, we try to look it up attributes in the instance, if there
    # is no instance, we look up the class. More info here:
    # https://scikit-learn.org/stable/developers/develop.html#estimator-types
    def __getattr__(self, key):
        if key != 'model_':
            if hasattr(self, 'model_'):
                return getattr(self.model_, key)
            else:
                return getattr(self.est_class, key)
        else:
            raise AttributeError(
                "'{}' object has no attribute 'model_'".format(type(self).__name__))

    # these two control logging

    def enable_logging(self):
        self._logging_enabled = True

    def disable_logging(self):
        self._logging_enabled = False

    # ignore the following two for now, more info in the Appendix

    def __getstate__(self):
        state = self.__dict__.copy()
        del state['_logger']
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._logger = logging.getLogger(__name__)
26/186: q = CustomModel(cols = ['a'], n_estimators=3)
26/187:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):

        self.est_class = est_class
        self.cols = cols
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'col', 3)

        self._param_names = ['est_class'] + list(self.keys())
        print(self._param_names)

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)

        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        check_is_fitted(self)

        # we use the fitted model and log if logging is enabled
        y_pred = self.model_.predict(X)

        if self._logging_enabled:
            self._logger.info('Logging predicted values: %s', y_pred)

        return y_pred

    # requiring a score method is not documented but throws an
    # error if not implemented
    def score(self, X, y, **kwargs):
        return self.model_.score(X, y, **kwargs)

    # some models implement custom methods. Anything that is not implemented here
    # will be delegated to the underlying model. There is one condition we have
    # to cover: if the underlying estimator has class attributes they won't
    # be accessible until we fit the model (since we instantiate the model there)
    # to fix it, we try to look it up attributes in the instance, if there
    # is no instance, we look up the class. More info here:
    # https://scikit-learn.org/stable/developers/develop.html#estimator-types
    def __getattr__(self, key):
        if key != 'model_':
            if hasattr(self, 'model_'):
                return getattr(self.model_, key)
            else:
                return getattr(self.est_class, key)
        else:
            raise AttributeError(
                "'{}' object has no attribute 'model_'".format(type(self).__name__))

    # these two control logging

    def enable_logging(self):
        self._logging_enabled = True

    def disable_logging(self):
        self._logging_enabled = False

    # ignore the following two for now, more info in the Appendix

    def __getstate__(self):
        state = self.__dict__.copy()
        del state['_logger']
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._logger = logging.getLogger(__name__)
26/188: q = CustomModel(cols = ['a'], n_estimators=3)
26/189:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):

        self.est_class = est_class
        self.cols = cols
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'col', 3)

        self._param_names = ['est_class'] + list(self.key)
        print(self._param_names)

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)

        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        check_is_fitted(self)

        # we use the fitted model and log if logging is enabled
        y_pred = self.model_.predict(X)

        if self._logging_enabled:
            self._logger.info('Logging predicted values: %s', y_pred)

        return y_pred

    # requiring a score method is not documented but throws an
    # error if not implemented
    def score(self, X, y, **kwargs):
        return self.model_.score(X, y, **kwargs)

    # some models implement custom methods. Anything that is not implemented here
    # will be delegated to the underlying model. There is one condition we have
    # to cover: if the underlying estimator has class attributes they won't
    # be accessible until we fit the model (since we instantiate the model there)
    # to fix it, we try to look it up attributes in the instance, if there
    # is no instance, we look up the class. More info here:
    # https://scikit-learn.org/stable/developers/develop.html#estimator-types
    def __getattr__(self, key):
        if key != 'model_':
            if hasattr(self, 'model_'):
                return getattr(self.model_, key)
            else:
                return getattr(self.est_class, key)
        else:
            raise AttributeError(
                "'{}' object has no attribute 'model_'".format(type(self).__name__))

    # these two control logging

    def enable_logging(self):
        self._logging_enabled = True

    def disable_logging(self):
        self._logging_enabled = False

    # ignore the following two for now, more info in the Appendix

    def __getstate__(self):
        state = self.__dict__.copy()
        del state['_logger']
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._logger = logging.getLogger(__name__)
26/190: q = CustomModel(cols = ['a'], n_estimators=3)
26/191:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):

        self.est_class = est_class
        self.cols = cols
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'col', 3)

        self._param_names = ['est_class'] + list(kwargs.keys()) + 'col'
        print(self._param_names)

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)

        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        check_is_fitted(self)

        # we use the fitted model and log if logging is enabled
        y_pred = self.model_.predict(X)

        if self._logging_enabled:
            self._logger.info('Logging predicted values: %s', y_pred)

        return y_pred

    # requiring a score method is not documented but throws an
    # error if not implemented
    def score(self, X, y, **kwargs):
        return self.model_.score(X, y, **kwargs)

    # some models implement custom methods. Anything that is not implemented here
    # will be delegated to the underlying model. There is one condition we have
    # to cover: if the underlying estimator has class attributes they won't
    # be accessible until we fit the model (since we instantiate the model there)
    # to fix it, we try to look it up attributes in the instance, if there
    # is no instance, we look up the class. More info here:
    # https://scikit-learn.org/stable/developers/develop.html#estimator-types
    def __getattr__(self, key):
        if key != 'model_':
            if hasattr(self, 'model_'):
                return getattr(self.model_, key)
            else:
                return getattr(self.est_class, key)
        else:
            raise AttributeError(
                "'{}' object has no attribute 'model_'".format(type(self).__name__))

    # these two control logging

    def enable_logging(self):
        self._logging_enabled = True

    def disable_logging(self):
        self._logging_enabled = False

    # ignore the following two for now, more info in the Appendix

    def __getstate__(self):
        state = self.__dict__.copy()
        del state['_logger']
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._logger = logging.getLogger(__name__)
26/192: q = CustomModel(cols = ['a'], n_estimators=3)
26/193:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):

        self.est_class = est_class
        self.cols = cols
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'col', 3)

        print(['est_class'] + list(kwargs.keys()))
        self._param_names = ['est_class'] + list(kwargs.keys())
        print(self._param_names)

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)

        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        check_is_fitted(self)

        # we use the fitted model and log if logging is enabled
        y_pred = self.model_.predict(X)

        if self._logging_enabled:
            self._logger.info('Logging predicted values: %s', y_pred)

        return y_pred

    # requiring a score method is not documented but throws an
    # error if not implemented
    def score(self, X, y, **kwargs):
        return self.model_.score(X, y, **kwargs)

    # some models implement custom methods. Anything that is not implemented here
    # will be delegated to the underlying model. There is one condition we have
    # to cover: if the underlying estimator has class attributes they won't
    # be accessible until we fit the model (since we instantiate the model there)
    # to fix it, we try to look it up attributes in the instance, if there
    # is no instance, we look up the class. More info here:
    # https://scikit-learn.org/stable/developers/develop.html#estimator-types
    def __getattr__(self, key):
        if key != 'model_':
            if hasattr(self, 'model_'):
                return getattr(self.model_, key)
            else:
                return getattr(self.est_class, key)
        else:
            raise AttributeError(
                "'{}' object has no attribute 'model_'".format(type(self).__name__))

    # these two control logging

    def enable_logging(self):
        self._logging_enabled = True

    def disable_logging(self):
        self._logging_enabled = False

    # ignore the following two for now, more info in the Appendix

    def __getstate__(self):
        state = self.__dict__.copy()
        del state['_logger']
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._logger = logging.getLogger(__name__)
26/194: q = CustomModel(cols = ['a'], n_estimators=3)
26/195:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):

        self.est_class = est_class
        self.cols = cols
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append(['s', '33'])

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)

        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        check_is_fitted(self)

        # we use the fitted model and log if logging is enabled
        y_pred = self.model_.predict(X)

        if self._logging_enabled:
            self._logger.info('Logging predicted values: %s', y_pred)

        return y_pred

    # requiring a score method is not documented but throws an
    # error if not implemented
    def score(self, X, y, **kwargs):
        return self.model_.score(X, y, **kwargs)

    # some models implement custom methods. Anything that is not implemented here
    # will be delegated to the underlying model. There is one condition we have
    # to cover: if the underlying estimator has class attributes they won't
    # be accessible until we fit the model (since we instantiate the model there)
    # to fix it, we try to look it up attributes in the instance, if there
    # is no instance, we look up the class. More info here:
    # https://scikit-learn.org/stable/developers/develop.html#estimator-types
    def __getattr__(self, key):
        if key != 'model_':
            if hasattr(self, 'model_'):
                return getattr(self.model_, key)
            else:
                return getattr(self.est_class, key)
        else:
            raise AttributeError(
                "'{}' object has no attribute 'model_'".format(type(self).__name__))

    # these two control logging

    def enable_logging(self):
        self._logging_enabled = True

    def disable_logging(self):
        self._logging_enabled = False

    # ignore the following two for now, more info in the Appendix

    def __getstate__(self):
        state = self.__dict__.copy()
        del state['_logger']
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._logger = logging.getLogger(__name__)
26/196: q = CustomModel(cols = ['a'], n_estimators=3)
26/197: q.get_params()
26/198:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):

        self.est_class = est_class
        self.cols = cols
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append(['est_class', '3'])

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)

        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        check_is_fitted(self)

        # we use the fitted model and log if logging is enabled
        y_pred = self.model_.predict(X)

        if self._logging_enabled:
            self._logger.info('Logging predicted values: %s', y_pred)

        return y_pred

    # requiring a score method is not documented but throws an
    # error if not implemented
    def score(self, X, y, **kwargs):
        return self.model_.score(X, y, **kwargs)

    # some models implement custom methods. Anything that is not implemented here
    # will be delegated to the underlying model. There is one condition we have
    # to cover: if the underlying estimator has class attributes they won't
    # be accessible until we fit the model (since we instantiate the model there)
    # to fix it, we try to look it up attributes in the instance, if there
    # is no instance, we look up the class. More info here:
    # https://scikit-learn.org/stable/developers/develop.html#estimator-types
    def __getattr__(self, key):
        if key != 'model_':
            if hasattr(self, 'model_'):
                return getattr(self.model_, key)
            else:
                return getattr(self.est_class, key)
        else:
            raise AttributeError(
                "'{}' object has no attribute 'model_'".format(type(self).__name__))

    # these two control logging

    def enable_logging(self):
        self._logging_enabled = True

    def disable_logging(self):
        self._logging_enabled = False

    # ignore the following two for now, more info in the Appendix

    def __getstate__(self):
        state = self.__dict__.copy()
        del state['_logger']
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._logger = logging.getLogger(__name__)
26/199: q = CustomModel(cols = ['a'], n_estimators=3)
26/200: q.get_params()
26/201:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):

        self.est_class = est_class
        self.cols = cols
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append(['est_class', '3'])
        print(self._param_names)

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)

        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        check_is_fitted(self)

        # we use the fitted model and log if logging is enabled
        y_pred = self.model_.predict(X)

        if self._logging_enabled:
            self._logger.info('Logging predicted values: %s', y_pred)

        return y_pred

    # requiring a score method is not documented but throws an
    # error if not implemented
    def score(self, X, y, **kwargs):
        return self.model_.score(X, y, **kwargs)

    # some models implement custom methods. Anything that is not implemented here
    # will be delegated to the underlying model. There is one condition we have
    # to cover: if the underlying estimator has class attributes they won't
    # be accessible until we fit the model (since we instantiate the model there)
    # to fix it, we try to look it up attributes in the instance, if there
    # is no instance, we look up the class. More info here:
    # https://scikit-learn.org/stable/developers/develop.html#estimator-types
    def __getattr__(self, key):
        if key != 'model_':
            if hasattr(self, 'model_'):
                return getattr(self.model_, key)
            else:
                return getattr(self.est_class, key)
        else:
            raise AttributeError(
                "'{}' object has no attribute 'model_'".format(type(self).__name__))

    # these two control logging

    def enable_logging(self):
        self._logging_enabled = True

    def disable_logging(self):
        self._logging_enabled = False

    # ignore the following two for now, more info in the Appendix

    def __getstate__(self):
        state = self.__dict__.copy()
        del state['_logger']
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._logger = logging.getLogger(__name__)
26/202: q = CustomModel(cols = ['a'], n_estimators=3)
26/203:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):

        self.est_class = est_class
        self.cols = cols
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('est_class', '3')
        print(self._param_names)

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)

        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        check_is_fitted(self)

        # we use the fitted model and log if logging is enabled
        y_pred = self.model_.predict(X)

        if self._logging_enabled:
            self._logger.info('Logging predicted values: %s', y_pred)

        return y_pred

    # requiring a score method is not documented but throws an
    # error if not implemented
    def score(self, X, y, **kwargs):
        return self.model_.score(X, y, **kwargs)

    # some models implement custom methods. Anything that is not implemented here
    # will be delegated to the underlying model. There is one condition we have
    # to cover: if the underlying estimator has class attributes they won't
    # be accessible until we fit the model (since we instantiate the model there)
    # to fix it, we try to look it up attributes in the instance, if there
    # is no instance, we look up the class. More info here:
    # https://scikit-learn.org/stable/developers/develop.html#estimator-types
    def __getattr__(self, key):
        if key != 'model_':
            if hasattr(self, 'model_'):
                return getattr(self.model_, key)
            else:
                return getattr(self.est_class, key)
        else:
            raise AttributeError(
                "'{}' object has no attribute 'model_'".format(type(self).__name__))

    # these two control logging

    def enable_logging(self):
        self._logging_enabled = True

    def disable_logging(self):
        self._logging_enabled = False

    # ignore the following two for now, more info in the Appendix

    def __getstate__(self):
        state = self.__dict__.copy()
        del state['_logger']
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._logger = logging.getLogger(__name__)
26/204: q = CustomModel(cols = ['a'], n_estimators=3)
26/205:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):

        self.est_class = est_class
        self.cols = cols
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append(['est_class', '3'])
        print(self._param_names)

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)

        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        check_is_fitted(self)

        # we use the fitted model and log if logging is enabled
        y_pred = self.model_.predict(X)

        if self._logging_enabled:
            self._logger.info('Logging predicted values: %s', y_pred)

        return y_pred

    # requiring a score method is not documented but throws an
    # error if not implemented
    def score(self, X, y, **kwargs):
        return self.model_.score(X, y, **kwargs)

    # some models implement custom methods. Anything that is not implemented here
    # will be delegated to the underlying model. There is one condition we have
    # to cover: if the underlying estimator has class attributes they won't
    # be accessible until we fit the model (since we instantiate the model there)
    # to fix it, we try to look it up attributes in the instance, if there
    # is no instance, we look up the class. More info here:
    # https://scikit-learn.org/stable/developers/develop.html#estimator-types
    def __getattr__(self, key):
        if key != 'model_':
            if hasattr(self, 'model_'):
                return getattr(self.model_, key)
            else:
                return getattr(self.est_class, key)
        else:
            raise AttributeError(
                "'{}' object has no attribute 'model_'".format(type(self).__name__))

    # these two control logging

    def enable_logging(self):
        self._logging_enabled = True

    def disable_logging(self):
        self._logging_enabled = False

    # ignore the following two for now, more info in the Appendix

    def __getstate__(self):
        state = self.__dict__.copy()
        del state['_logger']
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._logger = logging.getLogger(__name__)
26/206: q = CustomModel(cols = ['a'], n_estimators=3)
26/207: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/208: q.get_params()
26/209:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):

        self.est_class = est_class
        self.cols = cols
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('columns')
        print(self._param_names)

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)

        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        check_is_fitted(self)

        # we use the fitted model and log if logging is enabled
        y_pred = self.model_.predict(X)

        if self._logging_enabled:
            self._logger.info('Logging predicted values: %s', y_pred)

        return y_pred

    # requiring a score method is not documented but throws an
    # error if not implemented
    def score(self, X, y, **kwargs):
        return self.model_.score(X, y, **kwargs)

    # some models implement custom methods. Anything that is not implemented here
    # will be delegated to the underlying model. There is one condition we have
    # to cover: if the underlying estimator has class attributes they won't
    # be accessible until we fit the model (since we instantiate the model there)
    # to fix it, we try to look it up attributes in the instance, if there
    # is no instance, we look up the class. More info here:
    # https://scikit-learn.org/stable/developers/develop.html#estimator-types
    def __getattr__(self, key):
        if key != 'model_':
            if hasattr(self, 'model_'):
                return getattr(self.model_, key)
            else:
                return getattr(self.est_class, key)
        else:
            raise AttributeError(
                "'{}' object has no attribute 'model_'".format(type(self).__name__))

    # these two control logging

    def enable_logging(self):
        self._logging_enabled = True

    def disable_logging(self):
        self._logging_enabled = False

    # ignore the following two for now, more info in the Appendix

    def __getstate__(self):
        state = self.__dict__.copy()
        del state['_logger']
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._logger = logging.getLogger(__name__)
26/210: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/211: q.get_params()
26/212:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):

        self.est_class = est_class
        self.cols = cols
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('col')
        print(self._param_names)

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)

        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        check_is_fitted(self)

        # we use the fitted model and log if logging is enabled
        y_pred = self.model_.predict(X)

        if self._logging_enabled:
            self._logger.info('Logging predicted values: %s', y_pred)

        return y_pred

    # requiring a score method is not documented but throws an
    # error if not implemented
    def score(self, X, y, **kwargs):
        return self.model_.score(X, y, **kwargs)

    # some models implement custom methods. Anything that is not implemented here
    # will be delegated to the underlying model. There is one condition we have
    # to cover: if the underlying estimator has class attributes they won't
    # be accessible until we fit the model (since we instantiate the model there)
    # to fix it, we try to look it up attributes in the instance, if there
    # is no instance, we look up the class. More info here:
    # https://scikit-learn.org/stable/developers/develop.html#estimator-types
    def __getattr__(self, key):
        if key != 'model_':
            if hasattr(self, 'model_'):
                return getattr(self.model_, key)
            else:
                return getattr(self.est_class, key)
        else:
            raise AttributeError(
                "'{}' object has no attribute 'model_'".format(type(self).__name__))

    # these two control logging

    def enable_logging(self):
        self._logging_enabled = True

    def disable_logging(self):
        self._logging_enabled = False

    # ignore the following two for now, more info in the Appendix

    def __getstate__(self):
        state = self.__dict__.copy()
        del state['_logger']
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._logger = logging.getLogger(__name__)
26/213: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/214: q.get_params()
26/215: q.fit(df, y)
26/216: y_my = q.predict(df)
26/217:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):

        self.est_class = est_class
        self.cols = cols
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('col')
        print(self._param_names)

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)

        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        y_pred = self.model_.predict(X)
        return y_pred

    # requiring a score method is not documented but throws an
    # error if not implemented
    def score(self, X, y, **kwargs):
        return self.model_.score(X, y, **kwargs)

    # some models implement custom methods. Anything that is not implemented here
    # will be delegated to the underlying model. There is one condition we have
    # to cover: if the underlying estimator has class attributes they won't
    # be accessible until we fit the model (since we instantiate the model there)
    # to fix it, we try to look it up attributes in the instance, if there
    # is no instance, we look up the class. More info here:
    # https://scikit-learn.org/stable/developers/develop.html#estimator-types
    def __getattr__(self, key):
        if key != 'model_':
            if hasattr(self, 'model_'):
                return getattr(self.model_, key)
            else:
                return getattr(self.est_class, key)
        else:
            raise AttributeError(
                "'{}' object has no attribute 'model_'".format(type(self).__name__))

    # these two control logging

    def enable_logging(self):
        self._logging_enabled = True

    def disable_logging(self):
        self._logging_enabled = False

    # ignore the following two for now, more info in the Appendix

    def __getstate__(self):
        state = self.__dict__.copy()
        del state['_logger']
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._logger = logging.getLogger(__name__)
26/218: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/219: q.get_params()
26/220: q
26/221: q.fit(df, y)
26/222: y_my = q.predict(df)
26/223: y_my
26/224:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.est_class = est_class
        self.cols = cols
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('col')
        print(self._param_names)

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)

        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        y_pred = self.model_.predict(X)
        return y_pred

    # requiring a score method is not documented but throws an
    # error if not implemented
    def score(self, X, y, **kwargs):
        return self.model_.score(X, y, **kwargs)

    # some models implement custom methods. Anything that is not implemented here
    # will be delegated to the underlying model. There is one condition we have
    # to cover: if the underlying estimator has class attributes they won't
    # be accessible until we fit the model (since we instantiate the model there)
    # to fix it, we try to look it up attributes in the instance, if there
    # is no instance, we look up the class. More info here:
    # https://scikit-learn.org/stable/developers/develop.html#estimator-types
    def __getattr__(self, key):
        if key != 'model_':
            if hasattr(self, 'model_'):
                return getattr(self.model_, key)
            else:
                return getattr(self.est_class, key)
        else:
            raise AttributeError(
                "'{}' object has no attribute 'model_'".format(type(self).__name__))

    # these two control logging

    def enable_logging(self):
        self._logging_enabled = True

    def disable_logging(self):
        self._logging_enabled = False

    # ignore the following two for now, more info in the Appendix

    def __getstate__(self):
        state = self.__dict__.copy()
        del state['_logger']
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._logger = logging.getLogger(__name__)
26/225: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/226: q.get_params()
26/227: q
26/228: q.fit(df, y)
26/229: y_my = q.predict(df)
26/230: y_my
26/231: q
26/232: cross_validate(CustomModel(cols = ['a'], n_estimators=3), df, df['b'], cv=5)
26/233:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        #setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        #self._param_names.append('col')
        print(self._param_names)

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)

        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        y_pred = self.model_.predict(X)
        return y_pred

    # requiring a score method is not documented but throws an
    # error if not implemented
    def score(self, X, y, **kwargs):
        return self.model_.score(X, y, **kwargs)

    # some models implement custom methods. Anything that is not implemented here
    # will be delegated to the underlying model. There is one condition we have
    # to cover: if the underlying estimator has class attributes they won't
    # be accessible until we fit the model (since we instantiate the model there)
    # to fix it, we try to look it up attributes in the instance, if there
    # is no instance, we look up the class. More info here:
    # https://scikit-learn.org/stable/developers/develop.html#estimator-types
    def __getattr__(self, key):
        if key != 'model_':
            if hasattr(self, 'model_'):
                return getattr(self.model_, key)
            else:
                return getattr(self.est_class, key)
        else:
            raise AttributeError(
                "'{}' object has no attribute 'model_'".format(type(self).__name__))

    # these two control logging

    def enable_logging(self):
        self._logging_enabled = True

    def disable_logging(self):
        self._logging_enabled = False

    # ignore the following two for now, more info in the Appendix

    def __getstate__(self):
        state = self.__dict__.copy()
        del state['_logger']
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._logger = logging.getLogger(__name__)
26/234: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/235: q.get_params()
26/236: q
26/237: q.fit(df, y)
26/238: y_my = q.predict(df)
26/239: y_my
26/240: real_model = LGBMRegressor()
26/241: real_model2 = LGBMRegressor()
26/242: real_model.get_params
26/243:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
26/244: real_model.get_params == real_model2.get_params
26/245: real_model.get_params
26/246: q
26/247: cross_validate(CustomModel(cols = ['a'], n_estimators=3), df, df['b'], cv=5)
26/248: cross_validate(CustomModel(cols = [1], n_estimators=3), df, df['b'], cv=5)
26/249: cross_validate(CustomModel(cols = 1, n_estimators=3), df, df['b'], cv=5)
26/250: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/251: q.get_params()
26/252: q.fit(df, y)
26/253: cross_validate(q, df, df['b'], cv=5)
26/254:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        #setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        #self._param_names.append('col')
        print(self._param_names)

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)

        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        y_pred = self.model_.predict(X)
        return y_pred

    # requiring a score method is not documented but throws an
    # error if not implemented
    def score(self, X, y, **kwargs):
        return self.model_.score(X, y, **kwargs)

    # some models implement custom methods. Anything that is not implemented here
    # will be delegated to the underlying model. There is one condition we have
    # to cover: if the underlying estimator has class attributes they won't
    # be accessible until we fit the model (since we instantiate the model there)
    # to fix it, we try to look it up attributes in the instance, if there
    # is no instance, we look up the class. More info here:
    # https://scikit-learn.org/stable/developers/develop.html#estimator-types
    def __getattr__(self, key):
        if key != 'model_':
            if hasattr(self, 'model_'):
                return getattr(self.model_, key)
            else:
                return getattr(self.est_class, key)
        else:
            raise AttributeError(
                "'{}' object has no attribute 'model_'".format(type(self).__name__))

    # these two control logging

    def enable_logging(self):
        self._logging_enabled = True

    def disable_logging(self):
        self._logging_enabled = False

    # ignore the following two for now, more info in the Appendix

    def __getstate__(self):
        state = self.__dict__.copy()
        del state['_logger']
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._logger = logging.getLogger(__name__)
26/255: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/256: q.get_params()
26/257: q
26/258: q.fit(df, y)
26/259: y_my = q.predict(df)
26/260: y_my
26/261: real_model = LGBMRegressor()
26/262: real_model2 = LGBMRegressor()
26/263: real_model.get_params
26/264:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
26/265: real_model.get_params == real_model2.get_params
26/266: real_model.get_params
26/267: q
26/268: cross_validate(q, df, df['b'], cv=5)
26/269: cross_validate(q(cols=['a']), df, df['b'], cv=5)
26/270: q(cols=['a'])
26/271: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/272: cross_validate(q, df, df['b'], cv=5)
26/273: q
26/274: cross_validate(CustomModel(cols = ['a']), df, df['b'], cv=5)
26/275:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        #setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        #self._param_names.append('col')
        print(self._param_names)

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, **parameters, cols):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        self.cols = cols

        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        y_pred = self.model_.predict(X)
        return y_pred

    # requiring a score method is not documented but throws an
    # error if not implemented
    def score(self, X, y, **kwargs):
        return self.model_.score(X, y, **kwargs)

    # some models implement custom methods. Anything that is not implemented here
    # will be delegated to the underlying model. There is one condition we have
    # to cover: if the underlying estimator has class attributes they won't
    # be accessible until we fit the model (since we instantiate the model there)
    # to fix it, we try to look it up attributes in the instance, if there
    # is no instance, we look up the class. More info here:
    # https://scikit-learn.org/stable/developers/develop.html#estimator-types
    def __getattr__(self, key):
        if key != 'model_':
            if hasattr(self, 'model_'):
                return getattr(self.model_, key)
            else:
                return getattr(self.est_class, key)
        else:
            raise AttributeError(
                "'{}' object has no attribute 'model_'".format(type(self).__name__))

    # these two control logging

    def enable_logging(self):
        self._logging_enabled = True

    def disable_logging(self):
        self._logging_enabled = False

    # ignore the following two for now, more info in the Appendix

    def __getstate__(self):
        state = self.__dict__.copy()
        del state['_logger']
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._logger = logging.getLogger(__name__)
26/276: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/277:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        #setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        #self._param_names.append('col')
        print(self._param_names)

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, cols, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        self.cols = cols

        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        y_pred = self.model_.predict(X)
        return y_pred

    # requiring a score method is not documented but throws an
    # error if not implemented
    def score(self, X, y, **kwargs):
        return self.model_.score(X, y, **kwargs)

    # some models implement custom methods. Anything that is not implemented here
    # will be delegated to the underlying model. There is one condition we have
    # to cover: if the underlying estimator has class attributes they won't
    # be accessible until we fit the model (since we instantiate the model there)
    # to fix it, we try to look it up attributes in the instance, if there
    # is no instance, we look up the class. More info here:
    # https://scikit-learn.org/stable/developers/develop.html#estimator-types
    def __getattr__(self, key):
        if key != 'model_':
            if hasattr(self, 'model_'):
                return getattr(self.model_, key)
            else:
                return getattr(self.est_class, key)
        else:
            raise AttributeError(
                "'{}' object has no attribute 'model_'".format(type(self).__name__))

    # these two control logging

    def enable_logging(self):
        self._logging_enabled = True

    def disable_logging(self):
        self._logging_enabled = False

    # ignore the following two for now, more info in the Appendix

    def __getstate__(self):
        state = self.__dict__.copy()
        del state['_logger']
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._logger = logging.getLogger(__name__)
26/278: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/279: q.get_params()
26/280: q
26/281: q.fit(df, y)
26/282: y_my = q.predict(df)
26/283: y_my
26/284: real_model = LGBMRegressor()
26/285: real_model2 = LGBMRegressor()
26/286: real_model.get_params
26/287:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
26/288: real_model.get_params == real_model2.get_params
26/289: real_model.get_params
26/290: q(cols=['a'])
26/291: cross_validate(CustomModel(cols = ['a']), df, df['b'], cv=5)
26/292: q
26/293:
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
26/294:
from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier()
26/295:
class CustomModel (DecisionTreeClassifier):
    """LGBMRegressor wrapper"""
    def __init__(self,cols):
        print('You create me')
        self.cols = cols
        super().__init__(classifier=tree.DecisionTreeClassifier())
        
    def fit(self, X, y = None):
        model = clone(self.model)
        print(model == self.model)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model = model.fit(X_copy,y)
        print(self.model)

        return self
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.model.predict(X_copy))
26/296: q = CustomModel(['b'])
26/297: cross_validate(q, df, df['b'], cv=5)
26/298:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('col')

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, cols, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        self.cols = cols

        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        y_pred = self.model_.predict(X)
        return y_pred

    # requiring a score method is not documented but throws an
    # error if not implemented
    def score(self, X, y, **kwargs):
        return self.model_.score(X, y, **kwargs)

    # some models implement custom methods. Anything that is not implemented here
    # will be delegated to the underlying model. There is one condition we have
    # to cover: if the underlying estimator has class attributes they won't
    # be accessible until we fit the model (since we instantiate the model there)
    # to fix it, we try to look it up attributes in the instance, if there
    # is no instance, we look up the class. More info here:
    # https://scikit-learn.org/stable/developers/develop.html#estimator-types
    def __getattr__(self, key):
        if key != 'model_':
            if hasattr(self, 'model_'):
                return getattr(self.model_, key)
            else:
                return getattr(self.est_class, key)
        else:
            raise AttributeError(
                "'{}' object has no attribute 'model_'".format(type(self).__name__))

    # these two control logging

    def enable_logging(self):
        self._logging_enabled = True

    def disable_logging(self):
        self._logging_enabled = False

    # ignore the following two for now, more info in the Appendix

    def __getstate__(self):
        state = self.__dict__.copy()
        del state['_logger']
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._logger = logging.getLogger(__name__)
26/299: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/300: q.get_params()
26/301: q
26/302: q.fit(df, y)
26/303: y_my = q.predict(df)
26/304: y_my
26/305: real_model = LGBMRegressor()
26/306: real_model2 = LGBMRegressor()
26/307: real_model.get_params
26/308:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
26/309: real_model.get_params == real_model2.get_params
26/310: real_model.get_params
26/311: q(cols=['a'])
26/312: cross_validate(q, df, df['b'], cv=5)
26/313: q
26/314:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        #setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        #self._param_names.append('col')

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, cols, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        self.cols = cols
        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, cols, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        y_pred = self.model_.predict(X)
        return y_pred

    # requiring a score method is not documented but throws an
    # error if not implemented
    def score(self, X, y, **kwargs):
        return self.model_.score(X, y, **kwargs)
26/315: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/316: q.get_params()
26/317: q
26/318: q.fit(df, y)
26/319: y_my = q.predict(df)
26/320: y_my
26/321: real_model = LGBMRegressor()
26/322: real_model2 = LGBMRegressor()
26/323: real_model.get_params
26/324:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
26/325: real_model.get_params == real_model2.get_params
26/326: real_model.get_params
26/327: q
26/328: cross_validate(q, df, df['b'], cv=5)
26/329:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        #setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        #self._param_names.append('col')

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, cols, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        self.cols = cols
        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, cols, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        y_pred = self.model_.predict(X)
        return y_pred

    # requiring a score method is not documented but throws an
    # error if not implemented
    def score(self, X, y, **kwargs):
        return self.model_.score(X, y, **kwargs)
26/330: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/331: q.get_params()
26/332: q
26/333: q.fit(df, y)
26/334:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        #setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        #self._param_names.append('col')

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, cols, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        self.cols = cols
        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        y_pred = self.model_.predict(X)
        return y_pred

    # requiring a score method is not documented but throws an
    # error if not implemented
    def score(self, X, y, **kwargs):
        return self.model_.score(X, y, **kwargs)
26/335: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/336: q.get_params()
26/337: q
26/338: q.fit(df, y)
26/339: y_my = q.predict(df)
26/340: y_my
26/341: real_model = LGBMRegressor()
26/342: real_model2 = LGBMRegressor()
26/343: real_model.get_params
26/344:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
26/345: real_model.get_params == real_model2.get_params
26/346: real_model.get_params
26/347: q
26/348: cross_validate(q, df, df['b'], cv=5)
26/349:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        #setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        #self._param_names.append('col')

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, cols, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        self.cols = cols
        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        y_pred = self.model_.predict(X)
        return y_pred
26/350: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/351: q.get_params()
26/352: q
26/353: q.fit(df, y)
26/354: y_my = q.predict(df)
26/355: y_my
26/356: real_model = LGBMRegressor()
26/357: real_model2 = LGBMRegressor()
26/358: real_model.get_params
26/359:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
26/360: real_model.get_params == real_model2.get_params
26/361: real_model.get_params
26/362: q
26/363: cross_validate(q, df, df['b'], cv=5)
26/364:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        #setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        #self._param_names.append('col')
        super().__init__(LGBMRegressor)

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, cols, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        self.cols = cols
        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        y_pred = self.model_.predict(X)
        return y_pred
26/365: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/366: q.get_params()
26/367:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        #setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        #self._param_names.append('col')
        super().__init__(LGBMRegressor())

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, cols, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        self.cols = cols
        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        y_pred = self.model_.predict(X)
        return y_pred
26/368: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/369:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        #setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        #self._param_names.append('col')
        super().__init__(LGBMRegressor())

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, cols, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        self.cols = cols
        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        y_pred = self.model_.predict(X)
        return y_pred
26/370: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/371:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        #setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        #self._param_names.append('col')
        super().__init__(est_class)

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, cols, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        self.cols = cols
        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        y_pred = self.model_.predict(X)
        return y_pred
26/372: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/373:
class CustomModel(BaseEstimator):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        #setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        #self._param_names.append('col')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, cols, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        self.cols = cols
        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        y_pred = self.model_.predict(X)
        return y_pred
26/374: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/375: q.get_params()
26/376: q
26/377: q.fit(df, y)
26/378: y_my = q.predict(df)
26/379: y_my
26/380: real_model = LGBMRegressor()
26/381: real_model2 = LGBMRegressor()
26/382: real_model.get_params
26/383:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
26/384: real_model.get_params == real_model2.get_params
26/385: real_model.get_params
26/386: q
26/387: cross_validate(q, df, df['b'], cv=5)
26/388:
class CustomModel(lightgbm):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        #setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        #self._param_names.append('col')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, cols, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        self.cols = cols
        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        y_pred = self.model_.predict(X)
        return y_pred
26/389: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/390:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        #setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        #self._param_names.append('col')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, cols, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        self.cols = cols
        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        y_pred = self.model_.predict(X)
        return y_pred
26/391: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/392: q.get_params()
26/393: q
26/394: q.fit(df, y)
26/395: y_my = q.predict(df)
26/396: y_my
26/397: real_model = LGBMRegressor()
26/398: real_model2 = LGBMRegressor()
26/399: real_model.get_params
26/400:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
26/401: real_model.get_params == real_model2.get_params
26/402: real_model.get_params
26/403: q
26/404: cross_validate(q, df, df['b'], cv=5)
26/405:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        #self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        #setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        #self._param_names.append('col')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, cols, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        self.cols = cols
        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        # remember the trailing underscore
        self.model_ = self.est_class(**est_kwargs)
        self.model_.fit(X, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        y_pred = self.model_.predict(X)
        return y_pred
26/406: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/407: q.get_params()
26/408: q
26/409: q.fit(df, y)
26/410: y_my = q.predict(df)
26/411: y_my
26/412: real_model = LGBMRegressor()
26/413: real_model2 = LGBMRegressor()
26/414: real_model.get_params
26/415:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
26/416: real_model.get_params == real_model2.get_params
26/417: real_model.get_params
26/418: q
26/419: cross_validate(q, df, df['b'], cv=5)
26/420: cross_validate(CustomModel(cols=[√°]), df, df['b'], cv=5)
26/421: cross_validate(CustomModel(cols=['√°']), df, df['b'], cv=5)
26/422: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/423: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/424: q.get_params()
26/425: q
26/426:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        #self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        #setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        #self._param_names.append('col')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, cols, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        self.cols = cols
        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, cols, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model_.fit(X_copy, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        y_pred = self.model_.predict(X)
        return y_pred
26/427: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/428: q.get_params()
26/429: q
26/430: q.fit(df, y)
26/431:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        #self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        #setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        #self._param_names.append('col')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, cols, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        self.cols = cols
        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model_.fit(X_copy, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        y_pred = self.model_.predict(X)
        return y_pred
26/432: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/433: q.get_params()
26/434: q
26/435: q.fit(df, y)
26/436:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        #self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        #setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        #self._param_names.append('col')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, cols, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        self.cols = cols
        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model_.fit(X_copy, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X)
        return y_pred
26/437: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/438: q.get_params()
26/439: q
26/440: q.fit(df, y)
26/441: y_my = q.predict(df)
26/442: y_my
26/443:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        #self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        #setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        #self._param_names.append('col')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, cols, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        self.cols = cols
        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model_.fit(X_copy, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
26/444: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/445: q.get_params()
26/446: q
26/447: q.fit(df, y)
26/448: y_my = q.predict(df)
26/449: y_my
26/450: real_model = LGBMRegressor()
26/451: real_model2 = LGBMRegressor()
26/452: real_model.get_params
26/453:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
26/454: real_model.get_params == real_model2.get_params
26/455: real_model.get_params
26/456: q
26/457: cross_validate(CustomModel(cols=['√°']), df, df['b'], cv=5)
26/458: cross_validate(q, df, df['b'], cv=5)
26/459:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        #self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        #setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        #self._param_names.append('col')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, cols, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        self.cols = cols
        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model_.fit(X_copy, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
26/460: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/461: q.get_params()
26/462: q.fit(df, y)
26/463: y_my = q.predict(df)
26/464: y_my
26/465: real_model = LGBMRegressor()
26/466: real_model2 = LGBMRegressor()
26/467: real_model.get_params
26/468:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
26/469: real_model.get_params == real_model2.get_params
26/470: real_model.get_params
26/471: q
26/472: cross_validate(q, df, df['b'], cv=5)
26/473: q.cols
26/474:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        #self.cols = {}
        self.cols = cols
        self.est_class = LGBMRegressor(**kwargs)
        for key, value in kwargs.items():
            setattr(self, key, value)
        #setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        #self._param_names.append('col')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, cols, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        self.cols = cols
        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model_.fit(X_copy, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
26/475: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/476: q.get_params()
26/477:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        #self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        #setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        #self._param_names.append('col')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def set_params(self, cols, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        self.cols = cols
        return self

    # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model_.fit(X_copy, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
26/478: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/479: q.get_params()
26/480:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        #self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        #setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        #self._param_names.append('col')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

 #  def set_params(self, cols, **parameters):
 #      for parameter, value in parameters.items():
 #          setattr(self, parameter, value)
 #      self.cols = cols
 #      return self

 #  # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model_.fit(X_copy, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
26/481: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/482: q.get_params()
26/483: q.fit(df, y)
26/484: y_my = q.predict(df)
26/485: y_my
26/486: real_model = LGBMRegressor()
26/487: real_model2 = LGBMRegressor()
26/488: real_model.get_params
26/489:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
26/490: real_model.get_params == real_model2.get_params
26/491: real_model.get_params
26/492: q.cols
26/493: cross_validate(q, df, df['b'], cv=5)
26/494: df
26/495: cross_validate(q, df, df['b'], cv=5)
26/496: cross_validate(q, df, df['a'], cv=5)
26/497: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/498:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        #self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'col', 3)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('col')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

 #  def set_params(self, cols, **parameters):
 #      for parameter, value in parameters.items():
 #          setattr(self, parameter, value)
 #      self.cols = cols
 #      return self

 #  # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model_.fit(X_copy, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
26/499: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/500: q.get_params()
26/501: q.fit(df, y)
26/502: y_my = q.predict(df)
26/503: y_my
26/504: real_model = LGBMRegressor()
26/505: real_model2 = LGBMRegressor()
26/506: real_model.get_params
26/507:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
26/508: real_model.get_params == real_model2.get_params
26/509: real_model.get_params
26/510: cross_validate(q, df, df['a'], cv=5)
26/511:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        #self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

 #  def set_params(self, cols, **parameters):
 #      for parameter, value in parameters.items():
 #          setattr(self, parameter, value)
 #      self.cols = cols
 #      return self

 #  # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model_.fit(X_copy, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
26/512: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/513: q.get_params()
26/514: q.fit(df, y)
26/515: y_my = q.predict(df)
26/516: y_my
26/517: real_model = LGBMRegressor()
26/518: real_model2 = LGBMRegressor()
26/519: real_model.get_params
26/520:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
26/521: real_model.get_params == real_model2.get_params
26/522: real_model.get_params
26/523: cross_validate(q, df, df['a'], cv=5)
26/524:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        #self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

 #  def set_params(self, cols, **parameters):
 #      for parameter, value in parameters.items():
 #          setattr(self, parameter, value)
 #      self.cols = cols
 #      return self

 #  # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model_.fit(X_copy, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
26/525: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/526: q.get_params()
26/527: q.fit(df, y)
26/528: y_my = q.predict(df)
26/529: y_my
26/530: real_model = LGBMRegressor()
26/531: real_model2 = LGBMRegressor()
26/532: real_model.get_params
26/533:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
26/534: real_model.get_params == real_model2.get_params
26/535: real_model.get_params
26/536: cross_validate(q, df, df['a'], cv=5)
26/537: cross_validate(q, df, df['a'], cv=5, scoring='f1_macro')
26/538:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        #self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

 #  def set_params(self, cols, **parameters):
 #      for parameter, value in parameters.items():
 #          setattr(self, parameter, value)
 #      self.cols = cols
 #      return self

 #  # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model_.fit(X_copy, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs))
26/539:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        #self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

 #  def set_params(self, cols, **parameters):
 #      for parameter, value in parameters.items():
 #          setattr(self, parameter, value)
 #      self.cols = cols
 #      return self

 #  # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model_.fit(X_copy, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs)
26/540: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/541: q.get_params()
26/542: q.fit(df, y)
26/543: y_my = q.predict(df)
26/544: y_my
26/545: real_model = LGBMRegressor()
26/546: real_model2 = LGBMRegressor()
26/547: real_model.get_params
26/548:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
26/549: real_model.get_params == real_model2.get_params
26/550: real_model.get_params
26/551: cross_validate(q, df, df['a'], cv=5, scoring='f1_macro')
26/552:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        #self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

 #  def set_params(self, cols, **parameters):
 #      for parameter, value in parameters.items():
 #          setattr(self, parameter, value)
 #      self.cols = cols
 #      return self

 #  # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model_.fit(X_copy, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs)
26/553: q = CustomModel(cols = ['a'], n_estimators=3, max_depth = 3)
26/554: cross_validate(q, df, df['a'], cv=5)
26/555: q = CustomModel(cols = ['c'], n_estimators=3, max_depth = 3)
26/556: q.get_params()
26/557: q.fit(df, y)
26/558: y_my = q.predict(df)
26/559: y_my
26/560: real_model = LGBMRegressor()
26/561: real_model2 = LGBMRegressor()
26/562: real_model.get_params
26/563:
real_model.fit(df.drop('b', axis=1), df['b'])
y_real = real_model.predict(df.drop('b', axis=1))
26/564: real_model.get_params == real_model2.get_params
26/565: real_model.get_params
26/566: cross_validate(q, df, df['c'], cv=5)
26/567: cross_validate(real_model, df.drop('c', axis=1), df['c'], cv=5)
26/568: cross_validate(real_model, df.drop('a', axis=1), df['a'], cv=5)
26/569: cross_validate(real_model, df.drop('b', axis=1), df['b'], cv=5)
26/570: cross_validate(real_model, df.drop('d', axis=1), df['d'], cv=5)
26/571: cross_validate(q, df, df['d'], cv=5)
26/572: q = CustomModel(cols = ['d'], n_estimators=3, max_depth = 3)
26/573: cross_validate(q, df, df['d'], cv=5)
26/574: q
26/575: q = CustomModel(cols = ['d'], n_estimators=400, max_depth = 2)
26/576: q.get_params()
26/577: q.fit(df, y)
26/578: y_my = q.predict(df)
26/579: y_my
26/580: q
26/581: cross_validate(real_model, df.drop('d', axis=1), df['d'], cv=5)
26/582: cross_validate(q, df, df['d'], cv=5)
27/1:
# –ú—ã —Ö–æ—Ç–∏–º –Ω–∞–ø–∏—Å–∞—Ç—å –æ–±–µ—Ä—Ç–∫—É –¥–ª—è LGBMRegressor 
# –≤ –∫–æ—Ç–æ—Ä—É—é –º—ã –±—ã —É–º–µ–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞, 
# –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏. 

# –û—Å–Ω–æ–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å - –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —É–º–µ—Ç—å –ø—Ä–∏—Ç–≤–æ—Ä—è—Ç—å—Å—è 
# —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º sklearn —ç—Å—Ç–∏–º–∞—Ç–æ—Ä–æ–º.
# –¢–æ –µ—Å—Ç—å —Ä–∞–±–æ–∞—Ç—å –±–µ–∑ –ø—Ä–æ–±–ª–µ–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, c cross_validate.

# –ü—Ä–æ–¥—É–º–∞–π –∫–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –∫–ª–∞—Å—Å–µ,
# –∫–∞–∫ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LGBMRegressor —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ —Ç–¥.

# hint:
# 1) –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç sklearn.base.clone 
# 2) –ú–æ–∂–µ—Ç –ª–∏ –ø–æ–º–æ—á—å –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤?
27/2:
from sklearn.base import clone
from lightgbm import LGBMRegressor
import copy
27/3:
import numpy as np
import pandas as pd
27/4:
from sklearn import datasets
iris = datasets.load_iris()
27/5:
X = iris.data  # we only take the first two features.
y = iris.target
27/6: X, y = load_boston(return_X_y=True)
27/7:
from sklearn import datasets
iris = datasets.load_iris()
from sklearn.datasets import load_boston
27/8: X, y = load_boston(return_X_y=True)
27/9: X
27/10: X.shape
27/11: df = pd.DataFrame(X, columns = ['a','b','c','d','e','f','g','h','i','j','k','l','m'])
27/12: df
27/13: y
27/14:
from sklearn.base import clone
from lightgbm import LGBMRegressor
import copy
from sklearn.model_selection import cross_validate
27/15:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols,**kwargs):
        print('You create me')
        self.cols = cols
        #base_estimator = LGBMRegressor()
        super().__init__(cols,**kwargs)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self):
        print("die")
27/16:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        #self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

 #  def set_params(self, cols, **parameters):
 #      for parameter, value in parameters.items():
 #          setattr(self, parameter, value)
 #      self.cols = cols
 #      return self

 #  # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model_.fit(X_copy, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs)
27/17: q = CustomModel(cols = ['d'], n_estimators=400, max_depth = 2)
27/18: q.get_params()
27/19: q.fit(df, y)
27/20: y_my = q.predict(df)
27/21: cross_validate(q, df, df['d'], cv=5)
27/22:
class CustomModel(LGBMRegressor):
    """LGBMRegressor wrapper"""
    def __init__(self,cols,**kwargs):
        print('You create me')
        self.cols = cols
        #base_estimator = LGBMRegressor()
        super().__init__(cols,**kwargs)
        
    def fit(self, X, y):
        model = clone(self.base_estimator)
        print('you fit me')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        #self.model = model.fit(X_copy,y)
        print(self.base_estimator)

        self.base_estimator.fit(X_copy,y)
        return model
    
    def predict(self, X):
        print('predict')
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return(self.base_estimator.predict(X_copy))
    
    def __del__(self):
        print("die")
27/23:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        #self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

 #  def set_params(self, cols, **parameters):
 #      for parameter, value in parameters.items():
 #          setattr(self, parameter, value)
 #      self.cols = cols
 #      return self

 #  # our fit method instantiates the actual model, and
    # it forwards any extra keyword arguments
    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:2])
        self.model_.fit(X_copy, y, **kwargs)
        # fit must return self
        return self

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs)
27/24:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        #self.cols = {}
        self.cols = cols
        self.est_class = clone(est_class)
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model_.fit(X_copy, y, **kwargs)
        return self

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs)
27/25: q = CustomModel(cols = ['d'], n_estimators=400, max_depth = 2)
27/26:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        #self.cols = {}
        self.cols = cols
        self.est_class = clone(est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model_.fit(X_copy, y, **kwargs)
        return self

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs)
27/27: q = CustomModel(cols = ['d'], n_estimators=400, max_depth = 2)
27/28:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        #self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model_.fit(X_copy, y, **kwargs)
        return self

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs)
27/29: q = CustomModel(cols = ['d'], n_estimators=400, max_depth = 2)
27/30: q.get_params()
27/31: q.fit(df, y)
27/32: y_my = q.predict(df)
27/33: cross_validate(q, df, df['d'], cv=5)
27/34: q = CustomModel(cols = ['d', 'f'], n_estimators=400, max_depth = 2)
27/35: q.get_params()
27/36: q.fit(df, y)
27/37: y_my = q.predict(df)
27/38: cross_validate(q, df, df['d'], cv=5)
27/39:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        #self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:1])
        self.model_.fit(X_copy, y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs)
27/40: q = CustomModel(cols = ['d', 'f'], n_estimators=400, max_depth = 2)
27/41: q.get_params()
27/42: q.fit(df, y)
27/43: q = CustomModel(cols = ['a', 'd', 'f'], n_estimators=400, max_depth = 2)
27/44: q.get_params()
27/45: q.fit(df, y)
27/46: y_my = q.predict(df)
27/47: cross_validate(q, df, df['d'], cv=5)
27/48: X
27/49: df
27/50: df
27/51: df
27/52: cols_to_ignore = ['a', 'd', 'f']
27/53: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
27/54: q.get_params()
27/55: q.fit(df, y)
28/1:
# –ú—ã —Ö–æ—Ç–∏–º –Ω–∞–ø–∏—Å–∞—Ç—å –æ–±–µ—Ä—Ç–∫—É –¥–ª—è LGBMRegressor 
# –≤ –∫–æ—Ç–æ—Ä—É—é –º—ã –±—ã —É–º–µ–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞, 
# –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏. 

# –û—Å–Ω–æ–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å - –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —É–º–µ—Ç—å –ø—Ä–∏—Ç–≤–æ—Ä—è—Ç—å—Å—è 
# —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º sklearn —ç—Å—Ç–∏–º–∞—Ç–æ—Ä–æ–º.
# –¢–æ –µ—Å—Ç—å —Ä–∞–±–æ–∞—Ç—å –±–µ–∑ –ø—Ä–æ–±–ª–µ–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, c cross_validate.

# –ü—Ä–æ–¥—É–º–∞–π –∫–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –∫–ª–∞—Å—Å–µ,
# –∫–∞–∫ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LGBMRegressor —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ —Ç–¥.

# hint:
# 1) –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç sklearn.base.clone 
# 2) –ú–æ–∂–µ—Ç –ª–∏ –ø–æ–º–æ—á—å –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤?
28/2:
from sklearn.base import clone
from lightgbm import LGBMRegressor
import copy
from sklearn.model_selection import cross_validate
28/3:
import numpy as np
import pandas as pd
28/4:
from sklearn import datasets
iris = datasets.load_iris()
from sklearn.datasets import load_boston
28/5:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = LGBMRegressor(**kwargs)
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = clone(est_class)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:1])
        self.model_.fit(X_copy, y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs)
28/6:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
28/7: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
28/8: q.fit(df, y)
28/9:
# –ú—ã —Ö–æ—Ç–∏–º –Ω–∞–ø–∏—Å–∞—Ç—å –æ–±–µ—Ä—Ç–∫—É –¥–ª—è LGBMRegressor 
# –≤ –∫–æ—Ç–æ—Ä—É—é –º—ã –±—ã —É–º–µ–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞, 
# –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏. 

# –û—Å–Ω–æ–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å - –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —É–º–µ—Ç—å –ø—Ä–∏—Ç–≤–æ—Ä—è—Ç—å—Å—è 
# —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º sklearn —ç—Å—Ç–∏–º–∞—Ç–æ—Ä–æ–º.
# –¢–æ –µ—Å—Ç—å —Ä–∞–±–æ–∞—Ç—å –±–µ–∑ –ø—Ä–æ–±–ª–µ–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, c cross_validate.

# –ü—Ä–æ–¥—É–º–∞–π –∫–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –∫–ª–∞—Å—Å–µ,
# –∫–∞–∫ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LGBMRegressor —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ —Ç–¥.

# hint:
# 1) –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç sklearn.base.clone 
# 2) –ú–æ–∂–µ—Ç –ª–∏ –ø–æ–º–æ—á—å –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤?
28/10:
from sklearn.base import clone
from lightgbm import LGBMRegressor
import copy
from sklearn.model_selection import cross_validate
28/11:
import numpy as np
import pandas as pd
28/12:
from sklearn import datasets
iris = datasets.load_iris()
from sklearn.datasets import load_boston
28/13: X, y = load_boston(return_X_y=True)
28/14: df = pd.DataFrame(X, columns = ['a','b','c','d','e','f','g','h','i','j','k','l','m'])
28/15: from sklearn.base import BaseEstimator
28/16:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = LGBMRegressor(**kwargs)
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = clone(est_class)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:1])
        self.model_.fit(X_copy, y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs)
28/17:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
28/18: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
28/19: q.fit(df, y)
28/20:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = LGBMRegressor(**kwargs)
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = clone(self.est_class)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:1])
        self.model_.fit(X_copy, y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs)
28/21:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
28/22: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
28/23: q.fit(df, y)
28/24: y_my = q.predict(df)
28/25: cross_validate(q, df, df['d'], cv=5)
28/26:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        print(X_copy[:1])
        self.model_.fit(X_copy, y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs)
28/27:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
28/28: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
28/29: q.fit(df, y)
28/30: y_my = q.predict(df)
28/31: cross_validate(q, df, df['d'], cv=5)
28/32:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model_.fit(X_copy, y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs)
28/33:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
28/34: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
28/35: q.fit(df, y)
28/36: y_my = q.predict(df)
28/37: cross_validate(q, df, df['d'], cv=5)
28/38: cross_validate(q, df, df['a'], cv=5)
28/39: qq = sklearn.base.clone(q)
28/40: qq = clone(q)
28/41: qq == q
28/42: qq
28/43: qq.dtype
28/44: qq.get_params
28/45: qq.get_params == q.get_params
28/46: qq = clone(q)
29/1:
# –ú—ã —Ö–æ—Ç–∏–º –Ω–∞–ø–∏—Å–∞—Ç—å –æ–±–µ—Ä—Ç–∫—É –¥–ª—è LGBMRegressor 
# –≤ –∫–æ—Ç–æ—Ä—É—é –º—ã –±—ã —É–º–µ–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞, 
# –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏. 

# –û—Å–Ω–æ–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å - –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —É–º–µ—Ç—å –ø—Ä–∏—Ç–≤–æ—Ä—è—Ç—å—Å—è 
# —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º sklearn —ç—Å—Ç–∏–º–∞—Ç–æ—Ä–æ–º.
# –¢–æ –µ—Å—Ç—å —Ä–∞–±–æ–∞—Ç—å –±–µ–∑ –ø—Ä–æ–±–ª–µ–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, c cross_validate.

# –ü—Ä–æ–¥—É–º–∞–π –∫–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –∫–ª–∞—Å—Å–µ,
# –∫–∞–∫ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LGBMRegressor —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ —Ç–¥.

# hint:
# 1) –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç sklearn.base.clone 
# 2) –ú–æ–∂–µ—Ç –ª–∏ –ø–æ–º–æ—á—å –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤? -- –¥–∞, —É–Ω–∞—Å–ª–µ–¥–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥—ã –æ—Ç —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ
29/2:
from sklearn.base import clone
from lightgbm import LGBMRegressor
import copy
from sklearn.model_selection import cross_validate
29/3:
import numpy as np
import pandas as pd
29/4:
from sklearn import datasets
iris = datasets.load_iris()
from sklearn.datasets import load_boston
29/5: X, y = load_boston(return_X_y=True)
29/6: df = pd.DataFrame(X, columns = ['a','b','c','d','e','f','g','h','i','j','k','l','m'])
29/7: from sklearn.base import BaseEstimator
29/8:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model_.fit(X_copy, y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs)
29/9:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
29/10: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
29/11: q.fit(df, y)
29/12: y_my = q.predict(df)
29/13: cross_validate(q, df, df['a'], cv=5)
31/1:
# –ú—ã —Ö–æ—Ç–∏–º –Ω–∞–ø–∏—Å–∞—Ç—å –æ–±–µ—Ä—Ç–∫—É –¥–ª—è LGBMRegressor 
# –≤ –∫–æ—Ç–æ—Ä—É—é –º—ã –±—ã —É–º–µ–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞, 
# –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏. 

# –û—Å–Ω–æ–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å - –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —É–º–µ—Ç—å –ø—Ä–∏—Ç–≤–æ—Ä—è—Ç—å—Å—è 
# —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º sklearn —ç—Å—Ç–∏–º–∞—Ç–æ—Ä–æ–º.
# –¢–æ –µ—Å—Ç—å —Ä–∞–±–æ–∞—Ç—å –±–µ–∑ –ø—Ä–æ–±–ª–µ–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, c cross_validate.

# –ü—Ä–æ–¥—É–º–∞–π –∫–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –∫–ª–∞—Å—Å–µ,
# –∫–∞–∫ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LGBMRegressor —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ —Ç–¥.

# hint:
# 1) –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç sklearn.base.clone 
# 2) –ú–æ–∂–µ—Ç –ª–∏ –ø–æ–º–æ—á—å –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤? -- –¥–∞, —É–Ω–∞—Å–ª–µ–¥–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥—ã –æ—Ç —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ
31/2:
from sklearn.base import clone
from lightgbm import LGBMRegressor
import copy
from sklearn.model_selection import cross_validate
31/3:
import numpy as np
import pandas as pd
31/4: from sklearn.datasets import load_boston
31/5: X, y = load_boston(return_X_y=True)
31/6: df = pd.DataFrame(X, columns = ['a','b','c','d','e','f','g','h','i','j','k','l','m'])
31/7:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model_.fit(X_copy, y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs)
31/8:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
31/9: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
31/10: q.fit(df, y)
31/11: y_my = q.predict(df)
31/12: cross_validate(q, df, df['a'], cv=5)
32/1:
## –Ø –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞ format='%G-%V', —Ç.–∫. –¥–ª—è %V –Ω—É–∂–Ω–æ –ø–æ–¥–∫–ª—é—á–∞—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫—É, 
## —è —Ö–æ—Ç–µ–ª–∞ —Å–¥–µ–ª–∞—Ç—å –±–µ–∑ —Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ—á–µ–∫
##
##
32/2:
class CalWeek:
    def __init__(self, year_week, format='%G-%V'):
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
                
    def __sub__(self, other):
        try:
            if isinstance(other, int):
                year, week = divmod(other, 52)
                new_year, week = divmod(self.week - week + 52, 52)
                
                self.year = self.year - year - new_year + 1
                self.week = week
                self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
                return self
            elif isinstance(other, CalWeek):
                delta = (self.year * 52 + self.week) - (other.year * 52 + other.week)
                if (delta < 0): 
                    raise ValueError()
                else:
                    return(delta)
            else:
                raise TypeError()
        except ValueError:
            print('right date is bigger')
        except TypeError:
            print('Type problem')
32/3: CalWeek('2018-51')
32/4: str(CalWeek('2018-51'))
32/5: CalWeek('2018-51') - 3
32/6: CalWeek('2018-51') + 2
32/7: CalWeek('2019-04') - CalWeek('2018-51')
32/8:
    Class providing arithmetic operations for year-week date format
    
    Tests:
    >>> CalWeek('2018-51')
    2018-51
    
    >>> str(CalWeek('2018-51'))
    '2018-51'
    
    >>> CalWeek('2018-51') - 3
    2018-48
    
    >>> CalWeek('2018-51') + 2
    2019-01
    
    >>> CalWeek('2019-04') - CalWeek('2018-51')
    5
32/9: CalWeek('2018-51') + 200
32/10: CalWeek('2022-43') + 200
32/11: CalWeek('2022-43') - 200
32/12: CalWeek('2018-51') - 3
32/13: CalWeek('2022-01') - 3
32/14: CalWeek('2022-01') - 3 -52
32/15: CalWeek('2022-01') -55
32/16: CalWeek('2022-01') - 55
32/17: CalWeek('2022-01') - 3
32/18: CalWeek('2022-01') - 3
32/19:
class CalWeek:
    def __init__(self, year_week, format='%G-%V'):
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
                
    def __sub__(self, other):
        try:
            if isinstance(other, int):
                year, week = divmod(other, 52)
                new_year, week = divmod(self.week - week + 52, 52)
                
                self.year = self.year - year - new_year - 1
                self.week = week
                self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
                return self
            elif isinstance(other, CalWeek):
                delta = (self.year * 52 + self.week) - (other.year * 52 + other.week)
                if (delta < 0): 
                    raise ValueError()
                else:
                    return(delta)
            else:
                raise TypeError()
        except ValueError:
            print('right date is bigger')
        except TypeError:
            print('Type problem')
32/20: CalWeek('2018-51')
32/21: str(CalWeek('2018-51'))
32/22: CalWeek('2018-51') - 3
32/23: CalWeek('2018-51') + 2
32/24: CalWeek('2019-04') - CalWeek('2018-51')
32/25: CalWeek('2018-51') + 200
32/26: CalWeek('2022-43') - 200
32/27: CalWeek('2022-01') - 55
32/28: CalWeek('2022-01') - 3
32/29: CalWeek('2018-51') - 3
32/30: CalWeek('2018-51') - 03
32/31: CalWeek('2018-51') - 3
32/32: CalWeek('2018-51') - CalWeek('2018-51')
32/33: CalWeek('2018-51')-CalWeek('2018-51')
32/34: CalWeek('2018-51')-CalWeek('2016-48')
32/35: CalWeek('2018-51') - 3
32/36:
class CalWeek:
    def __init__(self, year_week, format='%G-%V'):
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
                
    def __sub__(self, other):
        try:
            if isinstance(other, int):
                year, week = divmod(other, 52)
                new_year, week = divmod(self.week - week + 52, 52)
                
                self.year = self.year - year + new_year - 1
                self.week = week
                self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
                return self
            elif isinstance(other, CalWeek):
                delta = (self.year * 52 + self.week) - (other.year * 52 + other.week)
                if (delta < 0): 
                    raise ValueError()
                else:
                    return(delta)
            else:
                raise TypeError()
        except ValueError:
            print('right date is bigger')
        except TypeError:
            print('Type problem')
32/37: CalWeek('2018-51')
32/38: str(CalWeek('2018-51'))
32/39: CalWeek('2018-51') - 3
32/40: CalWeek('2018-51') + 2
32/41: CalWeek('2019-04') - CalWeek('2018-51')
32/42: CalWeek('2018-51') + 200
32/43: CalWeek('2022-43') - 200
32/44: CalWeek('2018-51')-CalWeek('2016-48')
33/1:
class CalWeek:
    def __init__(self, year_week, format='%G-%V'):
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
                
    def __sub__(self, other):
        try:
            if isinstance(other, int):
                year, week = divmod(other, 52)
                new_year, week = divmod(self.week - week + 52, 52)
                
                self.year = self.year - year + new_year - 1
                self.week = week
                self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
                return self
            elif isinstance(other, CalWeek):
                delta = (self.year * 52 + self.week) - (other.year * 52 + other.week)
                if (delta < 0): 
                    raise ValueError()
                else:
                    return(delta)
            else:
                raise TypeError()
        except ValueError:
            print('right date is bigger')
        except TypeError:
            print('Type problem')
33/2: CalWeek('2018-51')
33/3: str(CalWeek('2018-51'))
33/4: CalWeek('2018-51') - 3
33/5: CalWeek('2018-51') + 2
33/6: CalWeek('2019-04') - CalWeek('2018-51')
33/7: CalWeek('2018-51') + 200
33/8: CalWeek('2022-43') - 200
34/1:
import os
import pandas as pd

import pyarrow as pa
import pyarrow.parquet as pq
34/2: !anaconda install pyarrow
34/3: !pip install pyarrow
34/4:
import os
import pandas as pd

import pyarrow as pa
import pyarrow.parquet as pq
34/5:
import os
import pandas as pd

import pyarrow as pa
import pyarrow.parquet as pq
34/6: !conda install pyarrow
34/7:
import os
import pandas as pd

import pyarrow as pa
import pyarrow.parquet as pq
34/8: !conda install -c anaconda libboost
35/1: !conda install -c conda-forge pyarrow
37/1:
import os
import pandas as pd

import pyarrow as pa
import pyarrow.parquet as pq
37/2: !conda install -c anaconda libboost
37/3: !conda install -c conda-forge pyarrow
39/1:
import os
import pandas as pd

import pyarrow as pa
import pyarrow.parquet as pq
39/2: from multiprocessing import cpu_count
39/3:
import os
import pandas as pd

import pyarrow as pa
import pyarrow.parquet as pq
39/4: !conda config --add channels conda-forge
39/5:
import os
import pandas as pd

import pyarrow as pa
import pyarrow.parquet as pq
39/6:
import os
import pandas as pd

import pyarrow as pa
import pyarrow.parquet as pq
39/7: !pip install couchbase==3.0.4b1
39/8:
import os
import pandas as pd

import pyarrow as pa
import pyarrow.parquet as pq
39/9: !conda update conda
41/1:
import os
import pandas as pd

import pyarrow as pa
import pyarrow.parquet as pq
42/1: from multiprocessing import cpu_count
42/2:
import os
import pandas as pd

import pyarrow as pa
import pyarrow.parquet as pq
42/3:
import os
import pandas as pd

import pyarrow as pa
42/4:
import os
import pandas as pd

import pyarrow
42/5: !conda install conda-forge::boost-cpp
42/6: !conda update pyarrow
42/7:
import os
import pandas as pd

import pyarrow
42/8:
import pandas as pd

import pyarrow
42/9: !conda install -c conda-forge pyarrowpip install pyarrow
42/10: !conda install install
42/11: !conda install pyarrowpip
43/1:
class CalWeek:
    def __init__(self, year_week, format='%G-%V'):
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
                
    def __sub__(self, other):
        try:
            if isinstance(other, int):
                year, week = divmod(other, 52)
                new_year, week = divmod(self.week - week + 52, 52)
                
                self.year = self.year - year + new_year - 1
                self.week = week
                self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
                return self
            elif isinstance(other, CalWeek):
                delta = (self.year * 52 + self.week) - (other.year * 52 + other.week)
                if (delta < 0): 
                    raise ValueError()
                else:
                    return(delta)
            else:
                raise TypeError()
        except ValueError:
            print('right date is bigger')
        except TypeError:
            print('Type problem')
43/2: CalWeek('2018-51')
43/3: str(CalWeek('2018-51'))
43/4: CalWeek('2018-51') - 3
43/5: CalWeek('2018-51') + 2
43/6: sum(CalWeek('2018-51'), 2)
43/7: sum(3,3)
43/8: sum([3,3])
43/9: sum([CalWeek('2018-51'), 2])
43/10:
class CalWeek:
    def __init__(self, year_week, format='%G-%V'):
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
                
    def __subi__(self, other):
        try:
            if isinstance(other, int):
                year, week = divmod(other, 52)
                new_year, week = divmod(self.week - week + 52, 52)
                
                self.year = self.year - year + new_year - 1
                self.week = week
                self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
                return self
            elif isinstance(other, CalWeek):
                delta = (self.year * 52 + self.week) - (other.year * 52 + other.week)
                if (delta < 0): 
                    raise ValueError()
                else:
                    return(delta)
            else:
                raise TypeError()
        except ValueError:
            print('right date is bigger')
        except TypeError:
            print('Type problem')
43/11: CalWeek('2018-51')
43/12: str(CalWeek('2018-51'))
43/13: CalWeek('2018-51') - 3
43/14:
class CalWeek:
    def __init__(self, year_week, format='%G-%V'):
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __addi__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
                
    def __sub__(self, other):
        try:
            if isinstance(other, int):
                year, week = divmod(other, 52)
                new_year, week = divmod(self.week - week + 52, 52)
                
                self.year = self.year - year + new_year - 1
                self.week = week
                self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
                return self
            elif isinstance(other, CalWeek):
                delta = (self.year * 52 + self.week) - (other.year * 52 + other.week)
                if (delta < 0): 
                    raise ValueError()
                else:
                    return(delta)
            else:
                raise TypeError()
        except ValueError:
            print('right date is bigger')
        except TypeError:
            print('Type problem')
43/15: CalWeek('2018-51')
43/16: str(CalWeek('2018-51'))
43/17: CalWeek('2018-51') - 3
43/18: CalWeek('2018-51') + 2
43/19:
class CalWeek:
    def __init__(self, year_week, format='%G-%V'):
        self.year = int(year_week[:4])
        self.week = int(year_week[-2:])
        self.year_week =  "{0}-{1:0>2d}".format(self.year, self.week)
        
    def __repr__ (self):
        return self.year_week
    
    def __add__(self, other):
        try:
            b = int(other)
            year, week = divmod(b, 52)
            new_year, week = divmod(self.week + week, 52)
            
            self.year = self.year + year + new_year
            self.week = week
            self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
            return self
        except ValueError:
            print('Could not convert str to an integer.')
        except:
            print('something is wrong')
                
    def __sub__(self, other):
        try:
            if isinstance(other, int):
                year, week = divmod(other, 52)
                new_year, week = divmod(self.week - week + 52, 52)
                
                self.year = self.year - year + new_year - 1
                self.week = week
                self.year_week = "{0}-{1:0>2d}".format(self.year, self.week)
                return self
            elif isinstance(other, CalWeek):
                delta = (self.year * 52 + self.week) - (other.year * 52 + other.week)
                if (delta < 0): 
                    raise ValueError()
                else:
                    return(delta)
            else:
                raise TypeError()
        except ValueError:
            print('right date is bigger')
        except TypeError:
            print('Type problem')
43/20: CalWeek('2018-51')
43/21: str(CalWeek('2018-51'))
43/22: CalWeek('2018-51') - 3
43/23: CalWeek('2018-51') + 2
43/24: sum([CalWeek('2018-51'), 2])
43/25: [1,2,3].sum()
46/1: #cross_validate(q, df, df['a'], cv=5)
46/2:
# –ú—ã —Ö–æ—Ç–∏–º –Ω–∞–ø–∏—Å–∞—Ç—å –æ–±–µ—Ä—Ç–∫—É –¥–ª—è LGBMRegressor 
# –≤ –∫–æ—Ç–æ—Ä—É—é –º—ã –±—ã —É–º–µ–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞, 
# –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏. 

# –û—Å–Ω–æ–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å - –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —É–º–µ—Ç—å –ø—Ä–∏—Ç–≤–æ—Ä—è—Ç—å—Å—è 
# —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º sklearn —ç—Å—Ç–∏–º–∞—Ç–æ—Ä–æ–º.
# –¢–æ –µ—Å—Ç—å —Ä–∞–±–æ–∞—Ç—å –±–µ–∑ –ø—Ä–æ–±–ª–µ–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, c cross_validate.

# –ü—Ä–æ–¥—É–º–∞–π –∫–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –∫–ª–∞—Å—Å–µ,
# –∫–∞–∫ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LGBMRegressor —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ —Ç–¥.

# hint:
# 1) –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç sklearn.base.clone 
# 2) –ú–æ–∂–µ—Ç –ª–∏ –ø–æ–º–æ—á—å –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤? -- –¥–∞, —É–Ω–∞—Å–ª–µ–¥–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥—ã –æ—Ç —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ
46/3:
from sklearn.base import clone
from lightgbm import LGBMRegressor
import copy
from sklearn.model_selection import cross_validate
46/4:
import numpy as np
import pandas as pd
46/5: from sklearn.datasets import load_boston
46/6: X, y = load_boston(return_X_y=True)
46/7: df = pd.DataFrame(X, columns = ['a','b','c','d','e','f','g','h','i','j','k','l','m'])
46/8: LGBMRegressor
46/9:
# –ú—ã —Ö–æ—Ç–∏–º –Ω–∞–ø–∏—Å–∞—Ç—å –æ–±–µ—Ä—Ç–∫—É –¥–ª—è LGBMRegressor 
# –≤ –∫–æ—Ç–æ—Ä—É—é –º—ã –±—ã —É–º–µ–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞, 
# –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏. 

# –û—Å–Ω–æ–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å - –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —É–º–µ—Ç—å –ø—Ä–∏—Ç–≤–æ—Ä—è—Ç—å—Å—è 
# —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º sklearn —ç—Å—Ç–∏–º–∞—Ç–æ—Ä–æ–º.
# –¢–æ –µ—Å—Ç—å —Ä–∞–±–æ–∞—Ç—å –±–µ–∑ –ø—Ä–æ–±–ª–µ–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, c cross_validate.

# –ü—Ä–æ–¥—É–º–∞–π –∫–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –∫–ª–∞—Å—Å–µ,
# –∫–∞–∫ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LGBMRegressor —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ —Ç–¥.

# hint:
# 1) –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç sklearn.base.clone 
# 2) –ú–æ–∂–µ—Ç –ª–∏ –ø–æ–º–æ—á—å –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤? -- –¥–∞, —É–Ω–∞—Å–ª–µ–¥–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥—ã –æ—Ç —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ
46/10:
from sklearn.base import clone
from lightgbm import LGBMRegressor
import copy
from sklearn.model_selection import cross_validate
46/11:
import numpy as np
import pandas as pd
46/12: from sklearn.datasets import load_boston
46/13: X, y = load_boston(return_X_y=True)
46/14: df = pd.DataFrame(X, columns = ['a','b','c','d','e','f','g','h','i','j','k','l','m'])
46/15:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model_.fit(X_copy, y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs)
46/16: LGBMRegressor
46/17:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
46/18: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
46/19: q.fit(df, y)
46/20: X, y = load_boston(return_X_y=True)
46/21: df = pd.DataFrame(X, columns = ['a','b','c','d','e','f','g','h','i','j','k','l','m'])
46/22: df.head()
46/23: q.fit(df, y)
46/24:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
46/25: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
46/26: q.fit(df, y)
47/1:
# –ú—ã —Ö–æ—Ç–∏–º –Ω–∞–ø–∏—Å–∞—Ç—å –æ–±–µ—Ä—Ç–∫—É –¥–ª—è LGBMRegressor 
# –≤ –∫–æ—Ç–æ—Ä—É—é –º—ã –±—ã —É–º–µ–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞, 
# –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏. 

# –û—Å–Ω–æ–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å - –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —É–º–µ—Ç—å –ø—Ä–∏—Ç–≤–æ—Ä—è—Ç—å—Å—è 
# —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º sklearn —ç—Å—Ç–∏–º–∞—Ç–æ—Ä–æ–º.
# –¢–æ –µ—Å—Ç—å —Ä–∞–±–æ–∞—Ç—å –±–µ–∑ –ø—Ä–æ–±–ª–µ–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, c cross_validate.

# –ü—Ä–æ–¥—É–º–∞–π –∫–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –∫–ª–∞—Å—Å–µ,
# –∫–∞–∫ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LGBMRegressor —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ —Ç–¥.

# hint:
# 1) –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç sklearn.base.clone 
# 2) –ú–æ–∂–µ—Ç –ª–∏ –ø–æ–º–æ—á—å –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤? -- –¥–∞, —É–Ω–∞—Å–ª–µ–¥–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥—ã –æ—Ç —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ
47/2:
from sklearn.base import clone
from lightgbm import LGBMRegressor
import copy
from sklearn.model_selection import cross_validate
47/3:
import numpy as np
import pandas as pd
47/4: from sklearn.datasets import load_boston
47/5: X, y = load_boston(return_X_y=True)
47/6: df = pd.DataFrame(X, columns = ['a','b','c','d','e','f','g','h','i','j','k','l','m'])
47/7:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model_.fit(X_copy, y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs)
47/8:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
47/9: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
47/10: q.fit(df, y)
48/1:
# –ú—ã —Ö–æ—Ç–∏–º –Ω–∞–ø–∏—Å–∞—Ç—å –æ–±–µ—Ä—Ç–∫—É –¥–ª—è LGBMRegressor 
# –≤ –∫–æ—Ç–æ—Ä—É—é –º—ã –±—ã —É–º–µ–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞, 
# –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏. 

# –û—Å–Ω–æ–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å - –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —É–º–µ—Ç—å –ø—Ä–∏—Ç–≤–æ—Ä—è—Ç—å—Å—è 
# —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º sklearn —ç—Å—Ç–∏–º–∞—Ç–æ—Ä–æ–º.
# –¢–æ –µ—Å—Ç—å —Ä–∞–±–æ–∞—Ç—å –±–µ–∑ –ø—Ä–æ–±–ª–µ–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, c cross_validate.

# –ü—Ä–æ–¥—É–º–∞–π –∫–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –∫–ª–∞—Å—Å–µ,
# –∫–∞–∫ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LGBMRegressor —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ —Ç–¥.

# hint:
# 1) –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç sklearn.base.clone 
# 2) –ú–æ–∂–µ—Ç –ª–∏ –ø–æ–º–æ—á—å –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤? -- –¥–∞, —É–Ω–∞—Å–ª–µ–¥–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥—ã –æ—Ç —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ
48/2:
from sklearn.base import clone
from lightgbm import LGBMRegressor
import copy
from sklearn.model_selection import cross_validate
48/3:
from sklearn.base import clone
from lightgbm import LGBMRegressor
import copy
from sklearn.model_selection import cross_validate
48/4:
import numpy as np
import pandas as pd
48/5: from sklearn.datasets import load_boston
48/6: X, y = load_boston(return_X_y=True)
48/7: df = pd.DataFrame(X, columns = ['a','b','c','d','e','f','g','h','i','j','k','l','m'])
48/8: df.head()
48/9:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model_.fit(X_copy, y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs)
48/10: LGBMRegressor
48/11:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
48/12: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
48/13: q.fit(df, y)
48/14: df.dtypes()
48/15: df
48/16: df.stype
48/17: df.dtype
48/18: df.dtypes()
48/19: df.dtype()
48/20: df.to_pandas()
48/21: df
48/22: df.drypes()
48/23: df.dtypes()
48/24: df.dtypes
48/25: !conda install pandas
49/1:
# –ú—ã —Ö–æ—Ç–∏–º –Ω–∞–ø–∏—Å–∞—Ç—å –æ–±–µ—Ä—Ç–∫—É –¥–ª—è LGBMRegressor 
# –≤ –∫–æ—Ç–æ—Ä—É—é –º—ã –±—ã —É–º–µ–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞, 
# –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏. 

# –û—Å–Ω–æ–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å - –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —É–º–µ—Ç—å –ø—Ä–∏—Ç–≤–æ—Ä—è—Ç—å—Å—è 
# —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º sklearn —ç—Å—Ç–∏–º–∞—Ç–æ—Ä–æ–º.
# –¢–æ –µ—Å—Ç—å —Ä–∞–±–æ–∞—Ç—å –±–µ–∑ –ø—Ä–æ–±–ª–µ–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, c cross_validate.

# –ü—Ä–æ–¥—É–º–∞–π –∫–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –∫–ª–∞—Å—Å–µ,
# –∫–∞–∫ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LGBMRegressor —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ —Ç–¥.

# hint:
# 1) –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç sklearn.base.clone 
# 2) –ú–æ–∂–µ—Ç –ª–∏ –ø–æ–º–æ—á—å –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤? -- –¥–∞, —É–Ω–∞—Å–ª–µ–¥–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥—ã –æ—Ç —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ
49/2:
from sklearn.base import clone
from lightgbm import LGBMRegressor
import copy
from sklearn.model_selection import cross_validate
49/3:
import numpy as np
import pandas as pd
49/4: from sklearn.datasets import load_boston
49/5: X, y = load_boston(return_X_y=True)
49/6: df = pd.DataFrame(X, columns = ['a','b','c','d','e','f','g','h','i','j','k','l','m'])
49/7: df.head()
49/8:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model_.fit(X_copy, y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs)
49/9: LGBMRegressor
49/10:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
49/11: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
49/12: df.dtypes
49/13: q.fit(df, y)
49/14: lgm = LGBMRegressor()
49/15: lgm.fit(X,y)
49/16: lgm.predict(X)
49/17: q.fit(df, y)
49/18:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
#        est_kwargs = self.get_params()
#        del est_kwargs['est_class']
#        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model_.fit(X_copy, y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs)
49/19: LGBMRegressor
49/20:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
49/21: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
49/22: df.dtypes
49/23: q.fit(df, y)
49/24:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
#        est_kwargs = self.get_params()
#        del est_kwargs['est_class']
#        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        self.model_.fit(X_copy, y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs)
49/25: LGBMRegressor
49/26:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
49/27: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
49/28: df.dtypes
49/29: q.fit(df, y)
49/30: y_my = q.predict(df)
49/31:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
#        est_kwargs = self.get_params()
#        del est_kwargs['est_class']
#        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        super().fit(X_copy, y)
#        self.model_.fit(X_copy, y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs)
49/32: LGBMRegressor
49/33:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
49/34: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
49/35: df.dtypes
49/36: q.fit(df, y)
49/37: y_my = q.predict(df)
49/38:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
#        est_kwargs = self.get_params()
#        del est_kwargs['est_class']
#        self.model_ = self.est_class(**est_kwargs)
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        super().fit(X_copy, y, **kwargs)
#        self.model_.fit(X_copy, y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs)
49/39: LGBMRegressor
49/40:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
49/41: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
49/42: df.dtypes
49/43: q.fit(df, y)
49/44:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
#        est_kwargs = self.get_params()
#        del est_kwargs['est_class']
#        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(self.cols, axis=1)
        super().fit(X_copy, y, **kwargs)
#        self.model_.fit(X_copy, y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs)
49/45: LGBMRegressor
49/46:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
49/47: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
49/48: df.dtypes
49/49: q.fit(df, y)
49/50:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__()
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
#        est_kwargs = self.get_params()
#        del est_kwargs['est_class']
#        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(self.cols, axis=1)
        super().fit(X_copy, y, **kwargs)
#        self.model_.fit(X_copy, y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs)
49/51: LGBMRegressor
49/52:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
49/53: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
49/54: q.fit(df, y)
49/55: q
49/56: lgm = LGBMRegressor()
49/57: lgm
49/58: q.super()
49/59: q.super
49/60: q
49/61: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
49/62: q
49/63:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
 #       super().__init__()
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
#        est_kwargs = self.get_params()
#        del est_kwargs['est_class']
#        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(self.cols, axis=1)
        super().fit(X_copy, y, **kwargs)
#        self.model_.fit(X_copy, y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs)
49/64:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
49/65: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
49/66: lgm = LGBMRegressor()
49/67: lgm
49/68: q
49/69: q.fit(df, y)
49/70:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
#        est_kwargs = self.get_params()
#        del est_kwargs['est_class']
#        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(self.cols, axis=1)
        super().fit(X_copy, y, **kwargs)
#        self.model_.fit(X_copy, y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs)
49/71:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
49/72: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
49/73: lgm = LGBMRegressor()
49/74: lgm
49/75: q
49/76: q.fit(df, y)
49/77:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
#        est_kwargs = self.get_params()
#        del est_kwargs['est_class']
#        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(self.cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        self.model_.fit(X_copy, y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs)
49/78:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
49/79: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
49/80: lgm = LGBMRegressor()
49/81: lgm
49/82: q
49/83: q.fit(df, y)
49/84:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(self.cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        self.model_.fit(X_copy, y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy)
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy, y, **kwargs)
49/85:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
49/86: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
49/87: lgm = LGBMRegressor()
49/88: lgm
49/89: q
49/90: q.fit(df, y)
49/91: X
49/92: X.dtype
49/93: X
49/94: q.fit(X, y)
49/95: df.dtype
49/96: X.dtype
49/97: X.to_numpy()
49/98: df.to_numpy()
49/99:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(self.cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
49/100:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
49/101: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
49/102: lgm = LGBMRegressor()
49/103: lgm
49/104: q
49/105: q.fit(X, y)
49/106: q.fit(df, y)
49/107: y_my = q.predict(df)
49/108: y_my
49/109:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, cols, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(self.cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
49/110: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
49/111: q
49/112: q.fit(df, y)
49/113: y_my = q.predict(df)
49/114: q.fit(cols_to_ignore, df, y)
49/115: y_my = q.predict(df)
49/116: y_my
49/117:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
49/118:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
49/119: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
49/120: q
49/121: q.fit(cols_to_ignore, df, y)
49/122: q.fit(df, y)
49/123: q.fit(df, y)
49/124:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(self.cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
49/125: q.fit(df, y)
49/126:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(self.cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
49/127:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
49/128: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
49/129: q
49/130: q.fit(df, y)
49/131:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        self.model_ = self.est_class(cols, **est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(self.cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
49/132:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
49/133: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
49/134: q
49/135: q.fit(df, y)
49/136:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        cols = self.cols
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
49/137:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
49/138: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
49/139: q
49/140: q.fit(df, y)
49/141: y_my = q.predict(df)
49/142:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        cols = self.cols
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(cols, axis=1)
        super().fit(X_copy, y, **kwargs)
#        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
49/143:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
49/144: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
49/145: q
49/146: q.fit(df, y)
49/147:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        cols = self.cols
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
49/148:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
49/149: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
49/150: q
49/151: q.fit(df, y)
49/152: y_my = q.predict(df)
49/153: y_my
49/154: #cross_validate(q, df, df['a'], cv=5)
49/155: cross_validate(q, df, df['a'], cv=5)
49/156:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        cols = self.cols
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
49/157:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
49/158: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
49/159: q
49/160: q.fit(df, y)
50/1:
# –ú—ã —Ö–æ—Ç–∏–º –Ω–∞–ø–∏—Å–∞—Ç—å –æ–±–µ—Ä—Ç–∫—É –¥–ª—è LGBMRegressor 
# –≤ –∫–æ—Ç–æ—Ä—É—é –º—ã –±—ã —É–º–µ–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞, 
# –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏. 

# –û—Å–Ω–æ–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å - –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —É–º–µ—Ç—å –ø—Ä–∏—Ç–≤–æ—Ä—è—Ç—å—Å—è 
# —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º sklearn —ç—Å—Ç–∏–º–∞—Ç–æ—Ä–æ–º.
# –¢–æ –µ—Å—Ç—å —Ä–∞–±–æ–∞—Ç—å –±–µ–∑ –ø—Ä–æ–±–ª–µ–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, c cross_validate.

# –ü—Ä–æ–¥—É–º–∞–π –∫–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –∫–ª–∞—Å—Å–µ,
# –∫–∞–∫ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LGBMRegressor —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ —Ç–¥.

# hint:
# 1) –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç sklearn.base.clone 
# 2) –ú–æ–∂–µ—Ç –ª–∏ –ø–æ–º–æ—á—å –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤? -- –¥–∞, —É–Ω–∞—Å–ª–µ–¥–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥—ã –æ—Ç —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ
50/2:
from sklearn.base import clone
from lightgbm import LGBMRegressor
import copy
from sklearn.model_selection import cross_validate
50/3:
import numpy as np
import pandas as pd
50/4: from sklearn.datasets import load_boston
50/5: X, y = load_boston(return_X_y=True)
50/6: df = pd.DataFrame(X, columns = ['a','b','c','d','e','f','g','h','i','j','k','l','m'])
50/7: df.head()
50/8:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        cols = self.cols
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
50/9:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
50/10: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
50/11: q
50/12: q.fit(df, y)
50/13: y_my = q.predict(df)
50/14: cross_validate(q, df, df['a'], cv=5)
50/15: from sklearn.datasets import make_regression
50/16: X, y = load_boston(return_X_y=True)
50/17: df = pd.DataFrame(X, columns = ['a','b','c','d','e','f','g','h','i','j','k','l','m'])
50/18: df.head()
50/19:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        cols = self.cols
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
50/20:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
50/21: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
50/22: q
50/23: q.fit(df, y)
50/24: y_my = q.predict(df)
50/25: cross_validate(q, df, df['a'], cv=5)
50/26:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        print(est_kwargs)
        cols = self.cols
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
50/27:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
50/28: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
50/29: q
50/30: q.fit(df, y)
50/31:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs[cols]
        print(est_kwargs)
        cols = self.cols
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
50/32:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
50/33: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
50/34: q
50/35: q.fit(df, y)
50/36:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
#        del est_kwargs[cols]
        print(est_kwargs)
        cols = self.cols
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
50/37:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
50/38: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
50/39: q
50/40: q.fit(df, y)
50/41:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        params_dic = {param: getattr(self, param)
                      for param in self._param_names}
        print(params_dic)
        return params_dic
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs[key]
        print(est_kwargs)
        cols = self.cols
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
50/42:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
50/43: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
50/44: q
50/45: q.fit(df, y)
50/46:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        params_dic = {param: getattr(self, param)
                      for param in self._param_names}
        del params_dic[cols]
        print(params_dic)
        return params_dic
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs[key]
        print(est_kwargs)
        cols = self.cols
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
50/47:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
50/48: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
50/49: q
50/50:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        params_dic = {param: getattr(self, param)
                      for param in self._param_names}
        del params_dic[cols]
        print(params_dic)
        return params_dic
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs[key]
        print(est_kwargs)
        cols = self.cols
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
50/51:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        params_dic = {param: getattr(self, param)
                      for param in self._param_names}
        del params_dic['"cols"']
        print(params_dic)
        return params_dic
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs[key]
        print(est_kwargs)
        cols = self.cols
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
50/52:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        params_dic = {param: getattr(self, param)
                      for param in self._param_names}
        del params_dic['cols']
        print(params_dic)
        return params_dic
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs[key]
        print(est_kwargs)
        cols = self.cols
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
50/53:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
50/54: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
50/55: q
50/56:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        params_dic = {param: getattr(self, param)
                      for param in self._param_names}
        del params_dic['cols']
        return params_dic
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs[key]
        print(est_kwargs)
        cols = self.cols
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
50/57:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
50/58: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
50/59: q
50/60: q.fit(df, y)
50/61:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        params_dic = {param: getattr(self, param)
                      for param in self._param_names}
        del params_dic['cols']
        return params_dic
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        cols = self.cols
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
50/62:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
50/63: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
50/64: q
50/65: q.fit(df, y)
50/66: y_my = q.predict(df)
50/67: cross_validate(q, df, df['a'], cv=5)
50/68:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        params_dic = {param: getattr(self, param)
                      for param in self._param_names}
        del params_dic['cols']
        return params_dic
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
#        cols = self.cols
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(self.cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
50/69:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
50/70: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
50/71: q
50/72: q.fit(df, y)
50/73: y_my = q.predict(df)
50/74: cross_validate(q, df, df['a'], cv=5)
50/75:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        params_dic = {param: getattr(self, param)
                      for param in self._param_names}
        del params_dic['cols']
        return params_dic
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
#        cols = self.cols
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(self.cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
50/76:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
50/77: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
50/78: q
50/79: q.fit(df, y)
50/80: y_my = q.predict(df)
50/81: cross_validate(q, df, df['a'], cv=5)
50/82:
class CustomModel(LGBMRegressor):
    def __init__(self, cols=[], est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        params_dic = {param: getattr(self, param)
                      for param in self._param_names}
        del params_dic['cols']
        return params_dic
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
#        cols = self.cols
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(self.cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
50/83:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
50/84: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
50/85: q
50/86: q.fit(df, y)
50/87: y_my = q.predict(df)
50/88: cross_validate(q, df, df['a'], cv=5)
50/89:
class CustomModel(LGBMRegressor):
    def __init__(self, cols=[], est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        params_dic = {param: getattr(self, param)
                      for param in self._param_names}
        del params_dic['cols']
        return params_dic
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
#        cols = self.cols
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        print(X.cols)
        X_copy = X.drop(self.cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
50/90:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
50/91: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
50/92: q
50/93: q.fit(df, y)
50/94: df
50/95: df.columns
50/96:
class CustomModel(LGBMRegressor):
    def __init__(self, cols=[], est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        params_dic = {param: getattr(self, param)
                      for param in self._param_names}
        del params_dic['cols']
        return params_dic
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
#        cols = self.cols
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        print(X.columns)
        X_copy = X.drop(self.cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
50/97:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
50/98: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
50/99: q
50/100: q.fit(df, y)
50/101:
class CustomModel(LGBMRegressor):
    def __init__(self, cols=[], est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        params_dic = {param: getattr(self, param)
                      for param in self._param_names}
        del params_dic['cols']
        return params_dic
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
#        cols = self.cols
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(self.cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        print(X_copy.columns)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
50/102:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
50/103: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
50/104: q
50/105: q.fit(df, y)
50/106: df.columns
50/107: df.columns
50/108: y_my = q.predict(df)
50/109: cross_validate(q, df, df['a'], cv=5)
50/110:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        params_dic = {param: getattr(self, param)
                      for param in self._param_names}
        del params_dic['cols']
        return params_dic
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
#        cols = self.cols
        del est_kwargs['est_class']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(self.cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        print(X_copy.columns)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
50/111:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
50/112: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
50/113: q
50/114: q.fit(df, y)
50/115: df.columns
50/116: y_my = q.predict(df)
50/117: cross_validate(q, df, df['a'], cv=5)
50/118:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                      for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
#        cols = self.cols
        del est_kwargs['est_class']
        del est_kwargs['cols']
        self.model_ = self.est_class(**est_kwargs)
#        X_copy = copy.deepcopy(X)
        X_copy = X.drop(self.cols, axis=1)
#        super().fit(X_copy, y, **kwargs)
        print(X_copy.columns)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
50/119:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
50/120: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
50/121: q
50/122: q.fit(df, y)
50/123: df.columns
50/124: y_my = q.predict(df)
50/125: cross_validate(q, df, df['a'], cv=5)
50/126:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                      for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        del est_kwargs['cols']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = X.drop(self.cols, axis=1)
        print(X_copy.columns)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X.drop(self.cols, axis=1, inplace=True)
        y_pred = self.model_.predict(X)
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
50/127:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
50/128: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
50/129: q
50/130: q.fit(df, y)
50/131: df.columns
50/132: y_my = q.predict(df)
50/133:
from sklearn.base import clone
from lightgbm import LGBMRegressor
import copy
from sklearn.model_selection import cross_validate
from sklearn.model_selection import train_test_split
50/134:
import numpy as np
import pandas as pd
50/135: from sklearn.datasets import make_regression
50/136: X, y = load_boston(return_X_y=True)
50/137: df = pd.DataFrame(X, columns = ['a','b','c','d','e','f','g','h','i','j','k','l','m'])
50/138: y
50/139: X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=0)
50/140: X_train
50/141:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                      for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        del est_kwargs['cols']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = X.drop(self.cols, axis=1)
        print(X_copy.columns)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X.drop(self.cols, axis=1, inplace=True)
        y_pred = self.model_.predictXX_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
50/142:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                      for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        del est_kwargs['cols']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = X.drop(self.cols, axis=1)
        print(X_copy.columns)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X.drop(self.cols, axis=1, inplace=True)
        y_pred = self.model_.predict(X.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = copy.deepcopy(X)
        X_copy = X_copy.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
50/143:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
50/144: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
50/145: q
50/146: q.fit(df, y)
50/147: df.columns
50/148: y_my = q.predict(df)
50/149:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                      for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        del est_kwargs['cols']
        self.model_ = self.est_class(**est_kwargs)
        X.drop(self.cols, axis=1)
        print(X_copy.columns, inplace=True)
        self.model_.fit(X.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X.drop(self.cols, axis=1, inplace=True)
        y_pred = self.model_.predict(X.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X.drop(self.cols, axis=1, inplace=True)
        return self.model_.score(X.to_numpy(), y, **kwargs)
50/150:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
50/151: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
50/152: q
50/153: q.fit(df, y)
50/154:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                      for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        del est_kwargs['cols']
        self.model_ = self.est_class(**est_kwargs)
        X.drop(self.cols, axis=1)
        print(X.columns, inplace=True)
        self.model_.fit(X.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X.drop(self.cols, axis=1, inplace=True)
        y_pred = self.model_.predict(X.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X.drop(self.cols, axis=1, inplace=True)
        return self.model_.score(X.to_numpy(), y, **kwargs)
50/155:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
50/156: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
50/157: q
50/158: q.fit(df, y)
50/159: X
50/160: df
50/161: X, y = load_boston(return_X_y=True)
50/162: df = pd.DataFrame(X, columns = ['a','b','c','d','e','f','g','h','i','j','k','l','m'])
50/163: df
50/164:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        X_copy = X.drop(self.cols, axis=1)
        super().__init__(X=X_copy, **kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                      for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        del est_kwargs['cols']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = X.drop(self.cols, axis=1)
        print(X_copy.columns)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X.drop(self.cols, axis=1, inplace=True)
        y_pred = self.model_.predict(X.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X.drop(self.cols, axis=1, inplace=True)
        return self.model_.score(X.to_numpy(), y, **kwargs)
50/165:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
50/166: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
50/167:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                      for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        del est_kwargs['cols']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = X.drop(self.cols, axis=1)
        print(X_copy.columns)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = X.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = X.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
50/168:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
50/169: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
50/170: from sklearn.datasets import make_regression
50/171: X, y = load_boston(return_X_y=True)
50/172: df = pd.DataFrame(X, columns = ['a','b','c','d','e','f','g','h','i','j','k','l','m'])
50/173: X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=0)
50/174: df
50/175:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys())
        self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                      for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        del est_kwargs['cols']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = X.drop(self.cols, axis=1)
        print(X_copy.columns)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = X.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = X.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
50/176:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
50/177: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
50/178: q
50/179: q.fit(df, y)
50/180: df.columns
50/181: y_my = q.predict(df)
50/182: cross_validate(q, df, df['a'], cv=5)
50/183:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys()) + ['cols']
  #      self._param_names.append('cols')
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                      for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        del est_kwargs['cols']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = X.drop(self.cols, axis=1)
        print(X_copy.columns)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = X.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = X.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
50/184:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
50/185: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
50/186: q
50/187: q.fit(df, y)
50/188: df.columns
50/189: y_my = q.predict(df)
50/190: cross_validate(q, df, df['a'], cv=5)
50/191:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys()) + ['cols']
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                      for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class', 'cols']
        del est_kwargs['cols']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = X.drop(self.cols, axis=1)
        print(X_copy.columns)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = X.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = X.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
50/192:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
50/193: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
50/194: q
50/195: q.fit(df, y)
50/196:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys()) + ['cols']
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                      for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        del est_kwargs['cols']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = X.drop(self.cols, axis=1)
#        print(X_copy.columns)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = X.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = X.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
50/197:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
50/198: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
50/199: q
50/200: q.fit(df, y)
50/201: df.columns
50/202: y_my = q.predict(df)
50/203: cross_validate(q, df, df['a'], cv=5)
51/1:
# –ú—ã —Ö–æ—Ç–∏–º –Ω–∞–ø–∏—Å–∞—Ç—å –æ–±–µ—Ä—Ç–∫—É –¥–ª—è LGBMRegressor 
# –≤ –∫–æ—Ç–æ—Ä—É—é –º—ã –±—ã —É–º–µ–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞, 
# –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏. 

# –û—Å–Ω–æ–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å - –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —É–º–µ—Ç—å –ø—Ä–∏—Ç–≤–æ—Ä—è—Ç—å—Å—è 
# —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º sklearn —ç—Å—Ç–∏–º–∞—Ç–æ—Ä–æ–º.
# –¢–æ –µ—Å—Ç—å —Ä–∞–±–æ–∞—Ç—å –±–µ–∑ –ø—Ä–æ–±–ª–µ–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, c cross_validate.

# –ü—Ä–æ–¥—É–º–∞–π –∫–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –∫–ª–∞—Å—Å–µ,
# –∫–∞–∫ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LGBMRegressor —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ —Ç–¥.

# hint:
# 1) –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç sklearn.base.clone 
# 2) –ú–æ–∂–µ—Ç –ª–∏ –ø–æ–º–æ—á—å –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤? -- –¥–∞, —É–Ω–∞—Å–ª–µ–¥–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥—ã –æ—Ç —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ
51/2:
from lightgbm import LGBMRegressor
from sklearn.model_selection import cross_validate
51/3:
import numpy as np
import pandas as pd
51/4: #from sklearn.datasets import make_regression
51/5:
from lightgbm import LGBMRegressor
from sklearn.model_selection import cross_validate
51/6:
import numpy as np
import pandas as pd
51/7: #from sklearn.datasets import make_regression
51/8: X, y = load_boston(return_X_y=True)
51/9: from sklearn.datasets import make_regression
51/10: X, y = load_boston(return_X_y=True)
51/11: X, y = make_regression(return_X_y=True)
51/12: from sklearn.datasets import load_boston
51/13: X, y = load_boston(return_X_y=True)
51/14: df = pd.DataFrame(X, columns = ['a','b','c','d','e','f','g','h','i','j','k','l','m'])
51/15:
class CustomModel(LGBMRegressor):
    def __init__(self, cols, est_class=LGBMRegressor, **kwargs):
        print('You create me')
        self.cols = {}
        self.cols = cols
        self.est_class = est_class
        for key, value in kwargs.items():
            setattr(self, key, value)
        setattr(self, 'cols', cols)
        self._param_names = ['est_class'] + list(kwargs.keys()) + ['cols']
        super().__init__(**kwargs)
        

    def get_params(self, deep=True):
        return {param: getattr(self, param)
                      for param in self._param_names}
    

    def fit(self, X, y, **kwargs):
        est_kwargs = self.get_params()
        del est_kwargs['est_class']
        del est_kwargs['cols']
        self.model_ = self.est_class(**est_kwargs)
        X_copy = X.drop(self.cols, axis=1)
#        print(X_copy.columns)
        self.model_.fit(X_copy.to_numpy(), y, **kwargs)
        return self
    

    def predict(self, X):
        X_copy = X.drop(self.cols, axis=1)
        y_pred = self.model_.predict(X_copy.to_numpy())
        return y_pred
    
    
    def score(self, X, y, **kwargs):
        X_copy = X.drop(self.cols, axis=1)
        return self.model_.score(X_copy.to_numpy(), y, **kwargs)
51/16:
### –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∞
cols_to_ignore = ['a', 'd', 'f']
51/17: q = CustomModel(cols = cols_to_ignore, n_estimators=400, max_depth = 2)
51/18: q
51/19: q.fit(df, y)
51/20: df.columns
51/21: df.columns
51/22: y_my = q.predict(df)
51/23: cross_validate(q, df, df['a'], cv=5)
52/1: import pandas as pd
52/2: df = pd.read_csv('transactions_data.txt', header = None)
52/3: df
52/4: df = pd.read_csv('transactions_data.txt')
52/5: df
52/6: df = pd.read_csv('transactions_data.txt', header = None)
52/7: df
52/8: df.resample('W').sum().head()
52/9: df.resample('W')
52/10:
df_1 = pd.DataFrame({'start_date': ['01/01/2019', '01/01/2019', '02/15/2019'],
                     'end_date': ['01/16/2019', '01/21/2019', '02/20/2019'],
                     'product_ID': ['A', 'B', 'A'],
                     'price': [100, 150, 150],
                     'org_price': [200, 300, 200]})
df_1
52/11: df_1.resample('W').sum().head()
52/12: df_1.dtypes()
52/13: df_1.dtypes
52/14:
df_1.start_date = pd.to_datetime(df_1.start_date)    
df_1.end_date = pd.to_datetime(df_1.end_date)
52/15: df_1.dtypes
52/16: df_1.resample('W').sum().head()
52/17: df_1
52/18: df_1.start_date.resample('W').sum().head()
52/19: df_1.story_point.resample('W', on='start_date').transform('sum')
52/20: df_1.start_date.resample('W', on='start_date').transform('sum')
52/21:
df_1['start_date'] = pd.to_datetime(df_1['start_date'])
weekly_summary = df_1.story_point.resample('W', on='start_date').sum()
52/22:
df_1['start_date'] = pd.to_datetime(df_1['start_date'])
weekly_summary = df_1.org_price.resample('W', on='start_date').sum()
52/23: df_1['start_date'] = pd.to_datetime(df_1['start_date'])
52/24:
df_1['start_date'] = pd.to_datetime(df_1['start_date'])
weekly_summary = df_1.org_price.resample('W', on='start_date').sum()
52/25: df_1
52/26:
df_1['start_date'] = pd.to_datetime(df_1['start_date'])
weekly_summary = df_1.org_price.resample('W', on='end_date').sum()
52/27:
df_1 = pd.DataFrame({'start_date': ['01/01/2019', '01/01/2019', '02/15/2019'],
                     'end_date': ['01/16/2019', '01/21/2019', '02/20/2019'],
                     'product_ID': ['A', 'B', 'A'],
                     'price': [100, 150, 150],
                     'org_price': [200, 300, 200]})
df_1
52/28:
df_1.start_date = pd.to_datetime(df_1.start_date)    
df_1.end_date = pd.to_datetime(df_1.end_date)
52/29: df_1
52/30: df_1.dtypes()
52/31: df_1.dtypes
52/32: df_1.resample('7D',on='start_date').sum()
52/33: df_1.resample('7D',on='start_date').mean()
52/34: df_1.resample('7D',on='start_date').max()
52/35: df_1.resample('W',on='start_date').max()
52/36:
df_1 = pd.DataFrame({'start_date': ['01/01/2019', '01/01/2019', '02/15/2019'],
                     'end_date': ['01/16/2019', '01/21/2019', '02/20/2019'],
                     'product_ID': ['A', 'B', 'A'],
                     'price': [100, 150, 150],
                     'org_price': [200, 300, 200]})
df_1
52/37:
df_1.start_date = pd.to_datetime(df_1.start_date)    
df_1.end_date = pd.to_datetime(df_1.end_date)
52/38: df_1.dtypes
52/39:
df_1['start_date'] = pd.to_datetime(df_1['start_date'])
df_1.dtypes()
52/40: df_1.resample('W',on='start_date').max()
52/41: df_1
52/42: dfdf_1['row'] = range(len(df_1))
52/43: df_1['row'] = range(len(df_1))
52/44: df_1
52/45: df = df_1
52/46:
starts = df[['START_TIME','TRIAL_No','itemnr','row']].rename(columns={'START_TIME':'Bin_time'})
ends = df[['END_TIME','TRIAL_No','itemnr','row']].rename(columns={'END_TIME':'Bin_time'})
df = pd.concat([starts, ends])
df = df.set_index('row', drop=True)
df = df.sort_index()
52/47:
starts = df[['start_date','price','product_ID','row']].rename(columns={'start_date':'Bin_time'})
ends = df[['end_date','price','product_ID','row']].rename(columns={'end_date':'Bin_time'})
df = pd.concat([starts, ends])
df = df.set_index('row', drop=True)
df = df.sort_index()
52/48: df
52/49: starts
52/50: ends
52/51: df['Bin_time'] = df['Bin_time'].astype('timedelta64[ms]')
52/52: df
52/53:
df = df.groupby(df.index).apply(lambda x: x.set_index('Bin_time').resample('100ms',how='first',fill_method='ffill'))

df = df.reset_index()
df = df.drop(['row'], axis=1)

#convert timedelta to integer back
df['Bin_time'] = (df['Bin_time'] / np.timedelta64(1, 'ms')).astype(int)
52/54:
import pandas as pd
import numpy as np
52/55:
df = df.groupby(df.index).apply(lambda x: x.set_index('Bin_time').resample('100ms',how='first',fill_method='ffill'))

df = df.reset_index()
df = df.drop(['row'], axis=1)

#convert timedelta to integer back
df['Bin_time'] = (df['Bin_time'] / np.timedelta64(1, 'ms')).astype(int)
52/56: df.set_index('Bin_time').resample('100ms',how='first',fill_method='ffill'))
52/57: df.set_index('Bin_time').resample('100ms',how='first',fill_method='ffill')
52/58: df.resample('W',on='Bin_time').max()
52/59: df.resample('D',on='Bin_time').max()
52/60: df_q
52/61: df_1
52/62: df
52/63: df
52/64:
starts = df[['start_date','price','product_ID','row']].rename(columns={'start_date':'Bin_time'})
ends = df[['end_date','price','product_ID','row']].rename(columns={'end_date':'Bin_time'})
df = pd.concat([starts, ends])
df = df.set_index('row', drop=True)
df = df.sort_index()
52/65: starts
52/66: df_1
52/67: df_1['row'] = range(len(df_1))
52/68: df = df_1
52/69:
starts = df[['start_date','price','product_ID','row']].rename(columns={'start_date':'Bin_time'})
ends = df[['end_date','price','product_ID','row']].rename(columns={'end_date':'Bin_time'})
df = pd.concat([starts, ends])
df = df.set_index('row', drop=True)
df = df.sort_index()
52/70: starts
52/71: ends
52/72: df
52/73: df.resample('D',on='Bin_time').max()
52/74: df_1
52/75:
starts = df[['start_date','price', 'org_price', 'product_ID','row']].rename(columns={'start_date':'Bin_time'})
ends = df[['end_date','price', 'org_price','product_ID','row']].rename(columns={'end_date':'Bin_time'})
df = pd.concat([starts, ends])
df = df.set_index('row', drop=True)
df = df.sort_index()
52/76: df_1['row'] = range(len(df_1))
52/77: df = df_1
52/78:
starts = df[['start_date','price', 'org_price', 'product_ID','row']].rename(columns={'start_date':'Bin_time'})
ends = df[['end_date','price', 'org_price','product_ID','row']].rename(columns={'end_date':'Bin_time'})
df = pd.concat([starts, ends])
df = df.set_index('row', drop=True)
df = df.sort_index()
52/79: starts
52/80: df
52/81: df.set_index('Bin_time').resample('D',how='first',fill_method='ffill')
52/82: df.set_index('Bin_time').resample('D',how='first')
52/83: df.set_index('Bin_time').resample('W',how='first',fill_method='ffill')
52/84: df.set_index('Bin_time').resample('D',how='first',fill_method='ffill')
52/85: df.set_index('Bin_time').resample('D',fill_method='ffill')
52/86: df.set_index('Bin_time').resample('D',how='last',fill_method='ffill')
52/87: df.set_index('Bin_time').resample('D')
52/88: df.set_index('Bin_time').resample('D').head(3)
52/89: df.set_index('Bin_time').resample('D',how='last',fill_method='ffill')
52/90: start_date  .resample('D').sum()
52/91: start_date.resample('D').sum()
52/92: df.start_date.resample('D').sum()
52/93: df.Bin_time.resample('D').sum()
52/94: df.Bin_time.resample('D')
52/95: df.Bin_time.resample('D',how='last',fill_method='ffill')
52/96:
df_1["date"] = df.apply(
    lambda x: pd.date_range(x["start_date"], x["end_date"]), axis=1
)
52/97: df_1
52/98:
df_1["date"] = df_1.apply(
    lambda x: pd.date_range(x["start_date"], x["end_date"]), axis=1
)
52/99: df_1
52/100:
df_1 = (
    df_1.explode("date", ignore_index=True)
    .drop(columns=["start_date", "end_date"])
)
52/101:
df_1 = (
    df_1.explode("date")
    .drop(columns=["start_date", "end_date"])
)
52/102: df_1
52/103: df_1.set_index('date').resample('d',).first().ffill().reset_index()
52/104: df_1.set_index('date').resample('w',).first().ffill().reset_index()
52/105: df_1.set_index('date').resample('w',).last().ffill().reset_index()
52/106: df_1
52/107:
df_1.resample('W', on='date')\
        .mean().ffill().astype(int)\
        .reset_index()
52/108: df_1
52/109:
df_1.resample('W', on='date')\
        .max().ffill().astype(int)\
        .reset_index()
52/110:
df_1.resample('W', on='date')\
        .sum().ffill().astype(int)\
        .reset_index()
52/111:
df_1.resample('W', on='date')\
        .avg().ffill().astype(int)\
        .reset_index()
52/112:
df_1.resample('W', on='date')\
        .mean().ffill().astype(int)\
        .reset_index()
52/113:
df_1.resample('D', on='date')\
        .mean().ffill().astype(int)\
        .reset_index()
52/114: df_1.set_index('date').resample('w',).last().ffill().reset_index()
52/115: df_1.set_index('date').resample('D',).last().ffill().reset_index()
52/116: df_1.set_index('date').resample('w',).last().ffill().reset_index()
52/117:
df_1.resample('–¶', on='date')\
        .mean().ffill().astype(int)\
        .reset_index()
52/118:
df_1.resample('W', on='date')\
        .mean().ffill().astype(int)\
        .reset_index()
52/119: df_1.set_index('date').resample('w',).first().ffill().reset_index()
52/120: df_1.set_index('date').resample('w',).last().ffill().reset_index()
52/121:
df_1 = (
    df_1.explode("date")
    .drop(columns=["start_date", "end_date"])
)
52/122: df_1
52/123:
df_1 = pd.DataFrame({'start_date': ['01/01/2019', '01/01/2019', '02/15/2019'],
                     'end_date': ['01/16/2019', '01/21/2019', '02/20/2019'],
                     'product_ID': ['A', 'B', 'A'],
                     'price': [100, 150, 150],
                     'org_price': [200, 300, 200]})
df_1
52/124:
df_1.start_date = pd.to_datetime(df_1.start_date)    
df_1.end_date = pd.to_datetime(df_1.end_date)
52/125: df_1.dtypes
52/126:
df_1['start_date'] = pd.to_datetime(df_1['start_date'])
df_1.dtypes()
52/127: df_1.resample('W',on='start_date').max()
52/128: df_1
52/129:
df_1["date"] = df_1.apply(
    lambda x: pd.date_range(x["start_date"], x["end_date"]), axis=1
)
52/130: df_1
52/131:
df_1 = (
    df_1.explode("date")
    .drop(columns=["start_date", "end_date"])
)
52/132: df_1
52/133: df_1.resample('W').asfreq()
52/134: df_1.date.resample('W').asfreq()
52/135: df_1.set_index('date', inplace=True)
52/136: df_1.resample('W').asfreq()
52/137:
df_1 = pd.DataFrame({'start_date': ['01/01/2019', '01/01/2019', '02/15/2019'],
                     'end_date': ['01/16/2019', '01/21/2019', '02/20/2019'],
                     'product_ID': ['A', 'B', 'A'],
                     'price': [100, 150, 150],
                     'org_price': [200, 300, 200]})
df_1
52/138:
df_1.start_date = pd.to_datetime(df_1.start_date)    
df_1.end_date = pd.to_datetime(df_1.end_date)
52/139: df_1.dtypes
52/140:
df_1['start_date'] = pd.to_datetime(df_1['start_date'])
df_1.dtypes()
52/141: df_1.resample('W',on='start_date').max()
52/142: df_1
52/143:
df_1["date"] = df_1.apply(
    lambda x: pd.date_range(x["start_date"], x["end_date"]), axis=1
)
52/144: df_1
52/145:
df_1 = (
    df_1.explode("date")
    .drop(columns=["start_date", "end_date"])
)
52/146: df_1.resample('W').asfreq()
52/147: df_1.set_index('date').resample('w',).first().ffill().reset_index()
52/148:
df_1.resample('W', on='date')\
        .mean().ffill().astype(int)\
        .reset_index()
52/149: df_1
52/150: df_1
52/151:
df_1.resample('W', on='date')\
        .mean().ffill().astype(int)\
52/152: df_1.resample('W', on='date').mean()
52/153: df_1.groupby('product_ID').resample('W', on='date').mean()
52/154: df_1.groupby('product_ID').resample('W', on='date').mean().reset_index()
52/155: df_1.groupby('product_ID')
52/156: df_1.groupby('product_ID').resample('W', on='date').max().reset_index()
52/157: df_1.groupby('product_ID').resample('W', on='date').sum().reset_index()
52/158: df_1.groupby('product_ID').resample('W', on='date').mean().reset_index()
52/159: df_1.groupby('product_ID').resample('W', on='date').bfill().reset_index()
52/160: df_1.groupby('product_ID').resample('W', on='date').mean().reset_index()
52/161: df_1.groupby('product_ID').resample('W', on='date').bfill().mean().reset_index()
52/162:
df_1.set_index('date').groupby('product_ID').resample('W').asfreq()
              .reset_index()
52/163: df_1.set_index('date').groupby('product_ID').resample('W').asfreq().reset_index()
52/164:
df_1.set_index('date').groupby('product_ID').resample('W').asfreq()
              .drop(['product_ID', 'index'], 1).reset_index()
52/165: df_1.set_index('date').groupby('product_ID').resample('W').asfreq().drop(['product_ID', 'index'], 1).reset_index()
52/166: df_1.set_index('date').groupby('product_ID').resample('W').asfreq().drop(['product_ID', 'date'], 1).reset_index()
52/167: df_1.set_index('date').groupby('product_ID').resample('W').asfreq().drop(['product_ID', 'index'], 1).reset_index()
52/168: df_1.set_index('date').groupby('product_ID').resample('W').asfreq().drop(['product_ID'], 1).reset_index()
52/169: df_1.set_index('date').groupby('product_ID').resample('W').asfreq().drop(['product_ID'], 1).ffill().astype(int).reset_index()
52/170: df_1.set_index('date').groupby('product_ID').resample('W').asfreq().drop(['product_ID'], 1).bfill().astype(int).reset_index()
52/171: df_1.set_index('date').groupby('product_ID').resample('W').asfreq().drop(['product_ID'], 1).fill(0).astype(int).reset_index()
52/172: df_1.set_index('date').groupby('product_ID').resample('W').asfreq().drop(['product_ID'], 1).ffill(0).astype(int).reset_index()
52/173: df_1.set_index('date').groupby('product_ID').resample('W').asfreq().drop(['product_ID'], 1).reset_index()
52/174: df_1
52/175: df_1['Date'] - pd.to_timedelta(df_1['Date'].dt.dayofweek, unit='d')
52/176: df_1['start_date'] - pd.to_timedelta(df_1['start_date'].dt.dayofweek, unit='d')
52/177: df_1
52/178: df
52/179:
df_1 = pd.DataFrame({'start_date': ['01/01/2019', '01/01/2019', '02/15/2019'],
                     'end_date': ['01/16/2019', '01/21/2019', '02/20/2019'],
                     'product_ID': ['A', 'B', 'A'],
                     'price': [100, 150, 150],
                     'org_price': [200, 300, 200]})
df_1
52/180: df_1['start_date'] - pd.to_timedelta(df_1['start_date'].dt.dayofweek, unit='d')
52/181:
df_1 = pd.DataFrame({'start_date': ['01/01/2019', '01/01/2019', '02/15/2019'],
                     'end_date': ['01/16/2019', '01/21/2019', '02/20/2019'],
                     'product_ID': ['A', 'B', 'A'],
                     'price': [100, 150, 150],
                     'org_price': [200, 300, 200]})
df_1
52/182:
df_1.start_date = pd.to_datetime(df_1.start_date)    
df_1.end_date = pd.to_datetime(df_1.end_date)
52/183: df_1.dtypes
52/184: df_1['start_date'] - pd.to_timedelta(df_1['start_date'].dt.dayofweek, unit='d')
52/185:
df_1 = pd.DataFrame({'start_date': ['01/01/2019', '01/01/2019', '02/15/2019'],
                     'end_date': ['01/16/2019', '01/21/2019', '02/20/2019'],
                     'product_ID': ['A', 'B', 'A'],
                     'price': [100, 150, 150],
                     'org_price': [200, 300, 200]})
df_1
52/186:
df_1.start_date = pd.to_datetime(df_1.start_date)    
df_1.end_date = pd.to_datetime(df_1.end_date)
52/187:
import datetime
def next_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead <= 0: # Target day already happened this week
        days_ahead += 7
    return d + datetime.timedelta(days_ahead)

d = datetime.date(2011, 7, 2)
next_monday = next_weekday(d, 0) # 0 = Monday, 1=Tuesday, 2=Wednesday...
print(next_monday)
52/188: df_1['try'] = next_weekday(df_1.start_date, 4)
52/189: df_1.start_date
52/190: df_1.start_date[0]
52/191: df_1['try'] = next_weekday(df_1.start_date[0], 4)
52/192: next_weekday(df_1.start_date[0], 4)
52/193: next_weekday(df_1.start_date[0], 5)
52/194: next_weekday(df_1.start_date[0], -2)
52/195:
def priv_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead <= 0: # Target day already happened this week
        days_ahead += 7
    return d + datetime.timedelta(days_ahead)
52/196:
def priv_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead <= 0: # Target day already happened this week
        days_ahead += 7
    return d - datetime.timedelta(days_ahead)
52/197: priv_weekday(df_1.start_date[0], 5)
52/198: next_weekday(df_1.start_date[0], 5)
52/199: priv_weekday(df_1.start_date[0], 5)
52/200: next_weekday(df_1.start_date[2], 5)
52/201: priv_weekday(df_1.start_date[2], 5)
52/202: start_date[0].weekday()
52/203: df_1.start_date[0].weekday()
52/204: next_weekday(df_1.start_date[2], 5)
52/205:
def priv_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    print(days_ahead)
    if days_ahead <= 0: # Target day already happened this week
        days_ahead += 7
    return d - datetime.timedelta(days_ahead)
52/206: df_1.start_date[0].weekday()
52/207: df_1.start_date[0]
52/208: next_weekday(df_1.start_date[2], 5)
52/209: priv_weekday(df_1.start_date[2], 5)
52/210:
def priv_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    print(d.weekday())
    print(days_ahead)
    if days_ahead <= 0: # Target day already happened this week
        days_ahead += 7
    return d - datetime.timedelta(days_ahead)
52/211: df_1.start_date[0].weekday()
52/212: df_1.start_date[0]
52/213: next_weekday(df_1.start_date[2], 5)
52/214: priv_weekday(df_1.start_date[2], 5)
52/215: df_1.start_date[2]
52/216:
def priv_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    print(d.weekday())
    print(days_ahead)
    if days_ahead <= 0: # Target day already happened this week
        days_ahead += 7
    print(days_ahead)
    return d - datetime.timedelta(days_ahead)
52/217: df_1.start_date[0].weekday()
52/218: df_1.start_date[2]
52/219: next_weekday(df_1.start_date[2], 5)
52/220: priv_weekday(df_1.start_date[2], 5)
52/221:
def priv_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    print(d.weekday())
    print(days_ahead)
    if days_ahead <= 0: # Target day already happened this week
        days_ahead += 7
    print(days_ahead)
    return d - datetime.timedelta(7) + datetime.timedelta(days_ahead)
52/222: df_1.start_date[0].weekday()
52/223: df_1.start_date[2]
52/224: next_weekday(df_1.start_date[2], 5)
52/225: priv_weekday(df_1.start_date[2], 5)
52/226: priv_weekday(df_1.start_date[2], 6)
52/227: priv_weekday(df_1.start_date[2], 4)
52/228: priv_weekday(df_1.start_date[2], 6)
52/229: priv_weekday(df_1.start_date[2], 6)
52/230: priv_weekday(df_1.start_date[2], 5)
52/231: priv_weekday(df_1.start_date[2], 4)
52/232:
def priv_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    print(d.weekday())
    print(days_ahead)
  #  if days_ahead <= 0: # Target day already happened this week
  #      days_ahead += 7
    print(days_ahead)
    return d - datetime.timedelta(7) + datetime.timedelta(days_ahead)
52/233: df_1.start_date[0].weekday()
52/234: df_1.start_date[2]
52/235: next_weekday(df_1.start_date[2], 5)
52/236: priv_weekday(df_1.start_date[2], 6)
52/237: priv_weekday(df_1.start_date[2], 4)
52/238: priv_weekday(df_1.start_date[2], 7)
52/239: priv_weekday(df_1.start_date[2], 1)
52/240:
def priv_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    print(d.weekday())
    print(days_ahead)
  #  if days_ahead <= 0: # Target day already happened this week
  #      days_ahead += 7
    print(days_ahead)
    return d datetime.timedelta(days_ahead)
52/241: df_1.start_date[0].weekday()
52/242: df_1.start_date[2]
52/243: next_weekday(df_1.start_date[2], 5)
52/244: priv_weekday(df_1.start_date[2], 6)
52/245: priv_weekday(df_1.start_date[2], 1)
52/246:
def priv_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    print(d.weekday())
    print(days_ahead)
  #  if days_ahead <= 0: # Target day already happened this week
  #      days_ahead += 7
    print(days_ahead)
    return d + datetime.timedelta(days_ahead)
52/247: df_1.start_date[0].weekday()
52/248: df_1.start_date[2]
52/249: next_weekday(df_1.start_date[2], 5)
52/250: priv_weekday(df_1.start_date[2], 6)
52/251: priv_weekday(df_1.start_date[2], 1)
52/252:
df_1.start_date = pd.to_datetime(df_1.start_date)    
df_1.end_date = pd.to_datetime(df_1.end_date)
52/253: df_1.dtypes
52/254:
def priv_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    print(d.weekday())
    print(days_ahead)
    if days_ahead > 0: # Target day already happened this week
        days_ahead += 7
    print(days_ahead)
    return d + datetime.timedelta(days_ahead)
52/255: df_1.start_date[0].weekday()
52/256: df_1.start_date[2]
52/257: next_weekday(df_1.start_date[2], 5)
52/258: priv_weekday(df_1.start_date[2], 6)
52/259: priv_weekday(df_1.start_date[2], 1)
52/260:
df_1.start_date = pd.to_datetime(df_1.start_date)    
df_1.end_date = pd.to_datetime(df_1.end_date)
52/261:
def priv_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    print(d.weekday())
    print(days_ahead)
    if days_ahead > 0: # Target day already happened this week
        days_ahead -= 7
    print(days_ahead)
    return d + datetime.timedelta(days_ahead)
52/262: df_1.start_date[0].weekday()
52/263: df_1.start_date[2]
52/264: next_weekday(df_1.start_date[2], 5)
52/265: priv_weekday(df_1.start_date[2], 6)
52/266: priv_weekday(df_1.start_date[2], 1)
52/267:
df_1 = pd.DataFrame({'start_date': ['01/01/2019', '01/01/2019', '02/15/2019'],
                     'end_date': ['01/16/2019', '01/21/2019', '02/20/2019'],
                     'product_ID': ['A', 'B', 'A'],
                     'price': [100, 150, 150],
                     'org_price': [200, 300, 200]})
df_1
52/268:
def prev_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    print(d.weekday())
    print(days_ahead)
    if days_ahead > 0: # Target day already happened this week
        days_ahead -= 7
    print(days_ahead)
    return d + datetime.timedelta(days_ahead)
52/269: df_1['prev_sat'] = df_1.apply(lambda row: df_1(row['next_weekday'], 5),axis=1)
52/270: df_1['prev_sat'] = df_1.apply(lambda row: prev_weekday(row['next_weekday'], 5),axis=1)
52/271: df_1['prev_sat'] = df_1.apply(lambda row: prev_weekday(row['start_date'], 5),axis=1)
52/272: df_1['prev_sat'] = df_1.apply(lambda row: prev_weekday(row['start_date'], 5))
52/273: df_1.start_date.apply(prev_weekday,1)
52/274: df_1.apply(lambda x: prev_weekday(x['start_date'], 5), axis=1)
52/275: next_weekday(df_1.start_date[2], 5)
52/276:
import pandas as pd
import numpy as np
52/277:
df_1 = pd.DataFrame({'start_date': ['01/01/2019', '01/01/2019', '02/15/2019'],
                     'end_date': ['01/16/2019', '01/21/2019', '02/20/2019'],
                     'product_ID': ['A', 'B', 'A'],
                     'price': [100, 150, 150],
                     'org_price': [200, 300, 200]})
df_1
52/278:
import datetime
def next_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead <= 0: # Target day already happened this week
        days_ahead += 7
    return d + datetime.timedelta(days_ahead)

d = datetime.date(2011, 7, 2)
next_monday = next_weekday(d, 0) # 0 = Monday, 1=Tuesday, 2=Wednesday...
print(next_monday)
52/279:
def prev_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    print(d.weekday())
    print(days_ahead)
    if days_ahead > 0: # Target day already happened this week
        days_ahead -= 7
    print(days_ahead)
    return d + datetime.timedelta(days_ahead)
52/280:
import datetime
def next_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead <= 0: # Target day already happened this week
        days_ahead += 7
    return d + datetime.timedelta(days_ahead)

d = datetime.date(2011, 7, 2)
next_monday = next_weekday(d, 0) # 0 = Monday, 1=Tuesday, 2=Wednesday...
print(next_monday)
52/281:
def prev_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead > 0: # Target day already happened this week
        days_ahead -= 7
    return d + datetime.timedelta(days_ahead)
52/282: df_1.start_date[0].weekday()
52/283: df_1.start_date[2]
52/284: next_weekday(df_1.start_date[2], 5)
52/285:
df_1.start_date = pd.to_datetime(df_1.start_date)    
df_1.end_date = pd.to_datetime(df_1.end_date)
52/286: priv_weekday(df_1.start_date[2], 6)
52/287: priv_weekday(df_1.start_date[2], 1)
52/288: next_weekday(df_1.start_date[2], 5)
52/289: df_1.apply(lambda x: prev_weekday(x['start_date'], 5), axis=1)
52/290: df_1['prev_sat'] = df_1.apply(lambda x: prev_weekday(x['start_date'], 5), axis=1)
52/291: df_1['next_sat'] = df_1.apply(lambda x: next_weekday(x['end_date'], 5), axis=1)
52/292: df_1.dtypes
52/293: df_1
52/294:
df_1["date"] = df_1.apply(
    lambda x: pd.date_range(x["start_date"], x["end_date"]), axis=1
)
52/295: df_1
52/296:
df_1["date_month"] = df_1.apply(
    lambda x: pd.date_range(x["prev_sat"], x["next_sat"]), axis=1
)
52/297: df_1
52/298:
df_1 = (
    df_1.explode("date")
    .drop(columns=["start_date", "end_date"])
)
52/299: df_1
52/300:
df_2 = (
    df_1.explode("date_month")
    .drop(columns=["prev_sat", "next_sat"])
)
52/301: df_2
52/302:
df_1 = pd.DataFrame({'start_date': ['01/01/2019', '01/01/2019', '02/15/2019'],
                     'end_date': ['01/16/2019', '01/21/2019', '02/20/2019'],
                     'product_ID': ['A', 'B', 'A'],
                     'price': [100, 150, 150],
                     'org_price': [200, 300, 200]})
df_1
52/303:
df_1.start_date = pd.to_datetime(df_1.start_date)    
df_1.end_date = pd.to_datetime(df_1.end_date)
52/304:
import datetime
def next_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead <= 0: # Target day already happened this week
        days_ahead += 7
    return d + datetime.timedelta(days_ahead)

d = datetime.date(2011, 7, 2)
next_monday = next_weekday(d, 0) # 0 = Monday, 1=Tuesday, 2=Wednesday...
print(next_monday)
52/305:
def prev_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead > 0: # Target day already happened this week
        days_ahead -= 7
    return d + datetime.timedelta(days_ahead)
52/306: df_1['prev_sat'] = df_1.apply(lambda x: prev_weekday(x['start_date'], 5), axis=1)
52/307: df_1['next_sat'] = df_1.apply(lambda x: next_weekday(x['end_date'], 5), axis=1)
52/308: df_1.dtypes
52/309: df_1
52/310: df_1['start_date'] - pd.to_timedelta(df_1['start_date'].dt.dayofweek, unit='d')
52/311:
df_1["date"] = df_1.apply(
    lambda x: pd.date_range(x["start_date"], x["end_date"]), axis=1
)
52/312:
df_1["date_month"] = df_1.apply(
    lambda x: pd.date_range(x["prev_sat"], x["next_sat"]), axis=1
)
52/313: df_1
52/314:
df_2 = (
    df_1.explode("date")
    .drop(columns=["start_date", "end_date"])
)
52/315:
df_3 = (
    df_1.explode("date_month")
    .drop(columns=["prev_sat", "next_sat"])
)
52/316: df_2
52/317: df_3
52/318: df_3.shape
52/319: df_2.shape
52/320:
import pandas as pd
import numpy as np
52/321:
df_1 = pd.DataFrame({'start_date': ['01/01/2019', '01/01/2019', '02/15/2019'],
                     'end_date': ['01/16/2019', '01/21/2019', '02/20/2019'],
                     'product_ID': ['A', 'B', 'A'],
                     'price': [100, 150, 150],
                     'org_price': [200, 300, 200]})
df_1
52/322:
df_1.start_date = pd.to_datetime(df_1.start_date)    
df_1.end_date = pd.to_datetime(df_1.end_date)
52/323:
def next_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead <= 0: # Target day already happened this week
        days_ahead += 7
    return d + datetime.timedelta(days_ahead)
52/324:
def prev_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead > 0: # Target day already happened this week
        days_ahead -= 7
    return d + datetime.timedelta(days_ahead)
52/325: df_week = df_1[:]
52/326: df_week
52/327: df_week
52/328: df_1['prev_sat'] = df_1.apply(lambda x: prev_weekday(x['start_date'], 5), axis=1)
52/329: df_1['next_sat'] = df_1.apply(lambda x: next_weekday(x['end_date'], 5), axis=1)
52/330: df_1.dtypes
52/331:
df_1 = pd.DataFrame({'start_date': ['01/01/2019', '01/01/2019', '02/15/2019'],
                     'end_date': ['01/16/2019', '01/21/2019', '02/20/2019'],
                     'product_ID': ['A', 'B', 'A'],
                     'price': [100, 150, 150],
                     'org_price': [200, 300, 200]})
df_1
52/332:
df_1.start_date = pd.to_datetime(df_1.start_date)    
df_1.end_date = pd.to_datetime(df_1.end_date)
52/333:
def next_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead <= 0: # Target day already happened this week
        days_ahead += 7
    return d + datetime.timedelta(days_ahead)
52/334:
def prev_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead > 0: # Target day already happened this week
        days_ahead -= 7
    return d + datetime.timedelta(days_ahead)
52/335: df_week = df_1[:]
52/336: df_week['prev_sat'] = df_week.apply(lambda x: prev_weekday(x['start_date'], 5), axis=1)
52/337: df_week['next_sat'] = df_week.apply(lambda x: next_weekday(x['end_date'], 5), axis=1)
52/338:
import pandas as pd
import numpy as np
import datetime
import copy
52/339: df_week = copy.deepcopy(df_1)
52/340: df_week['prev_sat'] = df_week.apply(lambda x: prev_weekday(x['start_date'], 5), axis=1)
52/341: df_week['next_sat'] = df_week.apply(lambda x: next_weekday(x['end_date'], 5), axis=1)
52/342: df_1.dtypes
52/343:
df_1["date"] = df_1.apply(
    lambda x: pd.date_range(x["start_date"], x["end_date"]), axis=1
)
52/344:
df_week["date_month"] = df_week.apply(
    lambda x: pd.date_range(x["prev_sat"], x["next_sat"]), axis=1
)
52/345: df_1
52/346:
df_week = (
    df_1.explode("date")
    .drop(columns=["start_date", "end_date"])
)
52/347:
df_1 = (
    df_1.explode("date")
    .drop(columns=["start_date", "end_date"])
)
52/348:
df_week = (
    df_week.explode("date_month")
    .drop(columns=["prev_sat", "next_sat"])
)
52/349: df_week
52/350:
df_1 = pd.DataFrame({'start_date': ['01/01/2019', '01/01/2019', '02/15/2019'],
                     'end_date': ['01/16/2019', '01/21/2019', '02/20/2019'],
                     'product_ID': ['A', 'B', 'A'],
                     'price': [100, 150, 150],
                     'org_price': [200, 300, 200]})
df_1
52/351:
df_1.start_date = pd.to_datetime(df_1.start_date)    
df_1.end_date = pd.to_datetime(df_1.end_date)
52/352:
def next_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead <= 0: # Target day already happened this week
        days_ahead += 7
    return d + datetime.timedelta(days_ahead)
52/353:
def prev_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead > 0: # Target day already happened this week
        days_ahead -= 7
    return d + datetime.timedelta(days_ahead)
52/354: df_week = copy.deepcopy(df_1)
52/355: df_week['prev_sat'] = df_week.apply(lambda x: prev_weekday(x['start_date'], 5), axis=1)
52/356: df_week['next_sat'] = df_week.apply(lambda x: next_weekday(x['end_date'], 5), axis=1)
52/357:
df_1["date"] = df_1.apply(
    lambda x: pd.date_range(x["start_date"], x["end_date"]), axis=1
)
52/358: df_1
52/359:
df_week["date_month"] = df_week.apply(
    lambda x: pd.date_range(x["prev_sat"], x["next_sat"]), axis=1
)
52/360: df_week
52/361:
df_1 = (
    df_1.explode("date")
    .drop(columns=["start_date", "end_date"])
)
52/362:
df_week = (
    df_week.explode("date_month")
    .drop(columns=["prev_sat", "next_sat"])
)
52/363: df_week
52/364:
df_1 = pd.DataFrame({'start_date': ['01/01/2019', '01/01/2019', '02/15/2019'],
                     'end_date': ['01/16/2019', '01/21/2019', '02/20/2019'],
                     'product_ID': ['A', 'B', 'A'],
                     'price': [100, 150, 150],
                     'org_price': [200, 300, 200]})
df_1
52/365:
df_1.start_date = pd.to_datetime(df_1.start_date)    
df_1.end_date = pd.to_datetime(df_1.end_date)
52/366:
def next_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead <= 0: # Target day already happened this week
        days_ahead += 7
    return d + datetime.timedelta(days_ahead)
52/367:
def prev_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead > 0: # Target day already happened this week
        days_ahead -= 7
    return d + datetime.timedelta(days_ahead)
52/368: df_week = copy.deepcopy(df_1)
52/369: df_week['prev_sat'] = df_week.apply(lambda x: prev_weekday(x['start_date'], 5), axis=1)
52/370: df_week['next_sat'] = df_week.apply(lambda x: next_weekday(x['end_date'], 5), axis=1)
52/371: df_1['start_date'] - pd.to_timedelta(df_1['start_date'].dt.dayofweek, unit='d')
52/372:
df_1["date"] = df_1.apply(
    lambda x: pd.date_range(x["start_date"], x["end_date"]), axis=1
)
52/373: df_1
52/374:
df_week["date_month"] = df_week.apply(
    lambda x: pd.date_range(x["prev_sat"], x["next_sat"]), axis=1
)
52/375: df_week
52/376:
df_1 = (
    df_1.explode("date")
    .drop(columns=["start_date", "end_date"])
)
52/377:
df_week = (
    df_week.explode("date_month")
    .drop(columns=["prev_sat", "next_sat", "start_date", "end_date"])
)
52/378: df_week
52/379: df_week.merge(df_1, on=['date_month', 'product_ID'])
52/380: df_week.merge(df_1, on=['date_month', 'product_ID'], how='left')
52/381:
df_1 = pd.DataFrame({'start_date': ['01/01/2019', '01/01/2019', '02/15/2019'],
                     'end_date': ['01/16/2019', '01/21/2019', '02/20/2019'],
                     'product_ID': ['A', 'B', 'A'],
                     'price': [100, 150, 150],
                     'org_price': [200, 300, 200]})
df_1
52/382:
df_1.start_date = pd.to_datetime(df_1.start_date)    
df_1.end_date = pd.to_datetime(df_1.end_date)
52/383:
def next_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead <= 0: # Target day already happened this week
        days_ahead += 7
    return d + datetime.timedelta(days_ahead)
52/384:
def prev_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead > 0: # Target day already happened this week
        days_ahead -= 7
    return d + datetime.timedelta(days_ahead)
52/385: df_week = copy.deepcopy(df_1)
52/386: df_week['prev_sat'] = df_week.apply(lambda x: prev_weekday(x['start_date'], 5), axis=1)
52/387: df_week['next_sat'] = df_week.apply(lambda x: next_weekday(x['end_date'], 5), axis=1)
52/388: df_1['start_date'] - pd.to_timedelta(df_1['start_date'].dt.dayofweek, unit='d')
52/389:
df_1["date"] = df_1.apply(
    lambda x: pd.date_range(x["start_date"], x["end_date"]), axis=1
)
52/390: df_1
52/391:
df_week["date"] = df_week.apply(
    lambda x: pd.date_range(x["prev_sat"], x["next_sat"]), axis=1
)
52/392: df_week
52/393:
df_1 = (
    df_1.explode("date")
    .drop(columns=["start_date", "end_date"])
)
52/394:
df_week = (
    df_week.explode("date")
    .drop(columns=["prev_sat", "next_sat", "start_date", "end_date"])
)
52/395: df_week.merge(df_1, on=['date', 'product_ID'], how='left')
52/396: df_1.head(2)
52/397:
df_1 = pd.DataFrame({'start_date': ['01/01/2019', '01/01/2019', '02/15/2019'],
                     'end_date': ['01/16/2019', '01/21/2019', '02/20/2019'],
                     'product_ID': ['A', 'B', 'A'],
                     'price': [100, 150, 150],
                     'org_price': [200, 300, 200]})
df_1
52/398:
df_1.start_date = pd.to_datetime(df_1.start_date)    
df_1.end_date = pd.to_datetime(df_1.end_date)
52/399:
def next_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead <= 0: # Target day already happened this week
        days_ahead += 7
    return d + datetime.timedelta(days_ahead)
52/400:
def prev_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead > 0: # Target day already happened this week
        days_ahead -= 7
    return d + datetime.timedelta(days_ahead)
52/401: df_week = copy.deepcopy(df_1)
52/402: df_week['prev_sat'] = df_week.apply(lambda x: prev_weekday(x['start_date'], 5), axis=1)
52/403: df_week['next_sat'] = df_week.apply(lambda x: next_weekday(x['end_date'], 5), axis=1)
52/404: df_1['start_date'] - pd.to_timedelta(df_1['start_date'].dt.dayofweek, unit='d')
52/405:
df_1["date"] = df_1.apply(
    lambda x: pd.date_range(x["start_date"], x["end_date"]), axis=1
)
52/406: df_1
52/407:
df_week["date"] = df_week.apply(
    lambda x: pd.date_range(x["prev_sat"], x["next_sat"]), axis=1
)
52/408: df_week
52/409:
df_1 = (
    df_1.explode("date")
    .drop(columns=["org_price", "start_date", "end_date"])
)
52/410:
df_week = (
    df_week.explode("date")
    .drop(columns=["price", "prev_sat", "next_sat", "start_date", "end_date"])
)
52/411: df_1.head(2)
52/412: df_week.merge(df_1, on=['date', 'product_ID'], how='left')
52/413: df = df_week.merge(df_1, on=['date', 'product_ID'], how='left')
52/414: df
52/415: df['price'].fillna(df['org_price'])
52/416: df
52/417: df['price'] = df['price'].fillna(df['org_price'])
52/418: df
52/419: df.resample('W', on='date').mean().reset_index()
52/420: df.resample('W-SAT', on='date').mean().reset_index()
52/421: df.resample('W-SAT', on='date').last().mean().reset_index()
52/422: df.resample('W-SAT', on='date').mean().reset_index()
52/423: df_1.groupby('product_ID').resample('W-SAT', on='date').mean().reset_index()
52/424: df
52/425: df.set_index('date').resample('d',).first().ffill().reset_index()
52/426: df.groupby([pd.Grouper(freq='W-SAT'), 'product_ID'])
52/427: df.groupby('product_ID').resample('W-SAT', on='date').mean().reset_index()
52/428: df.groupby('product_ID').resample('W-SAT', on='date', closed = "right", label = "right").mean().reset_index()
52/429: df.isnull().mean()
52/430: df.groupby('product_ID').resample('W-SAT', on='date', closed = "right").mean().reset_index()
52/431: df.groupby('product_ID').resample('W-SAT', on='date', label = "right").mean().reset_index()
52/432: df
53/1:
import pandas as pd
import numpy as np
import datetime
import copy
53/2:
df_1 = pd.DataFrame({'start_date': ['01/01/2019', '01/01/2019', '02/15/2019'],
                     'end_date': ['01/16/2019', '01/21/2019', '02/20/2019'],
                     'product_ID': ['A', 'B', 'A'],
                     'price': [100, 150, 150],
                     'org_price': [200, 300, 200]})
df_1
53/3:
df_1.start_date = pd.to_datetime(df_1.start_date)    
df_1.end_date = pd.to_datetime(df_1.end_date)
53/4:
def next_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead <= 0: # Target day already happened this week
        days_ahead += 7
    return d + datetime.timedelta(days_ahead)
53/5:
def prev_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead >= 0: # Target day already happened this week
        days_ahead -= 7
    return d + datetime.timedelta(days_ahead)
53/6: df_week = copy.deepcopy(df_1)
53/7: df_week['prev_sat'] = df_week.apply(lambda x: prev_weekday(x['start_date'], 5), axis=1)
53/8: df_week['next_sat'] = df_week.apply(lambda x: next_weekday(x['end_date'], 5), axis=1)
53/9: df_1['start_date'] - pd.to_timedelta(df_1['start_date'].dt.dayofweek, unit='d')
53/10:
df_1["date"] = df_1.apply(
    lambda x: pd.date_range(x["start_date"], x["end_date"]), axis=1
)
53/11: df_1
53/12:
df_week["date"] = df_week.apply(
    lambda x: pd.date_range(x["prev_sat"], x["next_sat"]), axis=1
)
53/13: df_week
53/14:
def prev_weekday(d, weekday):
    days_ahead = weekday - d.weekday()+1
    if days_ahead >= 0: # Target day already happened this week
        days_ahead -= 7
    return d + datetime.timedelta(days_ahead)
53/15: df_week = copy.deepcopy(df_1)
53/16: df_week['prev_sat'] = df_week.apply(lambda x: prev_weekday(x['start_date'], 5), axis=1)
53/17: df_week['next_sat'] = df_week.apply(lambda x: next_weekday(x['end_date'], 5), axis=1)
53/18: df_1['start_date'] - pd.to_timedelta(df_1['start_date'].dt.dayofweek, unit='d')
53/19:
df_1["date"] = df_1.apply(
    lambda x: pd.date_range(x["start_date"], x["end_date"]), axis=1
)
53/20: df_1
53/21:
df_week["date"] = df_week.apply(
    lambda x: pd.date_range(x["prev_sat"], x["next_sat"]), axis=1
)
53/22: df_week
53/23:
df_1 = pd.DataFrame({'start_date': ['01/01/2019', '01/01/2019', '02/15/2019'],
                     'end_date': ['01/16/2019', '01/21/2019', '02/20/2019'],
                     'product_ID': ['A', 'B', 'A'],
                     'price': [100, 150, 150],
                     'org_price': [200, 300, 200]})
df_1
53/24:
df_1.start_date = pd.to_datetime(df_1.start_date)    
df_1.end_date = pd.to_datetime(df_1.end_date)
53/25:
def next_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead <= 0:
        days_ahead += 7
    return d + datetime.timedelta(days_ahead)
53/26:
def prev_weekday(d, weekday):
    days_ahead = weekday - d.weekday()+1
    if days_ahead >= 0: # Target day already happened this week
        days_ahead -= 7
    return d + datetime.timedelta(days_ahead)
53/27: df_week = copy.deepcopy(df_1)
53/28: df_week['prev_sat'] = df_week.apply(lambda x: prev_weekday(x['start_date'], 5), axis=1)
53/29: df_week['next_sat'] = df_week.apply(lambda x: next_weekday(x['end_date'], 5), axis=1)
53/30: df_1['start_date'] - pd.to_timedelta(df_1['start_date'].dt.dayofweek, unit='d')
53/31:
df_1["date"] = df_1.apply(
    lambda x: pd.date_range(x["start_date"], x["end_date"]), axis=1
)
53/32: df_1
53/33:
df_week["date"] = df_week.apply(
    lambda x: pd.date_range(x["prev_sat"], x["next_sat"]), axis=1
)
53/34: df_week
53/35:
df_1 = (
    df_1.explode("date")
    .drop(columns=["org_price", "start_date", "end_date"])
)
53/36:
df_week = (
    df_week.explode("date")
    .drop(columns=["price", "prev_sat", "next_sat", "start_date", "end_date"])
)
53/37: df_1.head(2)
53/38: df = df_week.merge(df_1, on=['date', 'product_ID'], how='left')
53/39: df['price'] = df['price'].fillna(df['org_price'])
53/40: df
53/41: df.groupby('product_ID').resample('W-SAT', on='date', closed = "right", label = "right").mean().reset_index()
54/1:
import pandas as pd
import numpy as np
import datetime
import copy
54/2:
df_1 = pd.DataFrame({'start_date': ['01/01/2019', '01/01/2019', '02/15/2019'],
                     'end_date': ['01/16/2019', '01/21/2019', '02/20/2019'],
                     'product_ID': ['A', 'B', 'A'],
                     'price': [100, 150, 150],
                     'org_price': [200, 300, 200]})
df_1
54/3:
df_1.start_date = pd.to_datetime(df_1.start_date)    
df_1.end_date = pd.to_datetime(df_1.end_date)
54/4:
def next_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead <= 0:
        days_ahead += 7
    return d + datetime.timedelta(days_ahead)
54/5:
def prev_weekday(d, weekday):
    days_ahead = weekday - d.weekday()+1
    if days_ahead >= 0: # Target day already happened this week
        days_ahead -= 7
    return d + datetime.timedelta(days_ahead)
54/6: df_week = copy.deepcopy(df_1)
54/7: df_week['next_sat'] = df_week.apply(lambda x: next_weekday(x['end_date'], 5), axis=1)
54/8: df_week['next_sat'] = df_week.apply(lambda x: next_weekday(x['end_date'], 5), axis=1)
54/9: df_week['next_sat'] = df_week.apply(lambda x: next_weekday(x['end_date'], 5), axis=1)
55/1:
import pandas as pd
import numpy as np
import datetime
import copy
55/2:
df_1 = pd.DataFrame({'start_date': ['01/01/2019', '01/01/2019', '02/15/2019'],
                     'end_date': ['01/16/2019', '01/21/2019', '02/20/2019'],
                     'product_ID': ['A', 'B', 'A'],
                     'price': [100, 150, 150],
                     'org_price': [200, 300, 200]})
df_1
55/3:
df_1.start_date = pd.to_datetime(df_1.start_date)    
df_1.end_date = pd.to_datetime(df_1.end_date)
55/4:
def next_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead <= 0:
        days_ahead += 7
    return d + datetime.timedelta(days_ahead)
55/5:
def prev_weekday(d, weekday):
    days_ahead = weekday - d.weekday()+1
    if days_ahead >= 0: # Target day already happened this week
        days_ahead -= 7
    return d + datetime.timedelta(days_ahead)
55/6: df_week = copy.deepcopy(df_1)
55/7: df_1['start_date'] - pd.to_timedelta(df_1['start_date'].dt.dayofweek, unit='d')
55/8:
df_1["date"] = df_1.apply(
    lambda x: pd.date_range(x["start_date"], x["end_date"]), axis=1
)
55/9: df_1
55/10: df_1.start_date.min()
55/11:
df_week['prev_sat'] = df_week.apply(lambda x: prev_weekday(x['start_date'], 5), axis=1)
df_week['next_sat'] = df_week.apply(lambda x: next_weekday(x['end_date'], 5), axis=1)
55/12: df_week.start_date.min()
55/13: df_week.prev_sat.min()
55/14: df_week
55/15:
df_week["date"] = df_week.apply(
    lambda x: pd.date_range(df_week.prev_sat.min(), df_week.end_date.max()), axis=1
)
55/16: df_week
55/17:
df_1 = (
    df_1.explode("date")
    .drop(columns=["org_price", "start_date", "end_date"])
)
55/18:
df_week = (
    df_week.explode("date")
    .drop(columns=["price", "prev_sat", "next_sat", "start_date", "end_date"])
)
55/19: df_1.head(2)
55/20: df_week.merge(df_1, on=['date', 'product_ID'], how='left')
55/21: df = df_week.merge(df_1, on=['date', 'product_ID'], how='left')
55/22: df['price'] = df['price'].fillna(df['org_price'])
55/23: df
55/24: df.groupby('product_ID').resample('W-SAT', on='date', closed = "right", label = "right").mean().reset_index()
55/25: df_week
55/26: df_1
55/27:
df_1 = pd.DataFrame({'start_date': ['01/01/2019', '01/01/2019', '02/15/2019'],
                     'end_date': ['01/16/2019', '01/21/2019', '02/20/2019'],
                     'product_ID': ['A', 'B', 'A'],
                     'price': [100, 150, 150],
                     'org_price': [200, 300, 200]})
df_1
55/28:
df_1.start_date = pd.to_datetime(df_1.start_date)    
df_1.end_date = pd.to_datetime(df_1.end_date)
55/29:
def next_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead <= 0:
        days_ahead += 7
    return d + datetime.timedelta(days_ahead)
55/30:
def prev_weekday(d, weekday):
    days_ahead = weekday - d.weekday()+1
    if days_ahead >= 0: # Target day already happened this week
        days_ahead -= 7
    return d + datetime.timedelta(days_ahead)
55/31: df_week = copy.deepcopy(df_1)
55/32:
df_week['prev_sat'] = df_week.apply(lambda x: prev_weekday(x['start_date'], 5), axis=1)
df_week['next_sat'] = df_week.apply(lambda x: next_weekday(x['end_date'], 5), axis=1)
55/33: df_1['start_date'] - pd.to_timedelta(df_1['start_date'].dt.dayofweek, unit='d')
55/34:
df_1["date"] = df_1.apply(
    lambda x: pd.date_range(x["start_date"], x["end_date"]), axis=1
)
55/35: df_1
55/36: df_week.prev_sat.min()
55/37: df_week
55/38:
df_week["date"] = df_week.apply(
    lambda x: pd.date_range(df_week.prev_sat.min(), df_week.end_date.max()), axis=1
)
55/39: df_week
55/40:
df_1 = (
    df_1.explode("date")
    .drop(columns=["org_price", "start_date", "end_date"])
)
55/41:
df_week = (
    df_week.explode("date")
    .drop(columns=["price", "prev_sat", "next_sat", "start_date", "end_date"])
)
55/42: df_week
55/43:
df_1 = pd.DataFrame({'start_date': ['01/01/2019', '01/01/2019', '02/15/2019'],
                     'end_date': ['01/16/2019', '01/21/2019', '02/20/2019'],
                     'product_ID': ['A', 'B', 'A'],
                     'price': [100, 150, 150],
                     'org_price': [200, 300, 200]})
df_1
55/44:
df_1.start_date = pd.to_datetime(df_1.start_date)    
df_1.end_date = pd.to_datetime(df_1.end_date)
55/45:
def next_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead <= 0:
        days_ahead += 7
    return d + datetime.timedelta(days_ahead)
55/46:
def prev_weekday(d, weekday):
    days_ahead = weekday - d.weekday()+1
    if days_ahead >= 0: # Target day already happened this week
        days_ahead -= 7
    return d + datetime.timedelta(days_ahead)
55/47: df_week = copy.deepcopy(df_1)
55/48:
df_week['prev_sat'] = df_week.apply(lambda x: prev_weekday(x['start_date'], 5), axis=1)
df_week['next_sat'] = df_week.apply(lambda x: next_weekday(x['end_date'], 5), axis=1)
55/49: df_1['start_date'] - pd.to_timedelta(df_1['start_date'].dt.dayofweek, unit='d')
55/50:
df_1["date"] = df_1.apply(
    lambda x: pd.date_range(x["start_date"], x["end_date"]), axis=1
)
55/51: df_1
55/52: df_week.prev_sat.min()
55/53: df_week
55/54:
df_week["date"] = df_week.apply(
    lambda x: pd.date_range(df_week.prev_sat.min(), df_week.next_sat.max()), axis=1
)
55/55: df_week
55/56:
df_1 = (
    df_1.explode("date")
    .drop(columns=["org_price", "start_date", "end_date"])
)
55/57:
df_week = (
    df_week.explode("date")
    .drop(columns=["price", "prev_sat", "next_sat", "start_date", "end_date"])
)
55/58: df_week
55/59: df_1.head(2)
55/60: df_week.merge(df_1, on=['date', 'product_ID'], how='left')
55/61: df = df_week.merge(df_1, on=['date', 'product_ID'], how='left')
55/62: df['price'] = df['price'].fillna(df['org_price'])
55/63: df
55/64: df_1
55/65: df.groupby('product_ID').resample('W-SAT', on='date', closed = "right", label = "right").mean().reset_index()
56/1:
import pandas as pd
import numpy as np
import datetime
import copy
56/2:
df_1 = pd.DataFrame({'start_date': ['01/01/2019', '01/01/2019', '02/15/2019'],
                     'end_date': ['01/16/2019', '01/21/2019', '02/20/2019'],
                     'product_ID': ['A', 'B', 'A'],
                     'price': [100, 150, 150],
                     'org_price': [200, 300, 200]})
df_1
56/3:
df_1.start_date = pd.to_datetime(df_1.start_date)    
df_1.end_date = pd.to_datetime(df_1.end_date)
56/4:
def next_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead <= 0:
        days_ahead += 7
    return d + datetime.timedelta(days_ahead)
56/5:
def prev_weekday(d, weekday):
    days_ahead = weekday - d.weekday()+1
    if days_ahead >= 0: # Target day already happened this week
        days_ahead -= 7
    return d + datetime.timedelta(days_ahead)
56/6: df_week = copy.deepcopy(df_1)
56/7:
## 5 - Saturday
## –¥–æ–±–∞–≤–ª—è—é –¥–∞—Ç—ã –ø—Ä–æ—à–ª–æ–π –∏ —Å–ª–µ–¥. —Å—É–±–±–æ—Ç—ã (—è –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –¥–µ–ª–∞–ª–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏ —Ñ-—Ü–∏—é –∏ –≤ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
## –Ω–µ –ø–æ–ø–∞–¥–∞–ª–∏ –Ω–µ–¥–µ–ª–∏, –≤ –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –±—ã–ª–æ —Å–∫–∏–¥–æ–∫ –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∞. –ü–æ—Ç–æ–∏ –∏–∑–º–µ–Ω–µ–Ω–æ –Ω–∞ –≤–∑—è—Ç–∏–µ –º–∏–Ω–∏–º—É–º–∞ –∏ –º–∞–∫—Å–∏–º—É–º–∞)
df_week['prev_sat'] = df_week.apply(lambda x: prev_weekday(x['start_date'], 5), axis=1)
df_week['next_sat'] = df_week.apply(lambda x: next_weekday(x['end_date'], 5), axis=1)
56/8:
df_1["date"] = df_1.apply(
    lambda x: pd.date_range(x["start_date"], x["end_date"]), axis=1
)
56/9:
df_week["date"] = df_week.apply(
    lambda x: pd.date_range(df_week.prev_sat.min(), df_week.next_sat.max()), axis=1
)
56/10:
df_1 = (
    df_1.explode("date")
    .drop(columns=["org_price", "start_date", "end_date"])
)
56/11:
df_week = (
    df_week.explode("date")
    .drop(columns=["price", "prev_sat", "next_sat", "start_date", "end_date"])
)
56/12: df_week.head(5)
56/13: df_1.head(5)
56/14: df = df_week.merge(df_1, on=['date', 'product_ID'], how='left')
56/15: df['price'] = df['price'].fillna(df['org_price'])
56/16: df
56/17: df.head(5)
56/18: df.groupby('product_ID').resample('W-SAT', on='date', closed = "right", label = "right").mean().reset_index()
56/19:
df_final = df.groupby('product_ID').resample('W-SAT', on='date', closed = "right", label = "right")
.mean().reset_index()
56/20:
df_final = df.groupby('product_ID').resample('W-SAT', on='date', closed = "right", label = "right")#
.mean().reset_index()
56/21:
df_final = df.groupby('product_ID').resample('W-SAT', on='date', closed = "right", label = "right")\
.mean().reset_index()
56/22:
df_final = df.groupby('product_ID')\
             .resample('W-SAT', on='date', closed = "right", label = "right")\
             .mean().reset_index()
56/23: df_final
56/24:
## –Ω–∞ –≤—Ö–æ–¥ - –¥–∞—Ç–∞, –¥–µ–Ω—å –Ω–µ–¥–µ–ª–∏ (—Å 0 –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è)
## –≤—ã—Ö–æ–¥ - –¥–∞—Ç–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –¥–Ω—è –Ω–µ–¥–µ–ª–∏
def prev_weekday(d, weekday):
    days_ahead = weekday - d.weekday()+1
    if days_ahead >= 0: # Target day already happened this week
        days_ahead -= 7
    return d + datetime.timedelta(days_ahead)
56/25: df_week = copy.deepcopy(df_1)
56/26:
df_2 = pd.read_csv('transactions_data.txt', sep=' ', header=None)
df_2.columns = ['transaction_price', 'square_meters', 'num_bedrooms', 'floor', 'is_collab_firm']
df_2.head()
56/27: df_2
56/28: import seaborn as sns
56/29: sns.pairplot(df_2)
56/30: df_2.dtypes
56/31: sns.jointplot(x='transaction_price', y='square_meters', data=tips, kind='reg');
56/32: sns.jointplot(x='transaction_price', y='square_meters', data=df_2, kind='reg');
56/33: sns.jointplot(x='square_meters', y='transaction_price', data=df_2, kind='reg');
56/34: g = sns.pairplot(df_2, kind="reg", plot_kws=dict(scatter_kws=dict(s=2)))
56/35: df_2.is_collab_firm.value_counts()
56/36: g = sns.pairplot(df_2, kind="reg", hue='is_collab_firm', plot_kws=dict(scatter_kws=dict(s=2)))
56/37: sns.jointplot(x='square_meters', y='transaction_price', data=df_2, kind='reg', hue='is_collab_firm');
56/38: !pip install seaborn
56/39: !pip install --upgrade pip
56/40: !pip install --upgrade seaborn
56/41: sns.jointplot(x='square_meters', y='transaction_price', data=df_2, kind='reg');
56/42: g = sns.pairplot(df_2[['transaction_price', 'square_meters', 'num_bedrooms', 'floor']], kind="reg", hue='is_collab_firm', plot_kws=dict(scatter_kws=dict(s=2)))
56/43: g = sns.pairplot(df_2, kind="reg", hue='is_collab_firm', plot_kws=dict(scatter_kws=dict(s=2)))
56/44: from sklearn.linear_model import LinearRegression
56/45:

reg = LinearRegression()
56/46:

reg = LinearRegression()
56/47: reg = LinearRegression()
56/48: reg
56/49: df_2
56/50:
target = df_2['transaction_price']
train = df_2.drop(['transaction_price'],axis=1)
56/51: transaction_price
56/52: from sklearn.model_selection import train_test_split
56/53: df_2
56/54: df_collab = df_2[df_2.is_collab_firm == 1]
56/55:
target = df_collab['transaction_price']
train = df_collab.drop(['transaction_price'],axis=1)
56/56: x_train , x_test , y_train , y_test = train_test_split(train , target , test_size = 0.20,random_state=0)
56/57: reg.fit(x_train,y_train)
56/58: reg.score(x_test,y_test)
56/59: df_not_collab = df_2[df_2.is_collab_firm != 1]
56/60: df_not_collab[:, -1]
56/61: df_not_collab.loc[:, -1]
56/62: df_not_collab.loc[:, 1]
56/63: df_not_collab.loc[:, 1:]
56/64: df_not_collab.iloc[:, 1:]
56/65: reg.predict(df_not_collab.iloc[:, 1:])
56/66: df_not_collab['prd'] = reg.predict(df_not_collab.iloc[:, 1:])
56/67: df_not_collab
56/68: df_not_collab[df_not_collab.prd - df_not_collab.transaction_price]
56/69: df_not_collab['delta'] = [df_not_collab.prd - df_not_collab.transaction_price]
56/70: df_not_collab['delta'] = df_not_collab.prd - df_not_collab.transaction_price
56/71: df_not_collab
56/72: df_not_collab[:30]
56/73:
from sklearn.model_selection import train_test_split
from sklearn import ensemble
58/1:
import pandas as pd
import numpy as np
import datetime
import copy
58/2:
df_1 = pd.DataFrame({'start_date': ['01/01/2019', '01/01/2019', '02/15/2019'],
                     'end_date': ['01/16/2019', '01/21/2019', '02/20/2019'],
                     'product_ID': ['A', 'B', 'A'],
                     'price': [100, 150, 150],
                     'org_price': [200, 300, 200]})
df_1
58/3:
df_1.start_date = pd.to_datetime(df_1.start_date)    
df_1.end_date = pd.to_datetime(df_1.end_date)
58/4:
## –Ω–∞ –≤—Ö–æ–¥ - (–¥–∞—Ç–∞, –¥–µ–Ω—å –Ω–µ–¥–µ–ª–∏ (—Å 0 –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è))
## –≤—ã—Ö–æ–¥ - –¥–∞—Ç–∞ —Å–ª–µ–¥—É—é—â–µ–≥–æ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –¥–Ω—è –Ω–µ–¥–µ–ª–∏
def next_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead <= 0:
        days_ahead += 7
    return d + datetime.timedelta(days_ahead)
58/5:
## –Ω–∞ –≤—Ö–æ–¥ - (–¥–∞—Ç–∞, –¥–µ–Ω—å –Ω–µ–¥–µ–ª–∏ (—Å 0 –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è))
## –≤—ã—Ö–æ–¥ - –¥–∞—Ç–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –¥–Ω—è –Ω–µ–¥–µ–ª–∏
def prev_weekday(d, weekday):
    days_ahead = weekday - d.weekday()+1
    if days_ahead >= 0: # Target day already happened this week
        days_ahead -= 7
    return d + datetime.timedelta(days_ahead)
58/6: df_week = copy.deepcopy(df_1)
58/7:
## 5 - Saturday
## –¥–æ–±–∞–≤–ª—è—é –¥–∞—Ç—ã –ø—Ä–æ—à–ª–æ–π –∏ —Å–ª–µ–¥. —Å—É–±–±–æ—Ç—ã (—è –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –¥–µ–ª–∞–ª–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏ —Ñ-—Ü–∏—é –∏ –≤ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
## –Ω–µ –ø–æ–ø–∞–¥–∞–ª–∏ –Ω–µ–¥–µ–ª–∏, –≤ –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –±—ã–ª–æ —Å–∫–∏–¥–æ–∫ –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∞. –ü–æ—Ç–æ–∏ –∏–∑–º–µ–Ω–µ–Ω–æ –Ω–∞ –≤–∑—è—Ç–∏–µ –º–∏–Ω–∏–º—É–º–∞ –∏ –º–∞–∫—Å–∏–º—É–º–∞)
df_week['prev_sat'] = df_week.apply(lambda x: prev_weekday(x['start_date'], 5), axis=1)
df_week['next_sat'] = df_week.apply(lambda x: next_weekday(x['end_date'], 5), axis=1)
58/8:
df_1["date"] = df_1.apply(
    lambda x: pd.date_range(x["start_date"], x["end_date"]), axis=1
)
58/9:
df_week["date"] = df_week.apply(
    lambda x: pd.date_range(df_week.prev_sat.min(), df_week.next_sat.max()), axis=1
)
58/10:
df_1 = (
    df_1.explode("date")
    .drop(columns=["org_price", "start_date", "end_date"])
)
58/11:
df_week = (
    df_week.explode("date")
    .drop(columns=["price", "prev_sat", "next_sat", "start_date", "end_date"])
)
58/12: df = df_week.merge(df_1, on=['date', 'product_ID'], how='left')
58/13: df['price'] = df['price'].fillna(df['org_price'])
58/14: df.head(5)
58/15:
df_final = df.groupby('product_ID')\
             .resample('W-SAT', on='date', closed = "right", label = "right")\
             .mean().reset_index()
58/16: df_final
58/17:
df_2 = pd.read_csv('transactions_data.txt', sep=' ', header=None)
df_2.columns = ['transaction_price', 'square_meters', 'num_bedrooms', 'floor', 'is_collab_firm']
df_2.head()
58/18: df_2.is_collab_firm.value_counts()
58/19: df_2.is_collab_firm.value_counts()/len(df_2)
58/20: !pip install --upgrade seaborn
58/21: sns.jointplot(x='transaction_price', y='transaction_price', hue = 'is_collab_firm', data=df_2, kind='reg');
58/22: import seaborn as sns
58/23: sns.jointplot(x='transaction_price', y='transaction_price', hue = 'is_collab_firm', data=df_2, kind='reg');
58/24: sns.jointplot(x='transaction_price', y='transaction_price', hue = 'is_collab_firm', data=df_2);
58/25: sns.jointplot(x='transaction_price', y='transaction_price',  data=df_2, kind='reg');
58/26: sns.jointplot(x='transaction_price', y='transaction_price', data=df_2, kind='reg');
58/27: sns.jointplot(x='square_meters', y='transaction_price', data=df_2, kind='reg');
58/28: sns.jointplot(x='transaction_price', y='transaction_price', data=df_2, kind='reg');
58/29: sns.pairplot(df_2)
58/30: g = sns.pairplot(df_2, kind="reg", hue='is_collab_firm', plot_kws=dict(scatter_kws=dict(s=2)))
58/31:
from sklearn import ensemble
clf = ensemble.GradientBoostingRegressor(n_estimators = 400, max_depth = 5, min_samples_split = 2,
          learning_rate = 0.1, loss = 'ls')
58/32: clf.fit(x_train, y_train)
58/33: from sklearn.linear_model import LinearRegression
58/34:
from sklearn.model_selection import train_test_split
from sklearn import ensemble
58/35: reg = LinearRegression()
58/36: reg
58/37: df_2
58/38: df_collab = df_2[df_2.is_collab_firm == 1]
58/39: df_not_collab = df_2[df_2.is_collab_firm != 1]
58/40:
target = df_collab['transaction_price']
train = df_collab.drop(['transaction_price'],axis=1)
58/41: x_train , x_test , y_train , y_test = train_test_split(train , target , test_size = 0.20,random_state=0)
58/42: reg.fit(x_train,y_train)
58/43: reg.score(x_test,y_test)
58/44: clf.fit(x_train, y_train)
58/45: clf.score(x_test,y_test)
58/46: g = sns.pairplot(df_2, kind="reg", hue='is_collab_firm', diag_kind='hist'–± plot_kws=dict(scatter_kws=dict(s=2)))
58/47: g = sns.pairplot(df_2, kind="reg", hue='is_collab_firm', diag_kind='hist', plot_kws=dict(scatter_kws=dict(s=2)))
58/48: g = sns.pairplot(df_2, kind="reg", hue='is_collab_firm', plot_kws=dict(scatter_kws=dict(s=2)))
58/49:
## –ü–æ–¥—Ö–æ–¥ 1
–¢.–∫. –µ—Å—Ç—å –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç —Ñ–∏—Ä–º, —Ç–æ –º–æ–∂–µ–º –Ω–∞ –Ω–∏—Ö –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ (–ø—Ä–∏–º–µ—Ä –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –±–µ–∑ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤).
58/50: from sklearn.linear_model import LinearRegression
58/51:
from sklearn.model_selection import train_test_split
from sklearn import ensemble
58/52: from sklearn.model_selection import train_test_split
58/53: reg = LinearRegression()
58/54:
df_collab = df_2[df_2.is_collab_firm == 1]
df_not_collab = df_2[df_2.is_collab_firm != 1]
58/55:
target = df_collab['transaction_price']
train = df_collab.drop(['transaction_price', 'is_collab_firm'],axis=1)
58/56: x_train , x_test , y_train , y_test = train_test_split(train , target , test_size = 0.20,random_state=0)
58/57: reg.fit(x_train,y_train)
58/58: reg.score(x_test,y_test)
58/59: x_train , x_test , y_train , y_test = train_test_split(train , target , test_size = 0.20,random_state=45)
58/60: reg.fit(x_train,y_train)
58/61: reg.score(x_test,y_test)
58/62: x_train , x_test , y_train , y_test = train_test_split(train , target , test_size = 0.20,random_state=15)
58/63: reg.fit(x_train,y_train)
58/64: reg.score(x_test,y_test)
58/65: x_train , x_test , y_train , y_test = train_test_split(train , target , test_size = 0.20,random_state=-1)
58/66: reg.fit(x_train,y_train)
58/67: x_train , x_test , y_train , y_test = train_test_split(train , target , test_size = 0.20,random_state=1)
58/68: reg.fit(x_train,y_train)
58/69: reg.score(x_test,y_test)
58/70: x_train , x_test , y_train , y_test = train_test_split(train , target , test_size = 0.20,random_state=0)
58/71: reg.fit(x_train,y_train)
58/72: reg.score(x_test,y_test)
58/73: df_not_collab['prd'] = reg.predict(df_not_collab.iloc[:, 1:])
58/74: reg = LinearRegression(normalize=True)
58/75:
df_collab = df_2[df_2.is_collab_firm == 1]
df_not_collab = df_2[df_2.is_collab_firm != 1]
58/76:
target = df_collab['transaction_price']
train = df_collab.drop(['transaction_price', 'is_collab_firm'],axis=1)
58/77: x_train , x_test , y_train , y_test = train_test_split(train , target , test_size = 0.20,random_state=0)
58/78: reg.fit(x_train,y_train)
58/79: reg.score(x_test,y_test)
58/80: df_not_collab.iloc[:, 1:]
58/81: df_not_collab.iloc[:, 1:-1]
58/82: df_not_collab['pred'] = reg.predict(df_not_collab.iloc[:, 1:-1])
58/83: df_not_collab['delta'] = df_not_collab.pred - df_not_collab.transaction_price
58/84:
df_collab = df_2[df_2.is_collab_firm == 1]
df_not_collab = copy.deepcopy(df_2[df_2.is_collab_firm != 1])
58/85:
target = df_collab['transaction_price']
train = df_collab.drop(['transaction_price', 'is_collab_firm'],axis=1)
58/86: x_train , x_test , y_train , y_test = train_test_split(train , target , test_size = 0.20,random_state=0)
58/87: reg.fit(x_train,y_train)
58/88: reg.score(x_test,y_test)
58/89: df_not_collab.iloc[:, 1:-1]
58/90: df_not_collab['pred'] = reg.predict(df_not_collab.iloc[:, 1:-1])
58/91: df_not_collab['delta'] = df_not_collab.pred - df_not_collab.transaction_price
58/92: df_not_collab[:30]
58/93: df_not_collab[:10]
58/94: sns.scatterplot(data=df_not_collab, x="pred", y="transaction_price")
58/95: df_not_collab[:5]
58/96: df_not_collab[['transaction_price', 'pred']].describe()
58/97: df_not_collab[['transaction_price', 'pred']].describe()
58/98: df_not_collab[['transaction_price', 'pred']]
58/99: df_not_collab[['transaction_price', 'pred']].dtype
58/100: df_not_collab[['transaction_price', 'pred']].dtype()
58/101: df_not_collab[['transaction_price', 'pred']].dtypes()
58/102: df_not_collab[['transaction_price', 'pred']].dtypes
58/103: df_not_collab[['transaction_price', 'pred']].describe
58/104: df_not_collab[['transaction_price', 'pred']].describe()
58/105: !pip import numpy
58/106: !pip install numpy
58/107: !pip install pandas
58/108: df_not_collab[['transaction_price', 'pred']].describe()
58/109: df_not_collab.describe()
58/110: df_not_collab.delta.describe()
58/111: df_not_collab.transaction_price.describe()
58/112: df_not_collab['transaction_price', 'pred']
58/113: df_not_collab['transaction_price', 'pred'].describe()
58/114: df_not_collab[['transaction_price', 'pred']].describe()
58/115: df_not_collab.describe()
58/116: df_not_collab.transaction_price.describe
58/117: df_not_collab.numeric.describe()
58/118: df_not_collab.transaction_price.describe()
58/119: df_not_collab.pred.describe()
58/120: df_not_collab.delta.describe()
58/121: df_not_collab.pred.describe()
58/122: from sklearn.ensemble import IsolationForest
58/123:
iforest = IsolationForest(n_estimators=100, max_samples='auto', 
                          contamination=0.05, max_features=3, 
                          bootstrap=False, n_jobs=-1, random_state=1)
58/124: df_2
58/125:
pred= iforest.fit_predict(df_2)
df['scores']=iforest.decision_function(df_2)
df['anomaly_label']=pred
58/126: df[df.anomaly_label==-1]
58/127:
pred= iforest.fit_predict(df_2)
df_2['scores']=iforest.decision_function(df_2)
df_2['anomaly_label']=pred
58/128: df_2[df_2.anomaly_label==-1]
58/129: df_2
58/130: df_2.anomaly_label.value_counts()
58/131: df_not_collab = copy.deepcopy(df_2[df_2.is_collab_firm != 1])
58/132:
pred= iforest.fit_predict(df_not_collab)
df_not_collab['scores']=iforest.decision_function(df_not_collab)
df_not_collab['anomaly_label']=pred
58/133: df_not_collab[df_not_collab.anomaly_label==-1]
58/134: reg = LinearRegression(normalize=True)
58/135:
df_collab = df_2[df_2.is_collab_firm == 1]
df_not_collab = copy.deepcopy(df_2[df_2.is_collab_firm != 1])
58/136:
target = df_collab['transaction_price']
train = df_collab.drop(['transaction_price', 'is_collab_firm'],axis=1)
58/137: x_train , x_test , y_train , y_test = train_test_split(train , target , test_size = 0.20,random_state=0)
58/138: reg.fit(x_train,y_train)
58/139: reg.score(x_test,y_test)
58/140: df_not_collab.iloc[:, 1:-1]
58/141: df_not_collab['pred'] = reg.predict(df_not_collab.iloc[:, 1:-1])
58/142: df_not_collab['delta'] = df_not_collab.pred - df_not_collab.transaction_price
58/143: df_not_collab[:5]
58/144: df_not_collab.delta.describe()
58/145: from sklearn.ensemble import IsolationForest
58/146:
iforest = IsolationForest(n_estimators=100, max_samples='auto', 
                          contamination=0.05, max_features=3, 
                          bootstrap=False, n_jobs=-1, random_state=1)
58/147: df_not_collab_IF = copy.deepcopy(df_2[df_2.is_collab_firm != 1])
58/148:
pred= iforest.fit_predict(df_not_collab)
df_not_collab_IF['scores']=iforest.decision_function(df_not_collab)
df_not_collab_IF['anomaly_label']=pred
58/149: df_not_collab_IF[df_not_collab_IF.anomaly_label==-1]
58/150: df_not_collab
58/151: df_not_collab[df_not_collab == 515977]
58/152: df_not_collab[df_not_collab.transaction_price == 515977]
58/153: df_not_collab[df_not_collab.transaction_price == 398299]
58/154:
iforest = IsolationForest(n_estimators=20, max_samples='auto', 
                          contamination=0.05, max_features=3, 
                          bootstrap=False, n_jobs=-1, random_state=1)
58/155: df_not_collab_IF = copy.deepcopy(df_2[df_2.is_collab_firm != 1])
58/156:
pred= iforest.fit_predict(df_not_collab)
df_not_collab_IF['scores']=iforest.decision_function(df_not_collab)
df_not_collab_IF['anomaly_label']=pred
58/157: df_not_collab_IF[df_not_collab_IF.anomaly_label==-1]
58/158: df_not_collab[df_not_collab.transaction_price == 398299]
58/159:
iforest = IsolationForest(n_estimators=20, max_samples='auto', 
                          contamination=0.05, max_features=4, 
                          bootstrap=False, n_jobs=-1, random_state=1)
58/160: df_not_collab_IF = copy.deepcopy(df_2[df_2.is_collab_firm != 1])
58/161:
pred= iforest.fit_predict(df_not_collab)
df_not_collab_IF['scores']=iforest.decision_function(df_not_collab)
df_not_collab_IF['anomaly_label']=pred
58/162: df_not_collab_IF[df_not_collab_IF.anomaly_label==-1]
58/163: df_not_collab[df_not_collab.transaction_price == 398299]
58/164:
iforest = IsolationForest(n_estimators=20, max_samples='auto', 
                          contamination=0.05, max_features=2, 
                          bootstrap=False, n_jobs=-1, random_state=1)
58/165: df_not_collab_IF = copy.deepcopy(df_2[df_2.is_collab_firm != 1])
58/166:
pred= iforest.fit_predict(df_not_collab)
df_not_collab_IF['scores']=iforest.decision_function(df_not_collab)
df_not_collab_IF['anomaly_label']=pred
58/167: df_not_collab_IF[df_not_collab_IF.anomaly_label==-1]
58/168: df_not_collab[df_not_collab.transaction_price == 398299]
58/169:
iforest = IsolationForest(n_estimators=20, max_samples='auto', 
                          contamination=0.05, max_features=3, 
                          bootstrap=False, n_jobs=-1, random_state=1)
58/170: df_not_collab_IF = copy.deepcopy(df_2[df_2.is_collab_firm != 1])
58/171:
pred= iforest.fit_predict(df_not_collab)
df_not_collab_IF['scores']=iforest.decision_function(df_not_collab)
df_not_collab_IF['anomaly_label']=pred
58/172: df_not_collab_IF[df_not_collab_IF.anomaly_label==-1]
58/173: df_not_collab[df_not_collab.transaction_price == 398299]
58/174: df_not_collab_IF
58/175: df_not_collab_IF.sort_values('scores')
58/176: df_not_collab[df_not_collab.transaction_price == 246370]
58/177: df_not_collab[df_not_collab.transaction_price == 245901]
58/178: df_not_collab_IF.sort_values('scores')[40]
58/179: df_not_collab_IF.sort_values('scores')[40:]
58/180: df_not_collab_IF.sort_values('scores')[:-10]
58/181: df_not_collab_IF.sort_values('scores')[-10:]
58/182: df_not_collab[df_not_collab.transaction_price == 196323]
58/183: df_not_collab[df_not_collab.transaction_price == 249021]
58/184: df_not_collab_IF.sort_values('scores')[-30:]
58/185: df_not_collab[df_not_collab.transaction_price == 239824]
58/186: df_not_collab_IF.sort_values('scores')[-50:]
58/187: df_not_collab[df_not_collab.transaction_price == 248456]
58/188: from sklearn.ensemble import IsolationForest
58/189:
iforest = IsolationForest(n_estimators=20, max_samples='auto', 
                          contamination=0.05, max_features=3, 
                          bootstrap=False, n_jobs=-1, random_state=1)
58/190: df_not_collab_IF = copy.deepcopy(df_2[df_2.is_collab_firm != 1])
58/191:
pred= iforest.fit_predict(df_not_collab)
df_not_collab_IF['scores']=iforest.decision_function(df_not_collab)
df_not_collab_IF['anomaly_label']=pred
58/192:
pred= iforest.fit_predict(df_not_collab_IF)
df_not_collab_IF['scores']=iforest.decision_function(df_not_collab)
df_not_collab_IF['anomaly_label']=pred
58/193:
pred= iforest.fit_predict(df_not_collab_IF)
df_not_collab_IF['scores']=iforest.decision_function(df_not_collab_IF)
df_not_collab_IF['anomaly_label']=pred
58/194: df_not_collab_IF[df_not_collab_IF.anomaly_label==-1]
58/195:
# –ü—Ä–æ–≤–µ—Ä–∏–º, –∫–∞–∫–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–¥–µ–ª–∞–ª–∞ –º–æ–¥–µ–ª—å —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –¥–ª—è –∞–Ω–æ–º–∞–ª–∏–π
df_not_collab[(df_not_collab.transaction_price == 491735) | 
             (df_not_collab.transaction_price == 499486) |
             (df_not_collab.transaction_price == 957630) |
             (df_not_collab.transaction_price == 515977)]
58/196: import seaborn as sns
58/197:
df_2 = pd.read_csv('transactions_data.txt', sep=' ', header=None)
df_2.columns = ['transaction_price', 'square_meters', 'num_bedrooms', 'floor', 'is_collab_firm']
df_2.head()
58/198: df_2.is_collab_firm.value_counts()/len(df_2)
58/199: ### 20% –¥–æ–º–æ–≤ –±—ã–ª–∏ –ø—Ä–æ–¥–∞–Ω—ã —Å –ø–æ–º–æ—â—å—é —Ñ–∏—Ä–º –∏ –∏—Ö –¥–∞–Ω–Ω—ã–µ –∞–±—Å–æ–ª—é—Ç–Ω–æ –ø—Ä–æ–∑—Ä–∞—á–Ω—ã
58/200: g = sns.pairplot(df_2, kind="reg", hue='is_collab_firm', plot_kws=dict(scatter_kws=dict(s=2)))
58/201: from sklearn.linear_model import LinearRegression
58/202: from sklearn.model_selection import train_test_split
58/203: reg = LinearRegression(normalize=True)
58/204:
df_collab = df_2[df_2.is_collab_firm == 1]
df_not_collab = copy.deepcopy(df_2[df_2.is_collab_firm != 1])
58/205:
target = df_collab['transaction_price']
train = df_collab.drop(['transaction_price', 'is_collab_firm'],axis=1)
58/206: x_train , x_test , y_train , y_test = train_test_split(train , target , test_size = 0.20,random_state=0)
58/207: reg.fit(x_train,y_train)
58/208: reg.score(x_test,y_test)
58/209: df_not_collab.iloc[:, 1:-1]
58/210: df_not_collab['pred'] = reg.predict(df_not_collab.iloc[:, 1:-1])
58/211: df_not_collab['delta'] = df_not_collab.pred - df_not_collab.transaction_price
58/212: df_not_collab[:5]
58/213: df_not_collab.delta.describe()
58/214: from sklearn.ensemble import IsolationForest
58/215:
iforest = IsolationForest(n_estimators=20, max_samples='auto', 
                          contamination=0.05, max_features=3, 
                          bootstrap=False, n_jobs=-1, random_state=1)
58/216: df_not_collab_IF = copy.deepcopy(df_2[df_2.is_collab_firm != 1])
58/217:
pred= iforest.fit_predict(df_not_collab_IF)
df_not_collab_IF['scores']=iforest.decision_function(df_not_collab_IF)
df_not_collab_IF['anomaly_label']=pred
58/218:
# –ü–æ–¥—Å–≤–µ—á–∏–≤–∞–µ–º –∞–Ω–æ–º–∞–ª–∏–∏ 
df_not_collab_IF[df_not_collab_IF.anomaly_label==-1]
58/219:
# –ü—Ä–æ–≤–µ—Ä–∏–º, –∫–∞–∫–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–¥–µ–ª–∞–ª–∞ –º–æ–¥–µ–ª—å —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –¥–ª—è –∞–Ω–æ–º–∞–ª–∏–π
df_not_collab[(df_not_collab.transaction_price == 491735) | 
             (df_not_collab.transaction_price == 499486) |
             (df_not_collab.transaction_price == 957630) |
             (df_not_collab.transaction_price == 515977)]
58/220: df_not_collab
58/221: df_not_collab.sort_values('delta')
58/222:
iforest = IsolationForest(n_estimators=10, max_samples='auto', 
                          contamination=0.05, max_features=3, 
                          bootstrap=False, random_state=0)
58/223: df_not_collab_IF = copy.deepcopy(df_2[df_2.is_collab_firm != 1])
58/224:
pred= iforest.fit_predict(df_not_collab_IF)
df_not_collab_IF['scores']=iforest.decision_function(df_not_collab_IF)
df_not_collab_IF['anomaly_label']=pred
58/225:
# –ü–æ–¥—Å–≤–µ—á–∏–≤–∞–µ–º –∞–Ω–æ–º–∞–ª–∏–∏ 
df_not_collab_IF[df_not_collab_IF.anomaly_label==-1]
58/226:
# –ü—Ä–æ–≤–µ—Ä–∏–º, –∫–∞–∫–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–¥–µ–ª–∞–ª–∞ –º–æ–¥–µ–ª—å —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –¥–ª—è –∞–Ω–æ–º–∞–ª–∏–π
df_not_collab[(df_not_collab.transaction_price == 491735) | 
             (df_not_collab.transaction_price == 499486) |
             (df_not_collab.transaction_price == 957630) |
             (df_not_collab.transaction_price == 515977)]
58/227:
# –ü—Ä–æ–≤–µ—Ä–∏–º, –∫–∞–∫–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–¥–µ–ª–∞–ª–∞ –º–æ–¥–µ–ª—å —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –¥–ª—è –∞–Ω–æ–º–∞–ª–∏–π
df_not_collab[(df_not_collab.transaction_price == 398299) | 
             (df_not_collab.transaction_price == 499486) |
             (df_not_collab.transaction_price == 957630) |
             (df_not_collab.transaction_price == 515977)]
58/228:
iforest = IsolationForest(n_estimators=10, max_samples='auto', 
                          contamination=0.05, max_features=3, 
                          bootstrap=False, random_state=1)
58/229: df_not_collab_IF = copy.deepcopy(df_2[df_2.is_collab_firm != 1])
58/230:
pred= iforest.fit_predict(df_not_collab_IF)
df_not_collab_IF['scores']=iforest.decision_function(df_not_collab_IF)
df_not_collab_IF['anomaly_label']=pred
58/231:
# –ü–æ–¥—Å–≤–µ—á–∏–≤–∞–µ–º –∞–Ω–æ–º–∞–ª–∏–∏ 
df_not_collab_IF[df_not_collab_IF.anomaly_label==-1]
58/232:
# –ü—Ä–æ–≤–µ—Ä–∏–º, –∫–∞–∫–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–¥–µ–ª–∞–ª–∞ –º–æ–¥–µ–ª—å —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –¥–ª—è –∞–Ω–æ–º–∞–ª–∏–π
df_not_collab[(df_not_collab.transaction_price == 491735) | 
             (df_not_collab.transaction_price == 450027) |
             (df_not_collab.transaction_price == 957630) |
             (df_not_collab.transaction_price == 515977)]
59/1:
import pandas as pd
import numpy as np
import datetime
import copy
59/2:
df_1 = pd.DataFrame({'start_date': ['01/01/2019', '01/01/2019', '02/15/2019'],
                     'end_date': ['01/16/2019', '01/21/2019', '02/20/2019'],
                     'product_ID': ['A', 'B', 'A'],
                     'price': [100, 150, 150],
                     'org_price': [200, 300, 200]})
df_1
59/3:
df_1.start_date = pd.to_datetime(df_1.start_date)    
df_1.end_date = pd.to_datetime(df_1.end_date)
59/4:
## –Ω–∞ –≤—Ö–æ–¥ - (–¥–∞—Ç–∞, –¥–µ–Ω—å –Ω–µ–¥–µ–ª–∏ (—Å 0 –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è))
## –≤—ã—Ö–æ–¥ - –¥–∞—Ç–∞ —Å–ª–µ–¥—É—é—â–µ–≥–æ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –¥–Ω—è –Ω–µ–¥–µ–ª–∏
def next_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead <= 0:
        days_ahead += 7
    return d + datetime.timedelta(days_ahead)
59/5:
## –Ω–∞ –≤—Ö–æ–¥ - (–¥–∞—Ç–∞, –¥–µ–Ω—å –Ω–µ–¥–µ–ª–∏ (—Å 0 –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è))
## –≤—ã—Ö–æ–¥ - –¥–∞—Ç–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –¥–Ω—è –Ω–µ–¥–µ–ª–∏
def prev_weekday(d, weekday):
    days_ahead = weekday - d.weekday()+1
    if days_ahead >= 0: # Target day already happened this week
        days_ahead -= 7
    return d + datetime.timedelta(days_ahead)
59/6: df_week = copy.deepcopy(df_1)
59/7:
## 5 - Saturday
## –¥–æ–±–∞–≤–ª—è—é –¥–∞—Ç—ã –ø—Ä–æ—à–ª–æ–π –∏ —Å–ª–µ–¥. —Å—É–±–±–æ—Ç—ã (—è –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –¥–µ–ª–∞–ª–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏ —Ñ-—Ü–∏—é –∏ –≤ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
## –Ω–µ –ø–æ–ø–∞–¥–∞–ª–∏ –Ω–µ–¥–µ–ª–∏, –≤ –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –±—ã–ª–æ —Å–∫–∏–¥–æ–∫ –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∞. –ü–æ—Ç–æ–∏ –∏–∑–º–µ–Ω–µ–Ω–æ –Ω–∞ –≤–∑—è—Ç–∏–µ –º–∏–Ω–∏–º—É–º–∞ –∏ –º–∞–∫—Å–∏–º—É–º–∞)
df_week['prev_sat'] = df_week.apply(lambda x: prev_weekday(x['start_date'], 5), axis=1)
df_week['next_sat'] = df_week.apply(lambda x: next_weekday(x['end_date'], 5), axis=1)
59/8:
df_1["date"] = df_1.apply(
    lambda x: pd.date_range(x["start_date"], x["end_date"]), axis=1
)
59/9:
df_week["date"] = df_week.apply(
    lambda x: pd.date_range(df_week.prev_sat.min(), df_week.next_sat.max()), axis=1
)
59/10:
df_1 = (
    df_1.explode("date")
    .drop(columns=["org_price", "start_date", "end_date"])
)
59/11:
df_week = (
    df_week.explode("date")
    .drop(columns=["price", "prev_sat", "next_sat", "start_date", "end_date"])
)
59/12: df = df_week.merge(df_1, on=['date', 'product_ID'], how='left')
59/13: df['price'] = df['price'].fillna(df['org_price'])
59/14: df.head(5)
59/15:
df_final = df.groupby('product_ID')\
             .resample('W-SAT', on='date', closed = "right", label = "right")\
             .mean().reset_index()
59/16: df_final
59/17:
## 1 –≤ 1 —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º –∏–∑ –∑–∞–¥–∞–Ω–∏—è
df_final
59/18: import seaborn as sns
59/19:
df_2 = pd.read_csv('transactions_data.txt', sep=' ', header=None)
df_2.columns = ['transaction_price', 'square_meters', 'num_bedrooms', 'floor', 'is_collab_firm']
df_2.head()
59/20: df_2.is_collab_firm.value_counts()/len(df_2)
59/21: ### 20% –¥–æ–º–æ–≤ –±—ã–ª–∏ –ø—Ä–æ–¥–∞–Ω—ã —Å –ø–æ–º–æ—â—å—é —Ñ–∏—Ä–º –∏ –∏—Ö –¥–∞–Ω–Ω—ã–µ –∞–±—Å–æ–ª—é—Ç–Ω–æ –ø—Ä–æ–∑—Ä–∞—á–Ω—ã
59/22: g = sns.pairplot(df_2, kind="reg", hue='is_collab_firm', plot_kws=dict(scatter_kws=dict(s=2)))
59/23: from sklearn.linear_model import LinearRegression
59/24: from sklearn.model_selection import train_test_split
59/25: reg = LinearRegression(normalize=True)
59/26:
df_collab = df_2[df_2.is_collab_firm == 1]
df_not_collab = copy.deepcopy(df_2[df_2.is_collab_firm != 1])
59/27:
target = df_collab['transaction_price']
train = df_collab.drop(['transaction_price', 'is_collab_firm'],axis=1)
59/28: x_train , x_test , y_train , y_test = train_test_split(train , target , test_size = 0.20,random_state=0)
59/29: reg.fit(x_train,y_train)
59/30: reg.score(x_test,y_test)
59/31: df_not_collab.iloc[:, 1:-1]
59/32: df_not_collab['pred'] = reg.predict(df_not_collab.iloc[:, 1:-1])
59/33: df_not_collab['delta'] = df_not_collab.pred - df_not_collab.transaction_price
59/34: df_not_collab[:5]
59/35: df_not_collab.delta.describe()
59/36: from sklearn.ensemble import IsolationForest
59/37:
iforest = IsolationForest(n_estimators=10, max_samples='auto', 
                          contamination=0.05, max_features=3, 
                          bootstrap=False, random_state=1)
59/38: df_not_collab_IF = copy.deepcopy(df_2[df_2.is_collab_firm != 1])
59/39:
pred= iforest.fit_predict(df_not_collab_IF)
df_not_collab_IF['scores']=iforest.decision_function(df_not_collab_IF)
df_not_collab_IF['anomaly_label']=pred
59/40:
# –ü–æ–¥—Å–≤–µ—á–∏–≤–∞–µ–º –∞–Ω–æ–º–∞–ª–∏–∏ 
df_not_collab_IF[df_not_collab_IF.anomaly_label==-1]
59/41:
# –ü—Ä–æ–≤–µ—Ä–∏–º, –∫–∞–∫–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–¥–µ–ª–∞–ª–∞ –º–æ–¥–µ–ª—å —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –¥–ª—è –∞–Ω–æ–º–∞–ª–∏–π
df_not_collab[(df_not_collab.transaction_price == 491735) | 
             (df_not_collab.transaction_price == 450027) |
             (df_not_collab.transaction_price == 957630) |
             (df_not_collab.transaction_price == 515977)]
59/42: df_not_collab.sort_values('delta')
59/43: %history
60/1:
import pandas as pd
import numpy as np
import datetime
import copy
60/2:
df_1 = pd.DataFrame({'start_date': ['01/01/2019', '01/01/2019', '02/15/2019'],
                     'end_date': ['01/16/2019', '01/21/2019', '02/20/2019'],
                     'product_ID': ['A', 'B', 'A'],
                     'price': [100, 150, 150],
                     'org_price': [200, 300, 200]})
df_1
60/3:
df_1.start_date = pd.to_datetime(df_1.start_date)    
df_1.end_date = pd.to_datetime(df_1.end_date)
60/4:
## –Ω–∞ –≤—Ö–æ–¥ - (–¥–∞—Ç–∞, –¥–µ–Ω—å –Ω–µ–¥–µ–ª–∏ (—Å 0 –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è))
## –≤—ã—Ö–æ–¥ - –¥–∞—Ç–∞ —Å–ª–µ–¥—É—é—â–µ–≥–æ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –¥–Ω—è –Ω–µ–¥–µ–ª–∏
def next_weekday(d, weekday):
    days_ahead = weekday - d.weekday()
    if days_ahead <= 0:
        days_ahead += 7
    return d + datetime.timedelta(days_ahead)
60/5:
## –Ω–∞ –≤—Ö–æ–¥ - (–¥–∞—Ç–∞, –¥–µ–Ω—å –Ω–µ–¥–µ–ª–∏ (—Å 0 –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è))
## –≤—ã—Ö–æ–¥ - –¥–∞—Ç–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –¥–Ω—è –Ω–µ–¥–µ–ª–∏
def prev_weekday(d, weekday):
    days_ahead = weekday - d.weekday()+1
    if days_ahead >= 0: # Target day already happened this week
        days_ahead -= 7
    return d + datetime.timedelta(days_ahead)
60/6: df_week = copy.deepcopy(df_1)
60/7:
## 5 - Saturday
## –¥–æ–±–∞–≤–ª—è—é –¥–∞—Ç—ã –ø—Ä–æ—à–ª–æ–π –∏ —Å–ª–µ–¥. —Å—É–±–±–æ—Ç—ã (—è –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –¥–µ–ª–∞–ª–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏ —Ñ-—Ü–∏—é –∏ –≤ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
## –Ω–µ –ø–æ–ø–∞–¥–∞–ª–∏ –Ω–µ–¥–µ–ª–∏, –≤ –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –±—ã–ª–æ —Å–∫–∏–¥–æ–∫ –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∞. –ü–æ—Ç–æ–∏ –∏–∑–º–µ–Ω–µ–Ω–æ –Ω–∞ –≤–∑—è—Ç–∏–µ –º–∏–Ω–∏–º—É–º–∞ –∏ –º–∞–∫—Å–∏–º—É–º–∞)
df_week['prev_sat'] = df_week.apply(lambda x: prev_weekday(x['start_date'], 5), axis=1)
df_week['next_sat'] = df_week.apply(lambda x: next_weekday(x['end_date'], 5), axis=1)
60/8:
df_1["date"] = df_1.apply(
    lambda x: pd.date_range(x["start_date"], x["end_date"]), axis=1
)
60/9:
df_week["date"] = df_week.apply(
    lambda x: pd.date_range(df_week.prev_sat.min(), df_week.next_sat.max()), axis=1
)
60/10:
df_1 = (
    df_1.explode("date")
    .drop(columns=["org_price", "start_date", "end_date"])
)
60/11:
df_week = (
    df_week.explode("date")
    .drop(columns=["price", "prev_sat", "next_sat", "start_date", "end_date"])
)
60/12: df = df_week.merge(df_1, on=['date', 'product_ID'], how='left')
60/13: df['price'] = df['price'].fillna(df['org_price'])
60/14: df.head(5)
60/15:
df_final = df.groupby('product_ID')\
             .resample('W-SAT', on='date', closed = "right", label = "right")\
             .mean().reset_index()
60/16:
## 1 –≤ 1 —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º –∏–∑ –∑–∞–¥–∞–Ω–∏—è
df_final
60/17: import seaborn as sns
60/18:
df_2 = pd.read_csv('transactions_data.txt', sep=' ', header=None)
df_2.columns = ['transaction_price', 'square_meters', 'num_bedrooms', 'floor', 'is_collab_firm']
df_2.head()
60/19: df_2.is_collab_firm.value_counts()/len(df_2)
60/20: ### 20% –¥–æ–º–æ–≤ –±—ã–ª–∏ –ø—Ä–æ–¥–∞–Ω—ã —Å –ø–æ–º–æ—â—å—é —Ñ–∏—Ä–º –∏ –∏—Ö –¥–∞–Ω–Ω—ã–µ –∞–±—Å–æ–ª—é—Ç–Ω–æ –ø—Ä–æ–∑—Ä–∞—á–Ω—ã
60/21: g = sns.pairplot(df_2, kind="reg", hue='is_collab_firm', plot_kws=dict(scatter_kws=dict(s=2)))
60/22: from sklearn.linear_model import LinearRegression
60/23: from sklearn.model_selection import train_test_split
60/24: reg = LinearRegression(normalize=True)
60/25:
df_collab = df_2[df_2.is_collab_firm == 1]
df_not_collab = copy.deepcopy(df_2[df_2.is_collab_firm != 1])
60/26:
target = df_collab['transaction_price']
train = df_collab.drop(['transaction_price', 'is_collab_firm'],axis=1)
60/27: x_train , x_test , y_train , y_test = train_test_split(train , target , test_size = 0.20,random_state=0)
60/28: reg.fit(x_train,y_train)
60/29: reg.score(x_test,y_test)
60/30: df_not_collab.iloc[:, 1:-1]
60/31: df_not_collab['pred'] = reg.predict(df_not_collab.iloc[:, 1:-1])
60/32: df_not_collab['delta'] = df_not_collab.pred - df_not_collab.transaction_price
60/33: df_not_collab[:5]
60/34: df_not_collab.delta.describe()
60/35: df_not_collab.sort_values(delta)
60/36: df_not_collab.sort_values(delta)
60/37: df_not_collab.sort_values('delta')
60/38: df_not_collab.sort_values('delta', ascending=False)[:15]
60/39: ### –¢–û–ü-15 —á–µ–ª–æ–≤–µ–∫ —Å –ø—Ä–æ–¥–∞–∂–∞–º–∏ –Ω–∏–∂–µ —Ä—ã–Ω–∫–∞
60/40: df_not_collab.sort_values('delta', ascending=False)[:15]
60/41: from sklearn.ensemble import IsolationForest
60/42:
iforest = IsolationForest(n_estimators=10, max_samples='auto', 
                          contamination=0.05, max_features=3, 
                          bootstrap=False, random_state=1)
60/43: df_not_collab_IF = copy.deepcopy(df_2[df_2.is_collab_firm != 1])
60/44:
pred= iforest.fit_predict(df_not_collab_IF)
df_not_collab_IF['scores']=iforest.decision_function(df_not_collab_IF)
df_not_collab_IF['anomaly_label']=pred
60/45:
# –ü–æ–¥—Å–≤–µ—á–∏–≤–∞–µ–º –∞–Ω–æ–º–∞–ª–∏–∏ 
df_not_collab_IF[df_not_collab_IF.anomaly_label==-1]
60/46:
# –ü—Ä–æ–≤–µ—Ä–∏–º, –∫–∞–∫–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–¥–µ–ª–∞–ª–∞ –º–æ–¥–µ–ª—å —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –¥–ª—è –∞–Ω–æ–º–∞–ª–∏–π
df_not_collab[(df_not_collab.transaction_price == 491735) | 
             (df_not_collab.transaction_price == 450027) |
             (df_not_collab.transaction_price == 957630) |
             (df_not_collab.transaction_price == 515977)]
60/47: df_not_collab.sort_values('delta')
61/1:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
61/2: id_password
61/3: id_password.toPandas()
61/4: y = pd.DataFrame(data)
61/5: y = pd.DataFrame(id_password)
61/6: y
61/7: y.columns = [Id, Password]
61/8: y.columns = ['Id', 'Password']
61/9: id_name_verified.merge(y, on='Id')
61/10: id_name_verified = id_name_verified.merge(y, on='Id')
61/11: id_name_verified
61/12:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    y = pd.DataFrame(id_password)    
    y.columns = ['Id', 'Password']
    id_name_verified = id_name_verified.merge(y, on='Id')
    

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
61/13:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    y = pd.DataFrame(id_password)    
    y.columns = ['Id', 'Password']
    id_name_verified = id_name_verified.merge(y, on='Id')
    print(id_name_verified)
    

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
61/14:
import pandas as pd
import numpy as np
import copy

def login_table(id_name_verified, id_password):
    y = pd.DataFrame(id_password)    
    y.columns = ['Id', 'Password']
    id_name_verified = copy.deepcopy(id_name_verified.merge(y, on='Id'))
    
    

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
61/15:
import pandas as pd
import numpy as np
import copy

def login_table(id_name_verified, id_password):
    y = pd.DataFrame(id_password)    
    y.columns = ['Id', 'Password']
    id_name_verified.merge(y, on='Id')
    
    

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
61/16:
import pandas as pd
import numpy as np
import copy

def login_table(id_name_verified, id_password):
    y = pd.DataFrame(id_password)    
    y.columns = ['Id', 'Password']
    id_name_verified.merge(y, on='Id')
    return id_name_verified
    
    

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
61/17:
import pandas as pd
import numpy as np
import copy

def login_table(id_name_verified, id_password):
    y = pd.DataFrame(id_password)    
    y.columns = ['Id', 'Password']
    id_name_verified = id_name_verified.merge(y, on='Id')    
    

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
61/18:
import pandas as pd
import numpy as np
import copy

def login_table(id_name_verified, id_password):
    y = pd.DataFrame(id_password)    
    y.columns = ['Id', 'Password']
    id_name_verified = id_name_verified.merge(y, on='Id')    
    print(id_name_verified)

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
61/19:
import pandas as pd
import numpy as np
import copy

def login_table(id_name_verified, id_password):
    y = pd.DataFrame(id_password)    
    y.columns = ['Id', 'Password']
    id_name_verified = id_name_verified.merge(y, on='Id') 
    id_name_verified.drop('Verified', axis=1, inplace=True)
    print(id_name_verified)

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
61/20:
import pandas as pd
import numpy as np
import copy

def login_table(id_name_verified, id_password):
    y = pd.DataFrame(id_password)    
    y.columns = ['Id', 'Password']
    id_name_verified = id_name_verified.merge(y, on='Id') 
    id_name_verified.drop('Verified', axis=1, inplace=True)
    print(id_name_verified)
61/21:
id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
61/22: login_table(id_name_verified, id_password)
61/23: print(id_name_verified)
61/24:
import pandas as pd
import numpy as np
import copy

def login_table(id_name_verified, id_password):
    y = pd.DataFrame(id_password)    
    y.columns = ['Id', 'Password']
    id_name_verified = id_name_verified.merge(y, on='Id') 
    id_name_verified.drop('Verified', axis=1, inplace=True)
    print(id_name_verified)
    rerurn id_name_verified
61/25:
import pandas as pd
import numpy as np
import copy

def login_table(id_name_verified, id_password):
    y = pd.DataFrame(id_password)    
    y.columns = ['Id', 'Password']
    id_name_verified = id_name_verified.merge(y, on='Id') 
    id_name_verified.drop('Verified', axis=1, inplace=True)
    print(id_name_verified)
    return id_name_verified
61/26:
id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
61/27: login_table(id_name_verified, id_password)
61/28: print(id_name_verified)
61/29:
import pandas as pd
import numpy as np
import copy

def login_table(id_name_verified, id_password):
    y = pd.DataFrame(id_password)    
    y.columns = ['Id', 'Password']
    id_name_verified = id_name_verified.merge(y, on='Id') 
    id_name_verified.drop('Verified', axis=1, inplace=True)
    return id_name_verified
61/30:
id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
61/31: login_table(id_name_verified, id_password)
61/32: print(id_name_verified)
61/33: id_name_verified = login_table(id_name_verified, id_password)
61/34: print(id_name_verified)
63/1:
import pandas as pd
import numpy as np
63/2:
data = pd.read_csv('https://www.openml.org/data/get_csv/16826755/phpMYEkMl')
data.head()
63/3:
data = data.replace('?', np.nan)
data.isnull().sum()
64/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
64/2:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
64/3:
# let's load the dataset 

# Variable definitions:
#-------------------------
# disbursed_amount: loan amount given to the borrower
# interest: interest rate
# income: annual income
# number_open_accounts: open accounts (more on this later)
# number_credit_lines_12: accounts opened in the last 12 months
# target: loan status(paid or being repaid = 1, defaulted = 0)


data = pd.read_csv('../loan.csv')

data.head()
64/4:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
64/5:
# let's load the dataset 

# Variable definitions:
#-------------------------
# disbursed_amount: loan amount given to the borrower
# interest: interest rate
# income: annual income
# number_open_accounts: open accounts (more on this later)
# number_credit_lines_12: accounts opened in the last 12 months
# target: loan status(paid or being repaid = 1, defaulted = 0)


data = pd.read_csv('../loan.csv')

data.head()
64/6: fig = data['disbursed_amount'].hist(bins=50)
64/7: fig
64/8: fig.figure(figsize=(10,3))
64/9: fig.set_size_inches(18.5, 10.5)
64/10: fig.figure(figsize=(3,4))
64/11: fig
64/12: fig = data['disbursed_amount'].hist(bins=50, figsize=(10,2))
64/13:
# let's make a histogram to get familiar with the
# distribution of the variable

fig = data['disbursed_amount'].hist(bins=50)

fig.set_title('Loan Amount Requested')
fig.set_xlabel('Loan Amount')
fig.set_ylabel('Number of Loans')
64/14:
# Now, let's explore the income declared by the customers,
# that is, how much they earn yearly.

# this variable is also continuous

fig = data['income'].hist(bins=100)

# for better visualisation, I display only specific
# range in the x-axis
#fig.set_xlim(0, 400000)

# title and axis legends
fig.set_title("Customer's Annual Income")
fig.set_xlabel('Annual Income')
fig.set_ylabel('Number of Customers')
64/15:
# Now, let's explore the income declared by the customers,
# that is, how much they earn yearly.

# this variable is also continuous

fig = data['income'].hist(bins=100)

# for better visualisation, I display only specific
# range in the x-axis
fig.set_xlim(0, 400000)

# title and axis legends
fig.set_title("Customer's Annual Income")
fig.set_xlabel('Annual Income')
fig.set_ylabel('Number of Customers')
65/1:
import pandas as pd

import matplotlib.pyplot as plt
65/2:
# let's load the dataset

# Variable definitions:
#-------------------------
# loan_purpose: intended use of the loan
# market: the risk market assigned to the borrower (based in their financial situation)
# householder: whether the borrower owns or rents their property

data = pd.read_csv('../loan.csv')

data.head()
65/3: data['householder'].value_counts()
65/4: data['householder'].value_counts().mean()
65/5: data['householder'].value_counts()
65/6: data['householder'].value_counts() / len(data)
65/7: fig = data['householder'].value_counts().plot.bar()
65/8:
# let's make a bar plot, with the number of loans
# for each category of home ownership

# the code below counts the number of observations (borrowers)
# within each category and then makes a bar plot

fig = data['householder'].value_counts().plot.bar()
fig.set_title('Householder')
fig.set_ylabel('Number of customers')
65/9: fig = data['householder'].value_counts().plot.bar()
65/10: data['householder'].value_counts() / len(data)
65/11: data['householder'].value_counts()
65/12: data['householder'].value_counts()
65/13:
import numpy
print(numpy.__version__)
65/14: $pip install numpy --upgrade
65/15: !pip install numpy --upgrade
69/1:
import numpy
print(numpy.__version__)
69/2:
# let's make a bar plot, with the number of loans
# for each category of home ownership

# the code below counts the number of observations (borrowers)
# within each category and then makes a bar plot

fig = data['householder'].value_counts().plot.bar()
fig.set_title('Householder')
fig.set_ylabel('Number of customers')
69/3:
import pandas as pd

import matplotlib.pyplot as plt
69/4:
# let's load the dataset

# Variable definitions:
#-------------------------
# loan_purpose: intended use of the loan
# market: the risk market assigned to the borrower (based in their financial situation)
# householder: whether the borrower owns or rents their property

data = pd.read_csv('../loan.csv')

data.head()
69/5:
# let's inspect the variable householder,
# which indicates whether the borrowers own their home
# or if they are renting, among other things.

data['householder'].unique()
69/6: data['householder'].value_counts() / len(data)
69/7:
import numpy
print(numpy.__version__)
69/8:
# let's make a bar plot, with the number of loans
# for each category of home ownership

# the code below counts the number of observations (borrowers)
# within each category and then makes a bar plot

fig = data['householder'].value_counts().plot.bar()
fig.set_title('Householder')
fig.set_ylabel('Number of customers')
69/9: $pip install numpy --upgrade
69/10: data['householder'].value_counts()
69/11: data['householder'].value_counts().reset_index()
69/12:
# let's make a bar plot, with the number of loans
# for each category of home ownership

# the code below counts the number of observations (borrowers)
# within each category and then makes a bar plot

fig = data['householder'].value_counts().reset_index().plot.bar()
fig.set_title('Householder')
fig.set_ylabel('Number of customers')
69/13:
# let's make a bar plot, with the number of loans
# for each category of home ownership

# the code below counts the number of observations (borrowers)
# within each category and then makes a bar plot

fig = data['householder'].value_counts().reset_index().plot.hist()
fig.set_title('Householder')
fig.set_ylabel('Number of customers')
69/14:
df = pd.DataFrame({'lab':['A', 'B', 'C'], 'val':[10, 30, 20]})
ax = df.plot.bar(x='lab', y='val', rot=0)
69/15:
df = pd.DataFrame({'lab':['A', 'B', 'C'], 'val':[10, 30, 20]})
df
69/16:
df = pd.DataFrame({'lab':['A', 'B', 'C'], 'val':[10, 30, 20]})
ax = df.plot.bar(x='lab', y='val', rot=0)
69/17: !pip install --upgrade numpy==1.19.5
70/1:
import numpy
print(numpy.__version__)
70/2: !conda install --upgrade numpy==1.19.5
72/1:
import pandas as pd

import matplotlib.pyplot as plt
72/2:
# let's load the dataset

# Variable definitions:
#-------------------------
# loan_purpose: intended use of the loan
# market: the risk market assigned to the borrower (based in their financial situation)
# householder: whether the borrower owns or rents their property

data = pd.read_csv('../loan.csv')

data.head()
72/3:
# let's inspect the variable householder,
# which indicates whether the borrowers own their home
# or if they are renting, among other things.

data['householder'].unique()
72/4: data['householder'].value_counts() / len(data)
72/5:
import numpy
print(numpy.__version__)
72/6: !pip install --upgrade numpy==1.19.5
72/7:
df = pd.DataFrame({'lab':['A', 'B', 'C'], 'val':[10, 30, 20]})
ax = df.plot.bar(x='lab', y='val', rot=0)
72/8:
# let's make a bar plot, with the number of loans
# for each category of home ownership

# the code below counts the number of observations (borrowers)
# within each category and then makes a bar plot

fig = data['householder'].value_counts().reset_index().plot.hist()
fig.set_title('Householder')
fig.set_ylabel('Number of customers')
72/9:
# let's make a bar plot, with the number of loans
# for each category of home ownership

# the code below counts the number of observations (borrowers)
# within each category and then makes a bar plot

fig = data['householder'].value_counts().plot.hist()
fig.set_title('Householder')
fig.set_ylabel('Number of customers')
72/10: data['householder'].value_counts()
72/11:
# the "loan_purpose" variable is another categorical variable
# that indicates how the borrowers intend to use the
# money they are borrowing, for example to improve their
# house, or to cancel previous debt.

data['loan_purpose'].unique()
72/12:
# let's make a bar plot with the number of borrowers
# within each category

# the code below counts the number of observations (borrowers)
# within each category and then makes a plot

fig = data['loan_purpose'].value_counts().plot.bar()
fig.set_title('Loan Purpose')
fig.set_ylabel('Number of customers')
72/13: data['householder'].value_counts()
72/14:
# let's make a bar plot, with the number of loans
# for each category of home ownership

# the code below counts the number of observations (borrowers)
# within each category and then makes a bar plot

fig = data['householder'].value_counts().plot.hist()
fig.set_title('Householder')
fig.set_ylabel('Number of customers')
72/15:
# let's make a bar plot, with the number of loans
# for each category of home ownership

# the code below counts the number of observations (borrowers)
# within each category and then makes a bar plot

fig = data['householder'].value_counts().plot.bar()
fig.set_title('Householder')
fig.set_ylabel('Number of customers')
72/16: data['householder'].value_counts()
72/17:
# the "loan_purpose" variable is another categorical variable
# that indicates how the borrowers intend to use the
# money they are borrowing, for example to improve their
# house, or to cancel previous debt.

data['loan_purpose'].unique()
72/18:
# let's make a bar plot with the number of borrowers
# within each category

# the code below counts the number of observations (borrowers)
# within each category and then makes a plot

fig = data['loan_purpose'].value_counts().plot.bar()
fig.set_title('Loan Purpose')
fig.set_ylabel('Number of customers')
72/19:
# let's look at one additional categorical variable,
# "market", which represents the risk market or risk band
# assigned to the borrower

data['market'].unique()
72/20:
# let's make a bar plot with the number of borrowers
# within each category

fig = data['market'].value_counts().plot.bar()
fig.set_title('Status of the Loan')
fig.set_ylabel('Number of customers')
72/21:
# finally, let's look at a variable that is numerical,
# but its numbers have no real meaning
# their values are more "labels" than real numbers

data['customer_id'].head()
72/22:
# The variable has as many different id values as customers,
# in this case 10000, 

len(data['customer_id'].unique())
73/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
73/2:
# let's load the dataset

# Variable definitions:
#-------------------------
# disbursed amount: loan amount lent to the borrower
# market: risk band in which borrowers are placed
# loan purpose: intended use of the loan
# date_issued: date the loan was issued
# date_last_payment: date of last payment towards repyaing the loan

data = pd.read_csv('../loan.csv')

data.head()
73/3:
# pandas assigns type 'object' when reading dates
# and considers them strings.
# Let's have a look

data[['date_issued', 'date_last_payment']].dtypes
73/4:
# now let's parse the dates, currently coded as strings, into datetime format
# this will allow us to make some analysis afterwards

data['date_issued_dt'] = pd.to_datetime(data['date_issued'])
data['date_last_payment_dt'] = pd.to_datetime(data['date_last_payment'])

data[['date_issued', 'date_issued_dt', 'date_last_payment', 'date_last_payment_dt']].head()
73/5: data[['date_issued', 'date_issued_dt', 'date_last_payment', 'date_last_payment_dt']].dtypes
74/1:
import pandas as pd

import matplotlib.pyplot as plt
74/2:
# open_il_24m indicates:
# "Number of installment accounts opened in past 24 months".
# Installment accounts are those that, at the moment of acquiring them,
# there is a set period and amount of repayments agreed between the
# lender and borrower. An example of this is a car loan, or a student loan.
# the borrowers know that they are going to pay a fixed amount over a fixed period

data = pd.read_csv('../sample_s2.csv')
data.head()
74/3: data.shape
74/4:
# Fictitious meaning of the different letters / codes
# in the variable:
# 'A': couldn't identify the person
# 'B': no relevant data
# 'C': person seems not to have any account open

data.open_il_24m.unique()
74/5:
# Now, let's make a bar plot showing the different number of 
# borrowers for each of the values of the mixed variable

fig = data.open_il_24m.value_counts().plot.bar()
fig.set_title('Number of installment accounts open')
fig.set_ylabel('Number of borrowers')
74/6: data.dtype
74/7: data.dtype
74/8: data.dtypes
75/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

# to display the total number columns present in the dataset
pd.set_option('display.max_columns', None)
75/2:
# let's load the titanic dataset
data = pd.read_csv('../titanic.csv')

# let's inspect the first 5 rows
data.head()
76/1:
import pandas as pd
import numpy as np
76/2:
data = pd.read_csv('https://www.openml.org/data/get_csv/16826755/phpMYEkMl')
data.head()
76/3:
data = data.replace('?', np.nan)
data.isnull().sum()
76/4:
def get_first_cabin(row):
    try:
        return row.split()[0]
    except:
        return np.nan
76/5: data['cabin'] = data['cabin'].apply(get_first_cabin)
76/6: data.to_csv('../titanic.csv', index=False)
75/3:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

# to display the total number columns present in the dataset
pd.set_option('display.max_columns', None)
75/4:
# let's load the titanic dataset
data = pd.read_csv('../titanic.csv')

# let's inspect the first 5 rows
data.head()
75/5:
# we can quantify the total number of missing values using
# the isnull method plus the sum method on the dataframe

data.isnull().sum()
75/6: data.isnull().mean()
75/7:
# alternatively, we can use the mean method after isnull
# to visualise the percentage of
# missing values for each variable

data.isnull().mean()
77/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

# to build machine learning models
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier

# to evaluate the models
from sklearn.metrics import roc_auc_score

# to separate data into train and test
from sklearn.model_selection import train_test_split
77/2:
# let's load the titanic dataset

data = pd.read_csv('../titanic.csv')
data.head()
77/3:
# let's inspect the cardinality, this is the number
# of different labels, for the different categorical variables

print('Number of categories in the variable Name: {}'.format(
    len(data.name.unique())))

print('Number of categories in the variable Gender: {}'.format(
    len(data.sex.unique())))

print('Number of categories in the variable Ticket: {}'.format(
    len(data.ticket.unique())))

print('Number of categories in the variable Cabin: {}'.format(
    len(data.cabin.unique())))

print('Number of categories in the variable Embarked: {}'.format(
    len(data.embarked.unique())))

print('Total number of passengers in the Titanic: {}'.format(len(data)))
77/4:
# Let's re-map Cabin into numbers so we can use it to train ML models

# I will replace each cabin by a number
# to quickly demonstrate the effect of
# labels on machine learning algorithms

##############
# Note: this is neither the only nor the best
# way to encode categorical variables into numbers
# there is more on these techniques in the section
# "Encoding categorical variales"
##############

cabin_dict = {k: i for i, k in enumerate(X_train.cabin.unique(), 0)}
cabin_dict
77/5:
# Let's find out labels present only in the training set

unique_to_train_set = [
    x for x in X_train.cabin.unique() if x not in X_test.cabin.unique()
]

len(unique_to_train_set)
77/6:
# let's load the titanic dataset

data = pd.read_csv('../titanic.csv')
data.head()
77/7:
# let's inspect the cardinality, this is the number
# of different labels, for the different categorical variables

print('Number of categories in the variable Name: {}'.format(
    len(data.name.unique())))

print('Number of categories in the variable Gender: {}'.format(
    len(data.sex.unique())))

print('Number of categories in the variable Ticket: {}'.format(
    len(data.ticket.unique())))

print('Number of categories in the variable Cabin: {}'.format(
    len(data.cabin.unique())))

print('Number of categories in the variable Embarked: {}'.format(
    len(data.embarked.unique())))

print('Total number of passengers in the Titanic: {}'.format(len(data)))
77/8:
# let's explore the values / categories of Cabin

# we know from the previous cell that there are 148
# different cabins, therefore the variable
# is highly cardinal

data.cabin.unique()
77/9:
# let's capture the first letter of Cabin
data['Cabin_reduced'] = data['cabin'].astype(str).str[0]

data[['cabin', 'Cabin_reduced']].head()
77/10:
print('Number of categories in the variable Cabin: {}'.format(
    len(data.cabin.unique())))

print('Number of categories in the variable Cabin reduced: {}'.format(
    len(data.Cabin_reduced.unique())))
77/11:
# let's separate into training and testing set
# in order to build machine learning models

use_cols = ['cabin', 'Cabin_reduced', 'sex']

# this functions comes from scikit-learn
X_train, X_test, y_train, y_test = train_test_split(
    data[use_cols], 
    data['survived'],  
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
77/12:
# Let's find out labels present only in the training set

unique_to_train_set = [
    x for x in X_train.cabin.unique() if x not in X_test.cabin.unique()
]

len(unique_to_train_set)
77/13:
# Let's find out labels present only in the test set

unique_to_test_set = [
    x for x in X_test.cabin.unique() if x not in X_train.cabin.unique()
]

len(unique_to_test_set)
77/14:
# Let's find out labels present only in the training set
# for Cabin with reduced cardinality

unique_to_train_set = [
    x for x in X_train['Cabin_reduced'].unique()
    if x not in X_test['Cabin_reduced'].unique()
]

len(unique_to_train_set)
77/15:
# Let's find out labels present only in the test set
# for Cabin with reduced cardinality

unique_to_test_set = [
    x for x in X_test['Cabin_reduced'].unique()
    if x not in X_train['Cabin_reduced'].unique()
]

len(unique_to_test_set)
77/16:
# Let's re-map Cabin into numbers so we can use it to train ML models

# I will replace each cabin by a number
# to quickly demonstrate the effect of
# labels on machine learning algorithms

##############
# Note: this is neither the only nor the best
# way to encode categorical variables into numbers
# there is more on these techniques in the section
# "Encoding categorical variales"
##############

cabin_dict = {k: i for i, k in enumerate(X_train.cabin.unique(), 0)}
cabin_dict
77/17:
# replace the labels in Cabin, using the dic created above
X_train.loc[:, 'Cabin_mapped'] = X_train.loc[:, 'cabin'].map(cabin_dict)
X_test.loc[:, 'Cabin_mapped'] = X_test.loc[:, 'cabin'].map(cabin_dict)

X_train[['Cabin_mapped', 'cabin']].head(10)
77/18:
# Now I will replace the letters in the reduced cabin variable
# with the same procedure

# create replace dictionary
cabin_dict = {k: i for i, k in enumerate(X_train['Cabin_reduced'].unique(), 0)}

# replace labels by numbers with dictionary
X_train.loc[:, 'Cabin_reduced'] = X_train.loc[:, 'Cabin_reduced'].map(
    cabin_dict)
X_test.loc[:, 'Cabin_reduced'] = X_test.loc[:, 'Cabin_reduced'].map(cabin_dict)

X_train[['Cabin_reduced', 'cabin']].head(20)
77/19: X_train.loc[:, 'cabin']
77/20:
# check if there are missing values in these variables

X_train[['Cabin_mapped', 'Cabin_reduced', 'sex']].isnull().sum()
77/21:

X_train[['Cabin_mapped', 'Cabin_reduced', 'sex']]
77/22: X_train
77/23: X_train[X_train.Cabin_reduced > 0]
78/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

# to separate data intro train and test sets
from sklearn.model_selection import train_test_split
78/2:
# let's load the dataset with the variables
# we need for this demo

# Variable definitions:

# Neighborhood: Physical locations within Ames city limits
# Exterior1st: Exterior covering on house
# Exterior2nd: Exterior covering on house (if more than one material)

use_cols = ['Neighborhood', 'Exterior1st', 'Exterior2nd', 'SalePrice']

data = pd.read_csv('../houseprice.csv', usecols=use_cols)

data.head()
79/1:
import random
import pandas as pd
import numpy as np
79/2:
# load data
data = pd.read_csv('../crx.data', header=None)

# create variable names according to UCI Machine Learning
# Repo information
varnames = ['A'+str(s) for s in range(1,17)]

# add column names
data.columns = varnames

# replace ? by np.nan
data = data.replace('?', np.nan)

# re-cast some variables to the correct types 
data['A2'] = data['A2'].astype('float')
data['A14'] = data['A14'].astype('float')

# encode target to binary
data['A16'] = data['A16'].map({'+':1, '-':0})

data.head()
79/3:
import random
import pandas as pd
import numpy as np
79/4:
# load data
data = pd.read_csv('../crx.data', header=None)

# create variable names according to UCI Machine Learning
# Repo information
varnames = ['A'+str(s) for s in range(1,17)]

# add column names
data.columns = varnames

# replace ? by np.nan
data = data.replace('?', np.nan)

# re-cast some variables to the correct types 
data['A2'] = data['A2'].astype('float')
data['A14'] = data['A14'].astype('float')

# encode target to binary
data['A16'] = data['A16'].map({'+':1, '-':0})

data.head()
79/5:
## add more missing values to random positions
# this will help with the demos of the recipes

random.seed(9001)

values = set([random.randint(0, len(data)) for p in range(0, 100)])

for var in ['A3', 'A8', 'A9', 'A10']:
    data.loc[values, var] = np.nan
    
    
data.isnull().sum()
79/6:
# save the data
data.to_csv('../creditApprovalUCI.csv', index=False)
79/7: data.head()
79/8:
# find categorical variables
cat_cols = [c for c in data.columns if data[c].dtypes=='O']
data[cat_cols].head()
79/9:
# find numerical variables

num_cols = [c for c in data.columns if data[c].dtypes!='O']
data[num_cols].head()
78/3:
# let's load the dataset with the variables
# we need for this demo

# Variable definitions:

# Neighborhood: Physical locations within Ames city limits
# Exterior1st: Exterior covering on house
# Exterior2nd: Exterior covering on house (if more than one material)

use_cols = ['Neighborhood', 'Exterior1st', 'Exterior2nd', 'SalePrice']

data = pd.read_csv('../houseprice.csv', usecols=use_cols)

data.head()
78/4:
# let's load the dataset with the variables
# we need for this demo

# Variable definitions:

# Neighborhood: Physical locations within Ames city limits
# Exterior1st: Exterior covering on house
# Exterior2nd: Exterior covering on house (if more than one material)

use_cols = ['Neighborhood', 'Exterior1st', 'Exterior2nd', 'SalePrice']

data = pd.read_csv('../houseprice.csv', usecols=use_cols)

data.head()
78/5:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

# to separate data intro train and test sets
from sklearn.model_selection import train_test_split
78/6:
# let's load the dataset with the variables
# we need for this demo

# Variable definitions:

# Neighborhood: Physical locations within Ames city limits
# Exterior1st: Exterior covering on house
# Exterior2nd: Exterior covering on house (if more than one material)

use_cols = ['Neighborhood', 'Exterior1st', 'Exterior2nd', 'SalePrice']

data = pd.read_csv('../houseprice.csv', usecols=use_cols)

data.head()
78/7:
# let's load the dataset with the variables
# we need for this demo

# Variable definitions:

# Neighborhood: Physical locations within Ames city limits
# Exterior1st: Exterior covering on house
# Exterior2nd: Exterior covering on house (if more than one material)

use_cols = ['Neighborhood', 'Exterior1st', 'Exterior2nd', 'SalePrice']

data = pd.read_csv('../houseprice.csv', usecols=use_cols)

data.head()
78/8:
# let's look at the different number of labels
# in each variable (cardinality)

# these are the loaded categorical variables
cat_cols = ['Neighborhood', 'Exterior1st', 'Exterior2nd']

for col in cat_cols:
    print('variable: ', col, ' number of labels: ', data[col].nunique())

print('total houses: ', len(data))
78/9: total_houses
78/10: total_houses
78/11: df.groupby([var])['SalePrice'].mean().reset_index()
78/12:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

# to separate data intro train and test sets
from sklearn.model_selection import train_test_split
78/13:
# let's load the dataset with the variables
# we need for this demo

# Variable definitions:

# Neighborhood: Physical locations within Ames city limits
# Exterior1st: Exterior covering on house
# Exterior2nd: Exterior covering on house (if more than one material)

use_cols = ['Neighborhood', 'Exterior1st', 'Exterior2nd', 'SalePrice']

data = pd.read_csv('../houseprice.csv', usecols=use_cols)

data.head()
78/14:
# let's look at the different number of labels
# in each variable (cardinality)

# these are the loaded categorical variables
cat_cols = ['Neighborhood', 'Exterior1st', 'Exterior2nd']

for col in cat_cols:
    print('variable: ', col, ' number of labels: ', data[col].nunique())

print('total houses: ', len(data))
78/15: total_houses
78/16:
# let's plot how frequently each label
# appears in the dataset

# in other words, the percentage of houses in the data
# with each label

total_houses = len(data)

# for each categorical variable
for col in cat_cols:

    # count the number of houses per category
    # and divide by total houses

    # aka percentage of houses per category

    temp_df = pd.Series(data[col].value_counts() / total_houses)

    # make plot with the above percentages
    fig = temp_df.sort_values(ascending=False).plot.bar()
    fig.set_xlabel(col)

    # add a line at 5 % to flag the threshold for rare categories
    fig.axhline(y=0.05, color='red')
    fig.set_ylabel('Percentage of houses')
    plt.show()
78/17: df.groupby([var])['SalePrice'].mean().reset_index()
78/18:
# the following function calculates:

# 1) the percentage of houses per category
# 2) the mean SalePrice per category


def calculate_mean_target_per_category(df, var):

    # total number of houses
    total_houses = len(df)

    # percentage of houses per category
    temp_df = pd.Series(df[var].value_counts() / total_houses).reset_index()
    temp_df.columns = [var, 'perc_houses']

    # add the mean SalePrice
    temp_df = temp_df.merge(df.groupby([var])['SalePrice'].mean().reset_index(),
                            on=var,
                            how='left')

    return temp_df
78/19: data.groupby([var])['SalePrice'].mean().reset_index()
78/20: data.groupby([''Neighborhood''])['SalePrice'].mean().reset_index()
78/21: data.groupby(['Neighborhood'])['SalePrice'].mean().reset_index()
78/22:
# the following function calculates:

# 1) the percentage of houses per category
# 2) the mean SalePrice per category


def calculate_mean_target_per_category(df, var):

    # total number of houses
    total_houses = len(df)

    # percentage of houses per category
    temp_df = pd.Series(df[var].value_counts() / total_houses).reset_index()
    temp_df.columns = [var, 'perc_houses']

    # add the mean SalePrice
    temp_df = temp_df.merge(df.groupby([var])['SalePrice'].mean().reset_index(),
                            on=var,
                            how='left')

    return temp_df
78/23:
# now we use the function for the variable 'Neighborhood'
temp_df = calculate_mean_target_per_category(data, 'Neighborhood')
temp_df
78/24: temp_df
78/25: temp_df.index
78/26: cat_cols
80/1:
import pandas as pd
import numpy as np

# for plotting
import matplotlib.pyplot as plt
import seaborn as sns

# for the Q-Q plots
import scipy.stats as stats

# the dataset for the demo
from sklearn.datasets import load_boston

# for linear regression
from sklearn.linear_model import LinearRegression

# to split and standarize the dataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# to evaluate the regression model
from sklearn.metrics import mean_squared_error
80/2:
# load the the Boston House price data

# this is how we load the boston dataset from sklearn
boston_dataset = load_boston()

# create a dataframe with the independent variables
boston = pd.DataFrame(boston_dataset.data,
                      columns=boston_dataset.feature_names)

# add the target
boston['MEDV'] = boston_dataset.target

boston.head()
80/3:
# to train and evaluate the model, let's first split into
# train and test data, using 3 variables of choice:
# LSTAT, RM and CRIM

# let's separate into training and testing set
# using the sklearn function below

X_train, X_test, y_train, y_test = train_test_split(
    boston[['RM', 'LSTAT', 'CRIM']],
    boston['MEDV'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape, y_train.shape, y_test.shape
80/4:
# let's scale the features
# normal procedure for linear models
# I will explain this later in in the course

scaler = StandardScaler()
scaler.fit(X_train)
80/5:
# calculate the residuals

error = y_test - pred
80/6:
# let's build a linear model using the data as loaded from sklearn

# instantiate a lineear model
linreg = LinearRegression()

# train the model
linreg.fit(scaler.transform(X_train), y_train)

# make predictions on the train set and calculate
# the mean squared error
print('Train set')
pred = linreg.predict(scaler.transform(X_train))
print('Linear Regression mse: {}'.format(mean_squared_error(y_train, pred)))

# make predictions on the test set and calculate
# the mean squared error
print('Test set')
pred = linreg.predict(scaler.transform(X_test))
print('Linear Regression mse: {}'.format(mean_squared_error(y_test, pred)))
print()
80/7:
# calculate the residuals

error = y_test - pred
80/8:
# we will make a histogram to determine if the residuals
# are normally distributed with mean value at 0

sns.histplot(error, bins=30)
80/9: from yellowbrick.regressor import ResidualsPlot
80/10: ! conda install yellowbrick
81/1:
# load the titanic dataset

titanic = pd.read_csv('../titanic.csv',
                      usecols=['age', 'fare'])

# The variables age and fare have missing values,
# I will remove them for this demo
titanic.dropna(subset=['age', 'fare'], inplace=True)

titanic.head()
81/2:
# load the the Boston House price data

# load the boston dataset from sklearn
boston_dataset = load_boston()

# create a dataframe with the independent variables
# I will use only 3 of the total variables for this demo

boston = pd.DataFrame(boston_dataset.data,
                      columns=boston_dataset.feature_names)[[
                          'RM', 'LSTAT', 'CRIM'
                      ]]


boston.head()
81/3:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

# for Q-Q plots
import scipy.stats as stats

# boston house dataset for the demo
from sklearn.datasets import load_boston
81/4:
# load the the Boston House price data

# load the boston dataset from sklearn
boston_dataset = load_boston()

# create a dataframe with the independent variables
# I will use only 3 of the total variables for this demo

boston = pd.DataFrame(boston_dataset.data,
                      columns=boston_dataset.feature_names)[[
                          'RM', 'LSTAT', 'CRIM'
                      ]]


boston.head()
81/5:
# load the titanic dataset

titanic = pd.read_csv('../titanic.csv',
                      usecols=['age', 'fare'])

# The variables age and fare have missing values,
# I will remove them for this demo
titanic.dropna(subset=['age', 'fare'], inplace=True)

titanic.head()
82/1:
import pandas as pd
import numpy as np

# import several machine learning algorithms
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier

# to scale the features
from sklearn.preprocessing import MinMaxScaler

# to evaluate performance and separate into
# train and test set
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split
82/2:
# load numerical variables of the Titanic Dataset

data = pd.read_csv('../titanic.csv',
                   usecols=['pclass', 'age', 'fare', 'survived'])
data.head()
82/3:
# let's have a look at the values of those variables
# to get an idea of the feature magnitudes

data.describe()
83/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

# to show all the columns of the dataframe in the notebook
pd.set_option('display.max_columns', None)
83/2:
# let's load the House Prices dataset
# and explore its shape (rows and columns)

data = pd.read_csv('../houseprice.csv')
data.shape
83/3:
# let's visualise the dataset
data.head()
86/1:
import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
86/2: df = pd.read_csv('FuelEconomy.csv')
86/3: df
86/4: df.head(5)
86/5: df.tail(5)
86/6: df.describe()
86/7: df.info()
86/8: sns.jointplot(x='Horse Power', y='Fuel Ecomopy')
86/9: sns.jointplot(x='Horse Power', y='Fuel Ecomopy', data=df)
86/10: sns.jointplot(x='Horse Power', y='Fuel Ecomopy (MPG)', data=df)
86/11: sns.jointplot(x='Horse Power', y='Fuel Ecomopy (MPG)', data=df)
86/12: sns.jointplot(x='Horse Power', y='Fuel Economy (MPG)', data=df)
86/13: sns.jointplot(x='Horse Power', y='Fuel Economy (MPG)', data=df, color='blue')
86/14: sns.jointplot(x='Horse Power', y='Fuel Economy (MPG)', data=df, color='light blue')
86/15: sns.jointplot(x='Horse Power', y='Fuel Economy (MPG)', data=df, color='lightblue')
86/16: sns.jointplot(x='Horse Power', y='Fuel Economy (MPG)', data=df)
86/17: sns.pairplot(df)
86/18: sns.lmplot(x='Horse Power', y='Fuel Economy (MPG)', data=df)
86/19: df
86/20: X = df[['Horse Power']]
86/21: y = df[['Fuel Economy (MPG)']]
86/22: X
86/23: y = df['Fuel Economy (MPG)']
86/24: X
86/25: y
86/26: y.dtype
86/27: y.type
86/28: y
86/29: from sklearn.model_selection import train_test_split
86/30: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25)
86/31: X_test.shape
86/32: X_train.shape
86/33: X_test.shape
86/34: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)
86/35: X_train.shape
86/36: X_test.shape
86/37: from sklearn.linear_model import LinearRegression
86/38:
regressor = LinearRegression(fit_intercept=True)
regressor.fit(X_train, y_train)
86/39: regressor
86/40:
from sklearn.linear_model import LinearRegression
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train, y_train)
86/41: from sklearn.linear_model import LinearRegression
86/42:
regressor = LinearRegression(fit_intercept=True)
regressor.fit(X_train, y_train)
86/43: regressor.params
86/44: regressor.get_params
86/45: regressor.get_params()
86/46: regressor.coef_
86/47: print('Linear Model Coefficient (m):', regressor.coef_)
86/48:
print('Linear Model Coefficient (m):', regressor.coef_)
print('Linear Model Coefficient (b):', regressor.intercept_)
86/49: y_test
86/50: y_predict = regressor.predict(X_test)
86/51: y_predict
86/52: y_predict = regressor.predict(X_test)
86/53: y_predict
86/54: y_test
86/55: plt.scatter(X_train, y_train)
86/56: plt.scatter(X_train, y_train, color='red')
86/57:
plt.scatter(X_train, y_train, color='red')
plt.plot(X_train, regressor.predict(y_train), color='blue')
86/58:
plt.scatter(X_train, y_train, color='red')
plt.plot(X_train, regressor.predict(X_train), color='blue')
86/59:
plt.scatter(X_train, y_train, color='red')
plt.plot(X_train, regressor.predict(X_train), color='blue')
plt.xlabel('Horse Power (HP)')
plt.ylabel('MPG')
plt.title('HP vs MPG (train set)')
86/60:
plt.scatter(X_train, y_train, color='red')
plt.plot(X_train, regressor.predict(X_train), color='blue')
plt.xlabel('Horse Power (HP)')
plt.ylabel('MPG')
plt.title('HP vs MPG (Training set)')
86/61:
plt.scatter(X_test, y_test, color='red')
plt.plot(X_test, regressor.predict(X_test), color='blue')
plt.xlabel('Horse Power (HP)')
plt.ylabel('MPG')
plt.title('HP vs MPG (Testing set)')
86/62: regressor.predict(100)
86/63: regressor.predict([100])
86/64: regressor.predict(2)
86/65: regressor.predict(2.reshape(-1, 1))
86/66: a = 4
86/67: regressor.predict(a)
86/68: regressor.predict(a.reshape(-1, 1))
86/69: a = [4]
86/70: regressor.predict(a.reshape(-1, 1))
86/71: a = np.array(4)
86/72: regressor.predict(a.reshape(-1, 1))
86/73: a = np.array(130)
86/74: regressor.predict(a.reshape(-1, 1))
86/75: a = np.array(1)
86/76: regressor.predict(a.reshape(-1, 1))
86/77: a = np.array(130)
86/78: regressor.predict(a.reshape(-1, 1))
86/79: a = np.array(80)
86/80: regressor.predict(a.reshape(-1, 1))
86/81: a = np.array(500)
86/82: regressor.predict(a.reshape(-1, 1))
86/83: a = np.array(-500)
86/84: regressor.predict(a.reshape(-1, 1))
86/85: a = np.array(-5000)
86/86: regressor.predict(a.reshape(-1, 1))
86/87: a = np.array(-50000)
86/88: regressor.predict(a.reshape(-1, 1))
86/89: a = np.array(-10000)
86/90: regressor.predict(a.reshape(-1, 1))
86/91: a = np.array(-8000)
86/92: regressor.predict(a.reshape(-1, 1))
87/1:
import numpy as np
import pandas as pd
import matplotlib as plt
87/2: df = pd.read_csv('EconomiesOfScale.csv')
87/3: df
87/4: df.head(5)
87/5: df[:3]
87/6: df.describe()
87/7: df.info()
87/8:
import numpy as np
import pandas as pd
import matplotlib as plt
import seaborn as sns
87/9: df = pd.read_csv('EconomiesOfScale.csv')
87/10: df.head(5)
87/11: df[:3]
87/12: df.describe()
87/13: df.info()
87/14: sns.join
87/15: sns.jointplot(x = 'Number of Units', y = 'Manufacturing Cost', df)
87/16: sns.jointplot(x = 'Number of Units', y = 'Manufacturing Cost', data = df)
87/17: sns.lmplot(x = 'Number of Units', y = 'Manufacturing Cost', data = df)
87/18:
X = df[['Number of Units']]
y = df['Manufacturing Cost']
87/19: X.shape
87/20: y.shape
87/21: X.shape
87/22: from sklearn.linear_model import LinearRegression
87/23: regressor = LinearRegression()
87/24: regressor()
87/25: regressor = LinearRegression()
87/26: regressor()
87/27: regressor
87/28: regressor.params_
87/29: regressor.params
87/30: regressor.params()
87/31: regressor.list()
87/32: regressor.get_params()
87/33: regressor.fit(X, y)
87/34: regressor.coef_
87/35: regressor.intercept_
87/36:
print('Linear Model Coefficient (m): ', regressor.coef_)
print('Linear Model Coefficient (b): ', regressor.intercept_)
87/37: plt.scatter(X, y, color = 'red')
87/38: plt.scatter(X, y, color = 'red')
87/39:
import numpy as np
import pandas as pd
import metplotlib.pyplot as plt
import seaborn as sns
87/40:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
87/41: plt.scatter(X, y, color = 'red')
87/42:
plt.scatter(X, y, color = 'red')
plt.plot(X, regressor.predict(X), color = 'blue')
87/43:
plt.scatter(X, y, color = 'red')
plt.plot(X, regressor.predict(X), color = 'blue')
plt.xlabel('Number of Units')
87/44:
plt.scatter(X, y, color = 'red')
plt.plot(X, regressor.predict(X), color = 'blue')
plt.xlabel('Number of Units [in Millions]')
plt.ylabel('Cost Per Unit sold [$]')
87/45: from sklearn.preprocessing import PolynomialFeatures
87/46: poly_reg = PolynomialFeatures(degree=4)
87/47: poly_reg = PolynomialFeatures(degree=6)
87/48: poly_reg = PolynomialFeatures(degree=4)
87/49: X_poly = poly_reg.fit_transform(X)
87/50: X_poly
87/51:
regressor = LinearRegression()
regressor.fit(X_poly, y)
87/52: regressor.get_params()
87/53: regressor.coef_
87/54:
print('Linear Model Coefficient (m): ', regressor.coef_)
print('Linear Model Coefficient (b): ', regressor.intercept_)
87/55: X_poly.shape
87/56: X.shape
87/57: y.shape
87/58: plt.scatter(X, y, color='red')
87/59: y_predict = regressor.predict(poly_reg.fit_transform(X))
87/60: y_predict
87/61:
plt.scatter(X, y, color='red')
plt.plot(X, y_predict, color='blue')
87/62:
plt.scatter(X, y, color='red')
plt.plot(X, y_predict, color='blue')
plt.xlabel('Number of Units [in Millions]')
plt.ylabel('Cost Per Unit Sold [$]')
87/63: from sklearn.preprocessing import PolynomialFeatures
87/64: poly_reg = PolynomialFeatures(degree=9)
87/65: X_poly = poly_reg.fit_transform(X)
87/66: X_poly
87/67:
regressor = LinearRegression()
regressor.fit(X_poly, y)
87/68: regressor.get_params()
87/69: regressor.coef_
87/70:
print('Linear Model Coefficient (m): ', regressor.coef_)
print('Linear Model Coefficient (b): ', regressor.intercept_)
87/71: X.shape
87/72: y.shape
87/73: y_predict = regressor.predict(poly_reg.fit_transform(X))
87/74:
plt.scatter(X, y, color='red')
plt.plot(X, y_predict, color='blue')
plt.xlabel('Number of Units [in Millions]')
plt.ylabel('Cost Per Unit Sold [$]')
87/75: poly_reg = PolynomialFeatures(degree=2)
87/76: X_poly = poly_reg.fit_transform(X)
87/77: X_poly
87/78:
regressor = LinearRegression()
regressor.fit(X_poly, y)
87/79: regressor.get_params()
87/80: regressor.coef_
87/81:
print('Linear Model Coefficient (m): ', regressor.coef_)
print('Linear Model Coefficient (b): ', regressor.intercept_)
87/82: X.shape
87/83: y.shape
87/84: y_predict = regressor.predict(poly_reg.fit_transform(X))
87/85:
plt.scatter(X, y, color='red')
plt.plot(X, y_predict, color='blue')
plt.xlabel('Number of Units [in Millions]')
plt.ylabel('Cost Per Unit Sold [$]')
91/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
91/2: admission_df = pd.read_csv('Admission.csv')
91/3: admission_df.head(5)
91/4:
from mpl_toolkits.mplot3d import Axes3D
x_surf, y_surf = np.meshgrid(np.linspace(admission_df['GRE Score'].min(), admission_df['GRE Score'].max(), 100)  , np.linspace(admission_df['TOEFL Score'].min(), admission_df['TOEFL Score'].max(), 100)  )
onlyX = pd.DataFrame({'GRE Score': x_surf.ravel(), 'TOEFL Score':y_surf.ravel()})
fittedY = regressor.predict(onlyX)
fittedY = fittedY.reshape(x_surf.shape)
91/5: admission_df.tail(10)
91/6: admission_df.info()
91/7: admission_df.describe()
91/8: admission_df = admission_df.drop(['Serial No.'], axis = 1)
91/9: admission_df
91/10: admission_df.columns
91/11:
i = 1
fig, ax = plt.subplots(2, 4, figsize = (20, 20))

for column_header in column_headers:
    plt.subplot(2,4,i)
    sns.distplot(admission_df[column_header])
    i = i + 1
91/12: column_headers = admission_df.columns.values
91/13: column_headers
91/14:
i = 1
fig, ax = plt.subplots(2, 4, figsize = (20, 20))

for column_header in column_headers:
    plt.subplot(2,4,i)
    sns.distplot(admission_df[column_header])
    i = i + 1
91/15:
i = 1
fig, ax = plt.subplots(2, 4, figsize = (20, 20))

for column_header in column_headers:
    plt.subplot(2,4,i)
    sns.histplot(admission_df[column_header])
    i = i + 1
91/16:
i = 1
fig, ax = plt.subplots(2, 4, figsize = (20, 20))

for column_header in column_headers:
    plt.subplot(2,4,i)
    sns.histplot(admission_df[column_header], kde = True)
    i = i + 1
91/17:
i = 1
fig, ax = plt.subplots(2, 4, figsize = (20, 20))

for column_header in column_headers:
    plt.subplot(2,4,i)
    sns.histplot(admission_df[column_header], kde = True, multiple='layer')
    i = i + 1
91/18:
i = 1
fig, ax = plt.subplots(2, 4, figsize = (30, 30))

for column_header in column_headers:
    plt.subplot(2,4,i)
    sns.histplot(admission_df[column_header], kde = True, multiple='layer')
    i = i + 1
91/19:
i = 1
fig, ax = plt.subplots(4, 2, figsize = (20, 20))

for column_header in column_headers:
    plt.subplot(4,2,i)
    sns.histplot(admission_df[column_header], kde = True, multiple='layer')
    i = i + 1
91/20:
i = 1
fig, ax = plt.subplots(4, 2, figsize = (20, 40))

for column_header in column_headers:
    plt.subplot(4,2,i)
    sns.histplot(admission_df[column_header], kde = True, multiple='layer')
    i = i + 1
91/21:
i = 1
fig, ax = plt.subplots(2, 4, figsize = (20, 10))

for column_header in column_headers:
    plt.subplot(2,4,i)
    sns.boxplot(admission_df[column_header])
    i = i + 1
91/22:
i = 1
fig, ax = plt.subplots(2, 4, figsize = (20, 10))

for column_header in column_headers:
    plt.subplot(2,4,i)
    sns.boxplot(admission_df[[column_header]])
    i = i + 1
91/23:
i = 1
fig, ax = plt.subplots(2, 4, figsize = (20, 10))

for column_header in column_headers:
    plt.subplot(2,4,i)
    sns.boxplot(admission_df[column_header])
    i = i + 1
91/24:
for column_header in column_headers:
    print(column_header)
91/25:
for column_header in column_headers:
    print(column_header)
    print(admission_df[column_header].dtype)
91/26:
for column_header in column_headers:
    print(column_header)
    print(admission_df[column_header])
91/27:
for column_header in column_headers:
    print(column_header)
    print(admission_df[[column_header]])
91/28:
for column_header in column_headers:
    print(column_header)
    print(admission_df[[column_header]])
91/29:
for column_header in column_headers:
    print(column_header)
    print(admission_df[column_header])
91/30:
for column_header in column_headers:
    print(column_header)
    print(admission_df[column_header, :])
91/31:
for column_header in column_headers:
    print(column_header)
    print(admission_df[:, column_header])
91/32:
for column_header in column_headers:
    print(column_header)
    print(admission_df['column_header'])
91/33:
for column_header in column_headers:
    print(column_header)
    print(admission_df.loc[column_header])
91/34:
for column_header in column_headers:
    print(column_header)
    print(admission_df.loc[:, column_header])
91/35:
i = 1
fig, ax = plt.subplots(2, 4, figsize = (20, 10))

for column_header in column_headers:
    plt.subplot(2,4,i)
    sns.boxplot(admission_df.loc[:, column_header])
    i = i + 1
91/36:
for column_header in column_headers:
    print(column_header)
    print(admission_df.loc[:, column_header].to_frame())
91/37:
i = 1
fig, ax = plt.subplots(2, 4, figsize = (20, 10))

for column_header in column_headers:
    plt.subplot(2,4,i)
    sns.boxplot(admission_df.loc[:, column_header].to_frame())
    i = i + 1
91/38:
i = 1
fig, ax = plt.subplots(2, 4, figsize = (20, 10))

for column_header in column_headers:
    plt.subplot(2,4,i)
    sns.boxplot(admission_df[column_header].to_frame())
    i = i + 1
91/39:
i = 1
fig, ax = plt.subplots(2, 4, figsize = (20, 10))

for column_header in column_headers:
    plt.subplot(2,4,i)
    sns.boxplot(admission_df[column_header])
    i = i + 1
91/40: sns.boxplot(admission_df[['TOEFL Score']])
91/41: sns.boxplot(admission_df['TOEFL Score'])
91/42:
i = 1
fig, ax = plt.subplots(2, 4, figsize = (20, 10))

for column_header in column_headers:
    plt.subplot(2,4,i)
    var_trans = admission_df[column_header]
    print('var_trans.dtype')
    sns.boxplot(admission_df[column_header])
    i = i + 1
91/43:
i = 1
fig, ax = plt.subplots(2, 4, figsize = (20, 10))

for column_header in column_headers:
    plt.subplot(2,4,i)
    var_trans = admission_df[column_header]
    print(var_trans.dtype)
    sns.boxplot(admission_df[column_header])
    i = i + 1
91/44: admission_df['TOEFL Score'].dtype
91/45:
i = 1
fig, ax = plt.subplots(2, 4, figsize = (20, 10))

for column_header in column_headers:
    plt.subplot(2,4,i)
    var_trans = admission_df[column_header]
    print(var_trans.dtype)
    sns.boxplot(var_trans)
    i = i + 1
91/46:
i = 1
fig, ax = plt.subplots(2, 4, figsize = (20, 10))

for column_header in column_headers:
    plt.subplot(2,4,i)
    var_trans = admission_df[column_header]
    print(var_trans.dtype)
    sns.boxplot(y = var_trans)
    i = i + 1
91/47:
i = 1
fig, ax = plt.subplots(2, 4, figsize = (20, 10))

for column_header in column_headers:
    plt.subplot(2,4,i)
    var_trans = admission_df[column_header]
    print(var_trans.dtype)
    sns.boxplot(x = var_trans)
    i = i + 1
91/48:
i = 1
fig, ax = plt.subplots(2, 4, figsize = (20, 10))

for column_header in column_headers:
    plt.subplot(2,4,i)
    sns.boxplot(x = admission_df[column_header])
    i = i + 1
91/49: sns.heatmap(admission_df.isnull())
91/50: X = admission_df.drop(['Admission Chance'], axis = 1)
91/51: X
91/52: y = admission_df['Admission Chance']
91/53: y
91/54:
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)
91/55: X_train.shape
91/56: X_test.shape
91/57:
from sklearn.linear_model import LinearRegression
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train, y_train)
91/58: regressor.get_params()
91/59:
print('Linear Model Coeff (m)', regressor.coef_)
print('Linear Model Coeff (b)', regressor.intercept_)
91/60:
from sklearn.linear_model import LinearRegression
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train, y_train)
91/61: regressor.get_params()
91/62:
print('Linear Model Coeff (m)', regressor.coef_)
print('Linear Model Coeff (b)', regressor.intercept_)
91/63:
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)
91/64: X_train.shape
91/65: X_test.shape
91/66:
from sklearn.linear_model import LinearRegression
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train, y_train)
91/67: regressor.get_params()
91/68:
print('Linear Model Coeff (m)', regressor.coef_)
print('Linear Model Coeff (b)', regressor.intercept_)
91/69:
from sklearn.linear_model import LinearRegression
regressor = LinearRegression(fit_intercept = False)
regressor.fit(X_train, y_train)
91/70: regressor.get_params()
91/71:
print('Linear Model Coeff (m)', regressor.coef_)
print('Linear Model Coeff (b)', regressor.intercept_)
91/72:
from sklearn.linear_model import LinearRegression
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train, y_train)
91/73: regressor.get_params()
91/74:
print('Linear Model Coeff (m)', regressor.coef_)
print('Linear Model Coeff (b)', regressor.intercept_)
91/75:
Linear Model Coeff (m) [ 0.00136724  0.0029453   0.00160118 -0.00092029  0.02130753  0.12776349
  0.02921015]
Linear Model Coeff (b) -1.2156677150085793
91/76:
Linear Model Coeff (m) [ 0.00136724  0.0029453   0.00160118 -0.00092029  0.02130753  0.12776349
  0.02921015]
Linear Model Coeff (b) -1.2156677150085793
91/77: y_predict = regressor.predict(X_test)
91/78:
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from math import sqrt

k = X_test.shape[1]
n = len(X_test)
91/79: k
91/80: n
91/81:
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from math import sqrt

k = X_test.shape[1]
n = X_test.shape[0]
91/82: k
91/83: n
91/84:
RMSE = float(format(np.sqrt(mean_squared_error(y_test, y_predict)) , '.3f'))
MSE = mean_squared_error(y_test, y_predict)
MAE = mean_absolute_error(y_test, y_predict)
r2 = r2_score(y_test, y_predict)
adj_r2 = 1-(1-r2)*(n-1)/(n-k-1)
MAPE = np.mean( np.abs((y_test - y_predict) /y_test ) ) * 100

print('RMSE =',RMSE, '\nMSE =',MSE, '\nMAE =',MAE, '\nR2 =', r2, '\nAdjusted R2 =', adj_r2, '\nMean Absolute Percentage Error =', MAPE, '%')
91/85: RMSE = float(format(np.sqrt(mean_squared_error(y_test, y_predict))), '.3f'))
91/86: RMSE = float(format(np.sqrt(mean_squared_error(y_test, y_predict))), '.3f')
91/87: RMSE = float(format(np.sqrt(mean_squared_error(y_test, y_predict)), '.3f'))
91/88:
RMSE = float(format(np.sqrt(mean_squared_error(y_test, y_predict)), '.3f'))
MSE = mean_squared_error(y_test, y_predict)
MAE = mean_absolute_error(y_test, y_predict)
MAPE = 100 * (np.mean(np.abs(y_test - y_predict) / y_test ))
91/89: MAPE
91/90:
RMSE = float(format(np.sqrt(mean_squared_error(y_test, y_predict)) , '.3f'))
MSE = mean_squared_error(y_test, y_predict)
MAE = mean_absolute_error(y_test, y_predict)
r2 = r2_score(y_test, y_predict)
adj_r2 = 1-(1-r2)*(n-1)/(n-k-1)
MAPE = np.mean( np.abs((y_test - y_predict) /y_test ) ) * 100

print('RMSE =',RMSE, '\nMSE =',MSE, '\nMAE =',MAE, '\nR2 =', r2, '\nAdjusted R2 =', adj_r2, '\nMean Absolute Percentage Error =', MAPE, '%')
91/91:
X = admission_df[[ 'GRE Score', 'TOEFL Score' ]]
y = admission_df['Admission Chance']
91/92:
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)
91/93:
from sklearn.linear_model import LinearRegression
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train, y_train)
91/94: y_predict = regressor.predict(X_test)
91/95:
plt.scatter(y_test, y_predict, color = 'r')
plt.ylabel('Model Predictions')
plt.xlabel('True (ground truth)')
91/96:
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from math import sqrt

k = X_test.shape[1]
n = len(X_test)
RMSE = float(format(np.sqrt(mean_squared_error(y_test, y_predict)) , '.3f'))
MSE = mean_squared_error(y_test, y_predict)
MAE = mean_absolute_error(y_test, y_predict)
r2 = r2_score(y_test, y_predict)
adj_r2 = 1-(1-r2)*(n-1)/(n-k-1)
MAPE = np.mean( np.abs((y_test - y_predict) /y_test ) ) * 100

print('RMSE =',RMSE, '\nMSE =',MSE, '\nMAE =',MAE, '\nR2 =', r2, '\nAdjusted R2 =', adj_r2, '\nMean Absolute Percentage Error =', MAPE, '%')
91/97:
from mpl_toolkits.mplot3d import Axes3D
x_surf, y_surf = np.meshgrid(np.linspace(admission_df['GRE Score'].min(), admission_df['GRE Score'].max(), 100)  , np.linspace(admission_df['TOEFL Score'].min(), admission_df['TOEFL Score'].max(), 100)  )
onlyX = pd.DataFrame({'GRE Score': x_surf.ravel(), 'TOEFL Score':y_surf.ravel()})
fittedY = regressor.predict(onlyX)
fittedY = fittedY.reshape(x_surf.shape)
91/98:
fig = plt.figure()
ax = fig.add_subplot(111, projection = '3d')
ax.scatter(admission_df['GRE Score'], admission_df['TOEFL Score'], admission_df['Admission Chance'])
ax.plot_surface(x_surf, y_surf, fittedY, color = 'r', alpha = 0.3)
ax.set_xlabel('GRE Score')
ax.set_ylabel('TOEFL Score')
ax.set_zlabel('Acceptance Chance')
91/99:
ax.view_init(60, 70)
plt.show()
91/100:
fig = plt.figure()
ax = fig.add_subplot(111, projection = '3d')
ax.scatter(admission_df['GRE Score'], admission_df['TOEFL Score'], admission_df['Admission Chance'])
ax.plot_surface(x_surf, y_surf, fittedY, color = 'r', alpha = 0.3)
ax.set_xlabel('GRE Score')
ax.set_ylabel('TOEFL Score')
ax.set_zlabel('Acceptance Chance')
ax.view_init(60, 70)
plt.show()
91/101:
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from math import sqrt

k = X_test.shape[1]
n = len(X_test)
RMSE = float(format(np.sqrt(mean_squared_error(y_test, y_predict)) , '.3f'))
MSE = mean_squared_error(y_test, y_predict)
MAE = mean_absolute_error(y_test, y_predict)
r2 = r2_score(y_test, y_predict)
adj_r2 = 1-(1-r2)*(n-1)/(n-k-1)
MAPE = np.mean( np.abs((y_test - y_predict) /y_test ) ) * 100

print('RMSE =',RMSE, '\nMSE =',MSE, '\nMAE =',MAE, '\nR2 =', r2, '\nAdjusted R2 =', adj_r2, '\nMean Absolute Percentage Error =', MAPE, '%')
94/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
94/2: house_df = pd.read_csv('kc_house_data.csv', encoding = 'ISO-8859-1')
94/3: house_df
94/4: house_df.head(5)
94/5: house_df.tail(10)
94/6: house_df.info()
94/7: house_df.describe()
94/8: sns.scatterplot(x = 'sqft_living', y = 'price', data = house_df)
94/9: house_df.hist(bins = 20, figsize = (20,20), color = 'r')
94/10: house_df.hist(bins = 2, figsize = (20,20), color = 'r')
94/11: house_df.hist(bins = 20, figsize = (20,20), color = 'r')
94/12: house_df_sample = house_df[ ['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'yr_built']   ]
94/13: house_df_sample
94/14: house_df_sample = house_df[['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'yr_built']]
94/15: house_df_sample
94/16: sns.pairplot(house_df_sample)
94/17: selected_features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'sqft_above', 'sqft_basement']
94/18: X = house_df[selected_features]
94/19: X
94/20: y = house_df['price']
94/21: y
94/22: X.shape
94/23: y.shape
94/24:
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
94/25: sns.pairplot(X_scaled)
94/26: sns.pairplot(X_scaled.to_dataframe())
94/27: pd.DataFrame(X_scaled, columns = X.columns.values)
94/28: sns.pairplot(pd.DataFrame(X_scaled, columns = X.columns.values))
94/29: X_scaled.shape
94/30: scaler.data_max_
94/31: scaler.data_min_
94/32: y = y.values.reshape(-1,1)
94/33: y_scaled = scaler.fit_transform(y)
94/34: y_scaled
94/35: y_scaled
94/36: X_scale_df = pd.DataFrame(X_scaled, columns = X.columns.values)
94/37: X_scale_df
94/38: X_scale_df.hist(bins=15)
94/39:
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size = 0.25)
94/40: X_train.shape
94/41: X_test.shape
94/42:
import tensorflow.keras
from keras.models import Sequential 
from keras.layers import Dense

model = Sequential()
model.add(Dense(100, input_dim = 7, activation = 'relu'))
model.add(Dense(100, activation='relu'))
model.add(Dense(100, activation='relu'))
model.add(Dense(1, activation = 'linear'))
94/43: !conda install tensorflow
95/1:
import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt
import seaborn as sns
95/2: house_df = pd.read_csv('kc_house_data.csv', encoding = 'ISO-8859-1')
95/3: house_df
95/4: house_df.head(20)
95/5: house_df.tail(10)
95/6: house_df.describe()
95/7: house_df.info()
95/8: sns.scatterplot(x = 'bedrooms', y = 'price', data = house_df)
95/9: sns.scatterplot(x = 'sqft_living', y = 'price', data = house_df)
95/10: sns.scatterplot(x = 'sqft_lot', y = 'price', data = house_df)
95/11: house_df.hist(bins=20,figsize=(20,20), color = 'r')
95/12:
f, ax = plt.subplots(figsize=(20, 20))
sns.heatmap(house_df.corr(), annot = True)
95/13: sns.pairplot(house_df)
95/14:
# pick a sample of the data
house_df_sample =house_df[['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'yr_built']]
95/15: sns.pairplot(house_df_sample)
95/16:
selected_features = ['bedrooms','bathrooms','sqft_living','sqft_lot','floors', 'sqft_above', 'sqft_basement', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'yr_built', 
'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15']

X = house_df[selected_features]
95/17: X
95/18: y = house_df['price']
95/19: y
95/20: X.shape
95/21: y.shape
95/22:
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)
95/23:
from sklearn.linear_model import LinearRegression
regressor = LinearRegression(fit_intercept =True)
regressor.fit(X_train,y_train)
print('Linear Model Coefficient (m): ', regressor.coef_)
print('Linear Model Coefficient (b): ', regressor.intercept_)
95/24:
y_predict = regressor.predict( X_test)
y_predict
95/25:
plt.plot(y_test, y_predict, "^", color = 'r')
plt.xlim(0, 3000000)
plt.ylim(0, 3000000)

plt.xlabel("Model Predictions")
plt.ylabel("True Value (ground Truth)")
plt.title('Linear Regression Predictions')
plt.show()
95/26:
k = X_test.shape[1]
n = len(X_test)
95/27:
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from math import sqrt

RMSE = float(format(np.sqrt(mean_squared_error(y_test, y_predict)),'.3f'))
MSE = mean_squared_error(y_test, y_predict)
MAE = mean_absolute_error(y_test, y_predict)
r2 = r2_score(y_test, y_predict)
adj_r2 = 1-(1-r2)*(n-1)/(n-k-1)

print('RMSE =',RMSE, '\nMSE =',MSE, '\nMAE =',MAE, '\nR2 =', r2, '\nAdjusted R2 =', adj_r2)
95/28:
from sklearn.linear_model import Lasso, Ridge
regressor_ridge = Ridge(alpha = 50)
regressor_ridge.fit(X_train, y_train)
print('Linear Model Coefficient (m): ', regressor_ridge.coef_)
print('Linear Model Coefficient (b): ', regressor_ridge.intercept_)

y_predict = regressor_ridge.predict( X_test)
y_predict
95/29:
from sklearn.linear_model import Lasso
regressor_lasso = Lasso(alpha = 500)
regressor_lasso.fit(X_train,y_train)
print('Linear Model Coefficient (m): ', regressor_lasso.coef_)
print('Linear Model Coefficient (b): ', regressor_lasso.intercept_)

y_predict = regressor_lasso.predict( X_test)
y_predict
95/30:

plt.plot(y_test, y_predict, "^", color = 'r')
plt.xlim(0, 3000000)
plt.ylim(0, 3000000)

plt.xlabel("Model Predictions")
plt.ylabel("True Value (ground Truth)")
plt.title('Linear Regression Predictions')
plt.show()
95/31:
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from math import sqrt

RMSE = float(format(np.sqrt(mean_squared_error(y_test, y_predict)),'.3f'))
MSE = mean_squared_error(y_test, y_predict)
MAE = mean_absolute_error(y_test, y_predict)
r2 = r2_score(y_test, y_predict)
adj_r2 = 1-(1-r2)*(n-1)/(n-k-1)

print('RMSE =',RMSE, '\nMSE =',MSE, '\nMAE =',MAE, '\nR2 =', r2, '\nAdjusted R2 =', adj_r2)
95/32: regressor_lasso.intercept_
95/33:
y_predict = regressor_lasso.predict( X_test)
y_predict
95/34:

plt.plot(y_test, y_predict, "^", color = 'r')
plt.xlim(0, 3000000)
plt.ylim(0, 3000000)

plt.xlabel("Model Predictions")
plt.ylabel("True Value (ground Truth)")
plt.title('Linear Regression Predictions')
plt.show()
95/35:
y_predict = regressor.predict( X_test)
y_predict
95/36:
plt.plot(y_test, y_predict, "^", color = 'r')
plt.xlim(0, 3000000)
plt.ylim(0, 3000000)

plt.xlabel("Model Predictions")
plt.ylabel("True Value (ground Truth)")
plt.title('Linear Regression Predictions')
plt.show()
95/37:
k = X_test.shape[1]
n = len(X_test)
95/38:
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from math import sqrt

RMSE = float(format(np.sqrt(mean_squared_error(y_test, y_predict)),'.3f'))
MSE = mean_squared_error(y_test, y_predict)
MAE = mean_absolute_error(y_test, y_predict)
r2 = r2_score(y_test, y_predict)
adj_r2 = 1-(1-r2)*(n-1)/(n-k-1)

print('RMSE =',RMSE, '\nMSE =',MSE, '\nMAE =',MAE, '\nR2 =', r2, '\nAdjusted R2 =', adj_r2)
95/39:
from sklearn.linear_model import Lasso, Ridge
regressor_ridge = Ridge(alpha = 50)
regressor_ridge.fit(X_train, y_train)
print('Linear Model Coefficient (m): ', regressor_ridge.coef_)
print('Linear Model Coefficient (b): ', regressor_ridge.intercept_)

y_predict = regressor_ridge.predict( X_test)
y_predict
95/40:
y_predict_lasso = regressor_lasso.predict( X_test)
y_predict_lasso
95/41:

plt.plot(y_test, y_predict, "^", color = 'r')
plt.plot(y_test, y_predict_lasso, color = 'b')

plt.xlim(0, 3000000)
plt.ylim(0, 3000000)

plt.xlabel("Model Predictions")
plt.ylabel("True Value (ground Truth)")
plt.title('Linear Regression Predictions')
plt.show()
95/42:

plt.plot(y_test, y_predict, "^", color = 'r')
plt.plot(y_test, y_predict_lasso, 'o', color = 'b')

plt.xlim(0, 3000000)
plt.ylim(0, 3000000)

plt.xlabel("Model Predictions")
plt.ylabel("True Value (ground Truth)")
plt.title('Linear Regression Predictions')
plt.show()
95/43:
from sklearn.linear_model import Lasso
regressor_lasso = Lasso(alpha = 100)
regressor_lasso.fit(X_train,y_train)
print('Linear Model Coefficient (m): ', regressor_lasso.coef_)
print('Linear Model Coefficient (b): ', regressor_lasso.intercept_)

y_predict = regressor_lasso.predict( X_test)
y_predict
95/44:
y_predict = regressor.predict( X_test)
y_predict
95/45:
plt.plot(y_test, y_predict, "^", color = 'r')
plt.xlim(0, 3000000)
plt.ylim(0, 3000000)

plt.xlabel("Model Predictions")
plt.ylabel("True Value (ground Truth)")
plt.title('Linear Regression Predictions')
plt.show()
95/46:
y_predict_regressor = regressor.predict( X_test)
y_predict_regressor
95/47:

plt.plot(y_test, y_predict_regressor, "^", color = 'r')
plt.plot(y_test, y_predict_lasso, 'o', color = 'b')

plt.xlim(0, 3000000)
plt.ylim(0, 3000000)

plt.xlabel("Model Predictions")
plt.ylabel("True Value (ground Truth)")
plt.title('Linear Regression Predictions')
plt.show()
95/48:

plt.plot(y_test, y_predict_regressor, "^", color = 'r')
plt.plot(y_test, y_predict_lasso, 's', color = 'b')

plt.xlim(0, 3000000)
plt.ylim(0, 3000000)

plt.xlabel("Model Predictions")
plt.ylabel("True Value (ground Truth)")
plt.title('Linear Regression Predictions')
plt.show()
95/49:

plt.plot(y_test, y_predict_regressor, "^", color = 'r')
plt.plot(y_test, y_predict_lasso, 't', color = 'b')

plt.xlim(0, 3000000)
plt.ylim(0, 3000000)

plt.xlabel("Model Predictions")
plt.ylabel("True Value (ground Truth)")
plt.title('Linear Regression Predictions')
plt.show()
95/50:

plt.plot(y_test, y_predict_regressor, "^", color = 'r')
plt.plot(y_test, y_predict_lasso, 's', color = 'b')

plt.xlim(0, 3000000)
plt.ylim(0, 3000000)

plt.xlabel("Model Predictions")
plt.ylabel("True Value (ground Truth)")
plt.title('Linear Regression Predictions')
plt.show()
95/51:
from sklearn.linear_model import Lasso
regressor_lasso = Lasso(alpha = 1000)
regressor_lasso.fit(X_train,y_train)
print('Linear Model Coefficient (m): ', regressor_lasso.coef_)
print('Linear Model Coefficient (b): ', regressor_lasso.intercept_)

#y_predict = regressor_lasso.predict( X_test)
#y_predict
95/52:
y_predict_lasso = regressor_lasso.predict( X_test)
y_predict_lasso
95/53:

plt.plot(y_test, y_predict_regressor, "^", color = 'r')
plt.plot(y_test, y_predict_lasso, 's', color = 'b')

plt.xlim(0, 3000000)
plt.ylim(0, 3000000)

plt.xlabel("Model Predictions")
plt.ylabel("True Value (ground Truth)")
plt.title('Linear Regression Predictions')
plt.show()
95/54:

plt.plot(y_test, y_predict_regressor, "^", color = 'r')
plt.plot(y_test, y_predict_lasso, 's', color = 'b')

#plt.xlim(0, 3000000)
#plt.ylim(0, 3000000)

plt.xlabel("Model Predictions")
plt.ylabel("True Value (ground Truth)")
plt.title('Linear Regression Predictions')
plt.show()
96/1:
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split

from sklearn.feature_selection import VarianceThreshold
96/2:
# load our first dataset

# (feel free to write some code to explore the dataset and become
# familiar with it ahead of this demo)

data = pd.read_csv('../dataset_1.csv')
data.shape
96/3:
# load our first dataset

# (feel free to write some code to explore the dataset and become
# familiar with it ahead of this demo)

data = pd.read_csv('../dataset_1.csv')
data.shape
96/4:
# load our first dataset

# (feel free to write some code to explore the dataset and become
# familiar with it ahead of this demo)

data = pd.read_csv('../dataset_1.csv')
data.shape
96/5: data.head(3)
96/6: data.describe()
96/7:
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),
    data['target'],
    test_size=.3,
    random_state=0)
96/8:
# separate dataset into train and test

X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),  # drop the target
    data['target'],  # just the target
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
96/9:
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),
    data['target'],
    test_size=.3,
    random_state=0)

X_train.shape, X_test.shape
96/10:
sel = VarianceThreshold(threshold=0)

sel.fit(X_train)  # fit finds the features with zero variance
96/11:
# get_support is a boolean vector that indicates which features are retained
# if we sum over get_support, we get the number of features that are not constant

# (go ahead and print the result of sel.get_support() to understand its output)

sum(sel.get_support())
96/12:
sel = VarianceThreshold(threshold=0)

sel.fit(X_train)  # fit finds the features with zero variance
96/13: sel
96/14: sel.get_support()
96/15:
# now let's print the number of constant feautures
# (see how we use ~ to exclude non-constant features)

constant = X_train.columns[~sel.get_support()]

len(constant)
96/16: constant
96/17: X_train
96/18:
# capture non-constant feature names

feat_names = X_train.columns[sel.get_support()]
96/19:
X_train = sel.transform(X_train)
X_test = sel.transform(X_test)

X_train.shape, X_test.shape
96/20: X_train
96/21:
# reconstitute de dataframe

X_train = pd.DataFrame(X_train, columns=feat_names)
X_train.head()
96/22:
# separate train and test (again, as we transformed the previous ones)

X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),
    data['target'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
96/23:
const_f = [
    feat for feat in X_train.columns if X_train[feat].std( == 0)
]
96/24:
const_f = [
    feat for feat in X_train.columns if X_train[feat].std() == 0
]
96/25:
# short and easy: find constant features

# in this dataset, all features are numeric,
# so this bit of code will suffice:

constant_features = [
    feat for feat in X_train.columns if X_train[feat].std() == 0
]

len(const_f)
96/26:
# short and easy: find constant features

# in this dataset, all features are numeric,
# so this bit of code will suffice:

constant_features = [
    feat for feat in X_train.columns if X_train[feat].std() == 0
]

len(constant_features)
96/27:
const_f = [
    feat for feat in X_train.columns if X_train[feat].std() == 0
]
104/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
104/2:
# let's load the dataset 

# Variable definitions:
#-------------------------
# disbursed_amount: loan amount given to the borrower
# interest: interest rate
# income: annual income
# number_open_accounts: open accounts (more on this later)
# number_credit_lines_12: accounts opened in the last 12 months
# target: loan status(paid or being repaid = 1, defaulted = 0)


data = pd.read_csv('../loan.csv')

data.head()
104/3:
# let's look at the values of the variable disbursed_amount
# this is the amount of money requested by the borrower

# this variable is continuous, it can take in principle
# any value

data['disbursed_amount'].unique()
104/4: fig = data['disbursed_amount'].hist(bins=50, figsize=(10,2))
104/5:
# let's make a histogram to get familiar with the
# distribution of the variable

fig = data['disbursed_amount'].hist(bins=50)

fig.set_title('Loan Amount Requested')
fig.set_xlabel('Loan Amount')
fig.set_ylabel('Number of Loans')
104/6:
# let's look at the values of the variable disbursed_amount
# this is the amount of money requested by the borrower

# this variable is continuous, it can take in principle
# any value

data['disbursed_amount'].unique()
104/7: fig = data['disbursed_amount'].hist(bins=50, figsize=(10,2))
104/8:
# let's make a histogram to get familiar with the
# distribution of the variable

fig = data['disbursed_amount'].hist(bins=50)

fig.set_title('Loan Amount Requested')
fig.set_xlabel('Loan Amount')
fig.set_ylabel('Number of Loans')
104/9:
# let's do the same exercise for the variable interest rate,
# which is the interest charged by the finance company to the borrowers

# this variable is also continuous, it can take in principle
# any value within the range

data['interest'].unique()
104/10:
# let's make a histogram to get familiar with the
# distribution of the variable

fig = data['interest'].hist(bins=30)

fig.set_title('Interest Rate')
fig.set_xlabel('Interest Rate')
fig.set_ylabel('Number of Loans')
104/11:
# Now, let's explore the income declared by the customers,
# that is, how much they earn yearly.

# this variable is also continuous

fig = data['income'].hist(bins=100)

# for better visualisation, I display only specific
# range in the x-axis
fig.set_xlim(0, 400000)

# title and axis legends
fig.set_title("Customer's Annual Income")
fig.set_xlabel('Annual Income')
fig.set_ylabel('Number of Customers')
104/12:
# Now, let's explore the income declared by the customers,
# that is, how much they earn yearly.

# this variable is also continuous

fig = data['income'].hist(bins=100)

# for better visualisation, I display only specific
# range in the x-axis
fig.set_xlim(0, 400000)

# title and axis legends
fig.set_title("Customer's Annual Income")
fig.set_xlabel('Annual Income')
fig.set_ylabel('Number of Customers')
104/13:
# Now, let's explore the income declared by the customers,
# that is, how much they earn yearly.

# this variable is also continuous

fig = data['income'].hist(bins=10)

# for better visualisation, I display only specific
# range in the x-axis
fig.set_xlim(0, 400000)

# title and axis legends
fig.set_title("Customer's Annual Income")
fig.set_xlabel('Annual Income')
fig.set_ylabel('Number of Customers')
104/14:
# Now, let's explore the income declared by the customers,
# that is, how much they earn yearly.

# this variable is also continuous

fig = data['income'].hist(bins=100)

# for better visualisation, I display only specific
# range in the x-axis
fig.set_xlim(0, 400000)

# title and axis legends
fig.set_title("Customer's Annual Income")
fig.set_xlabel('Annual Income')
fig.set_ylabel('Number of Customers')
104/15:
# Now, let's explore the income declared by the customers,
# that is, how much they earn yearly.

# this variable is also continuous

fig = data['income'].hist(bins=100)

# for better visualisation, I display only specific
# range in the x-axis
#fig.set_xlim(0, 400000)

# title and axis legends
fig.set_title("Customer's Annual Income")
fig.set_xlabel('Annual Income')
fig.set_ylabel('Number of Customers')
104/16:
# Now, let's explore the income declared by the customers,
# that is, how much they earn yearly.

# this variable is also continuous

fig = data['income'].hist(bins=100)

# for better visualisation, I display only specific
# range in the x-axis
fig.set_xlim(0, 400000)

# title and axis legends
fig.set_title("Customer's Annual Income")
fig.set_xlabel('Annual Income')
fig.set_ylabel('Number of Customers')
104/17: data['number_credit_lines_12'].—Çunique()
104/18: data['number_credit_lines_12'].nunique()
104/19: data['number_credit_lines_12'].value_counts()
104/20: data['number_credit_lines_12'].value_counts(dropna=False)
104/21: data
104/22: data.dropna(how=√°ny)
104/23: data.dropna(how=√°ny)
104/24: data.dropna(how='any')
104/25: data.isnull().count()
104/26: data.isnull()
104/27: data.isnull().mean()
104/28: data.isnull().sum()
104/29: data.notnull().sum()
104/30: data['target'].value_counts().mean()
104/31: data['target'].value_counts()
104/32: data['target'].value_counts()/len(data)
105/1:
import pandas as pd

import matplotlib.pyplot as plt
105/2:
# let's load the dataset

# Variable definitions:
#-------------------------
# loan_purpose: intended use of the loan
# market: the risk market assigned to the borrower (based in their financial situation)
# householder: whether the borrower owns or rents their property

data = pd.read_csv('../loan.csv')

data.head()
105/3:
# let's inspect the variable householder,
# which indicates whether the borrowers own their home
# or if they are renting, among other things.

data['householder'].unique()
105/4: data['householder'].value_counts() / len(data)
105/5: data['householder'].value_counts()
105/6: fig = data.householder.value_counts().plot.bar()
105/7: data.householder.bar()
105/8: data.householder.hist()
106/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
106/2:
# let's load the dataset

# Variable definitions:
#-------------------------
# disbursed amount: loan amount lent to the borrower
# market: risk band in which borrowers are placed
# loan purpose: intended use of the loan
# date_issued: date the loan was issued
# date_last_payment: date of last payment towards repyaing the loan

data = pd.read_csv('../loan.csv')

data.head()
106/3:
# pandas assigns type 'object' when reading dates
# and considers them strings.
# Let's have a look

data[['date_issued', 'date_last_payment']].dtypes
106/4:
# now let's parse the dates, currently coded as strings, into datetime format
# this will allow us to make some analysis afterwards

data['date_issued_dt'] = pd.to_datetime(data['date_issued'])
data['date_last_payment_dt'] = pd.to_datetime(data['date_last_payment'])

data[['date_issued', 'date_issued_dt', 'date_last_payment', 'date_last_payment_dt']].head()
106/5: data[['date_issued', 'date_issued_dt', 'date_last_payment', 'date_last_payment_dt']].dtypes
106/6: data[['date_issued', 'date_issued_dt', 'date_last_payment', 'date_last_payment_dt']].dtypes()
106/7: data[['date_issued', 'date_issued_dt', 'date_last_payment', 'date_last_payment_dt']].dtypes
106/8:
# let's extract the month and the year from the variable date
# to make nicer plots

# more on this in section 12 of the course

data['month'] = data['date_issued_dt'].dt.month
data['year'] = data['date_issued_dt'].dt.year
106/9:
# let's see how much money Lending has disbursed
# (i.e., lent) over the years to the different risk
# markets (grade variable)

fig = data.groupby(['year','month', 'market'])['disbursed_amount'].sum().unstack().plot(
    figsize=(14, 8), linewidth=2)

fig.set_title('Disbursed amount in time')
fig.set_ylabel('Disbursed Amount')
106/10: data
106/11: data.groupby(['year','month', 'market'])['disbursed_amount'].sum()
106/12: data.market.value_counts()
106/13: data.market.value_counts(dropna=False)
106/14: data.market.value_counts(dropna=False)
106/15: data.market.value_counts(dropna=True)
106/16: data.market.value_counts(dropna=False)
106/17: data.groupby(['year','month', 'market'])['disbursed_amount'].sum().unstack()
106/18:
index = pd.MultiIndex.from_tuples([('one', 'a'), ('one', 'b'),
                                   ('two', 'a'), ('two', 'b')])
106/19: s = pd.Series(np.arange(1.0, 5.0), index=index)
106/20: —ã
106/21: s
106/22:
index = pd.MultiIndex.from_tuples([('one', 'a'), ('one', 'b'),
                                   ('two', 'a'), ('two', 'b'),
                                   ('three', 'a'), ('three', 'b')])
106/23: s = pd.Series(np.arange(1.0, 7.0), index=index)
106/24: s
106/25: s.unstack()
106/26:
index = pd.MultiIndex.from_tuples([('one', 'a', '1'), ('one', 'b', '1'),
                                   ('two', 'a', '2'), ('two', 'b', '2'),
                                   ('three', 'a', '3'), ('three', 'b', '3')])
106/27: s = pd.Series(np.arange(1.0, 11.0), index=index)
106/28: s
106/29:
index = pd.MultiIndex.from_tuples([('one', 'a', '1'), ('one', 'b', '1'), ('one', 'b', '2'),
                                   ('two', 'a', '2'), ('two', 'b', '2'), ('two', 'b', '3'),
                                   ('three', 'a', '3'), ('three', 'b', '3'), ('three', 'b', '1')])
106/30: s = pd.Series(np.arange(1.0, 11.0), index=index)
106/31: s = pd.Series(np.arange(1.0, 10.0), index=index)
106/32: s
106/33: s.unstack()
106/34: s.unstack(level=0)
106/35: s.unstack(level=-1)
106/36: s.unstack(level=1)
109/1:
import numpy as np
import pandas as pd
109/2:
# p.170 multiindex
index = pd.MultiIndex.from_product([[2013, 2014], [1, 2]],
                                   names=['year', 'visit'])
109/3:
columns = pd.MultiIndex.from_product([['Bob', 'Guido', 'Sue'],
                                      ['HR', 'Temp']],
                                     names=['subject', 'type'])
109/4: data = np.round(np.random.randn(4, 6), 1)
109/5:
data = np.round(np.random.randn(4, 6), 1)
data
109/6:
data = np.round(np.random.randn(4, 6), 10)
data
109/7:
data = np.round(np.random.randn(4, 6), 1)
data
109/8:
data = np.round(np.random.randn(4, 6), 2)
data
109/9:
data = np.round(np.random.randn(4, 6), 1)
data[:, ::2] *= 10
109/10: data
109/11: health_data = pd.DataFrame(data, index=index, columns=columns)
109/12: health_data
109/13:
data = np.round(np.random.randn(4, 6), 1)
data[:, ::2] *= 10
data += 37
109/14: health_data = pd.DataFrame(data, index=index, columns=columns)
109/15: health_data
109/16:
data = np.round(np.random.randn(4, 6), 1)
data[:, ::2] *= 10
#data += 37
109/17: health_data = pd.DataFrame(data, index=index, columns=columns)
109/18: health_data
109/19:
data = np.round(np.random.randn(4, 6), 1)
data[:, ::2] *= 10
data += 37
109/20: health_data = pd.DataFrame(data, index=index, columns=columns)
109/21: health_data
109/22:
data = np.round(np.random.randn(4, 6), 1)
data[:, ::2] *= 10
#data += 37
109/23: health_data = pd.DataFrame(data, index=index, columns=columns)
109/24: health_data
109/25:
data += 37
health_data
109/26:
data = np.round(np.random.randn(4, 6), 1)
data[:, ::2] *= 10
data += 37
109/27: health_data = pd.DataFrame(data, index=index, columns=columns)
109/28: health_data
109/29: health_data['Bob']
109/30: health_data['Bob', 'HR']
109/31: health_data.iloc[:2, :2]
109/32: health_data
109/33: health_data.iloc[:3, :2]
109/34: health_data.iloc[:2, :2]
109/35: health_data.loc[:, 'Bob']
109/36: health_data.loc[:, ('Bob', 'HR')]
109/37:
idx = pd.IndexSlice
health_data.loc[idx[:, 1], idx[:, 'HR']]
109/38: health_data.loc[idx[:, 1], idx[:, 'Sue']]
109/39: health_data.loc[:, ('Bob', 'HR')]
109/40: health_data.loc[idx[:, 1], idx[:, 'HR']]
109/41: health_data.loc[2013, idx[:, 'HR']]
109/42: health_data.loc[[2013, 1], idx[:, 'HR']]
109/43: health_data.loc[[2013, idx[:, 'HR']]
109/44: health_data.loc[2013, idx[:, 'HR']]
109/45: health_data.loc[[2013, 1], idx[:, 'HR']]
109/46: health_data.loc[(2013, 1), idx[:, 'HR']]
109/47: health_data.loc[idx[:, 2], idx['Sue', :]]
109/48: health_data.loc[idx[:, 2], idx['Sue', 'HR']]
109/49: health_data.loc[idx[:, 2], idx['Sue']]
109/50: health_data.loc[idx[:, 2], idx['Sue', :]]
109/51: health_data.unstack(level=0)
109/52: health_data.unstack(level=1)
109/53: health_data.unstack(level=-1)
109/54: health_data.unstack(level=-2)
109/55: new_data = health_data.unstack(level=-2)
109/56: new_data
109/57: health_data.iloc[1]
109/58: health_data.iloc[1, idx=['sue']]
109/59: health_data.iloc[1, idx['sue']]
109/60: health_data.iloc[1, idx['Sue']]
109/61: health_data.iloc[1, idx['Sue', :, :]]
109/62: health_data.iloc[1, idx['Sue', :]]
109/63: health_data.iloc[1]
109/64: health_data.iloc[1, 'Sue']
109/65: health_data.iloc[1]
109/66: health_data.loc[1]
109/67: health_data.loc[1, ['Sue']]
109/68: health_data.loc[idx[1], ['Sue']]
109/69: health_data.loc[idx[1], ['Sue']]
109/70: health_data.loc[:, ['Sue']]
109/71: health_data.loc[idx[2013, 1], ['Sue']]
109/72: health_data.loc[idx[2013, 1], ['Sue', 'HR']]
109/73: health_data.loc[idx[2013, 1], ['Sue', 'HR', :]]
109/74: health_data.loc[idx[2013, 1], ['Sue', 'HR']]
109/75: health_data.loc[idx[2013, 1], ['Sue']]
109/76: health_data.loc[idx[2013, 1], idx['Sue', 'HR']]
109/77: health_data.loc[idx[2013, 1], idx['Sue', 'HR']] = 90
109/78: health_data
109/79: new
109/80: new_data
109/81: health_data.reset_index()
109/82: health_data.reset_index()[1]
109/83: health_data.reset_index()
109/84: data_mean = health_data.mean(level=1)
109/85: data_mean
109/86: health_data
109/87: data_mean = health_data.mean(level=0)
109/88: data_mean
109/89: health_data.mean(level='visit')
109/90: data_mean.mean(axis=1, level='type')
109/91: data_mean
109/92: data_mean.mean(axis=1, level=1)
109/93: data_mean.mean(axis=1, level=0)
109/94: data_mean
109/95: health_data
109/96: health_data.reset_index()
109/97: health_data.reset_index().groupby('year')
109/98: health_data.reset_index().groupby('year').reset_index()
109/99: health_data.reset_index().groupby('year')
109/100:
health_data
.groupby('year')
109/101: health_data.groupby('year')
109/102: health_data.groupby('year').reset_index()
109/103: health_data
109/104: health_data.reset_index()
109/105: health_data.to_flat_index()
109/106: health_data.reset_index()
109/107:
idx = pd.MultiIndex.from_arrays([i.unique().repeat(len(df.index.levels[1]) + 1), k])
df = df.reindex(idx).fillna('')
109/108:
idx = pd.MultiIndex.from_arrays([i.unique().repeat(len(df.index.levels[1]) + 1), k])
df = health_data.reindex(idx).fillna('')
109/109: health_data.droplevel()
109/110: health_data.droplevel(level=1)
109/111: health_data.droplevel(level=0)
109/112: health_data.droplevel(level=3)
109/113: health_data.droplevel(level=2)
109/114: health_data.droplevel(level=1)
109/115:
m = {t: '_'.join(t) for t in health_data.columns}
health_data.groupby(m, axis=1).mean().add_suffix('_O').reset_index()
109/116:
m = {t: '_'.join(t) for t in health_data.columns}
health_data.groupby(m, axis=1).mean().reset_index()
109/117: health_data.groupby(m, axis=1).mean()
109/118: health_data.groupby(axis=1).mean()
109/119: health_data.groupby(type, axis=1).mean()
109/120: health_data.groupby(['Temp', 'HR'], axis=1).mean()
109/121: health_data.groupby(level=1, axis=1).mean().reset_index()
109/122: health_data
109/123: health_data.groupby(level=0, axis=1).mean().reset_index()
109/124: health_data.groupby(level=0, axis=1).mean().reset_index().groupby('year')
109/125: health_data.groupby(level=0, axis=1).mean().reset_index().groupby('year').reset_index()
109/126: health_data.groupby(level=0, axis=1).mean().reset_index().groupby('year').mean().reset_index()
109/127: health_data
109/128: health_data
109/129: health_data.groupby(level=1, axis=0).mean().reset_index()
109/130: health_data.groupby(level=1, axis=0).mean().reset_index().groupby(level=1, axis=1).sum().reset_index()
109/131: health_data.groupby(level=1, axis=0).mean().reset_index()
109/132: temp_data = health_data.groupby(level=1, axis=0).mean().reset_index().groupby(level=1, axis=1).sum().reset_index()
109/133: temp_data
109/134: temp_data.index
109/135: temp_data.columns
109/136: temp_data
109/137: temp_data.iloc[:, 2:]
109/138: temp_data.iloc[:, 1:]
109/139: ######################
109/140:
import seaborn as sns
planets = sns.load_dataset('planets')
planets.shape
109/141: planets.head(5)
109/142: planets.describe()
109/143: planets.dropna().describe()
109/144: planets.groupby('method')['orbital_period'].mean().reset_index()
109/145: planets.groupby('method')['orbital_period'].median().reset_index()
109/146: planets.groupby('method')['orbital_period'].median()
109/147: planets.groupby('method')['year'].describe()
109/148: planets.groupby('method')['year']
109/149: planets
109/150: planets.groupby('method')['year'].describe().unstack()
109/151: planets.groupby('method')['year'].describe()
109/152:
rng = np.random.RandomState(0)
df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],
                   'data1': range(6),
                   'data2': rng.randint(0, 10, 6)},
                   columns = ['key', 'data1', 'data2'])
109/153: df
109/154: df.groupby('key').aggregate(['min', np.mean, np.median, 'max'])
109/155:
df.groupby('key').aggregate({'data1': 'min',
                             'data2': ['min', 'max']})
109/156:
def filter_fun(x):
    return x[data2].std() > 4
109/157: df.groupby('key').filter('filter_fun')
109/158: df.groupby('key').filter(filter_fun)
109/159:
def filter_fun(x):
    return x['data2'].std() > 4
109/160: df.groupby('key').filter(filter_fun)
109/161: df
109/162: df.describe()
109/163: df.groupby('key').describe()
109/164: df
110/1:
import numpy as np
import pandas as pd
110/2:
with open ('../resipes_datasets/test.json', 'r') as f:
    data = (line.strip() for line in f)
    data_json = "[{0}]".format(','.join(data))
recipes = pd.read_json(data_json)
110/3:
with open ('resipes_datasets/test.json', 'r') as f:
    data = (line.strip() for line in f)
    data_json = "[{0}]".format(','.join(data))
recipes = pd.read_json(data_json)
110/4:
with open ('/resipes_datasets/test.json', 'r') as f:
    data = (line.strip() for line in f)
    data_json = "[{0}]".format(','.join(data))
recipes = pd.read_json(data_json)
110/5:
with open ('resipes_datasets/test.json', 'r') as f:
    data = (line.strip() for line in f)
    data_json = "[{0}]".format(','.join(data))
recipes = pd.read_json(data_json)
110/6:
with open ('resipes_datasets/test.json', 'r') as f:
    data = (line.strip() for line in f)
    data_json = "[{0}]".format(','.join(data))
recipes = pd.read_json('data_json')
110/7:
with open ('resipes_datasets/test.json', 'r') as f:
    data = (line.strip() for line in f)
    data_json = "[{0}]".format(','.join(data))
recipes = pd.read_json(data_json, lines=True)
110/8: data= pd.read_json('resipes_datasets/test.json', lines=True)
110/9: data= pd.read_json('resipes_datasets/test.json', lines=True, orient='records')
110/10: data= pd.read_json('resipes_datasets/test.json', orient='records')
110/11: data
110/12: !curl -O
110/13:
!curl -O
https://s3.amazonaws.com/openrecipes/20170107-061401-recipeitems.json.gz
!gunzip 20170107-061401-recipeitems.json.gz
110/14:
!curl -O
https://s3.amazonaws.com/openrecipes/20170107-061401-recipeitems.json.gz
!gunzip 20170107-061401-recipeitems.json.gz
110/15:
!curl -O
https://s3.amazonaws.com/openrecipes/20170107-061401-recipeitems.json.gz
!gunzip 20170107-061401-recipeitems.json.gz
110/16:
!curl -O
https:/s3.amazonaws.com/openrecipes/20170107-061401-recipeitems.json.gz
!gunzip 20170107-061401-recipeitems.json.gz
110/17:
!curl -O
https://s3.amazonaws.com/openrecipes/20170107-061401-recipeitems.json.gz
!gunzip 20170107-061401-recipeitems.json.gz
110/18:
!wget -O
https://s3.amazonaws.com/openrecipes/20170107-061401-recipeitems.json.gz
!gunzip 20170107-061401-recipeitems.json.gz
110/19:
!wget --limit-rate=20k
https://s3.amazonaws.com/openrecipes/20170107-061401-recipeitems.json.gz
!gunzip 20170107-061401-recipeitems.json.gz
110/20: !curl -O https://s3.amazonaws.com/openrecipes/20170107-061401-recipeitems.json.gz
111/1:
import numpy as np
import pandas as pd
111/2:
from io import StringIO
with open('20170107-061401-recipeitems.json', 'r', encoding="utf-8") as f:
    data = (line.strip() for line in f)
    data_json = "[{0}]".format(','.join(data))
recipesDF = pd.read_json(StringIO(data_json))
111/3: !curl -O https://s3.amazonaws.com/openrecipes/20170107-061401-recipeitems.json.gz
112/1:
import numpy as np
import pandas as pd
112/2: !wget -O https://s3.amazonaws.com/openrecipes/20170107-061401-recipeitems.json.gz
112/3:
from io import StringIO
with open('20170107-061401-recipeitems.json', 'r', encoding="utf-8") as f:
    data = (line.strip() for line in f)
    data_json = "[{0}]".format(','.join(data))
recipesDF = pd.read_json(StringIO(data_json))
113/1: !conda install pandas-datareader
116/1:
import numpy as np
import pandas as pd
116/2:
from io import StringIO
with open('20170107-061401-recipeitems.json', 'r', encoding="utf-8") as f:
    data = (line.strip() for line in f)
    data_json = "[{0}]".format(','.join(data))
recipesDF = pd.read_json(StringIO(data_json))
115/1: from pandas_datareader import data
115/2: from pandas-datareader import data
115/3: from pandas_datareader import data
115/4: from pandas_datareader import data
115/5: !conda install pandas-datareader
116/3:
from io import StringIO
with open('20170107-061401-recipeitems.json.gz', 'r', encoding="utf-8") as f:
    data = (line.strip() for line in f)
    data_json = "[{0}]".format(','.join(data))
recipesDF = pd.read_json(StringIO(data_json))
117/1: pandas-datareader
117/2: pandas-datareader?
117/3: import pandas_datareader as pdr
117/4: !pip install pandas-datareader
117/5: import pandas_datareader as pdr
117/6:
import pandas_datareader as pdr
from pandas_datareader import data
117/7: goog = data.DataReader('GOOG', start='2004', end='2016', data_source='google')
117/8: goog = data.DataReader('GOOG', start='2004', end='2016', data_source='yahoo')
117/9: goog
117/10:
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn; seaborn.set()
117/11: goog.plot()
117/12: goog['Close'].plot()
117/13: goog.plot(alpha=.5, style='-')
117/14: goog['Close'].plot(alpha=.5, style='-')
117/15:
goog['Close'].plot(alpha=.5, style='-')
goog.resample('BA').mean().plot(style=':')
117/16:
goog['Close'].plot(alpha=.5, style='-')
goog['Close'].resample('BA').mean().plot(style=':')
117/17:
goog['Close'].plot(alpha=.5, style='-')
goog['Close'].resample('BA').mean().plot(style=':')
goog['Close'].asfreq('BA').plot(style='--')
117/18:
goog['Close'].plot(alpha=.5, style='-')
goog['Close'].resample('BA').max().plot(style=':')
goog['Close'].asfreq('BA').plot(style='--')
117/19:
goog['Close'].plot(alpha=.5, style='-')
goog['Close'].resample('BA').mean().plot(style=':')
goog['Close'].asfreq('BA').plot(style='--')
117/20:
goog['Close'].plot(alpha=.8, style='-')
goog['Close'].resample('BA').mean().plot(style=':')
goog['Close'].asfreq('BA').plot(style='--')
117/21:
goog['Close'].plot(alpha=.6, style='-')
goog['Close'].resample('BA').mean().plot(style=':')
goog['Close'].asfreq('BA').plot(style='--')
117/22: goog['Close'].resample('BA')
117/23: goog['Close'].resample('BA').reset_index()
117/24: goog['Close'].resample('BA').mean()
117/25: goog['Close'].resample('BA').reset_index()
117/26: goog['Close'].resample('BA').mean(.reset_index()
117/27: goog['Close'].resample('BA').mean().reset_index()
117/28:
fig, ax = plt.subplots(2, sharex=True)
data = goog['Close'].iloc[:10]
data['Close'].asfreq('D').plot(as=as[0], marker='o')
117/29:
fig, ax = plt.subplots(2, sharex=True)
data = goog['Close'].iloc[:10]
data['Close'].asfreq('D').plot(ax=ax[0], marker='o')
117/30: data['Close'].asfreq('D')
117/31:
fig, ax = plt.subplots(2, sharex=True)
data = goog['Close'].iloc[:10]
data.asfreq('D').plot(ax=ax[0], marker='o')
117/32:
fig, ax = plt.subplots(2, sharex=True)
data = goog['Close'].iloc[:10]
data.asfreq('D').plot(ax=ax[0], marker='-o')
117/33:
fig, ax = plt.subplots(2, sharex=True)
data = goog['Close'].iloc[:10]
data.asfreq('D').plot(ax=ax[0], marker='o')
data.asfreq('D', method='bfill').plot(ax=ax[1], style='-o')
117/34:
fig, ax = plt.subplots(2, sharex=True)
data = goog['Close'].iloc[:10]
data.asfreq('D').plot(ax=ax[0], marker='o')
data.asfreq('D', method='bfill').plot(ax=ax[1], style='-o')
data.asfreq('D', method='ffill').plot(ax=ax[1], style='--o')
117/35:
fig, ax = plt.subplots(2, sharex=True)
data = goog['Close'].iloc[:10]
data.asfreq('D').plot(ax=ax[0], marker='o')
data.asfreq('D', method='bfill').plot(ax=ax[1], style='-o')
data.asfreq('D', method='ffill').plot(ax=ax[1], style=':o')
117/36:
fig, ax = plt.subplots(2, sharex=True)
data = goog['Close'].iloc[:10]
data.asfreq('D').plot(ax=ax[0], marker='o')
data.asfreq('D', method='bfill').plot(ax=ax[1], style='-o')
data.asfreq('D', method='ffill').plot(ax=ax[1], style=':s')
117/37: pd.__version__
117/38: plt.__version__
117/39: matplotlib.__version__
117/40: print('matplotlib: {}'.format(matplotlib.__version__))
117/41: print('matplotlib: {}'.format(plt.__version__))
117/42:
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
import seaborn; seaborn.set()
117/43:
import pandas_datareader as pdr
from pandas_datareader import data
117/44: print('matplotlib: {}'.format(plt.__version__))
117/45: print('matplotlib: {}'.format(matplotlib.__version__))
117/46: !pip install --upgrade pyOpenSSL
117/47: !conda install --upgrade pyOpenSSL
117/48: !conda update pyOpenSSL
119/1: #!pip install pandas-datareader
119/2:
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
import seaborn; seaborn.set()
119/3:
import pandas_datareader as pdr
from pandas_datareader import data
119/4: print('matplotlib: {}'.format(matplotlib.__version__))
119/5: goog = data.DataReader('GOOG', start='2004', end='2016', data_source='yahoo')
119/6: goog
119/7: goog['Close'].plot()
119/8:
goog['Close'].plot(alpha=.6, style='-')
goog['Close'].resample('BA').mean().plot(style=':')
goog['Close'].asfreq('BA').plot(style='--')
119/9: goog['Close'].resample('BA').mean().reset_index()
119/10:
fig, ax = plt.subplots(2, sharex=True)
data = goog['Close'].iloc[:10]
data.asfreq('D').plot(ax=ax[0], marker='o')
data.asfreq('D', method='bfill').plot(ax=ax[1], style='-o')
data.asfreq('D', method='ffill').plot(ax=ax[1], style=':s')
119/11:
fig, ax = plt.subplots(2, sharex=True)
data = goog['Close'].iloc[:10]
data.asfreq('D').plot(ax=ax[0])
data.asfreq('D', method='bfill').plot(ax=ax[1], style='-o')
data.asfreq('D', method='ffill').plot(ax=ax[1], style=':s')
119/12:
fig, ax = plt.subplots(2, sharex=True)
data = goog['Close'].iloc[:10]
data.asfreq('D').plot(ax=ax[0])
#data.asfreq('D', method='bfill').plot(ax=ax[1], style='-o')
#data.asfreq('D', method='ffill').plot(ax=ax[1], style=':s')
119/13:
fig, ax = plt.subplots(2, sharex=True)
data = goog['Close'].iloc[:10]
#data.asfreq('D').plot(ax=ax[0])
#data.asfreq('D', method='bfill').plot(ax=ax[1], style='-o')
#data.asfreq('D', method='ffill').plot(ax=ax[1], style=':s')
119/14:
fig, ax = plt.subplots(2, sharex=True)
data = goog['Close'].iloc[:10]
data.asfreq('D').plot(ax=ax[0])
#data.asfreq('D', method='bfill').plot(ax=ax[1], style='-o')
#data.asfreq('D', method='ffill').plot(ax=ax[1], style=':s')
119/15: pandas.__version__
119/16:
import pandas as pd
import pandas_datareader as pdr
from pandas_datareader import data
119/17:
fig, ax = plt.subplots(2, sharex=True)
data = goog['Close'].iloc[:10]
data.asfreq('D').plot(ax=ax[0])
#data.asfreq('D', method='bfill').plot(ax=ax[1], style='-o')
#data.asfreq('D', method='ffill').plot(ax=ax[1], style=':s')
119/18: pandas.__version__
119/19: pd.__version__
119/20: !conda update pandas
120/1: !pip install pandas
120/2:
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
import seaborn; seaborn.set()
120/3:
import pandas as pd
import pandas_datareader as pdr
from pandas_datareader import data
120/4: pd.__version__
121/1: !pip install pandas
121/2:
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
import seaborn; seaborn.set()
121/3:
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
import seaborn
121/4:
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
121/5: !conda install seaborn
121/6:
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
121/7:
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
#import seaborn as sns
121/8: import seaborn as sns
121/9: !conda install scipy=1.6.1
121/10:
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
#import seaborn as sns
121/11: import seaborn as sns
121/12: !conda install scipy
121/13:
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
#import seaborn as sns
121/14: import seaborn as sns
121/15: !conda update scipy
121/16: !conda update scikit-learn
121/17: import seaborn as sns
121/18: !conla list
121/19: !conda list
121/20: import seaborn
122/1:
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
#import seaborn as sns
122/2: import seaborn
123/1:
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
#import seaborn as sns
123/2: import seaborn
123/3:
import pandas as pd
import pandas_datareader as pdr
from pandas_datareader import data
123/4: pd.__version__
123/5: print('matplotlib: {}'.format(matplotlib.__version__))
123/6: goog = data.DataReader('GOOG', start='2004', end='2016', data_source='yahoo')
123/7: goog
123/8: goog['Close'].plot()
123/9:
goog['Close'].plot(alpha=.6, style='-')
goog['Close'].resample('BA').mean().plot(style=':')
goog['Close'].asfreq('BA').plot(style='--')
123/10: goog['Close'].resample('BA').mean().reset_index()
123/11:
fig, ax = plt.subplots(2, sharex=True)
data = goog['Close'].iloc[:10]
data.asfreq('D').plot(ax=ax[0])
#data.asfreq('D', method='bfill').plot(ax=ax[1], style='-o')
#data.asfreq('D', method='ffill').plot(ax=ax[1], style=':s')
123/12:
fig, ax = plt.subplots(2, sharex=True)
data = goog['Close'].iloc[:10]
data.asfreq('D').plot(ax=ax[0], marker='o')
#data.asfreq('D', method='bfill').plot(ax=ax[1], style='-o')
#data.asfreq('D', method='ffill').plot(ax=ax[1], style=':s')
123/13: !conda update pandas
124/1:
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
#import seaborn as sns
124/2: import seaborn
124/3:
import pandas as pd
import pandas_datareader as pdr
from pandas_datareader import data
124/4: pd.__version__
124/5: print('matplotlib: {}'.format(matplotlib.__version__))
124/6: goog = data.DataReader('GOOG', start='2004', end='2016', data_source='yahoo')
124/7: goog
124/8: goog['Close'].plot()
124/9:
goog['Close'].plot(alpha=.6, style='-')
goog['Close'].resample('BA').mean().plot(style=':')
goog['Close'].asfreq('BA').plot(style='--')
124/10: goog['Close'].resample('BA').mean().reset_index()
124/11:
fig, axes = plt.subplots(2, sharex=True)
data = goog['Close'].iloc[:10]
data.asfreq('D').plot(ax=axes[0], marker='o')
#data.asfreq('D', method='bfill').plot(ax=ax[1], style='-o')
#data.asfreq('D', method='ffill').plot(ax=ax[1], style=':s')
124/12:
fig, axes = plt.subplots(2, sharex=True)
data = goog['Close'].iloc[:10]
data.asfreq('D').plot(marker='o')
#data.asfreq('D', method='bfill').plot(ax=ax[1], style='-o')
#data.asfreq('D', method='ffill').plot(ax=ax[1], style=':s')
124/13:
fig, axes = plt.subplots(2, sharex=True)
data = goog['Close'].iloc[:10]
data.asfreq('D').plot(ax=axes[0] )
#data.asfreq('D', method='bfill').plot(ax=ax[1], style='-o')
#data.asfreq('D', method='ffill').plot(ax=ax[1], style=':s')
124/14:
fig, axes = plt.subplots(2, 1)
data = goog['Close'].iloc[:10]
data.asfreq('D').plot(ax=axes[0], marker='o')
#data.asfreq('D', method='bfill').plot(ax=ax[1], style='-o')
#data.asfreq('D', method='ffill').plot(ax=ax[1], style=':s')
124/15:
fig, axes = plt.subplots(2, 1)
data = goog['Close'].iloc[:10]
data.asfreq('D').plot(ax=axes[0])
#data.asfreq('D', method='bfill').plot(ax=ax[1], style='-o')
#data.asfreq('D', method='ffill').plot(ax=ax[1], style=':s')
124/16:
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
#import seaborn as sns
124/17: import seaborn
124/18:
import pandas as pd
import pandas_datareader as pdr
from pandas_datareader import data
124/19: pd.__version__
124/20: print('matplotlib: {}'.format(matplotlib.__version__))
124/21: goog = data.DataReader('GOOG', start='2004', end='2016', data_source='yahoo')
124/22: goog
124/23: goog['Close'].plot()
124/24:
goog['Close'].plot(alpha=.6, style='-')
goog['Close'].resample('BA').mean().plot(style=':')
goog['Close'].asfreq('BA').plot(style='--')
124/25: goog['Close'].resample('BA').mean().reset_index()
124/26:
fig, axes = plt.subplots(2, 1)
data = goog['Close'].iloc[:10]
data.asfreq('D').plot(ax=axes[0], marker='o')
#data.asfreq('D', method='bfill').plot(ax=ax[1], style='-o')
#data.asfreq('D', method='ffill').plot(ax=ax[1], style=':s')
125/1:
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
#import seaborn as sns
125/2: import seaborn
125/3:
import pandas as pd
import pandas_datareader as pdr
from pandas_datareader import data
125/4: pd.__version__
125/5: print('matplotlib: {}'.format(matplotlib.__version__))
125/6: goog = data.DataReader('GOOG', start='2004', end='2016', data_source='yahoo')
125/7: goog
125/8: goog['Close'].plot()
125/9:
goog['Close'].plot(alpha=.6, style='-')
goog['Close'].resample('BA').mean().plot(style=':')
goog['Close'].asfreq('BA').plot(style='--')
125/10: goog['Close'].resample('BA').mean().reset_index()
125/11:
fig, axes = plt.subplots(2, 1)
data = goog['Close'].iloc[:10]
data.asfreq('D').plot(ax=axes[0], marker='o')
#data.asfreq('D', method='bfill').plot(ax=ax[1], style='-o')
#data.asfreq('D', method='ffill').plot(ax=ax[1], style=':s')
126/1:
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
#import seaborn as sns
126/2: import seaborn
126/3:
import pandas as pd
import pandas_datareader as pdr
from pandas_datareader import data
126/4: pd.__version__
126/5: print('matplotlib: {}'.format(matplotlib.__version__))
126/6: goog = data.DataReader('GOOG', start='2004', end='2016', data_source='yahoo')
126/7: goog
126/8: goog['Close'].plot()
126/9:
goog['Close'].plot(alpha=.6, style='-')
goog['Close'].resample('BA').mean().plot(style=':')
goog['Close'].asfreq('BA').plot(style='--')
126/10: goog['Close'].resample('BA').mean().reset_index()
126/11:
fig, axes = plt.subplots(2, 1)
data = goog['Close'].iloc[:10]
data.asfreq('D').plot(ax=axes[0], marker='o')
#data.asfreq('D', method='bfill').plot(ax=ax[1], style='-o')
#data.asfreq('D', method='ffill').plot(ax=ax[1], style=':s')
127/1:
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
#import seaborn as sns
127/2: import seaborn
127/3:
import pandas as pd
import pandas_datareader as pdr
from pandas_datareader import data
127/4: pd.__version__
127/5: print('matplotlib: {}'.format(matplotlib.__version__))
127/6: goog = data.DataReader('GOOG', start='2004', end='2016', data_source='yahoo')
127/7: goog
127/8: goog['Close'].plot()
127/9:
goog['Close'].plot(alpha=.6, style='-')
goog['Close'].resample('BA').mean().plot(style=':')
goog['Close'].asfreq('BA').plot(style='--')
127/10: goog['Close'].resample('BA').mean().reset_index()
127/11:
fig, axes = plt.subplots(2, 1)
data = goog['Close'].iloc[:10]
data.asfreq('D').plot(ax=axes[0], marker='o')
#data.asfreq('D', method='bfill').plot(ax=ax[1], style='-o')
#data.asfreq('D', method='ffill').plot(ax=ax[1], style=':s')
128/1:
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
#import seaborn as sns
128/2: import seaborn
128/3:
import pandas as pd
import pandas_datareader as pdr
from pandas_datareader import data
128/4: pd.__version__
128/5: print('matplotlib: {}'.format(matplotlib.__version__))
128/6: goog = data.DataReader('GOOG', start='2004', end='2016', data_source='yahoo')
128/7: goog
128/8: goog['Close'].plot()
128/9:
goog['Close'].plot(alpha=.6, style='-')
goog['Close'].resample('BA').mean().plot(style=':')
goog['Close'].asfreq('BA').plot(style='--')
128/10: goog['Close'].resample('BA').mean().reset_index()
128/11:
fig, axes = plt.subplots(2, 1)
data = goog['Close'].iloc[:10]
data.asfreq('D').plot(ax=axes[0], marker='o')
#data.asfreq('D', method='bfill').plot(ax=ax[1], style='-o')
#data.asfreq('D', method='ffill').plot(ax=ax[1], style=':s')
128/12:
fig, axes = plt.subplots(2, 1)
data = goog['Close'].iloc[:10]
data.asfreq('D').plot(ax=axes[0], marker='o')
data.asfreq('D', method='bfill').plot(ax=ax[1], style='-o')
data.asfreq('D', method='ffill').plot(ax=ax[1], style=':s')
128/13:
fig, axes = plt.subplots(2, 1)
data = goog['Close'].iloc[:10]
data.asfreq('D').plot(ax=axes[0], marker='o')
data.asfreq('D', method='bfill').plot(ax=axes[1], style='-o')
data.asfreq('D', method='ffill').plot(ax=axes[1], style=':s')
128/14:
fig, ax = plt.subplots(3, sharex=True)
# –∑–∞–¥–∞–µ–º –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
goog = goog['Close'].asfreq('D', method='pad')

goog.plot(as=as[0])
goog.shift(900).plot(ax=ax[1])
goog.tshift(900).plot(ax=ax[2])
128/15:
fig, ax = plt.subplots(3, sharex=True)
# –∑–∞–¥–∞–µ–º –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
goog = goog['Close'].asfreq('D', method='pad')

goog.plot(ax=ax[0])
goog.shift(900).plot(ax=ax[1])
goog.tshift(900).plot(ax=ax[2])
128/16:
fig, ax = plt.subplots(3, sharex=True)
# –∑–∞–¥–∞–µ–º –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
goog = goog['Close'].asfreq('D', method='pad')

goog.plot(ax=ax[0])
goog.shift(900).plot(ax=ax[1])
goog.tshift(900).plot(ax=ax[2])

local_max = pd.to_datetime('2007-11-05')
offset = pd.Timedelta(900, 'D')

ax[0].legend(['input'], loc=2)
ax[0].get_xticklabels()[4].set(weight='heavy', color='red')
ax[0].axvline(local_max, alpha=.3, color='red')
128/17: goog
128/18:
fig, ax = plt.subplots(3, sharex=True)
# –∑–∞–¥–∞–µ–º –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
goog = goog.asfreq('D', method='pad')

goog.plot(ax=ax[0])
goog.shift(900).plot(ax=ax[1])
goog.tshift(900).plot(ax=ax[2])

local_max = pd.to_datetime('2007-11-05')
offset = pd.Timedelta(900, 'D')

ax[0].legend(['input'], loc=2)
ax[0].get_xticklabels()[4].set(weight='heavy', color='red')
ax[0].axvline(local_max, alpha=.3, color='red')
128/19:
fig, ax = plt.subplots(3, sharex=True)
# –∑–∞–¥–∞–µ–º –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
goog = goog.asfreq('D', method='pad')

goog.plot(ax=ax[0])
goog.shift(900).plot(ax=ax[1])
goog.tshift(900).plot(ax=ax[2])

local_max = pd.to_datetime('2007-11-05')
offset = pd.Timedelta(900, 'D')

ax[0].legend(['input'], loc=2)
ax[0].get_xticklabels()[3].set(weight='heavy', color='red')
ax[0].axvline(local_max, alpha=.3, color='red')
128/20: ax[0].get_xticklabels()
128/21: ax[0].get_xticklabels()[:]
128/22:
fig, ax = plt.subplots(3, sharex=True)
# –∑–∞–¥–∞–µ–º –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
goog = goog.asfreq('D', method='pad')

goog.plot(ax=ax[0])
goog.shift(900).plot(ax=ax[1])
goog.tshift(900).plot(ax=ax[2])

local_max = pd.to_datetime('2007-11-05')
offset = pd.Timedelta(900, 'D')

ax[0].legend(['input'], loc=2)
#ax[0].get_xticklabels()[3].set(weight='heavy', color='red')
ax[0].axvline(local_max, alpha=.3, color='red')

ax[1].legend(['shift(900)'], loc=2)
#ax[1].get_xticklabels()[3].set(weight='heavy', color='red')
ax[1].axvline(local_max + offset, alpha=.3, color='red')
128/23:
fig, ax = plt.subplots(3, sharex=True)
# –∑–∞–¥–∞–µ–º –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
goog = goog.asfreq('D', method='pad')

goog.plot(ax=ax[0])
goog.shift(900).plot(ax=ax[1])
goog.tshift(900).plot(ax=ax[2])

local_max = pd.to_datetime('2007-11-05')
offset = pd.Timedelta(900, 'D')

ax[0].legend(['input'], loc=2)
#ax[0].get_xticklabels()[3].set(weight='heavy', color='red')
ax[0].axvline(local_max, alpha=.3, color='red')

ax[1].legend(['shift(900)'], loc=2)
#ax[1].get_xticklabels()[3].set(weight='heavy', color='red')
ax[1].axvline(local_max + offset, alpha=.3, color='red')

ax[2].legend(['tshift(900)'], loc=2)
#ax[1].get_xticklabels()[3].set(weight='heavy', color='red')
ax[2].axvline(local_max + offset, alpha=.3, color='red')
128/24: goog
128/25: goog.shift(900)
128/26:
fig, ax = plt.subplots(3, sharex=True)
# –∑–∞–¥–∞–µ–º –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
goog = goog.asfreq('D', method='pad')

goog.plot(ax=ax[0])
goog.shift(1800).plot(ax=ax[1])
goog.tshift(1800).plot(ax=ax[2])

local_max = pd.to_datetime('2007-11-05')
offset = pd.Timedelta(1800, 'D')

ax[0].legend(['input'], loc=2)
#ax[0].get_xticklabels()[3].set(weight='heavy', color='red')
ax[0].axvline(local_max, alpha=.3, color='red')

ax[1].legend(['shift(900)'], loc=2)
#ax[1].get_xticklabels()[3].set(weight='heavy', color='red')
ax[1].axvline(local_max + offset, alpha=.3, color='red')

ax[2].legend(['tshift(900)'], loc=2)
#ax[1].get_xticklabels()[3].set(weight='heavy', color='red')
ax[2].axvline(local_max + offset, alpha=.3, color='red')
128/27: goog.shift(1800)
128/28: goog.shift(1800).notnull()
128/29: goog[goog.shift(1800).notnull()]
128/30: goog.shift(1800).plot()
128/31:
fig, ax = plt.subplots(3, sharex=False)
# –∑–∞–¥–∞–µ–º –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
goog = goog.asfreq('D', method='pad')

goog.plot(ax=ax[0])
goog.shift(1800).plot(ax=ax[1])
goog.tshift(1800).plot(ax=ax[2])

local_max = pd.to_datetime('2007-11-05')
offset = pd.Timedelta(1800, 'D')

ax[0].legend(['input'], loc=2)
#ax[0].get_xticklabels()[3].set(weight='heavy', color='red')
ax[0].axvline(local_max, alpha=.3, color='red')

ax[1].legend(['shift(900)'], loc=2)
#ax[1].get_xticklabels()[3].set(weight='heavy', color='red')
ax[1].axvline(local_max + offset, alpha=.3, color='red')

ax[2].legend(['tshift(900)'], loc=2)
#ax[1].get_xticklabels()[3].set(weight='heavy', color='red')
ax[2].axvline(local_max + offset, alpha=.3, color='red')
128/32: goog[goog.tshift(1800).notnull()]
128/33: goog.tshift(1800)
128/34:
### –ü—Ä–∏–±—ã–ª—å –æ—Ç –≤–ª–æ–∂–µ–Ω–∏–π
ROI = 100 * (goog.tshift(-365) / goog - 1)
ROI.plot()
128/35:
### –ü—Ä–∏–±—ã–ª—å –æ—Ç –≤–ª–æ–∂–µ–Ω–∏–π
ROI = 100 * (goog.tshift(-365) / goog - 1)
ROI.plot()
plt.ylabel('% Return on Investment')
128/36:
#—Å–∫–æ–ª—å–∑—è—â–∏–µ –æ–∫–Ω–∞
rolling = goog.rolling(365, center=True)
data = pd.DataFrame({'input': goog,
                     'one-year rolling_mean': rolling.mean(),
                     'one-year rolling_std': rolling.std()
                    })
ax = data.plot(style=['-', '--', ':'])
ax.lines[0].set_alpha(.3)
128/37:
rolling = goog.rolling(30, center=True)
data = pd.DataFrame({'input': goog,
                     'one-year rolling_mean': rolling.mean(),
                     'one-year rolling_std': rolling.std()
                    })
ax = data.plot(style=['-', '--', ':'])
ax.lines[0].set_alpha(.3)
128/38: data
128/39: data[:60]
128/40: data[:30]
128/41: data[:30].input.mean()
128/42: data[1:31].input.mean()
128/43:
#####
data = pd.read_csv('data/Fremont_Bridge_Bicycle_Counter.csv', index_col='Date', parse_dates=True)
128/44: data.head(5)
128/45: data.shape
128/46: data.isnull().mean()
128/47: data.dropna(inplace=True)
128/48: data.shape
128/49: data.isnull().mean()
128/50: data.columns[0]
128/51: data.columns[0] = 'Total'
128/52: data.columns
128/53: data.columns = ['Total', 'East Sidewalk', 'West Sidewalk']
128/54: data.head(5)
128/55: data.shape
128/56: data['Total'].plot()
128/57:
weekly = data.resample('W').sum()
weekly.plot(style=[':', '--', '-'])
plt.ylabel('Weekly bicycle count')
128/58:
weekly = data.resample('W').sum()
weekly.plot(style=[':', '--', '--'])
plt.ylabel('Weekly bicycle count')
128/59:
weekly = data.resample('W').sum()
weekly.plot(style=['--', ':', ':'])
plt.ylabel('Weekly bicycle count')
128/60: weekly
128/61:
weekly = data.resample('W').mean()
weekly.plot(style=['--', ':', ':'])
plt.ylabel('Weekly bicycle count')
128/62: data
128/63: data.Date
128/64: data.strftime('%A')
128/65: data.index
128/66: data.index.strftime('%A')
128/67: data.index.strftime('%A')[:30]
128/68: data['day_of_week'] = data.index.strftime('%A')
128/69: data
128/70: data.groupby('day_of_week')['Total'].mean().reset_index()
128/71:
data['day_of_week'] = data.index.strftime('%A')
data['year'] = data.index.strftime('%Y')
128/72: data.groupby(['year', 'day_of_week'])['Total'].mean().reset_index()
128/73: data.groupby('day_of_week')['Total'].mean().reset_index()
128/74: data.groupby(['year', 'day_of_week'])['Total'].mean().reset_index().plot()
128/75: data.resample('D').sum()
128/76: data.resample('A').sum()
128/77: data.resample('D').sum()
128/78: data.resample('D').sum()
128/79: data.resample('D').sum()
128/80:
daily = data.resample('D').sum()
daily.rolling(30, center=True).sum().plot
128/81:
daily = data.resample('D').sum()
daily.rolling(30, center=True).sum().plot(style=['--', ':', ':'])
128/82:
daily = data.resample('D').sum()
daily.rolling(30, center=True).sum().plot(style=['--', ':', ':'])
plt.ylabel('=_=')
128/83: data.resample('D').sum()
128/84: data.resample('D').sum().rolling(30, center=True).sum()
128/85: data.resample('D').sum().rolling(30, center=True).sum().isnull().mean()
128/86: data.resample('D').sum().rolling(30, center=True).sum()
128/87: data.resample('D').sum().rolling(30, center=True).sum()[40:]
128/88: daily
128/89: daily.rolling(50, center=True, win_type='gaussian').sum(std=10).plot(style=['--', ':', ':'])
128/90: data.groupby(data.index.time).mean()
128/91: data.groupby(data.index.time).mean()['Total'].plot()
128/92: data.groupby(data.index.time).mean().plot()
128/93:
by_time = data.groupby(data.index.time).mean()
hourly_ticks = 4 * 60 * 60 * np.arrange(6)
by_time.plot(xticks=hourly_ticks)
128/94:
import seaborn
import numpy as np
128/95:
by_time = data.groupby(data.index.time).mean()
hourly_ticks = 4 * 60 * 60 * np.arrange(6)
by_time.plot(xticks=hourly_ticks)
128/96:
by_time = data.groupby(data.index.time).mean()
hourly_ticks = 4 * 60 * 60 * np.arrange(6)
by_time.plot(xticks=hourly_ticks)
128/97:
by_time = data.groupby(data.index.time).mean()
hourly_ticks = 4 * 60 * 60 * np.arange(6)
by_time.plot(xticks=hourly_ticks)
128/98: 4 * 60 * 60 * np.arange(6)
128/99:
by_time = data.groupby(data.index.time).mean()
hourly_ticks = 2 * 60 * 60 * np.arange(6)
by_time.plot(xticks=hourly_ticks)
128/100:
by_time = data.groupby(data.index.time).mean()
hourly_ticks = 2 * 60 * 60 * np.arange(12)
by_time.plot(xticks=hourly_ticks)
128/101:
df_weekday = data.groupby(data.index.dayofweek).mean()
by_weekday.index = ['Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun']
by_weekday.plot()
128/102:
df_weekday = data.groupby(data.index.dayofweek).mean()
df_weekday.index = ['Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun']
df_weekday.plot()
129/1: #!conda install seaborn
129/2: #!conda update pandas
129/3: #!conda update scikit-learn
129/4:
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
#import seaborn as sns
129/5:
import seaborn
import numpy as np
129/6:
import pandas as pd
import pandas_datareader as pdr
from pandas_datareader import data
129/7: pd.__version__
129/8: print('matplotlib: {}'.format(matplotlib.__version__))
129/9: goog = data.DataReader('GOOG', start='2004', end='2016', data_source='yahoo')
129/10: goog
129/11: goog['Close'].plot()
129/12:
goog['Close'].plot(alpha=.6, style='-')
goog['Close'].resample('BA').mean().plot(style=':')
goog['Close'].asfreq('BA').plot(style='--')
129/13: goog['Close'].resample('BA').mean().reset_index()
129/14:
fig, axes = plt.subplots(2, 1)
data = goog['Close'].iloc[:10]
data.asfreq('D').plot(ax=axes[0], marker='o')
data.asfreq('D', method='bfill').plot(ax=axes[1], style='-o')
data.asfreq('D', method='ffill').plot(ax=axes[1], style=':s')
129/15: goog
129/16: goog[goog.shift(1800).notnull()]
129/17: goog.tshift(1800)
129/18: goog.shift(1800).plot()
129/19: goog.tshift(800)
129/20: goog[goog.shift(1800).notnull()]
129/21: goog.tshift(800)
129/22: goog.shift(1800).plot()
129/23: goog.shift(800).plot()
129/24:
fig, ax = plt.subplots(3, sharex=False)
# –∑–∞–¥–∞–µ–º –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
goog = goog.asfreq('D', method='pad')

goog.plot(ax=ax[0])
goog.shift(1800).plot(ax=ax[1])
goog.tshift(1800).plot(ax=ax[2])

local_max = pd.to_datetime('2007-11-05')
offset = pd.Timedelta(1800, 'D')

ax[0].legend(['input'], loc=2)
#ax[0].get_xticklabels()[3].set(weight='heavy', color='red')
ax[0].axvline(local_max, alpha=.3, color='red')

ax[1].legend(['shift(900)'], loc=2)
#ax[1].get_xticklabels()[3].set(weight='heavy', color='red')
ax[1].axvline(local_max + offset, alpha=.3, color='red')

ax[2].legend(['tshift(900)'], loc=2)
#ax[1].get_xticklabels()[3].set(weight='heavy', color='red')
ax[2].axvline(local_max + offset, alpha=.3, color='red')
129/25:
weekly = data.resample('W').sum()
weekly.plot(style=['--', ':', ':'])
plt.ylabel('Weekly bicycle count')
129/26:
weekly = data.resample('W').mean()
weekly.plot(style=['--', ':', ':'])
plt.ylabel('Weekly bicycle count')
129/27:
#####
data = pd.read_csv('data/Fremont_Bridge_Bicycle_Counter.csv', index_col='Date', parse_dates=True)
129/28: data.columns = ['Total', 'East Sidewalk', 'West Sidewalk']
129/29: data.dropna(inplace=True)
129/30: data.head(5)
129/31: data.shape
129/32: data['Total'].plot()
129/33:
weekly = data.resample('W').sum()
weekly.plot(style=['--', ':', ':'])
plt.ylabel('Weekly bicycle count')
129/34:
weekly = data.resample('W').mean()
weekly.plot(style=['--', ':', ':'])
plt.ylabel('Weekly bicycle count')
129/35:
data['day_of_week'] = data.index.strftime('%A')
data['year'] = data.index.strftime('%Y')
129/36: data.index.strftime('%A')
129/37: data.groupby(['year', 'day_of_week'])['Total'].mean().reset_index()
129/38: data.groupby(['year', 'day_of_week'])['Total'].mean().reset_index().plot()
129/39:
daily = data.resample('D').sum()
daily.rolling(30, center=True).sum().plot(style=['--', ':', ':'])
plt.ylabel('=_=')
129/40: daily.rolling(50, center=True, win_type='gaussian').sum(std=10).plot(style=['--', ':', ':'])
129/41: data.groupby(data.index.time).mean().plot()
129/42:
by_time = data.groupby(data.index.time).mean()
hourly_ticks = 2 * 60 * 60 * np.arange(12)
by_time.plot(xticks=hourly_ticks)
129/43:
df_weekday = data.groupby(data.index.dayofweek).mean()
df_weekday.index = ['Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun']
df_weekday.plot()
130/1:
import pandas as pd
import numpy as np
130/2: data = pd.read_csv('data/Fremont_Bridge_Bicycle_Counter.csv', index_col='Date', parse_dates=True)
130/3: data.head()
130/4: data.columns
130/5: data.columns = ['Total', 'East', 'West']
130/6: pd.eval('data.Total > 10')
130/7: data.eval('Total > 10')
130/8: data[data.eval('Total > 10')]
130/9: data[data.eval('Total > 100')]
130/10: data.query('Total > 100')
131/1:
import pandas as pd
import numpy as np
131/2: from sklearn.datasets import load_iris
131/3:
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
#import seaborn as sns
131/4: iris = load_iris()
131/5: iris.head(3)
131/6: iris
131/7: features = iris.data.T
131/8: features
131/9: features[0]
131/10: features[4]
131/11: features[4]
131/12: features
131/13: plt.scatter(features[0], features[1], alpha=.3, s=100*features[3], c=iris.target, cmap='viridis')
131/14: plt.scatter(features[0], features[1], alpha=.3, s=100*features[3], c=iris.target )
131/15:
plt.scatter(features[0], features[1], alpha=.3, s=100*features[3], c=iris.target, cmap='viridis')
plt.xlabel('x')
plt.ylabel('y')
131/16:
plt.scatter(features[0], features[1], alpha=.3, s=100*features[3], c=iris.target, cmap='viridis')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
131/17:
plt.scatter(features[0], features[1], alpha=.3, s=100*features[3], c=iris.target, cmap='viridis')
plt.xlabel('x')
plt.ylabel('y')
legend1 = plt.legend(*scatter.legend_elements(num=5),
                    loc="upper left", title="Ranking")
plt.add_artist(legend1)
131/18:
scatter = plt.scatter(features[0], features[1], alpha=.3, s=100*features[3], c=iris.target, cmap='viridis')
plt.xlabel('x')
plt.ylabel('y')
legend1 = plt.legend(*scatter.legend_elements(num=5),
                    loc="upper left", title="Ranking")
plt.add_artist(legend1)
131/19:
scatter = plt.scatter(features[0], features[1], alpha=.3, s=100*features[3], c=iris.target, cmap='viridis')
plt.xlabel('x')
plt.ylabel('y')
legend1 = plt.legend(*scatter.legend_elements,
                    loc="upper left", title="Ranking")
plt.add_artist(legend1)
131/20:
scatter = plt.scatter(features[0], features[1], alpha=.3, s=100*features[3], c=iris.target, cmap='viridis')
plt.xlabel('x')
plt.ylabel('y')
plt.legend(iris.target,
           ('Low Outlier', 'LoLo', 'Lo', 'Average'),
           scatterpoints=1,
           loc='lower left',
           ncol=3,
           fontsize=8)
131/21:
scatter = plt.scatter(features[0], features[1], alpha=.3, s=100*features[3], c=iris.target, cmap='viridis',
                     label=iris.target_names[target])
plt.xlabel('x')
plt.ylabel('y')
plt.legend(iris.target,
           ('Low Outlier', 'LoLo', 'Lo', 'Average'),
           scatterpoints=1,
           loc='lower left',
           ncol=3,
           fontsize=8)
131/22:
scatter = plt.scatter(features[0], features[1], alpha=.3, s=100*features[3], c=iris.target, cmap='viridis',
                     label=iris.target)
plt.xlabel('x')
plt.ylabel('y')
plt.legend(iris.target,
           ('Low Outlier', 'LoLo', 'Lo', 'Average'),
           scatterpoints=1,
           loc='lower left',
           ncol=3,
           fontsize=8)
131/23:
import warnings
warnings.filterwarnings('ignore')
131/24:
scatter = plt.scatter(features[0], features[1], alpha=.3, s=100*features[3], c=iris.target, cmap='viridis',
                     label=iris.target)
plt.xlabel('x')
plt.ylabel('y')
plt.legend(iris.target,
           ('Low Outlier', 'LoLo', 'Lo', 'Average'),
           scatterpoints=1,
           loc='lower left',
           ncol=3,
           fontsize=8)
131/25:
scatter = plt.scatter(features[0], features[1], alpha=.3, s=100*features[3], c=iris.target, cmap='viridis',
                     label=iris.target)
plt.xlabel('x')
plt.ylabel('y')
131/26:
scatter = plt.scatter(features[0], features[1], alpha=.3, s=100*features[3], c=iris.target, cmap='viridis',
                     label=color)
plt.xlabel('x')
plt.ylabel('y')
131/27: iris.target
131/28: iris.target_names
131/29:
scatter = plt.scatter(features[0], features[1], alpha=.3, s=100*features[3], c=iris.target, cmap='viridis',
                     label=iris.target_names)
plt.xlabel('x')
plt.ylabel('y')
131/30:
scatter = plt.scatter(features[0], features[1], alpha=.3, s=100*features[3], c=iris.target, cmap='viridis',
                     label=iris.target_names)
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
131/31: pd.to_datetime(iris)
131/32: pd.DataFrame(iris)
131/33: iris
131/34: iris.dtype
131/35: iris.type
131/36: pd.DataFrame(iris)
131/37: iris
131/38: pd.DataFrame(iris.data)
131/39: pd.DataFrame(iris.data + iris.target)
131/40:
df = pd.DataFrame(data=data.data, columns=data.feature_names)
df.head()
131/41:
df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
df.head()
131/42:
df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
df['Class'] = iris.target
df.head()
131/43: features = iris.data.T
131/44: np.unique(df.target_names
131/45: np.unique(df.Class)
131/46:
df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
df['Class'] = iris.target
df['Class_name'] = iris.target_names
df.head()
131/47: iris
131/48: iris
131/49:
df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
df['Class'] = iris.target
df['Class_name'] = df['Class'].rename(columns = {0: 'Iris Setosa',
                                            1: 'Iris Versicolour',
                                            2: 'Iris Virginica'})
df.head()
131/50:
mapping = {0: 'Iris Setosa',
           1: 'Iris Versicolour',
           2: 'Iris Virginica'}
131/51:
df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
df['Class'] = iris.target
df['Class_name'] = df['Class'].replace({0: mapping, 1: mapping, 2: mapping})
df.head()
131/52:
df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
df['Class'] = iris.target
df['Class_name'] = df.replace({0: mapping, 1: mapping, 2: mapping})
df.head()
131/53:
df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
df['Class'] = iris.target
#df['Class_name'] = df.replace({0: mapping, 1: mapping, 2: mapping})
df.head()
131/54: df.Class
131/55: df.Class
131/56:
df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
df['Class'] = iris.target
df['Class_name'] = df['Class'].map(mapping)
df.head()
131/57:
for i in np.unique(df.Class_name):
    plt.scatter(df[np.where(labels==i),0], df[np.where(labels==i),1], label=i)
131/58:
for i in np.unique(df.Class_name):
    plt.scatter(df[np.where(Class_name==i),0], df[np.where(Class_name==i),1], label=i)
131/59:
for i in np.unique(df.Class_name):
    print(i)
    plt.scatter(df[np.where(Class_name==i),0], df[np.where(Class_name==i),1], label=i)
131/60: df[df.Class_name == Iris Setosa]
131/61: df[df.Class_name == 'Iris Setosa']
131/62: df[df.Class_name == 'Iris Setosa'][0]
131/63: df[df.Class_name == 'Iris Setosa']
131/64: df[df.Class_name == 'Iris Setosa'][:,0]
131/65: df[df.Class_name == 'Iris Setosa'].illoc[:,0]
131/66: df[df.Class_name == 'Iris Setosa'].iloc[:,0]
131/67:
for i in np.unique(df.Class_name):
    plt.scatter(df[df.Class_name == i].iloc[:,0], 
                df[df.Class_name == i].iloc[:,1], 
                label=i)
131/68:
for i in np.unique(df.Class_name):
    plt.scatter(df[df.Class_name == i].iloc[:,0], 
                df[df.Class_name == i].iloc[:,1],
                alpha=0.3,
                c=df['Class']
                label=i)
131/69:
for i in np.unique(df.Class_name):
    plt.scatter(df[df.Class_name == i].iloc[:,0], 
                df[df.Class_name == i].iloc[:,1],
                alpha=0.3,
                c=df['Class'],
                label=i)
131/70:
for i in np.unique(df.Class_name):
    plt.scatter(df[df.Class_name == i].iloc[:,0], 
                df[df.Class_name == i].iloc[:,1],
                alpha=0.3,
                c=[df.Class_name == i]['Class'],
                label=i)
131/71: [df.Class_name == 'Iris Setosa']['Class']
131/72:
for i in np.unique(df.Class_name):
    plt.scatter(df[df.Class_name == i].iloc[:,0], 
                df[df.Class_name == i].iloc[:,1],
                alpha=0.3,
                c=df[df.Class_name == i]['Class'],
                label=i)
131/73:
for i in np.unique(df.Class_name):
    plt.scatter(df[df.Class_name == i].iloc[:,0], 
                df[df.Class_name == i].iloc[:,1],
                alpha=0.3,
                c=df[df.Class_name == i]['Class'],
                cmap='viridis',
                label=i)
131/74:
for i in np.unique(df.Class_name):
    plt.scatter(df[df.Class_name == i].iloc[:,0], 
                df[df.Class_name == i].iloc[:,1],
                alpha=0.3,
                c=df[df.Class_name == i]['Class'],
                cmap='viridis',
                label=i)
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
131/75: color_list = ['r', 'b', 'y']
131/76:
for i in np.unique(df.Class):
    plt.scatter(df[df.Class == i].iloc[:,0], 
                df[df.Class == i].iloc[:,1],
                alpha=0.3,
                c=color_list[i],
                cmap='viridis',
                label=i)
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
131/77: mapping[0]
131/78:
for i in np.unique(df.Class):
    plt.scatter(df[df.Class == i].iloc[:,0], 
                df[df.Class == i].iloc[:,1],
                alpha=0.3,
                c=color_list[i],
                cmap='viridis',
                label=mapping[i])
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
131/79:
## histograms
df.head()
131/80: plt.hist(df.iloc[:,1])
131/81: plt.hist(df.iloc[:,1], bins=30)
131/82: plt.hist(df.iloc[:,1], bins=30, normed=True)
131/83: plt.hist(df.iloc[:,1], bins=30, alpha=.5)
131/84: plt.hist(df.iloc[:,1], bins=10, alpha=.5)
131/85: plt.hist(df.iloc[:,1], bins=20, alpha=.5)
131/86:
for i in np.unique(df.Class):
    plt.hist(df[df.Class == i].iloc[:,0],
             alpha=0.3,
             c=color_list[i],
             cmap='viridis',
             label=mapping[i])
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
131/87:
for i in np.unique(df.Class):
    plt.hist(df[df.Class == i].iloc[:,0],
             alpha=0.3,
             c=color_list[i],
             label=mapping[i])
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
131/88:
for i in np.unique(df.Class):
    plt.hist(df[df.Class == i].iloc[:,0],
             alpha=0.3,
             c=color_list[i],
             #label=mapping[i])
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
131/89:
for i in np.unique(df.Class):
    plt.hist(df[df.Class == i].iloc[:,0],
             alpha=0.3,
             c=color_list[i],
             #label=mapping[i])
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
131/90:
for i in np.unique(df.Class):
    plt.hist(df[df.Class == i].iloc[:,0],
             alpha=0.3,
             c=color_list[i]
             #label=mapping[i]
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
131/91:
for i in np.unique(df.Class):
    plt.hist(df[df.Class == i].iloc[:,0],
             alpha=0.3,
             color=color_list[i]
             #label=mapping[i]
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
131/92:
for i in np.unique(df.Class):
    plt.hist(df[df.Class == i].iloc[:,0],
             bins=15,
             alpha=0.3,
             color=color_list[i]
             #label=mapping[i]
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
131/93:
for i in np.unique(df.Class):
    plt.hist(df[df.Class == i].iloc[:,0],
             bins=15,
             alpha=0.3,
             color=color_list[i],
             stacked=True
             #label=mapping[i]
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
131/94:
for i in np.unique(df.Class):
    plt.hist(df[df.Class == i].iloc[:,0],
             bins=15,
             alpha=0.3,
             color=color_list[i],
             stacked=False
             #label=mapping[i]
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
131/95:
for i in np.unique(df.Class):
    plt.hist(df[df.Class == i].iloc[:,0],
             bins=15,
             alpha=0.3,
             color=color_list[i],
             rwidth=.1
             #label=mapping[i]
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
131/96:
for i in np.unique(df.Class):
    plt.hist(df[df.Class == i].iloc[:,0],
             bins=15,
             alpha=0.3,
             color=color_list[i],
             rwidth=.8
             #label=mapping[i]
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
131/97:
for i in np.unique(df.Class):
    plt.hist(df[df.Class == i].iloc[:,0],
             bins=15,
             alpha=0.3,
             color=color_list[i],
             histtype='bar'
             #rwidth=.8
             #label=mapping[i]
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
131/98:
for i in np.unique(df.Class):
    plt.hist(df[df.Class == i].iloc[:,0],
             bins=15,
             alpha=0.3,
             color=color_list[i],
             histtype='barstacked'
             #rwidth=.8
             #label=mapping[i]
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
131/99:
for i in np.unique(df.Class):
    plt.hist(df[df.Class == i].iloc[:,0],
             bins=15,
             alpha=0.3,
             color=color_list[i],
             histtype='barstacked'
             #rwidth=.8
             label=mapping[i]
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
131/100:
for i in np.unique(df.Class):
    plt.hist(df[df.Class == i].iloc[:,0],
             bins=15,
             alpha=0.3,
             color=color_list[i],
             histtype='barstacked',
             #rwidth=.8
             label=mapping[i]
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
131/101:
for i in np.unique(df.Class):
    plt.hist(df[df.Class == i].iloc[:,0],
             bins=15,
             alpha=0.3,
             color=color_list[i],
             histtype='stepfilled',
             #rwidth=.8
             label=mapping[i]
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
131/102: df.Class
131/103: df.Class_name
131/104: df[df.Class == 'Iris Setosa']
131/105: df.Class_name
131/106: df[df.Class == 'Iris Virginica']
131/107: df[df.Class_name == 'Iris Virginica']
131/108: df[df.Class_name == 'Iris Virginica'].iloc[:,0]
131/109: df[df.Class_name == 'Iris Virginica'].iloc[:,0]
131/110: np.histogram(df[df.Class_name == 'Iris Virginica'].iloc[:,0], bins=5)
131/111: np.histogram(df[df.Class_name == 'Iris Setosa'].iloc[:,0], bins=5)
131/112:
####
california_data = pd.read_csv('california_cities.csv')
131/113: california_data
131/114:
####
california_data = pd.read_csv('california_cities.csv', index_col=False)
131/115: california_data
131/116:
####
california_data = pd.read_csv('california_cities.csv', index_col=0)
131/117: california_data
131/118: california_data.hqad()
131/119: california_data.head()
131/120: lat, lon = california_data['latd'], california_data['longd']
131/121:
lat, lon = california_data['latd'], california_data['longd']
population, area = california_data['population_total'], california_data['area_total_km2']
131/122:
plt.scatter(lon, lat, label=None, c=np.log10(population),
            cmap='viridis', s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
131/123: c=np.log10(population)
131/124: c
131/125:
plt.scatter(lon, lat, label=None, c=np.log10(population),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
131/126:
plt.scatter(lon, lat, label=None, c=np.log10(population)-2,
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
131/127: c=np.log10(population)+10
131/128: c
131/129:
plt.scatter(lon, lat, label=None, 
            c=np.log10(population)+100,
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
131/130:
plt.scatter(lon, lat, label=None, 
            c=np.log10(population)+1000,
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
131/131:
plt.scatter(lon, lat, label=None, 
            c=np.log10(population)+10000,
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
131/132:
plt.scatter(lon, lat, label=None, 
            c=np.log10(population)-10000,
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
131/133:
plt.scatter(lon, lat, label=None, 
            #c=np.log10(population)-10000,
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
131/134:
plt.scatter(lon, lat, label=None, 
            #c=np.log10(population)-10000,
            cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
131/135:
plt.scatter(lon, lat, label=None, 
            c=np.log10(population)-10000,
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
131/136:
plt.scatter(lon, lat, label=None, 
            c=np.log10(population*10),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
131/137:
plt.scatter(lon, lat, label=None, 
            c=np.log10(population*100),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
131/138:
plt.scatter(lon, lat, label=None, 
            c=np.log10(population*-100),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
131/139:
plt.scatter(lon, lat, label=None, 
            c=np.log10(population),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
131/140:
plt.scatter(lon, lat, label=None, 
            c=np.log(population),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
131/141:
plt.scatter(lon, lat, label=None, 
            c=2,
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
131/142:
plt.scatter(lon, lat, label=None, 
            c=population,
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
131/143: population
131/144: population.describe()
131/145: population.hist()
131/146:
plt.scatter(lon, lat, label=None, 
            c=np.log(population),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
131/147:
plt.scatter(lon, lat, label=None, 
            c=np.log(population),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
plt.xlabel('long')
plt.ylabel('latitude')
plt.colorbar(label='log$_{10}$(population)')
plt.clim(3, 7)
131/148:
plt.scatter(lon, lat, label=None, 
            c=np.log10(population),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
plt.xlabel('long')
plt.ylabel('latitude')
plt.colorbar(label='log$_{10}$(population)')
plt.clim(3, 7)
131/149:
plt.scatter(lon, lat, label=None, 
            c=np.log10(population),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
plt.xlabel('long')
plt.ylabel('latitude')
plt.colorbar(label='log$_{222}$(population)')
plt.clim(3, 7)
131/150:
plt.scatter(lon, lat, label=None, 
            c=np.log10(population),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
plt.xlabel('long')
plt.ylabel('latitude')
plt.colorbar(label='log$_{10}$(population)')
plt.clim(3, 7)
131/151:
plt.scatter(lon, lat, label=None, 
            c=np.log10(population),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
plt.xlabel('long')
plt.ylabel('latitude')
plt.colorbar(label='log$_{10}$(population)')
plt.clim(3, 7)
for area in [100, 300, 500]:
    plt.scatter([], [], c='k', alpha=.3, s=area, label=str(area)+'km$^2"$')
plt.legend(scatterpoints=1, frameon=False, labelspacing=1, title='√áity Area')
131/152:
plt.scatter(lon, lat, label=None, 
            c=np.log10(population),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
plt.xlabel('long')
plt.ylabel('latitude')
plt.colorbar(label='log$_{10}$(population)')
plt.clim(3, 7)
for area in [100, 300, 500]:
    plt.scatter([], [], c='c', alpha=.3, s=area, label=str(area)+'km$^2"$')
plt.legend(scatterpoints=1, frameon=False, labelspacing=1, title='√áity Area')
131/153:
plt.scatter(lon, lat, label=None, 
            c=np.log10(population),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
plt.xlabel('long')
plt.ylabel('latitude')
plt.colorbar(label='log$_{10}$(population)')
plt.clim(3, 7)
for area in [100, 300, 500]:
    plt.scatter([], [], c='k', alpha=.3, s=area, label=str(area)+'km$^2"$')
plt.legend(scatterpoints=1, frameon=False, labelspacing=1, title='√áity Area')
131/154:
plt.scatter(lon, lat, label=None, 
            c=np.log10(population),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
plt.xlabel('long')
plt.ylabel('latitude')
plt.colorbar(label='log$_{10}$(population)')
plt.clim(3, 7)
131/155:
for i in np.unique(df.Class):
    plt.hist(df[df.Class == i].iloc[:,0],
             bins=15,
             alpha=0.3,
             color=color_list[i],
             histtype='stepfilled',
             #rwidth=.8
             label=mapping[i]
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
131/156: np.histogram(df[df.Class_name == 'Iris Setosa'].iloc[:,0], bins=5)
131/157:
####
california_data = pd.read_csv('california_cities.csv', index_col=0)
131/158: california_data.head()
131/159:
lat, lon = california_data['latd'], california_data['longd']
population, area = california_data['population_total'], california_data['area_total_km2']
131/160:
plt.scatter(lon, lat, label=None, 
            c=np.log10(population),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
plt.xlabel('long')
plt.ylabel('latitude')
plt.colorbar(label='log$_{10}$(population)')
plt.clim(3, 7)
for area in [100, 300, 500]:
    plt.scatter([], [], c='k', alpha=.3, s=area, label=str(area)+'km$^2"$')
plt.legend(scatterpoints=1, frameon=False, labelspacing=1, title='√áity Area')
131/161:
for i in np.unique(df.Class):
    plt.hist(df[df.Class == i].iloc[:,0],
             bins=15,
             alpha=0.3,
             color=color_list[i],
             histtype='stepfilled',
             #rwidth=.8
             label=mapping[i]
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
131/162: np.histogram(df[df.Class_name == 'Iris Setosa'].iloc[:,0], bins=5)
131/163:
####
california_data = pd.read_csv('california_cities.csv', index_col=0)
131/164: california_data.head()
131/165:
lat, lon = california_data['latd'], california_data['longd']
population, area = california_data['population_total'], california_data['area_total_km2']
131/166:
plt.scatter(lon, lat, label=None, 
            c=np.log10(population),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
plt.xlabel('long')
plt.ylabel('latitude')
plt.colorbar(label='log$_{10}$(population)')
plt.clim(3, 7)
for area in [100, 300, 500]:
    plt.scatter([], [], c='r', alpha=.3, s=area, label=str(area)+'km$^2"$')
plt.legend(scatterpoints=1, frameon=False, labelspacing=1, title='√áity Area')
131/167:
plt.scatter(lon, lat, label=None, 
            c=np.log10(population),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
plt.xlabel('long')
plt.ylabel('latitude')
plt.colorbar(label='log$_{10}$(population)')
plt.clim(3, 7)
for area in [100, 300, 500]:
    plt.scatter([], [], c='g', alpha=.3, s=area, label=str(area)+'km$^2"$')
plt.legend(scatterpoints=1, frameon=False, labelspacing=1, title='√áity Area')
131/168:
for i in np.unique(df.Class):
    plt.scatter(df[df.Class == i].iloc[:,0], 
                df[df.Class == i].iloc[:,1],
                alpha=0.3,
                c=color_list[i],
                cmap='viridis',
                label=mapping[i])
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
131/169:
## histograms
df.head()
131/170: plt.hist(df.iloc[:,1], bins=20, alpha=.5)
131/171:
for i in np.unique(df.Class):
    plt.hist(df[df.Class == i].iloc[:,0],
             bins=15,
             alpha=0.3,
             color=color_list[i],
             histtype='stepfilled',
             #rwidth=.8
             label=mapping[i]
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
131/172: np.histogram(df[df.Class_name == 'Iris Setosa'].iloc[:,0], bins=5)
131/173:
####
california_data = pd.read_csv('california_cities.csv', index_col=0)
131/174: california_data.head()
131/175:
lat, lon = california_data['latd'], california_data['longd']
population, area = california_data['population_total'], california_data['area_total_km2']
131/176:
plt.scatter(lon, lat, label=None, 
            c=np.log10(population),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
plt.xlabel('long')
plt.ylabel('latitude')
plt.colorbar(label='log$_{10}$(population)')
plt.clim(3, 7)
for area in [100, 300, 500]:
    plt.scatter([], [], c='k', alpha=.3, s=area, label=str(area)+'km$^2"$')
plt.legend(scatterpoints=1, frameon=False, labelspacing=1, title='√áity Area')
131/177:
plt.scatter(lon, lat, label=None, 
            c=np.log10(population),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
plt.xlabel('long')
plt.ylabel('latitude')
plt.colorbar(label='log$_10$(population)')
plt.clim(3, 7)
for area in [100, 300, 500]:
    plt.scatter([], [], c='k', alpha=.3, s=area, label=str(area)+'km$^2"$')
plt.legend(scatterpoints=1, frameon=False, labelspacing=1, title='√áity Area')
131/178:
plt.clf()
plt.scatter(lon, lat, label=None, 
            c=np.log10(population),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
plt.xlabel('long')
plt.ylabel('latitude')
plt.colorbar(label='log$_10$(population)')
plt.clim(3, 7)
for area in [100, 300, 500]:
    plt.scatter([], [], c='k', alpha=.3, s=area, label=str(area)+'km$^2"$')
plt.legend(scatterpoints=1, frameon=False, labelspacing=1, title='√áity Area')
131/179:
plt.clf()
plt.cla()
plt.close()
plt.scatter(lon, lat, label=None, 
            c=np.log10(population),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
plt.xlabel('long')
plt.ylabel('latitude')
plt.colorbar(label='log$_10$(population)')
plt.clim(3, 7)
for area in [100, 300, 500]:
    plt.scatter([], [], c='k', alpha=.3, s=area, label=str(area)+'km$^2"$')
plt.legend(scatterpoints=1, frameon=False, labelspacing=1, title='√áity Area')
131/180:
plt.clf()
plt.cla()
plt.close()
plt.scatter(lon, lat, label=None, 
            c=np.log10(population),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
plt.xlabel('long')
plt.ylabel('latitude')
plt.colorbar(label='log$_10$(population)')
plt.clim(3, 7)
for area in [100, 300, 500]:
    plt.scatter([], [], c='k', alpha=.3, s=area, label=str(area)+'km$^2"$')
plt.legend(scatterpoints=1, frameon=False, labelspacing=1, title='√áity Area')
plt.show()
131/181:
for i in np.unique(df.Class):
    plt.hist(df[df.Class == i].iloc[:,0],
             bins=15,
             alpha=0.3,
             color=color_list[i],
             histtype='stepfilled',
             #rwidth=.8
             label=mapping[i]
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
131/182: np.histogram(df[df.Class_name == 'Iris Setosa'].iloc[:,0], bins=5)
131/183:
####
california_data = pd.read_csv('california_cities.csv', index_col=0)
131/184: california_data.head()
131/185:
lat, lon = california_data['latd'], california_data['longd']
population, area = california_data['population_total'], california_data['area_total_km2']
131/186:
plt.clf()
plt.cla()
plt.close()
plt.scatter(lon, lat, label=None, 
            c=np.log10(population),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
plt.xlabel('long')
plt.ylabel('latitude')
plt.colorbar(label='log$_10$(population)')
plt.clim(3, 7)
for area in [100, 300, 500]:
    plt.scatter([], [], c='k', alpha=.3, s=area, label=str(area)+'km$^2"$')
plt.legend(scatterpoints=1, frameon=False, labelspacing=1, title='√áity Area')
plt.show()
131/187:
plt.clf()
plt.cla()
plt.close()
plt.scatter(lon, lat, label=None, 
            c=np.log10(population),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
plt.xlabel('long')
plt.ylabel('latitude')
plt.colorbar(label='log$_10$(population)')
plt.clim(3, 7)
for area in [100, 300, 500]:
    plt.scatter([], [], c='k', alpha=.3, s=area, label=str(area)+'km$^2"$')
plt.legend(scatterpoints=1, frameon=False, labelspacing=1, title='√áity Area')
plt.show()
131/188:

plt.scatter(lon, lat, label=None, 
            c=np.log10(population),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
plt.xlabel('long')
plt.ylabel('latitude')
plt.colorbar(label='log$_10$(population)')
plt.clim(3, 7)
131/189:

plt.scatter(lon, lat, label=None, 
            c=np.log10(population),
            cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
plt.xlabel('long')
plt.ylabel('latitude')
plt.colorbar(label='log$_10$(population)')
plt.clim(3, 7)
131/190:
for i in np.unique(df.Class):
    plt.hist(df[df.Class == i].iloc[:,0],
             bins=15,
             alpha=0.3,
             color=color_list[i],
             histtype='stepfilled',
             #rwidth=.8
             label=mapping[i]
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
131/191: np.histogram(df[df.Class_name == 'Iris Setosa'].iloc[:,0], bins=5)
131/192:
####
california_data = pd.read_csv('california_cities.csv', index_col=0)
131/193: california_data.head()
131/194:
lat, lon = california_data['latd'], california_data['longd']
population, area = california_data['population_total'], california_data['area_total_km2']
131/195:

plt.scatter(lon, lat, label=None, 
            c=np.log10(population),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
plt.xlabel('long')
plt.ylabel('latitude')
plt.colorbar(label='log$_10$(population)')
plt.clim(3, 7)
for area in [100, 300, 500]:
    plt.scatter([], [], c='k', alpha=.3, s=area, label=str(area)+'km$^2"$')
plt.legend(scatterpoints=1, frameon=False, labelspacing=1, title='√áity Area')
plt.show()
131/196:
plt.scatter(lon, lat, label=None, 
            c=np.log10(population),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
plt.xlabel('long')
plt.ylabel('latitude')
plt.colorbar(label='log$_{10}$(population)')
plt.clim(3, 7)
for area in [100, 300, 500]:
    plt.scatter([], [], c='k', alpha=.3, s=area, label=str(area)+'km$^2$')
plt.legend(scatterpoints=1, frameon=False, labelspacing=1, title='√áity Area')
131/197:
for i in np.unique(df.Class):
    plt.hist(df[df.Class == i].iloc[:,0],
             bins=15,
             alpha=0.3,
             color=color_list[i],
             histtype='stepfilled',
             #rwidth=.8
             label=mapping[i]
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
131/198: np.histogram(df[df.Class_name == 'Iris Setosa'].iloc[:,0], bins=5)
131/199:
####
california_data = pd.read_csv('california_cities.csv', index_col=0)
131/200: california_data.head()
131/201:
lat, lon = california_data['latd'], california_data['longd']
population, area = california_data['population_total'], california_data['area_total_km2']
131/202:
plt.scatter(lon, lat, label=None, 
            c=np.log10(population),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
plt.xlabel('long')
plt.ylabel('latitude')
plt.colorbar(label='log$_{10}$(population)')
plt.clim(3, 7)
for area in [100, 300, 500]:
    plt.scatter([], [], c='k', alpha=.3, s=area, label=str(area)+'km$^2$')
plt.legend(scatterpoints=1, frameon=False, labelspacing=1, title='√áity Area')
131/203: area
131/204:
lat, lon = california_data['latd'], california_data['longd']
population, area = california_data['population_total'], california_data['area_total_km2']
131/205:
plt.scatter(lon, lat, label=None, 
            c=np.log10(population),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
plt.xlabel('long')
plt.ylabel('latitude')
plt.colorbar(label='log$_{10}$(population)')
plt.clim(3, 7)
for area_list in [100, 300, 500]:
    plt.scatter([], [], c='k', alpha=.3, s=area_list, label=str(area)+'km$^2$')
plt.legend(scatterpoints=1, frameon=False, labelspacing=1, title='√áity Area')
131/206:
plt.scatter(lon, lat, label=None, 
            c=np.log10(population),
            #cmap='viridis', 
            s=area, linewidths=0, alpha=.5)
plt.axis(aspect='equal')
plt.xlabel('long')
plt.ylabel('latitude')
plt.colorbar(label='log$_{10}$(population)')
plt.clim(3, 7)
for area_list in [100, 300, 500]:
    plt.scatter([], [], c='k', alpha=.3, s=area_list, label=str(area_list)+'km$^2$')
plt.legend(scatterpoints=1, frameon=False, labelspacing=1, title='√áity Area')
131/207: california_data.sort_values('population')
131/208: california_data.sort_values('population_total')
130/11:
import pandas as pd
import numpy as np
130/12: data = pd.read_csv('data/Fremont_Bridge_Bicycle_Counter.csv', index_col='Date', parse_dates=True)
130/13: data.head()
130/14: data.columns = ['Total', 'East', 'West']
130/15: pd.eval('data.Total > 10')
130/16: data[data.eval('Total > 100')]
130/17: data.query('Total > 100')
133/1:
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits import mplot3d

from sklearn.datasets import load_diabetes
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV
133/2:
# if you want more information about the dataset for this demo:

# scikit-learn dataset
# https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset

# in short, regression problem, trying to predict disease progression based on
# a bunch of variables

# load dataset
diabetes_X, diabetes_y = load_diabetes(return_X_y=True)
X = pd.DataFrame(diabetes_X)
y = diabetes_y

X.head()
133/3:
# random forests
rf_model = RandomForestRegressor(
    n_estimators=100, max_depth=3, random_state=0, n_jobs=4)

# hyperparameter space
rf_param_grid = dict(
    n_estimators=[2, 5, 10, 20, 50, 100, 200, 300],
#     max_depth=[1, 2, 3, 4],
#     min_samples_split=[2,4,8,12,24]
)

# search
reg = GridSearchCV(rf_model, rf_param_grid,
                   scoring='r2', cv=5)

search = reg.fit(X, y)

# best hyperparameters
search.best_params_
133/4:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

results['mean_test_score'].plot(yerr=[results['std_test_score'], results['std_test_score']], subplots=True)

plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/5: search
133/6: search.cv_results_
133/7: results['std_test_score']
133/8: results['std_test_score']
133/9:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

results['mean_test_score'].plot(yerr=[results['std_test_score'], results['std_test_score']], subplots=True)

plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/10: pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
133/11: results
133/12:
# plot the resuls

results2 = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

results['mean_test_score'].plot(yerr=[results['std_test_score'], results['std_test_score']], subplots=True)

plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/13: results2
133/14:
# plot the resuls

results2 = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results2.index = rf_param_grid['n_estimators']

results['mean_test_score'].plot(yerr=[results['std_test_score'], results['std_test_score']], subplots=True)

plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/15: results2
133/16: results2
133/17: results2.mean_test_score
133/18:
# plot the resuls

results2 = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results2.index = rf_param_grid['n_estimators']

results['mean_test_score'].plot(yerr=[results['std_test_score']*2, results['std_test_score']], subplots=True)

plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/19:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

results['mean_test_score'].plot(yerr=[results['std_test_score']*2, results['std_test_score']], subplots=True)

plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/20:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

results['mean_test_score'].plot(yerr=[results['std_test_score']*2, results['std_test_score']*6], subplots=True)

plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/21:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

results['mean_test_score'].plot(yerr=[results['std_test_score']*2, results['std_test_score']*6], subplots=False)

plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/22:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

results['mean_test_score'].plot(yerr=[results['std_test_score'], results['std_test_score']], subplots=True)

plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/23: asymmetric_error = [results['std_test_score']*2, results['std_test_score']]
133/24: asymmetric_error
133/25: asymmetric_error = [results['std_test_score'], results['std_test_score']]
133/26: asymmetric_error
133/27: results
133/28: asymmetric_error = [results['std_test_score']*2, results['std_test_score']]
133/29: asymmetric_error
133/30:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

results['mean_test_score'].plot(yerr=asymmetric_error, subplots=True)

plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/31:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

results['mean_test_score'].plot(yerr=asymmetric_error)

plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/32:
# example data
x = np.arange(0.1, 4, 0.5)
y = np.exp(-x)

# example error bar values that vary with x-position
error = 0.1 + 0.2 * x
133/33: import numpy as np
133/34:
# example data
x = np.arange(0.1, 4, 0.5)
y = np.exp(-x)

# example error bar values that vary with x-position
error = 0.1 + 0.2 * x
133/35:
lower_error = 0.4 * error
upper_error = error
133/36: asymmetric_error = [lower_error, upper_error]
133/37: asymmetric_error
133/38: asymmetric_error = [results['std_test_score']*2, results['std_test_score']]
133/39: asymmetric_error
133/40: asymmetric_error[0]
133/41: asymmetric_error = [results['std_test_score']*2, results['std_test_score']]
133/42:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

results['mean_test_score'].plot(yerr=[asymmetric_error[0], asymmetric_error[1]], subplots=True)

plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/43:
lower_error = 0.4 * error
upper_error = error
133/44: asymmetric_error = [lower_error, upper_error]
133/45: asymmetric_error
133/46:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

results['mean_test_score'].plot(yerr=[asymmetric_error[0], asymmetric_error[1]], subplots=True)

plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/47:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

results['mean_test_score'].plot(yerr=[asymmetric_error[0], asymmetric_error[1]])

plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/48:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

results['mean_test_score'].plot(yerr=[asymmetric_error[0], asymmetric_error[1]])

plt.ylim(0.2, 1.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/49:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

results['mean_test_score'].plot(yerr=[asymmetric_error[0], asymmetric_error[1]])

plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/50:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

results['mean_test_score'].plot(yerr=asymmetric_error)

plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/51: asymmetric_error
133/52: results.index
133/53: results
133/54:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

#results['mean_test_score'].plot(yerr=asymmetric_error)
ax1.errorbar(results.index, results.mean_test_score, xerr=asymmetric_error, fmt='o')
plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/55:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

#results['mean_test_score'].plot(yerr=asymmetric_error)
plt.errorbar(results.index, results.mean_test_score, xerr=asymmetric_error, fmt='o')
plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/56:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

#results['mean_test_score'].plot(yerr=asymmetric_error)
plt.errorbar(results.index, results.mean_test_score, xerr=asymmetric_error)
plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/57:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

#results['mean_test_score'].plot(yerr=asymmetric_error)
plt.errorbar(results.index, results.mean_test_score, yerr=asymmetric_error)
plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/58: asymmetric_error = [results['std_test_score']*0.5, results['std_test_score']]
133/59: asymmetric_error[0]
133/60:
lower_error = 0.4 * error
upper_error = error
133/61: asymmetric_error = [results['std_test_score']*0.5, results['std_test_score']]
133/62: asymmetric_error[0]
133/63: asymmetric_error
133/64:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

#results['mean_test_score'].plot(yerr=asymmetric_error)
plt.errorbar(results.index, results.mean_test_score, yerr=asymmetric_error)
plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/65:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

#results['mean_test_score'].plot(yerr=asymmetric_error)
plt.errorbar(results.index, results.mean_test_score, yerr=asymmetric_error, fmt='o')
plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/66:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

#results['mean_test_score'].plot(yerr=asymmetric_error)
plt.errorbar(results.index, results.mean_test_score, yerr=asymmetric_error, fmt='-o')
plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/67:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

#results['mean_test_score'].plot(yerr=asymmetric_error)
plt.errorbar(results.index, results.mean_test_score, yerr=asymmetric_error, fmt='-s')
plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/68:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

#results['mean_test_score'].plot(yerr=asymmetric_error)
plt.errorbar(results.index, results.mean_test_score, yerr=asymmetric_error, fmt='-t')
plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/69:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

#results['mean_test_score'].plot(yerr=asymmetric_error)
plt.errorbar(results.index, results.mean_test_score, yerr=asymmetric_error, fmt='-<')
plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/70:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

#results['mean_test_score'].plot(yerr=asymmetric_error)
plt.errorbar(results.index, results.mean_test_score, yerr=asymmetric_error, fmt='-^')
plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/71:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

#results['mean_test_score'].plot(yerr=asymmetric_error)
plt.errorbar(results.index, results.mean_test_score, yerr=asymmetric_error, fmt='--^')
plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/72:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

#results['mean_test_score'].plot(yerr=asymmetric_error)
plt.errorbar(results.index, results.mean_test_score, yerr=asymmetric_error, fmt='--^', c='k')
plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/73:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

#results['mean_test_score'].plot(yerr=asymmetric_error)
plt.errorbar(results.index, results.mean_test_score, yerr=asymmetric_error, fmt='--^', c='b')
plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/74:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

#results['mean_test_score'].plot(yerr=asymmetric_error)
plt.errorbar(results.index, results.mean_test_score, yerr=asymmetric_error, fmt='--^', ecolor='p', c='b')
plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/75:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

#results['mean_test_score'].plot(yerr=asymmetric_error)
plt.errorbar(results.index, results.mean_test_score, yerr=asymmetric_error, fmt='--^', ecolor='r', c='b')
plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/76:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

#results['mean_test_score'].plot(yerr=asymmetric_error)
plt.errorbar(results.index, results.mean_test_score, yerr=asymmetric_error, fmt='--^', ecolor='y', c='b')
plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
133/77:
# plot the resuls

results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]
results.index = rf_param_grid['n_estimators']

#results['mean_test_score'].plot(yerr=asymmetric_error)
plt.errorbar(results.index, results.mean_test_score, yerr=asymmetric_error, fmt='--^', ecolor='y', c='b',
            barsabove=True)
plt.ylim(0.2, 0.6)
plt.ylabel('Mean r2 score')
plt.xlabel('Number of trees')
134/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix, make_scorer
134/2:
# if you want more information about the dataset for this demo:

# scikit-learn dataset
# https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset

# dataset information: UCI Machine Learning Repository
# https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)
    
# in short, classification problem, trying to predict whether the tumor
# is malignant or benign

# load dataset
breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y=True)
X = pd.DataFrame(breast_cancer_X)
y = pd.Series(breast_cancer_y).map({0:1, 1:0})

X.head()
134/3:
# percentage of benign (0) and malign tumors (1)

y.value_counts() / len(y)
135/1:
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
135/2:
# if you want more information about the dataset for this demo:

# scikit-learn dataset
# https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset

# dataset information: UCI Machine Learning Repository
# https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)
    
# in short, classification problem, trying to predict whether the tumor
# is malignant or benign

# load dataset
breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y=True)
X = pd.DataFrame(breast_cancer_X)
y = pd.Series(breast_cancer_y).map({0:1, 1:0})

X.head()
135/3:
# percentage of benign (0) and malign tumors (1)

y.value_counts() / len(y)
135/4: y
135/5:
# random forests
rf_model = RandomForestClassifier(
    n_estimators=100, max_depth=1, random_state=0, n_jobs=4)

# hyperparameter space
rf_param_grid = dict(
    n_estimators=[20, 50, 100, 200, 500, 1000],
    max_depth=[2, 3, 4],
)

# search
clf = GridSearchCV(rf_model,
                   rf_param_grid,
                   scoring='roc_auc',
                   cv=5)

search = clf.fit(X, y)

# best hyperparameters
search.best_params_
135/6: rf_param_grid
135/7:
rf_param_grid = dict{
    'n_estimators': [20, 50, 100, 200, 500, 1000],
    'max_depth': [2, 3, 4]
}
135/8:
rf_param_grid = {
    'n_estimators': [20, 50, 100, 200, 500, 1000],
    'max_depth': [2, 3, 4]
}
135/9: rf_param_grid
135/10:
rf_param_grid_d = {
    'n_estimators': [20, 50, 100, 200, 500, 1000],
    'max_depth': [2, 3, 4]
}
135/11:
# random forests
rf_model = RandomForestClassifier(
    n_estimators=100, max_depth=1, random_state=0, n_jobs=4)

# hyperparameter space
rf_param_grid = dict(
    n_estimators=[20, 50, 100, 200, 500, 1000],
    max_depth=[2, 3, 4],
)

# search
clf = GridSearchCV(rf_model,
                   rf_param_grid,
                   scoring='roc_auc',
                   cv=5)

search = clf.fit(X, y)

# best hyperparameters
search.best_params_
135/12:
rf_param_grid_d = {
    'n_estimators': [20, 50, 100, 200, 500, 1000],
    'max_depth': [2, 3, 4]
}
135/13: rf_param_grid_d
135/14: rf_param_grid
135/15: rf_param_grid.nbytes
135/16: rf_param_grid.nbytes
135/17: import sys
135/18: import sys
135/19:
# if you want more information about the dataset for this demo:

# scikit-learn dataset
# https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset

# dataset information: UCI Machine Learning Repository
# https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)
    
# in short, classification problem, trying to predict whether the tumor
# is malignant or benign

# load dataset
breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y=True)
X = pd.DataFrame(breast_cancer_X)
y = pd.Series(breast_cancer_y).map({0:1, 1:0})

X.head()
135/20: sys.getsizeof(rf_param_grid)
135/21: sys.getsizeof(rf_param_grid_d)
135/22: search
135/23: search.cv_results_
135/24:
results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]

results.head()
135/25:
results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]

results
135/26:
results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]

results
135/27:
results.sort_values(by='mean_test_score', ascending=False, inplace=True)

results.reset_index(drop=True, inplace=True)
135/28: results
135/29: results.index
135/30: plt.errorbar(results.index, results.mean_test_score, yerr=[results.std_test_score], fmt='-o')
135/31: plt.errorbar(results.index, results.mean_test_score, yerr=results.std_test_score, fmt='-o')
135/32:
plt.errorbar(results.index, results.mean_test_score, yerr=results.std_test_score, fmt='-o')
plt.ylim(0.8, 1,1)
135/33:
plt.errorbar(results.index, results.mean_test_score, yerr=results.std_test_score, fmt='-o')
plt.ylim(0.9, 1,1)
135/34:
plt.errorbar(results.index, results.mean_test_score, yerr=results.std_test_score, fmt='-o')
plt.ylim(0.98, 1,1)
135/35:
plt.errorbar(results.index, results.mean_test_score, yerr=results.std_test_score, fmt='-o')
plt.ylim(0.97, 1,1)
135/36:
plt.errorbar(results.index, results.mean_test_score, yerr=results.std_test_score, fmt='--o', c='r')
plt.ylim(0.97, 1,1)
135/37:
plt.errorbar(results.index, results.mean_test_score, yerr=results.std_test_score, fmt='--o', c='r', ecolor='black')
plt.ylim(0.97, 1,1)
135/38:
plt.errorbar(results.index, results.mean_test_score, yerr=results.std_test_score, fmt='--o', c='r', ecolor='blue')
plt.ylim(0.97, 1,1)
135/39:
plt.errorbar(results.index, results.mean_test_score, yerr=results.std_test_score, fmt='--o', c='r', ecolor='pink')
plt.ylim(0.97, 1,1)
136/1:
import numpy as np
import pandas as pd
from scipy.special import comb

from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

from sklearn.model_selection import (
    KFold,
    RepeatedKFold,
    LeaveOneOut,
    LeavePOut,
    StratifiedKFold,
    cross_validate,
    train_test_split,
)
136/2:
# if you want more information about the dataset for this demo:

# scikit-learn dataset
# https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset

# dataset information: UCI Machine Learning Repository
# https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)
    
# in short, classification problem, trying to predict whether the tumor
# is malignant or benign

# load dataset
breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y=True)
X = pd.DataFrame(breast_cancer_X)
y = pd.Series(breast_cancer_y).map({0:1, 1:0})

X.head()
136/3:
# percentage of benign (0) and malign tumors (1)

y.value_counts() / len(y)
136/4:
# split dataset into a train and test set

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=0)

X_train.shape, X_test.shape
137/1:
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

from sklearn.model_selection import (
    KFold,
    RepeatedKFold,
    LeaveOneOut,
    LeavePOut,
    StratifiedKFold,
    GridSearchCV,
    train_test_split,
)
137/2:
# if you want more information about the dataset for this demo:

# scikit-learn dataset
# https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset

# dataset information: UCI Machine Learning Repository
# https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)
    
# in short, classification problem, trying to predict whether the tumor
# is malignant or benign

# load dataset
breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y=True)
X = pd.DataFrame(breast_cancer_X)
y = pd.Series(breast_cancer_y).map({0:1, 1:0})

X.head()
137/3:
# percentage of benign (0) and malign tumors (1)

y.value_counts() / len(y)
137/4:
# split dataset into a train and test set

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=0)

X_train.shape, X_test.shape
137/5:
# Logistic Regression
logit = LogisticRegression(
    penalty ='l2', C=1, solver='liblinear', random_state=4, max_iter=10000)

# hyperparameter space
param_grid = dict(
    penalty=['l1', 'l2'],
    C=[0.1, 1, 10],
)

# K-Fold Cross-Validation
kf = KFold(n_splits=5, shuffle=True, random_state=4)

# search
clf =  GridSearchCV(
    logit,
    param_grid,
    scoring='accuracy',
    cv=kf, # k-fold
    refit=True, # refits best model to entire dataset
)

search = clf.fit(X_train, y_train)

# best hyperparameters
search.best_params_
137/6: search.cv_results_
138/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

from sklearn.model_selection import (
    GroupKFold,
    LeaveOneGroupOut,
    cross_validate,
    GridSearchCV,
)
138/2:
# load dataset
breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y=True)
X = pd.DataFrame(breast_cancer_X)
y = pd.Series(breast_cancer_y).map({0:1, 1:0})

# add patients arbitrarily for the demo
patient_list = [p for p in range(10)]

np.random.seed(1)
X["patient"] = np.random.choice(patient_list, size=len(X))

X.head()
138/3:
# number of patients
X["patient"].nunique()
138/4:
# observations per patient

X["patient"].value_counts().plot.bar()
plt.show()
138/5:
# split dataset into a train and test set
# this time, we leave data from 1 patient out

# all patients except 7
X_train = X[X['patient']!=7]
y_train = y.iloc[X_train.index]

# patient 7
X_test = X[X['patient']==7]
y_test = y.iloc[X_test.index]

# the test set will not be used in the cross-validation!
138/6:
# Logistic Regression
logit = LogisticRegression(
    penalty ='l2', C=10, solver='liblinear', random_state=4, max_iter=10000)

# Group K-Fold Cross-Validation
gkf = GroupKFold(n_splits=5)

# estimate generalization error
clf =  cross_validate(
    logit,
    X_train.drop('patient', axis=1), # drop the patient column, this is not a predictor
    y_train,
    scoring='accuracy',
    return_train_score=True,
    cv=gkf.split(X_train.drop('patient', axis=1), y_train, groups=X_train['patient']),
)

clf['test_score']
138/7: y_train
138/8: y_train.value_counts()
138/9: X_train.drop('patient', axis=1)
139/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

from sklearn.model_selection import (
    KFold,
    GridSearchCV,
    train_test_split,
)
139/2:
# if you want more information about the dataset for this demo:

# scikit-learn dataset
# https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset

# dataset information: UCI Machine Learning Repository
# https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)
    
# in short, classification problem, trying to predict whether the tumor
# is malignant or benign

# load dataset
breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y=True)
X = pd.DataFrame(breast_cancer_X)
y = pd.Series(breast_cancer_y).map({0:1, 1:0})

X.head()
139/3:
# percentage of benign (0) and malign tumors (1)

y.value_counts() / len(y)
139/4:
# split dataset into a train and test set

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=0)

X_train.shape, X_test.shape
139/5:
X_train.reset_index(drop=True, inplace=True)
y_train.reset_index(drop=True, inplace=True)
139/6:
# if you want more information about the dataset for this demo:

# scikit-learn dataset
# https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset

# dataset information: UCI Machine Learning Repository
# https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)
    
# in short, classification problem, trying to predict whether the tumor
# is malignant or benign

# load dataset
breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y=True)
X = pd.DataFrame(breast_cancer_X)
y = pd.Series(breast_cancer_y).map({0:1, 1:0})

X.head()
139/7:
# percentage of benign (0) and malign tumors (1)

y.value_counts() / len(y)
139/8:
# split dataset into a train and test set

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=0)

X_train.shape, X_test.shape
139/9: X_train
139/10:
X_train.reset_index(drop=True, inplace=True)
y_train.reset_index(drop=True, inplace=True)
139/11: X_train
139/12:
X_train.reset_index(drop=True, inplace=True)
y_train.reset_index(drop=True, inplace=True)
139/13:
def nested_cross_val(model, grid):

    # configure the outer loop cross-validation procedure
    cv_outer = KFold(n_splits=5, shuffle=True, random_state=1)

    # configure the inner loop cross-validation procedure
    cv_inner = KFold(n_splits=5, shuffle=True, random_state=1)

    # enumerate splits
    outer_results = list()
    inner_results = list()

    for train_ix, test_ix in cv_outer.split(X_train):

        # split data
        xtrain, xtest = X_train.loc[train_ix, :], X_train.loc[test_ix, :]
        ytrain, ytest = y_train[train_ix], y_train[test_ix]

        # define search
        search = GridSearchCV(
            model, grid, scoring='accuracy', cv=cv_inner, refit=True)

        # execute search
        search.fit(xtrain, ytrain)

        # evaluate model on the hold out dataset
        yhat = search.predict(xtest)

        # evaluate the model
        accuracy = accuracy_score(ytest, yhat)

        # store the result
        outer_results.append(accuracy)
        
        inner_results.append(search.best_score_)

        # report progress
        print(' >> accuracy_outer=%.3f, accuracy_inner=%.3f, cfg=%s' %
              (accuracy, search.best_score_, search.best_params_))

    # summarize the estimated performance of the model
    print()
    print('accuracy_outer: %.3f +- %.3f' %
          (np.mean(outer_results), np.std(outer_results)))
    print('accuracy_inner: %.3f +- %.3f' %
          (np.mean(inner_results), np.std(inner_results)))

    return search.fit(X_train, y_train)
139/14:
# Logistic Regression
logit = LogisticRegression(
    penalty ='l2', C=1, solver='liblinear', random_state=4, max_iter=10000)

# hyperparameter space
logit_param = dict(
    penalty=['l1', 'l2'],
    C=[0.1, 1, 10],
)
139/15: logit_search = nested_cross_val(logit, logit_param)
139/16:
# let's get the predictions

X_train_preds = logit_search.predict(X_train)
X_test_preds = logit_search.predict(X_test)

# let's examine the accuracy
print('Train accuracy: ', accuracy_score(y_train, X_train_preds))
print('Test accuracy: ', accuracy_score(y_test, X_test_preds))
139/17:
rf_param = dict(
    n_estimators=[10, 50, 100, 200],
    min_samples_split=[0.1, 0.3, 0.5, 1.0],
    max_depth=[1,2,3,None],
    )

rf = RandomForestClassifier(
    n_estimators=100,
    min_samples_split=2,
    max_depth=3,
    random_state=0,
    n_jobs=-1,
    )
139/18: rf_search = nested_cross_val(rf, rf_param)
139/19:
# let's get the predictions

X_train_preds = rf_search.predict(X_train)
X_test_preds = rf_search.predict(X_test)

# let's examine the accuracy
print('Train accuracy: ', accuracy_score(y_train, X_train_preds))
print('Test accuracy: ', accuracy_score(y_test, X_test_preds))
139/20: rf_search
139/21: rf_search.cv_results_
140/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

from sklearn.model_selection import (
    KFold,
    cross_validate,
    train_test_split,
)
140/2:
# if you want more information about the dataset for this demo:

# scikit-learn dataset
# https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset

# dataset information: UCI Machine Learning Repository
# https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)
    
# in short, classification problem, trying to predict whether the tumor
# is malignant or benign

# load dataset
breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y=True)
X = pd.DataFrame(breast_cancer_X)
y = pd.Series(breast_cancer_y).map({0:1, 1:0})

X.head()
140/3:
# percentage of benign (0) and malign tumors (1)

y.value_counts() / len(y)
140/4:
# split dataset into a train and test set

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=0)

X_train.shape, X_test.shape
141/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
141/2:
from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix

from sklearn.model_selection import (
    KFold,
    cross_validate,
    train_test_split,
)
141/3: load_breast_cancer
141/4:
breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y=True)
X = pd.DataFrame(breast_cancer_X)
y = pd.Series(breast_cancer_y).map({0:1, 1:0})

X.head()
141/5: X[0]
141/6: X[[0]]
141/7: X[[:, 0]]
141/8: X[2, :]
141/9: X[1:2]
141/10: X[1:3]
141/11: X[[1:3]]
141/12: X[[1]]
141/13: X[[1:8]]
141/14: X.iloc[1, :]
141/15: X.iloc[[1, :]]
141/16: X.iloc[[1]]
141/17: X.iloc[[1:3]]
141/18: X[0]
141/19: X[[0]]
141/20: X[[6]]
141/21: X[1:2]
141/22: X.iloc[:, 4]
141/23: X.iloc[:, 4:6]
141/24: X.iloc[2:3, 4:6]
141/25: X.iloc[2:5, 4:6]
141/26: X.iloc[2:5, 4:6]
141/27: X.loc[2:5, 4:6]
141/28: y
141/29: y.value_counts().mean()
141/30: y.value_counts() / len(y)
141/31: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=0)
141/32: X_train.shape
141/33: X_test.shape
141/34: X_test.shape()
141/35: X_test.shape
141/36: # Manual Search
141/37:
# Logistic Regression
logit = LogisticRegression(
    penalty='l2', C=0.1, solver='liblinear', random_state=4, max_iter=10000)

# K-fold CV
kf = KFold(n_splits=5, shuffle=True, random_state=0)

#estimate generalization error
clf = cross_validate(
    logit,
    X_train,
    y_train,
    scoring='accuracy',
    return_train_score=True,
    cv=kf,
)
141/38: clf
141/39: clf['train_score']
141/40: clf['test_score']
141/41: print('mean train set accuracy: ', np.mean(clf['train_score']), '+- ', np.std(clf['train_score']))
141/42: print('mean train set accuracy: ', f'{np.mean(clf['train_score']):9.4f}, '+- ', np.std(clf['train_score']))
141/43: print('mean train set accuracy: ', f'{np.mean(clf['train_score']):9.4f}', '+- ', np.std(clf['train_score']))
141/44: print('mean train set accuracy: ', '% 6.2f' % np.mean(clf['train_score']), '+- ', np.std(clf['train_score']))
141/45: print('mean train set accuracy: ', '% .2f' % np.mean(clf['train_score']), '+- ', np.std(clf['train_score']))
141/46: print('mean train set accuracy: ', '% .5f' % np.mean(clf['train_score']), '+- ', np.std(clf['train_score']))
141/47: print('mean train set accuracy: ', '% .5f' % np.mean(clf['train_score']), '+- ', '% .5f' % np.std(clf['train_score']))
141/48: print('mean train set accuracy:', '% .5f' % np.mean(clf['train_score']), '+- ', '% .5f' % np.std(clf['train_score']))
141/49: print('mean train set accuracy:', '%.5f' % np.mean(clf['train_score']), '+- ', '% .5f' % np.std(clf['train_score']))
141/50: print('mean train set accuracy:', '%.5f' % np.mean(clf['train_score']), '+-', '%.5f' % np.std(clf['train_score']))
141/51:
print('mean train set accuracy:', '%.5f' % np.mean(clf['train_score']), '+-', '%.5f' % np.std(clf['train_score']))
print('mean test set accuracy:', '%.5f' % np.mean(clf['test_score']), '+-', '%.5f' % np.std(clf['test_score']))
141/52:
# let's get the predictions
logit
141/53: clf
141/54:
# let's get the predictions
logit.fit(X_train, y_train)

train_preds = logit.predict(X_train)
test_preds = logit.predict(X_test)
141/55: logit
141/56:
print('Train accuracy:', '%.5f' % accuracy_score(y_train, train_preds)
print('Test accuracy:', '%.5f' % accuracy_score(y_test, test_preds)
141/57:
print('Train accuracy:', '%.5f' % accuracy_score(y_train, train_preds))
print('Test accuracy:', '%.5f' % accuracy_score(y_test, test_preds))
141/58: ### RANDOM FOREST
141/59:
rf = RandomForestClassifier(
    n_estimators=500,
    max_depth=10,
    random_state=0,
    n_jobs=4,
    )

clf = cross_validate(
    rf,
    X_train,
    y_train,
    scoring='accuracy',
    return_train_score=True,
    cv=kf
)
141/60:
rf = RandomForestClassifier(
    n_estimators=500,
    max_depth=10,
    random_state=0,
    n_jobs=4,
    )

clf = cross_validate(
    rf,
    X_train,
    y_train,
    scoring='accuracy',
    return_train_score=True,
    cv=kf
)
141/61:
print('mean train set accuracy:', '%.5f' % np.mean(clf['train_score']), '+-', '%.5f' % np.std(clf['train_score']))
print('mean test set accuracy:', '%.5f' % np.mean(clf['test_score']), '+-', '%.5f' % np.std(clf['test_score']))
141/62: X_train.shape
141/63:
rf = RandomForestClassifier(
    n_estimators=500,
    max_depth=4,
    random_state=0,
    n_jobs=4,
    )

clf = cross_validate(
    rf,
    X_train,
    y_train,
    scoring='accuracy',
    return_train_score=True,
    cv=kf
)
141/64:
print('mean train set accuracy:', '%.5f' % np.mean(clf['train_score']), '+-', '%.5f' % np.std(clf['train_score']))
print('mean test set accuracy:', '%.5f' % np.mean(clf['test_score']), '+-', '%.5f' % np.std(clf['test_score']))
141/65:
rf = RandomForestClassifier(
    n_estimators=1500,
    max_depth=4,
    random_state=0,
    n_jobs=4,
    )

clf = cross_validate(
    rf,
    X_train,
    y_train,
    scoring='accuracy',
    return_train_score=True,
    cv=kf
)
141/66: print('mean test set accuracy:', '%.5f' % np.mean(clf['test_score']), '+-', '%.5f' % np.std(clf['test_score']))
141/67:
rf = RandomForestClassifier(
    n_estimators=500,
    max_depth=4,
    random_state=0,
    n_jobs=4,
    )

clf = cross_validate(
    rf,
    X_train,
    y_train,
    scoring='accuracy',
    return_train_score=True,
    cv=kf
)
141/68:
print('mean train set accuracy:', '%.5f' % np.mean(clf['train_score']), '+-', '%.5f' % np.std(clf['train_score']))
print('mean test set accuracy:', '%.5f' % np.mean(clf['test_score']), '+-', '%.5f' % np.std(clf['test_score']))
141/69:
rf.fit(X_train, y_train)

y_train_pred = rf.predict(X_train)
y_test_pred = rf.predict(X_test)
141/70:
print('Train accuracy:', '%.5f' % accuracy_score(y_train, y_train_pred))
print('Test accuracy:', '%.5f' % accuracy_score(y_test, y_test_pred))
141/71:
rf = RandomForestClassifier(
    n_estimators=1000,
    max_depth=4,
    random_state=0,
    n_jobs=4,
    )

clf = cross_validate(
    rf,
    X_train,
    y_train,
    scoring='accuracy',
    return_train_score=True,
    cv=kf
)
141/72:
print('mean train set accuracy:', '%.5f' % np.mean(clf['train_score']), '+-', '%.5f' % np.std(clf['train_score']))
print('mean test set accuracy:', '%.5f' % np.mean(clf['test_score']), '+-', '%.5f' % np.std(clf['test_score']))
141/73:
rf.fit(X_train, y_train)

y_train_pred = rf.predict(X_train)
y_test_pred = rf.predict(X_test)
141/74:
print('Train accuracy:', '%.5f' % accuracy_score(y_train, y_train_pred))
print('Test accuracy:', '%.5f' % accuracy_score(y_test, y_test_pred))
141/75:
rf = RandomForestClassifier(
    n_estimators=10,
    max_depth=4,
    random_state=0,
    n_jobs=4,
    )

clf = cross_validate(
    rf,
    X_train,
    y_train,
    scoring='accuracy',
    return_train_score=True,
    cv=kf
)
141/76:
print('mean train set accuracy:', '%.5f' % np.mean(clf['train_score']), '+-', '%.5f' % np.std(clf['train_score']))
print('mean test set accuracy:', '%.5f' % np.mean(clf['test_score']), '+-', '%.5f' % np.std(clf['test_score']))
141/77:
rf.fit(X_train, y_train)

y_train_pred = rf.predict(X_train)
y_test_pred = rf.predict(X_test)
141/78:
print('Train accuracy:', '%.5f' % accuracy_score(y_train, y_train_pred))
print('Test accuracy:', '%.5f' % accuracy_score(y_test, y_test_pred))
143/1:
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import roc_auc_score

from sklearn.model_selection import (
    GridSearchCV,
    train_test_split,
)
143/2:
breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y=True)
X = pd.DataFrame(breast_cancer_X)
y = pd.Series(breast_cancer_y).map({0:1, 1:0})

X.head()
143/3:
# split dataset into a train and test set

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=0)

X_train.shape, X_test.shape
143/4: # GRID SEARCH
143/5:
gbm = GradientBoostingClassifier(random_state=0)

param_grid = dict(
    n_estimators=[10, 20, 50, 100],
    min_samples_split=[.1, .3, .5],
    max_depth=[1, 2, 3, 4, None]
    )
print('Number of hyperparam combinations: ', 
      len(param_grid['n_estimators'])
      *len(param_grid['min_samples_split'])
      *len(param_grid['max_depth']))
143/6:
gbm = GradientBoostingClassifier(random_state=0)

param_grid = dict(
    n_estimators=[10, 20, 50, 100],
    min_samples_split=[.1, .3, .5],
    max_depth=[1, 2, 3, 4, None]
    )
print('Number of hyperparam combinations: ', '%.2f'%
      len(param_grid['n_estimators'])
      *len(param_grid['min_samples_split'])
      *len(param_grid['max_depth']))
143/7:
gbm = GradientBoostingClassifier(random_state=0)

param_grid = dict(
    n_estimators=[10, 20, 50, 100],
    min_samples_split=[.1, .3, .5],
    max_depth=[1, 2, 3, 4, None]
    )
print('Number of hyperparam combinations: ', '%.2f'%
      (len(param_grid['n_estimators'])
      *len(param_grid['min_samples_split'])
      *len(param_grid['max_depth'])))
143/8:
# set up the search
search = GridSearchCV(gbm, param_grid, 
                      scoring='roc_auc',
                      cv=5,
                      refit=True)
143/9: search.fit(X_train, y_train)
143/10: search.best_estimator_
143/11: search.cv_results_
143/12:
df_results = pd.DataFrame(search.cv_results_)
df_results.head(4)
143/13: df_results.map('{:.2f}'.format
143/14: df_results.map('{:.2f}'.format)
143/15: df_results.map('{.2f}'.format)
143/16: df_results.map('{:,d}'.format)
143/17: df_results[:].map('{:,d}'.format)
143/18: df_results.style.format("{.2f")
143/19: df_results.style.format("{.2f}")
143/20: df_results.style.format("{.2d}")
143/21: df_results.style.format('%.5f')
143/22: df_results.style.format("{.2f}")
143/23: df_results.style.format("{:.2f}")
143/24: df_results.style.format("{:,.2f}")
143/25: df_results
143/26: df_results.dtypes
143/27: df_results[df_results.dtypes == 'float64']
143/28: df_results.select_dtypes(include=['float64'])
143/29: df_results.select_dtypes(include=['float64']).columns
143/30: float_col = df_results.select_dtypes(include=['float64']).columns
143/31: pd.options.display.float_format = lambda x : '{:.0f}'.format(x) if round(x,0) == x else '{:,.2f}'.format(x)
143/32: df_results
143/33: pd.options.display.float_format = lambda x : '{:.0f}'.format(x) if round(x,0) == x else '{:,.4f}'.format(x)
143/34: df_results
143/35: df_results.mean_fit_time
143/36: df_results['mean_fit_time'].map('{:.1f}'.format)
143/37: df_results
143/38:
for col in float_col:
    df_results[float_col] = df_results[float_col].map('{:.1f}'.format)
143/39:
for col in float_col:
    df_results[col] = df_results[col].map('{:.1f}'.format)
143/40: df_results
143/41:
df_results = pd.DataFrame(search.cv_results_)
df_results.head(4)
143/42:
df_results = pd.DataFrame(search.cv_results_)
df_results.head(4)
143/43:
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import roc_auc_score

from sklearn.model_selection import (
    GridSearchCV,
    train_test_split,
)
143/44:
breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y=True)
X = pd.DataFrame(breast_cancer_X)
y = pd.Series(breast_cancer_y).map({0:1, 1:0})

X.head()
143/45: df_results
144/1:
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import roc_auc_score

from sklearn.model_selection import (
    GridSearchCV,
    train_test_split,
)
144/2:
breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y=True)
X = pd.DataFrame(breast_cancer_X)
y = pd.Series(breast_cancer_y).map({0:1, 1:0})

X.head()
144/3:
# split dataset into a train and test set

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=0)

X_train.shape, X_test.shape
144/4: # GRID SEARCH
144/5:
gbm = GradientBoostingClassifier(random_state=0)

param_grid = dict(
    n_estimators=[10, 20, 50, 100],
    min_samples_split=[.1, .3, .5],
    max_depth=[1, 2, 3, 4, None]
    )
print('Number of hyperparam combinations: ', '%.2f'%
      (len(param_grid['n_estimators'])
      *len(param_grid['min_samples_split'])
      *len(param_grid['max_depth']))
     )
144/6:
# set up the search
search = GridSearchCV(gbm, param_grid, 
                      scoring='roc_auc',
                      cv=5,
                      refit=True)
144/7: search.fit(X_train, y_train)
144/8: search.best_estimator_
144/9:
df_results = pd.DataFrame(search.cv_results_)
df_results.head(4)
144/10: df_results
144/11:
float_col = df_results.select_dtypes(include=['float64']).columns
for col in float_col:
    df_results[col] = df_results[col].map('{:.4f}'.format)
144/12: df_results
144/13:
df_results = pd.DataFrame(search.cv_results_)
df_results.head(4)
144/14: df_results
144/15:
float_col = df_results.select_dtypes(include=['float64']).columns
for col in float_col:
    df_results[col] = df_results[col].map('{:.4f}'.format)
144/16: df_results
144/17: df_results
144/18: df_results.shape
144/19:  df_results
144/20:  df_results.sort_values(by=['mean_test_score', 'std_test_score'], ascending=False, inplace=True)
144/21:  df_results
144/22:
plt.errorbar(df_results.index, df_results.mean_test_score, yerr=df_results.std_test_score,
             , )
144/23:
plt.errorbar(df_results.index, df_results.mean_test_score, yerr=df_results.std_test_score,
              )
144/24:
plt.errorbar(df_results.index, 
             df_results.mean_test_score, 
             yerr=df_results.std_test_score,
              )
144/25:
plt.errorbar(df_results.index, 
             df_results.mean_test_score, 
             yerr=[df_results.std_test_score, df_results.std_test_score],
              )
144/26: df_results.std_test_score
144/27:
df_results = pd.DataFrame(search.cv_results_)
df_results.head(4)
144/28:
float_col = df_results.select_dtypes(include=['float64']).columns
for col in float_col:
    df_results[col] = df_results[col].map('{:.4f}'.format)
144/29: df_results.shape
144/30: df_results.std_test_score
144/31:
float_col = df_results.select_dtypes(include=['float64']).columns
for col in float_col:
    df_results[col] = df_results[col].map('{:.4f}'.format)
144/32: df_results
144/33: df_results.dtypes
144/34:
df_results = pd.DataFrame(search.cv_results_)
df_results.head(4)
144/35: df_results.dtypes
144/36:
float_col = df_results.select_dtypes(include=['float64']).columns
for col in float_col:
    df_results[col] = df_results[col].map('{:.4f}'.format)
144/37: df_results.dtypes
144/38:
float_col = df_results.select_dtypes(include=['float64']).columns
for col in float_col:
    df_results[col] = df_results[col].map('{:.4d}'.format)
144/39:
df_results = pd.DataFrame(search.cv_results_)
df_results.head(4)
144/40: df_results.dtypes
144/41: #pd.options.display.float_format = lambda x : '{:.0f}'.format(x) if round(x,0) == x else '{:,.4f}'.format(x)
144/42:
float_col = df_results.select_dtypes(include=['float64']).columns
for col in float_col:
    df_results[col] = df_results[col].map('{:.4d}'.format)
144/43:
float_col = df_results.select_dtypes(include=['float64']).columns
for col in float_col:
    df_results[col] = df_results[col].map('{0:,.2f}'.format)
144/44: df_results.dtypes
144/45:
df_results = pd.DataFrame(search.cv_results_)
df_results.head(4)
144/46: df_results.dtypes
144/47: pd.options.display.float_format = lambda x : '{:.0f}'.format(x) if round(x,0) == x else '{:,.4f}'.format(x)
144/48: df_results.dtypes
144/49:
#float_col = df_results.select_dtypes(include=['float64']).columns
#for col in float_col:
#    df_results[col] = df_results[col].map('{0:,.2f}'.format)
144/50: df_results.shape
144/51:  df_results.sort_values(by=['mean_test_score', 'std_test_score'], ascending=False, inplace=True)
144/52:  df_results
144/53: df_results.std_test_score
144/54:
plt.errorbar(df_results.index, 
             df_results.mean_test_score, 
             yerr=[df_results.std_test_score, df_results.std_test_score],
              )
144/55:
plt.errorbar(range(0, 17), 
             df_results.mean_test_score, 
             yerr=[df_results.std_test_score, df_results.std_test_score],
              )
144/56:
plt.errorbar(range(0, 15), 
             df_results.mean_test_score, 
             yerr=[df_results.std_test_score, df_results.std_test_score],
              )
144/57: range(0, 15)
144/58: list(range(11, 17))
144/59: list(range(0, 17))
144/60: list(range(0, 60))
144/61:
plt.errorbar(list(range(0, 60)), 
             df_results.mean_test_score, 
             yerr=[df_results.std_test_score, df_results.std_test_score],
              )
144/62:
plt.errorbar(list(range(0, 60)), 
             df_results.mean_test_score, 
             yerr=[df_results.std_test_score, df_results.std_test_score],
             c='p')
144/63:
plt.errorbar(list(range(0, 60)), 
             df_results.mean_test_score, 
             yerr=[df_results.std_test_score, df_results.std_test_score],
             c='pink')
144/64:
plt.errorbar(list(range(0, 60)), 
             df_results.mean_test_score, 
             yerr=[df_results.std_test_score, df_results.std_test_score],
             c='pink', ecolor='r')
144/65:
plt.errorbar(list(range(0, 60)), 
             df_results.mean_test_score, 
             yerr=[df_results.std_test_score, df_results.std_test_score],
             c='r', ecolor='pink')
144/66:
plt.errorbar(list(range(0, 60)), 
             df_results.mean_test_score, 
             yerr=[df_results.std_test_score, df_results.std_test_score],
             c='r', ecolor='pink', fmt='0')
144/67:
plt.errorbar(list(range(0, 60)), 
             df_results.mean_test_score, 
             yerr=[df_results.std_test_score, df_results.std_test_score],
             c='r', ecolor='pink', fmt='o')
144/68:
plt.errorbar(list(range(0, 60)), 
             df_results.mean_test_score, 
             yerr=[df_results.std_test_score, df_results.std_test_score],
             c='r', ecolor='pink', fmt='>')
144/69:
plt.errorbar(list(range(0, 30)), 
             df_results.mean_test_score, 
             yerr=[df_results.std_test_score, df_results.std_test_score],
             c='r', ecolor='pink', fmt='>')
144/70:
plt.errorbar(list(range(0, 60)), 
             df_results.mean_test_score, 
             yerr=[df_results.std_test_score, df_results.std_test_score],
             c='r', ecolor='pink', fmt='>')
144/71:
plt.errorbar(list(range(0, 60)), 
             df_results.mean_test_score, 
             yerr=[df_results.std_test_score, df_results.std_test_score],
             c='r', ecolor='pink', fmt='->')
144/72:
plt.errorbar(list(range(0, 60)), 
             df_results.mean_test_score, 
             yerr=[df_results.std_test_score, df_results.std_test_score],
             c='r', ecolor='pink', fmt='-')
144/73:
plt.errorbar(list(range(0, 60)), 
             df_results.mean_test_score, 
             yerr=[df_results.std_test_score, df_results.std_test_score],
             mfc='red',
         mec='green', ms=20, mew=4)
144/74:
plt.errorbar(list(range(0, 60)), 
             df_results.mean_test_score, 
             yerr=[df_results.std_test_score, df_results.std_test_score],
             c='r', ecolor='pink', linestyle='-')
144/75:
plt.errorbar(list(range(0, 60)), 
             df_results.mean_test_score, 
             yerr=[df_results.std_test_score, df_results.std_test_score],
             c='r', ecolor='pink', linestyle=['-', '-'])
144/76:
plt.errorbar(list(range(0, 60)), 
             df_results.mean_test_score, 
             yerr=[df_results.std_test_score, df_results.std_test_score],
             c='r', ecolor='pink', fmt='-')
144/77:
plt.errorbar(list(range(0, 60)), 
             df_results.mean_test_score, 
             yerr=[df_results.std_test_score, df_results.std_test_score],
             c='r', ecolor='pink', fmt='-0')
144/78:
plt.errorbar(list(range(0, 60)), 
             df_results.mean_test_score, 
             yerr=[df_results.std_test_score, df_results.std_test_score],
             c='r', ecolor='pink', fmt='-o')
144/79:
plt.errorbar(list(range(0, 60)), 
             df_results.mean_test_score, 
             yerr=[df_results.std_test_score, df_results.std_test_score],
             c='r', ecolor='pink', fmt='-')
144/80:
import numpy as np
import matplotlib.pyplot as plt


fig = plt.figure()
x = np.arange(10)
y = 2.5 * np.sin(x / 20 * np.pi)
yerr = np.linspace(0.05, 0.2, 10)

plt.errorbar(x, y + 3, yerr=yerr, label='both limits (default)')

plt.errorbar(x, y + 2, yerr=yerr, uplims=True, label='uplims=True')

plt.errorbar(x, y + 1, yerr=yerr, uplims=True, lolims=True,
             label='uplims=True, lolims=True')

upperlimits = [True, False] * 5
lowerlimits = [False, True] * 5
plt.errorbar(x, y, yerr=yerr, uplims=upperlimits, lolims=lowerlimits,
             label='subsets of uplims and lolims')

plt.legend(loc='lower right')
144/81:
import numpy as np
import matplotlib.pyplot as plt


fig = plt.figure()
x = np.arange(10)
y = 2.5 * np.sin(x / 20 * np.pi)
yerr = np.linspace(0.05, 0.2, 10)

plt.errorbar(x, y + 3, yerr=yerr, label='both limits (default)')

plt.errorbar(x, y + 2, yerr=yerr, label='uplims=True')

plt.errorbar(x, y + 1, yerr=yerr, uplims=True, lolims=True,
             label='uplims=True, lolims=True')

upperlimits = [True, False] * 5
lowerlimits = [False, True] * 5
plt.errorbar(x, y, yerr=yerr, uplims=upperlimits, lolims=lowerlimits,
             label='subsets of uplims and lolims')

plt.legend(loc='lower right')
144/82:
plt.errorbar(list(range(0, 60)), 
             df_results.mean_test_score, 
             yerr=[df_results.std_test_score, df_results.std_test_score],
             c='r', ecolor='pink', fmt='-', uplims=True)
144/83:
plt.errorbar(list(range(0, 60)), 
             df_results.mean_test_score, 
             yerr=[df_results.std_test_score, df_results.std_test_score],
             c='r', ecolor='pink', fmt='-', )
144/84:
import numpy as np
import matplotlib.pyplot as plt


fig = plt.figure()
x = np.arange(10)
y = 2.5 * np.sin(x / 20 * np.pi)
yerr = np.linspace(0.05, 0.2, 10)

plt.errorbar(x, y + 3, yerr=yerr, label='both limits (default)')

plt.errorbar(x, y + 2, yerr=yerr, uplims=True, label='uplims=True')

plt.errorbar(x, y + 1, yerr=yerr, uplims=True, lolims=True,
             label='uplims=True, lolims=True')

upperlimits = [True, False] * 5
lowerlimits = [False, True] * 5
plt.errorbar(x, y, yerr=yerr, uplims=upperlimits, lolims=lowerlimits,
             label='subsets of uplims and lolims')

plt.legend(loc='lower right')
144/85:
plt.errorbar(list(range(0, 60)), 
             df_results.mean_test_score, 
             yerr=[df_results.std_test_score, df_results.std_test_score],
             c='r', ecolor='pink', fmt='-', 
             #uplims=True, 
             uplims=upperlimits, lolims=lowerlimits,
            )
144/86:
plt.errorbar(list(range(0, 60)), 
             df_results.mean_test_score, 
             yerr=[df_results.std_test_score, df_results.std_test_score],
             c='r', ecolor='pink', fmt='-', 
             #uplims=True, 
             uplims=True, lolims=True,
            )
144/87:
y_pred_train = search.predict_proba(X_train)[:, 1]
y_pred_test = search.predict_proba(X_test)[:, 1]
144/88:
print('Train roc_auc: ', roc_auc_score(y_train, y_pred_train))
print('Test roc_auc: ', roc_auc_score(y_test, y_pred_test))
144/89: df_results
144/90: df_results['param_max_depth']
144/91: df_results.groupby('param_max_depth')
144/92:
df_results.groupby('param_max_depth').agg({'mean_test_score': 'mean',
                                           'mean_test_score': 'std'})
144/93: df_results.groupby('param_max_depth').agg({'mean_test_score': 'mean', 'mean_test_score': 'max'})
144/94: df_results.groupby('param_max_depth').agg({'mean_test_score': 'mean', 'param_n_estimators': 'max'})
144/95: df_results.groupby('param_max_depth').agg({'mean_test_score': ['mean', 'std'], 'param_n_estimators': 'max'})
144/96: df_results.groupby('param_max_depth').agg({'mean_test_score': ['mean', 'std']})
144/97: df_results.groupby('param_n_estimators').agg({'mean_test_score': ['mean', 'std']})
144/98: df_results.groupby('param_n_estimators').agg({'mean_test_score': ['mean', 'std']}).unstack()
144/99: df_results.groupby('param_n_estimators').agg({'mean_test_score': ['mean', 'std']}).unstack(level_q)
144/100: df_results.groupby('param_n_estimators').agg({'mean_test_score': ['mean', 'std']}).unstack(level_1)
144/101: df_results.groupby('param_n_estimators').agg({'mean_test_score': ['mean', 'std']}).unstack(level=1)
144/102: df_results.groupby('param_n_estimators').agg({'mean_test_score': ['mean', 'std']})
144/103: df_results.groupby('param_n_estimators').agg({'mean_test_score': ['mean', 'std']}).unstack(level=0)
144/104: df_results.groupby('param_n_estimators').agg({'mean_test_score': ['mean', 'std']}).reset_index()
144/105: df_results.groupby('param_n_estimators').agg({'mean_test_score': ['mean', 'std']}).drop('level_0', axis=1)
144/106: df_results.groupby('param_n_estimators').agg({'mean_test_score': ['mean', 'std']}).droplevel(0)
144/107: df_results.groupby('param_n_estimators').agg({'mean_test_score': ['mean', 'std']}).droplevel1)
144/108: df_results.groupby('param_n_estimators').agg({'mean_test_score': ['mean', 'std']})
144/109: df_results.groupby('param_n_estimators').agg({'mean_test_score': ['mean', 'std']}).droplevel(0)
144/110: df_results.groupby('param_n_estimators').agg({'mean_test_score': ['mean', 'std']})
144/111: df_results.groupby('param_n_estimators').agg({'mean_test_score': ['mean', 'std']}).columns
144/112: df_results.groupby('param_n_estimators').agg({'mean_test_score': ['mean', 'std']})
144/113: df_results.groupby('param_n_estimators').agg({'mean_test_score': ['mean', 'std']}).droplevel(1)
144/114: df_results.groupby('param_n_estimators').agg({'mean_test_score': ['mean', 'std']})
144/115:
df_results.groupby('param_n_estimators').agg(
             mean_test_score_mean=('mean_test_score', 'mean'),
             mean_test_score_std=('mean_test_score', 'std'))
144/116:
def one_param_analysis(hyper_param):
    return df_results.groupby(hyper_param).agg(
            mean_test_score_mean=('mean_test_score', 'mean'),
            mean_test_score_std=('mean_test_score', 'std')
            )
144/117:
tmp = summarize_by_param('param_n_estimators')

tmp.head()
144/118:
tmp = one_param_analysis('param_n_estimators')

tmp.head()
144/119:
tmp = one_param_analysis('param_n_estimators')
tmp.head()
144/120:
tmp = one_param_analysis('param_n_estimators')
tmp
144/121:
plt.errorbar(tmp.index, tmp.mean_test_score_mean,
             yerr=[tmp.mean_test_score_std, tmp.mean_test_score_std],
             )
144/122:
plt.errorbar(tmp.index, tmp.mean_test_score_mean,
             yerr=[tmp.mean_test_score_std, tmp.mean_test_score_std],
             fmt='--o')
144/123:
plt.errorbar(tmp.index, tmp.mean_test_score_mean,
             yerr=[tmp.mean_test_score_std, tmp.mean_test_score_std],
             fmt='--o')
plt.ylim(.96, 1)
145/1:
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import load_breast_cancer
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

from sklearn.model_selection import (
    GridSearchCV,
    train_test_split,
)
145/2:
breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y=True)
X = pd.DataFrame(breast_cancer_X)
y = pd.Series(breast_cancer_y).map({0:1, 1:0})

X.head()
145/3:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=0)
X_train.shape, X_test.shape
145/4: ### SVC
145/5:
svm = SVC(random_state=0)
param_grid = [
    {'C': [.1, 10, 100, 1000], 'kernel': ['linear']},
    {'C': [.1, 10, 100, 1000], 'gamma': [.001, .0001], 'kernel': ['rbf']}        
    }]
145/6:
svm = SVC(random_state=0)
param_grid = [
    {'C': [.1, 10, 100, 1000], 'kernel': ['linear']},
    {'C': [.1, 10, 100, 1000], 'gamma': [.001, .0001], 'kernel': ['rbf']}        
    ]
145/7:
search = GridSearchCV(svm,
                      param_grid,
                      scoring='accuracy'
                      cv=5,
                      refit=True
                     )
145/8:
search = GridSearchCV(svm,
                      param_grid,
                      scoring='accuracy',
                      cv=5,
                      refit=True
                     )
145/9: search
145/10: search.fit(X_train, y_train)
145/11: search.best_params_
145/12: search.cv_results_
145/13: pd.DataFrame(search.cv_results_)
145/14: df_search = pd.DataFrame(search.cv_results_)
145/15: df_search
145/16: df_search.columns
145/17: df_search['param_C', 'param_kernel', 'param_gamma', 'mean_test_score', 'std_test_score', 'rank_test_score']
145/18: df_search[['param_C', 'param_kernel', 'param_gamma', 'mean_test_score', 'std_test_score', 'rank_test_score']]
145/19: df_search = df_search[['param_C', 'param_kernel', 'param_gamma', 'mean_test_score', 'std_test_score', 'rank_test_score']]
145/20: df_search
145/21: df_search.sort_values('rank_test_score')
145/22:
plt.errorbar(df_search['rank_test_score'],
             df_search['mean_test_score'],
             yerr=[df_search'std_test_score', df_search'std_test_score'])
145/23:
plt.errorbar(df_search['rank_test_score'],
             df_search['mean_test_score'],
             yerr=[df_search'std_test_score'], df_search'std_test_score'])
145/24:
plt.errorbar(df_search['rank_test_score'],
             df_search['mean_test_score'],
             yerr=[df_search'std_test_score'], df_search['std_test_score'])
145/25:
plt.errorbar(df_search['rank_test_score'],
             df_search['mean_test_score'],
             yerr=[df_search['std_test_score'], df_search['std_test_score'])
145/26:
plt.errorbar(df_search['rank_test_score'],
             df_search['mean_test_score'],
             yerr=[df_search['std_test_score'], df_search['std_test_score']])
145/27:
plt.errorbar(df_search['rank_test_score']-1,
             df_search['mean_test_score'],
             yerr=[df_search['std_test_score'], df_search['std_test_score']])
145/28:
plt.errorbar(df_search['rank_test_score']-1,
             df_search['mean_test_score'],
            # yerr=[df_search['std_test_score'], df_search['std_test_score']]
             )
145/29: df_search.sort_values('rank_test_score').reset_indext_index(inplace=True)
145/30: df_search.sort_values('rank_test_score').reset_indext_index()
145/31: df_search.sort_values('rank_test_score').reset_indext(inplace=True)
145/32: df_search.sort_values('rank_test_score').reset_indext()
145/33: df_search.sort_values('rank_test_score')
145/34: df_search = df_search.sort_values('rank_test_score')
145/35: df_search.reset_index()
145/36: df_search.reset_index(inplace=True)
145/37:
plt.errorbar(df_search.index,
             df_search['mean_test_score'],
            # yerr=[df_search['std_test_score'], df_search['std_test_score']]
             )
145/38:
plt.errorbar(df_search.index,
             df_search['mean_test_score'],
             yerr=[df_search['std_test_score'], df_search['std_test_score']]
             )
145/39: df_search
145/40:
y_train_pred = search.predict(X_train)
y_test_pred = search.predict(X_test)

print('Train accuracy: ', '%.4f'%accuracy_score(y_train, y_train_pred))
145/41:
y_train_pred = search.predict(X_train)
y_test_pred = search.predict(X_test)

print('Train accuracy: ', '%.2f'%accuracy_score(y_train, y_train_pred))
145/42:
y_train_pred = search.predict(X_train)
y_test_pred = search.predict(X_test)

print('Train accuracy: ', '%.5f'%accuracy_score(y_train, y_train_pred))
print('Test accuracy: ', '%.5f'%accuracy_score(y_test, y_test_pred))
145/43:
search = GridSearchCV(svm,
                      param_grid,
                      scoring='accuracy',
                      cv=3,
                      refit=True
                     )
145/44: search.fit(X_train, y_train)
145/45: search.best_params_
145/46: df_search = pd.DataFrame(search.cv_results_)
145/47: df_search.columns
145/48: df_search = df_search[['param_C', 'param_kernel', 'param_gamma', 'mean_test_score', 'std_test_score', 'rank_test_score']]
145/49: df_search = df_search.sort_values('rank_test_score')
145/50: df_search.reset_index(inplace=True)
145/51:
plt.errorbar(df_search.index,
             df_search['mean_test_score'],
             yerr=[df_search['std_test_score'], df_search['std_test_score']]
             )
145/52: df_search
147/1:
import numpy as np
import pandas as pd

from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import roc_auc_score

from sklearn.model_selection import (
    RandomizedSearchCV,
    train_test_split,
)
147/2:
breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y=True)
X = pd.DataFrame(breast_cancer_X)
y = pd.Series(breast_cancer_y).map({0:1, 1:0})

X.head()
147/3:
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=0)

X_train.shape, X_test.shape
147/4:
#####
stats
147/5: from scipy import stats
147/6:
#####
stats
147/7:
#####
stats.randint.rvs(2,5)
147/8:
#####
stats.randint.rvs(2,5)
147/9: ##RANDOM SEARCH
147/10:
gbm = GradientBoostingClassifier(random_state=0)

param_grid = dict(
    n_estimators=stats.randint(10, 120),
    min_samples_split=stats.uniform(0, 1),
    max_depth=stats.randint(1, 5),
    loss=('deviance', 'exponential')
    )
147/11:
search = RandomizedSearchCV(gbm,
                            param_grid,
                            scoring='roc_auc',
                            cv=5,
                            n_iter=60,
                            random_state=0,
                            refit=True)
147/12: search.fit(X_train, y_train)
147/13: search.best_params_
147/14:
# we also find the data for all models evaluated

results = pd.DataFrame(search.cv_results_)

print(results.shape)

results.head()
151/1:
import numpy as np
import pandas as pd

from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import cross_val_score, train_test_split

from skopt import dummy_minimize # for the randomized search
from skopt.plots import plot_convergence
from skopt.space import Real, Integer, Categorical
from skopt.utils import use_named_args
151/2: !conda install skopt
151/3: !pip install skopt
151/4:
import numpy as np
import pandas as pd

from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import cross_val_score, train_test_split

from skopt import dummy_minimize # for the randomized search
from skopt.plots import plot_convergence
from skopt.space import Real, Integer, Categorical
from skopt.utils import use_named_args
151/5: !conda install -c bjodah skopt
151/6: !conda config --append channels conda-forge
151/7: !conda install slycot control
153/1: !conda install -c bjodah skopt
153/2: !conda install hyperopt
154/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import load_breast_cancer
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score, train_test_split

import xgboost as xgb

from hyperopt import hp, rand, fmin, Trials
154/2:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import load_breast_cancer
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score, train_test_split

import xgboost as xgb

from hyperopt import hp, rand, fmin, Trials
154/3:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import load_breast_cancer
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score, train_test_split

import xgboost as xgb

from hyperopt import hp, rand, fmin, Trials
156/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import load_breast_cancer
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score, train_test_split

import xgboost as xgb

from hyperopt import hp, rand, fmin, Trials
156/2:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import load_breast_cancer
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score, train_test_split

import xgboost as xgb

from hyperopt import hp, rand, fmin, Trials
157/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import load_breast_cancer
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score, train_test_split

import xgboost as xgb

from hyperopt import hp, rand, fmin, Trials
158/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import load_breast_cancer
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score, train_test_split

import xgboost as xgb

from hyperopt import hp, rand, fmin, Trials
158/2:
# load dataset

breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y=True)
X = pd.DataFrame(breast_cancer_X)
y = pd.Series(breast_cancer_y).map({0:1, 1:0})

X.head()
158/3:
# the target:
# percentage of benign (0) and malign tumors (1)

y.value_counts() / len(y)
158/4:
# split dataset into a train and test set

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=0)

X_train.shape, X_test.shape
162/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
162/2: df = pd.read_csv('Train_Titanic.csv')
162/3: df
162/4: df.describe()
162/5: df.isnull().mean()
162/6: df.head(4)
162/7: df.head(4)
162/8: df.tail(4)
162/9: print('Total', len(df))
162/10:
print('Total = ', len(df))
print('Survived = ', len(df.Survived == 1))
162/11:
print('Total = ', len(df))
print('Survived = ', len(df[df.Survived == 1]))
162/12:
print('Total = ', len(df))
print('Survived = ', len(df[df.Survived == 1]))
print('Survived = ', len(df.where(Survived == 1)))
162/13:
print('Total = ', len(df))
print('Survived = ', len(df[df.Survived == 1]))
print('Survived = ', len(df.where(df.Survived == 1)))
162/14:
print('Total = ', len(df))
print('Survived = ', len(df[df.Survived == 1]))
print('Survived = ', len(df.where(df.Survived = 1)))
162/15:
print('Total = ', len(df))
print('Survived = ', len(df[df.Survived == 1]))
print('Survived = ', len(df.where(df['Survived'] == 1)))
162/16:
print('Total = ', len(df))
print('Survived = ', len(df[df.Survived == 1]))
print('Survived = ', len(df.where(df['Survived'] != 1)))
162/17: condition_1 = df.Survived == 1
162/18: df.where(filter)
162/19: df.where(condition_1)
162/20: df.where(condition_1).Survived.sum()
162/21: sns.countplot(x='Pclass', data=df)
162/22: df.dtype
162/23: df.dtype()
162/24: df.dtypes()
162/25: df.dtypes
162/26: df[df.dtypes == 'object']
162/27: df.select_dtypes('object')
162/28: df.select_dtypes('object').columns
162/29: list(df.select_dtypes('object').columns)
162/30: obj_features = list(df.select_dtypes('object').columns)
162/31:
for obj in obj_features:
    sns.countplot(x=obj, data=df)
162/32:
for obj in obj_features:
    plt.figure()
    sns.countplot(x=obj, data=df)
162/33: df.dtypes
162/34: sns.countplot(x='Pclass', data=df, hue='Survived')
162/35:
plt.figure(figsize=[6, 12])
plt.subplot(211)
sns.countplot(x='Pclass', data=df)
plt.subplot(212)
sns.countplot(x='Pclass', data=df, hue='Survived')
162/36:
plt.figure(figsize=[6, 6])
plt.subplot(211)
sns.countplot(x='Pclass', data=df)
plt.subplot(212)
sns.countplot(x='Pclass', data=df, hue='Survived')
162/37:
plt.figure(figsize=[6, 6])
plt.subplot(211)
sns.countplot(x='SibSp', data=df)
plt.subplot(212)
sns.countplot(x='SibSp', data=df, hue='Survived')
162/38:
plt.figure(figsize=(40,30))
sns.countplot(x='Age', hue='Survived', data=df)
162/39: df['Age'].hist(30)
162/40: df['Age'].hist(bins = 30)
162/41:
plt.figure(figsize=(40,30))
sns.countplot(x='Fare', hue='Survived', data=df)
162/42: df.Fare.hist(bins=30)
162/43: df.Fare.hist(bins=60)
162/44:

sns.heatmap(df.isnull(), yticklabels=False, cbar=False)
162/45:

sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='b')
162/46:

sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='blue')
162/47:

sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='Blues')
162/48: df.drop('√áabin', axis=1, inplace=True)
162/49: df.drop('Cabin', axis=1, inplace=True)
162/50: sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='Blues')
162/51: df.drop(['Name', 'Ticket', 'Emberked'], axis=1, inplace=True)
162/52: df.drop(['Name', 'Ticket', 'Embarked'], axis=1, inplace=True)
162/53: df.isnull().mean()
162/54: sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='Red')
162/55: sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='Reds')
162/56: sns.heatmap(df.isnull(), cbar=False, cmap='Reds')
162/57: sns.heatmap(df.isnull(), yticklabels=False, cmap='Reds')
162/58: df.isnull()
162/59:
plt.figure(figsize=(15, 10))
sns.boxplot(x='Sex', y='Age')
162/60:
plt.figure(figsize=(15, 10))
sns.boxplot(x='Sex', y='Age', data=df)
162/61: pd.isnull(age)
162/62: pd.isnull(data[0])
162/63: pd.isnull(df[0])
162/64:
def FillAge(data):
    age = data[0]
    sex = data[1]
    
    if pd.isnull(age):
        if sex is 'male':
            return 29
        else:
            return 25
    else:
        return age
162/65: df['Age'] = df[['Age', 'Sex']].apply(FillAge, axis=1)
162/66: df[['Age', 'Sex']][0]
162/67: df[['Age', 'Sex']]
162/68: ddf  = df[['Age', 'Sex']]
162/69: ddf[0]
162/70: pd.isnull(df.Age)
162/71: df.Age.hist(bins=30)
162/72: df.drop('PassengerId', axis=1, inplace=True)
162/73: df
162/74: male = pd.get_dummies(df.Sex)
162/75: male
162/76: male = pd.get_dummies(df.Sex, drop_first=True)
162/77: male
162/78: df.drop['Sex', axis=1, inplace=True]
162/79: df
162/80: df.drop('Sex', axis=1, inplace=True)
162/81: pd.concat([df, male], axis=1)
162/82: df = pd.concat([df, male], axis=1)
162/83: df
162/84: X = df.drop('Survived', axis=1, inplace=True)
162/85: X
162/86: X = df.drop('Survived', axis=1, inplace=True)
162/87: X
162/88: df
162/89: df = pd.concat([df, male], axis=1)
162/90: df
162/91: df = pd.read_csv('Train_Titanic.csv')
162/92: df
162/93: df.describe()
162/94: df.isnull().mean()
162/95: df.tail(4)
162/96: #### step 2
162/97: condition_1 = df.Survived == 1
162/98: df.where(condition_1).Survived.sum()
162/99:
print('Total = ', len(df))
print('Survived = ', len(df[df.Survived == 1]))
print('Survived = ', len(df.where(df['Survived'] != 1)))
162/100: ##
162/101: df.dtypes
162/102: list(df.select_dtypes('object').columns)
162/103: obj_features = list(df.select_dtypes('object').columns)
162/104:
for obj in obj_features:
    plt.figure()
    sns.countplot(x=obj, data=df)
162/105:
plt.figure(figsize=[6, 6])
plt.subplot(211)
sns.countplot(x='Pclass', data=df)
plt.subplot(212)
sns.countplot(x='Pclass', data=df, hue='Survived')
162/106:
plt.figure(figsize=[6, 6])
plt.subplot(211)
sns.countplot(x='SibSp', data=df)
plt.subplot(212)
sns.countplot(x='SibSp', data=df, hue='Survived')
162/107:
plt.figure(figsize=(40,30))
sns.countplot(x='Age', hue='Survived', data=df)
162/108: df['Age'].hist(bins = 30)
162/109:
plt.figure(figsize=(40,30))
sns.countplot(x='Fare', hue='Survived', data=df)
162/110: df.Fare.hist(bins=60)
162/111: ### step 3
162/112: sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='Blues')
162/113: df.drop('Cabin', axis=1, inplace=True)
162/114: df.drop(['Name', 'Ticket', 'Embarked'], axis=1, inplace=True)
162/115: df.isnull().mean()
162/116: sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='Reds')
162/117:
plt.figure(figsize=(15, 10))
sns.boxplot(x='Sex', y='Age', data=df)
162/118:
def FillAge(data):
    age = data[0]
    sex = data[1]
    
    if pd.isnull(age):
        if sex is 'male':
            return 29
        else:
            return 25
    else:
        return age
162/119: df['Age'] = df[['Age', 'Sex']].apply(FillAge, axis=1)
162/120: df.Age.hist(bins=30)
162/121: ## STEP
162/122: df.drop('PassengerId', axis=1, inplace=True)
162/123: male = pd.get_dummies(df.Sex, drop_first=True)
162/124: df.drop('Sex', axis=1, inplace=True)
162/125: df = pd.concat([df, male], axis=1)
162/126: df
162/127: X = df.drop('Survived', axis=1).values
162/128: dfX
162/129: X
162/130: y = df.Survived.values
162/131: y
162/132: y.mean()
162/133:
from sklearn.model_selection import train_test_split
train_test_split(X, y, test_size=0.2, random_state=0)
162/134: from sklearn.model_selection import train_test_split
162/135: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
162/136:
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state=0)
162/137: classifier.fit(X_train, y_train)
162/138: y_predict = classifier.predict(X_test)
162/139: cm = confusion_matrix(y_test, y_predict_test)
162/140: from sklearn.metrics import confusion_matrix
162/141: cm = confusion_matrix(y_test, y_predict_test)
162/142: cm = confusion_matrix(y_test, y_predict)
162/143: cm
162/144: sns.heatmap(cm, annot=True, fmt='d')
162/145: sns.heatmap(cm, annot=True, fmt='b')
162/146: sns.heatmap(cm, annot=True, fmt='d')
162/147: classifier.fit(X_train, y_train)
162/148: #STEP 5
162/149: y_predict = classifier.predict(X_test)
162/150: from sklearn.metrics import confusion_matrix
162/151: cm = confusion_matrix(y_test, y_predict)
162/152: cm
162/153: sns.heatmap(cm, annot=True, fmt='d')
162/154: from sklearn.metrics import classification_report
162/155: print(classification_report(y_test, y_predict))
162/156: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)
162/157:
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state=0)
162/158: classifier.fit(X_train, y_train)
162/159: #STEP 5
162/160: y_predict = classifier.predict(X_test)
162/161: from sklearn.metrics import confusion_matrix
162/162: cm = confusion_matrix(y_test, y_predict)
162/163: cm
162/164: sns.heatmap(cm, annot=True, fmt='d')
162/165: from sklearn.metrics import classification_report
162/166: print(classification_report(y_test, y_predict))
164/1:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
164/2: from sklearn.datasets import load_breast_cancer
164/3: df = load_breast_cancer
164/4: df
164/5: df = load_breast_cancer()
164/6: df
164/7: df.keys
164/8: df.keys()
164/9: print(df['DESCR'])
164/10: print(df.target_names)
164/11: print(df.target)
164/12: print(df.feature_names)
164/13: print(df['data'])
164/14: print(df['data'].shape)
164/15: df_cancer = pd.DataFrame(np.c_[df['data'], df.target], columns=np.append(df.feature_names))
164/16: df_cancer = pd.DataFrame(np.c_[df['data'], df['target']], columns=np.append(df.feature_names))
164/17: df_cancer = pd.DataFrame(np.c_[df['data'], df['target']], columns=np.append(df.feature_names, ['target']))
164/18: df_cancer
164/19: df_cancer.head()
164/20: #### Visualizing
164/21: sns.pairplot(df_cancer, hue='target')
164/22: df_cancer.columns
164/23: df_cancer.columns()
164/24: df_cancer.columns
164/25:
sns.pairplot(df_cancer, hue='target', vars=['mean radius', 'mean texture', 'mean perimeter', 'mean area',
       'mean smoothness', 'mean compactness', 'mean concavity',
       'mean concave points', 'mean symmetry', 'mean fractal dimension'])
164/26:
sns.pairplot(df_cancer, hue='target', vars=['mean radius', 'mean texture', 'mean perimeter', 'mean area',
       'mean smoothness'])
164/27: sns.countplot(df_cancer.target, label='count')
164/28: sns.scatterplot(x='mean area', y='mean smoothness', hue='target', df=df_cancer)
164/29: sns.scatterplot(x='mean area', y='mean smoothness', hue='target', df=df_cancer)
164/30: df_cancer['mean area']
164/31: sns.scatterplot(x = 'mean area', y='mean smoothness', hue='target', df=df_cancer)
164/32: df_cancer.head()
164/33: sns.scatterplot(x = 'mean texture', y='mean smoothness', hue='target', df=df_cancer)
164/34: df_cancer.columns
164/35: sns.scatterplot(x = 'mean area', y='mean smoothness', hue='target', df=df_cancer)
163/1:
# import libraries 
import pandas as pd # Import Pandas for data manipulation using dataframes
import numpy as np # Import Numpy for data statistical analysis 
import matplotlib.pyplot as plt # Import matplotlib for data visualisation
import seaborn as sns # Statistical data visualization
# %matplotlib inline
163/2:
# Import Cancer data drom the Sklearn library
from sklearn.datasets import load_breast_cancer
cancer = load_breast_cancer()
163/3: cancer
163/4: cancer.keys()
163/5: print(cancer['DESCR'])
163/6: print(cancer['target_names'])
163/7: print(cancer['target'])
163/8: print(cancer['feature_names'])
163/9: print(cancer['data'])
163/10: cancer['data'].shape
163/11: df_cancer = pd.DataFrame(np.c_[cancer['data'], cancer['target']], columns = np.append(cancer['feature_names'], ['target']))
163/12: df_cancer.head()
163/13: df_cancer.tail()
163/14:
x = np.array([1,2,3])
x.shape
163/15:
Example = np.c_[np.array([1,2,3]), np.array([4,5,6])]
Example.shape
163/16: sns.pairplot(df_cancer, hue = 'target', vars = ['mean radius', 'mean texture', 'mean area', 'mean perimeter', 'mean smoothness'] )
163/17: sns.countplot(df_cancer['target'], label = "Count")
163/18: sns.scatterplot(x = 'mean area', y = 'mean smoothness', hue = 'target', data = df_cancer)
164/36: sns.scatterplot(x = 'mean area', y = 'mean smoothness', hue = 'target', data = df_cancer)
164/37: sns.scatterplot(x = 'mean area', y = 'mean smoothness', hue = 'target', data = df_cancer)
164/38: sns.scatterplot(x='mean area', y='mean smoothness', hue='target', df=df_cancer)
164/39: sns.scatterplot(x = 'mean area', y='mean smoothness', hue='target', df=df_cancer)
164/40: sns.scatterplot(x = 'mean area', y='mean smoothness', hue='target', data=df_cancer)
164/41:
sns.scatterplot(x='mean texture', y='mean perimeter', hue=
                target, data=df_cancer)
164/42:
sns.scatterplot(x='mean texture', y='mean perimeter', hue=
                'target', data=df_cancer)
164/43:
sns.scatterplot(x='mean texture', y='mean radius', hue=
                'target', data=df_cancer)
164/44: sns.heatmap(df_cancer.corr(), annot=True)
164/45:
plt.figure(figsize=(20, 10))
sns.heatmap(df_cancer.corr(), annot=True)
164/46: from sklearn.model_selection import train_test_split
164/47: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=5)
164/48:
X = df_cancer.drop('target', axis=1)
y = df_cancer['target']
164/49: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=5)
164/50: X_train.shape
164/51: y_train.shape
164/52:
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix
164/53: svc_model = SVC()
164/54: svc_model.fit(X_train, y_train)
164/55: svc_model.predict(X_test)
164/56: y_predict = svc_model.predict(X_test)
164/57: y_predict
164/58: confusion_matrix(y_test, y_predict)
164/59: cm = confusion_matrix(y_test, y_predict)
164/60: sns.heatmap(cm)
164/61: sns.heatmap(cm, annot=True)
164/62: sns.heatmap(cm, annot=True)
164/63: sns.heatmap(cm, annot=True, cbar=False)
164/64: print(classification_report(y_test, y_predict))
164/65: df_cancer.describe()
164/66: min_train = X_train.min()
164/67: min_train
164/68: range_train = (X_train - min_train).max
164/69: range_train = (X_train - min_train).max()
164/70: range_train
164/71: X_train_scaled = (X_train - min_train)/range_train
164/72: X_train_scaled
164/73: sns.scatterplot(x = X_train['mean_area'], y = X_train['mean smoothness'], hue = 'target')
164/74: sns.scatterplot(x = X_train['mean_area'], y = X_train['mean smoothness'], hue = y_train)
164/75: sns.scatterplot(x = X_train['mean area'], y = X_train['mean smoothness'], hue = y_train)
164/76: sns.scatterplot(x = X_train_scaled['mean area'], y = X_train_scaled['mean smoothness'], hue = y_train)
164/77: min_test = X_test.min()
164/78: range_test = (X_test - min_test).max
164/79: range_test
164/80: X_test_scaled = (X_test - min_test)/range_test
164/81: range_test = (X_test - min_test).max()
164/82: X_test_scaled = (X_test - min_test)/range_test
164/83: X_test_scaled
164/84: X_test_scaled.describe()
164/85:
min_test = X_test.min()
max_test = X_test.max()
164/86: X_test_scaled = (X_test - min_test)/(max_test - min_test)
164/87: X_test_scaled
164/88: X_test_scaled.describe()
164/89: max_test
164/90: (X_test - X_test.min())/(X_test.max() - X_test.min())
164/91: (X_test - X_test.min())/(X_test.max() - X_test.min()).describe()
164/92: (X_test - X_test.min())/(X_test.max() - X_test.min())
164/93: ddff = (X_test - X_test.min())/(X_test.max() - X_test.min())
164/94: ddff
164/95: ddff.describe()
164/96: ddff = (X_test - X_test.min())/(X_test.max() - X_test.min())
164/97: X_test_scaled
164/98:
svc_model_scaled = SVC()
svc_model_scaled.fit(X_train_scaled, y_train)
164/99: y_predict_scaled = svc_model_scaled.predict(X_test_scaled)
164/100: confusion_matrix(y_test, y_predict_scaled)
164/101: cm_scaled = confusion_matrix(y_test, y_predict_scaled)
164/102: sns.heatmap(cm_scaled)
164/103: sns.heatmap(cm_scaled, annot=True)
164/104: cm
164/105: cm_scaled
164/106: tn, fp, fn, tp = confusion_matrix(y_test, y_predict_scaled).ravel()
164/107: fp
164/108: ##
164/109:
param_grid = {'C': [.1, 1, 10, 100],
              'gamma': [1, .1, .01, .001],
              'kernel': 'rbf'}
164/110: from sklearn.model_selection import GridSearchCV
164/111: grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=4)
164/112:
param_grid = {'C': [.1, 1, 10, 100],
              'gamma': [1, .1, .01, .001],
              'kernel': ['rbf']}
164/113: from sklearn.model_selection import GridSearchCV
164/114: grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=4)
164/115: grid.fit(X_test_scaled, y_train)
164/116: grid.fit(X_train_scaled, y_train)
164/117: grid.best_params_
164/118: grid.best_estimator_
164/119: y_grid = grid.predict(X_test_scaled)
164/120: confusion_matrix(y_test, y_grid)
164/121: sns.heatmap(confusion_matrix(y_test, y_grid), annot=True)
164/122: print(classification_report(y_test, y_grid))
165/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
165/2: df = pd.read_csv("Tshirt_Sizing_Dataset.csv")
165/3: from sklearn.preprocessing import LabelEncoder
165/4: df
165/5: df.head(3)
165/6: df.tail(3)
165/7: from sklearn.metrics import classification_report, confusion_matrix
165/8:
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
165/9: X = df[:, 0:2]
165/10: X = df.iloc[:, 0:2].values
165/11: X
165/12: y = df.iloc[:, 2].values
165/13: y
165/14:
label_enc = LabelEncoder()
y = label_enc.fit_transform(y)
165/15: y
165/16: from sklearn.model_selection import train_test_split
165/17: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25)
165/18: X_train.shape
165/19: X_test.shape
165/20: ## model training
165/21: from sklearn.neighbors import KNeighborsClassifier
165/22: knn = KNeighborsClassifier(n_neighbors= 5, metric = 'minkowski', p = 2)
165/23: knn.fit(X_train, y_train)
165/24: y_pred = knn.pred(X_test)
165/25: y_pred = knn.predict(X_test)
165/26: y_pred
165/27: y_test
165/28: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, random_state = 5)
165/29: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, random_state = 5)
165/30: X_train.shape
165/31: X_test.shape
165/32: ## model training
165/33: from sklearn.neighbors import KNeighborsClassifier
165/34: knn = KNeighborsClassifier(n_neighbors= 5, metric = 'minkowski', p = 2)
165/35: knn.fit(X_train, y_train)
165/36: y_pred = knn.predict(X_test)
165/37: y_pred
165/38: y_test
165/39: confusion_matrix(y_test, y_pred)
165/40: y_pred = knn.predict(X_test)
165/41: y_pred
165/42: sns.heatmap(confusion_matrix(y_test, y_pred), annot=True)
165/43: knn = KNeighborsClassifier(n_neighbors= 3, metric = 'minkowski', p = 2)
165/44: knn.fit(X_train, y_train)
165/45: y_pred = knn.predict(X_test)
165/46: y_pred
165/47: y_test
165/48: confusion_matrix(y_test, y_pred)
165/49: sns.heatmap(confusion_matrix(y_test, y_pred), annot=True)
167/1:
# import libraries 
import pandas as pd # Import Pandas for data manipulation using dataframes
import numpy as np # Import Numpy for data statistical analysis 
import matplotlib.pyplot as plt # Import matplotlib for data visualisation
import seaborn as sns # Statistical data visualization
# %matplotlib inline
167/2: df_alexa = pd.read_csv('amazon_alexa.tsv', sep='\t')
167/3: df_alexa.head()
167/4: df_alexa.keys()
167/5: df_alexa.tail()
167/6: df_alexa['verified_reviews']
167/7: positive = df_alexa[df_alexa['feedback']==1]
167/8: negative = df_alexa[df_alexa['feedback']==0]
167/9: negative
168/1:
# import libraries 
import pandas as pd # Import Pandas for data manipulation using dataframes
import numpy as np # Import Numpy for data statistical analysis 
import matplotlib.pyplot as plt # Import matplotlib for data visualisation
import seaborn as sns # Statistical data visualization
# %matplotlib inline
168/2: df_alexa = pd.read_csv('amazon_alexa.tsv', sep='\t')
168/3:
# Let's drop the date
df_alexa = df_alexa.drop(['date', 'rating'],axis=1)
168/4: variation_dummies = pd.get_dummies(df_alexa['variation'], drop_first = True)
168/5: df_alexa.drop(['variation'], axis=1, inplace=True)
168/6: df_alexa = pd.concat([df_alexa, variation_dummies], axis=1)
168/7: df_alexa
168/8: from sklearn.feature_extraction.text import CountVectorizer
168/9: vec = CountVectorizer()
168/10: alexa_vec = CountVectorizer.fit_transform(df_alexa['verified_reviews'])
168/11: alexa_vec = vec.fit_transform(df_alexa['verified_reviews'])
168/12: alexa_vec
168/13: print(alexa_vec.toarray())
168/14:
# first let's drop the column
df_alexa.drop(['verified_reviews'], axis=1, inplace=True)
reviews = pd.DataFrame(alexa_countvectorizer.toarray())
168/15:
# first let's drop the column
df_alexa.drop(['verified_reviews'], axis=1, inplace=True)
reviews = pd.DataFrame(alexa_vec.toarray())
168/16:
# first let's drop the column
#df_alexa.drop(['verified_reviews'], axis=1, inplace=True)
reviews = pd.DataFrame(alexa_vec.toarray())
168/17: df_alexa = pd.concat([df_alexa, reviews], axis=1)
168/18: df_alexa
168/19:
# Let's drop the target label coloumns
X = df_alexa.drop(['feedback'],axis=1)
167/10: X.shape
167/11:
# Let's drop the target label coloumns
X = df_alexa.drop(['feedback'],axis=1)
167/12: # X = pd.DataFrame(alexa_countvectorizer.toarray())
167/13: X.shape
167/14:
# Let's drop the target label coloumns
X = df_alexa.drop(['feedback'],axis=1)
167/15: # X = pd.DataFrame(alexa_countvectorizer.toarray())
167/16: X.shape
167/17:
# Now let's concatenate them together
df_alexa = pd.concat([df_alexa, reviews], axis=1)
167/18: df_alexa
168/20:
# Let's drop the target label coloumns
X = df_alexa.drop(['feedback'],axis=1)
168/21: X.shape
168/22: y = df_alexa.feedback
168/23: from sklearn.model_selection import train_test_split
168/24: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_sample = 2)
168/25: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 2)
168/26:
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.ensemble import RandomForestClassifier
168/27: rfc = RandomForestClassifier()
168/28: rfc = RandomForestClassifier(n_estimators=100, criterion='entropy')
168/29: rfc.fit(X_train, y_train)
168/30: rfc.predict(y_test)
168/31: y_pred = rfc.predict(X_test)
168/32: cm = confusion_matrix(y_test, y_pred)
168/33: sns.heatmap(cm, annot=True)
168/34: print(classification_report(y_test, y_pred))
168/35: from sklearn.model_selection import GridSearchCV
168/36: RFC = RandomForestClassifier()
168/37:
param_grid = {'n_estimators': [10, 50, 100, 150, 200],
              'criterion': ['gini', 'entropy']}
168/38: gs = GridSearchCV(RFC, param_grid)
168/39: gs.fit(X_train, y_train)
168/40:
######
df_alexa = pd.read_csv('amazon_alexa.tsv', sep = '\t')
168/41: df_alexa = pd.concat([df_alexa, pd.DataFrame(alexa_vec.toarray())], axis=1)
168/42: df_alexa
168/43: df_alexa['length'] df_alexa['verified_reviews'].apply(len)
168/44: df_alexa['length'] = df_alexa['verified_reviews'].apply(len)
168/45: df_alexa
168/46: X = df_alexa.drop(['rating', 'date', 'variation', 'verified_reviews'], axis=1)
168/47: y = df_alexa.feedback
168/48: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 2)
168/49:
rfc = RandomForestClassifier(n_estimators=100)
rfc.fit(X_train, y_train)
y_predict = rfc.predic(X_test)
print(classification_report(y_test, y_predict))
168/50:
rfc = RandomForestClassifier(n_estimators=100)
rfc.fit(X_train, y_train)
y_predict = rfc.predict(X_test)
print(classification_report(y_test, y_predict))
168/51: sns.heatmap(confusion_matrix(y_test, y_predict), annot=True)
170/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
170/2: spam_df = pd.read_csv("emails.csv")
170/3: ham = spam_df[spam_df['spam']==0]
170/4: spam = spam_df[spam_df['spam']==1]
170/5: sns.countplot(spam_df['spam'], label = "Count")
170/6: spam_df.head(5)
170/7:
from sklearn.feature_extraction.text import CountVectorizer
sample_data = ['This is the first document.','This document is the second document.','And this is the third one.','Is this the first document?']

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(sample_data)
170/8: ham.head(5)
170/9:
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()
spamham_countvectorizer = vectorizer.fit_transform(spam_df['text'])
170/10: print(vectorizer.get_feature_names())
170/11: print(spamham_countvectorizer.toarray())
170/12:
from sklearn.naive_bayes import MultinomialNB

NB_classifier = MultinomialNB()
label = spam_df['spam'].values
NB_classifier.fit(spamham_countvectorizer, label)
170/13: spam_df
170/14: spam_df.values
170/15: spamham_countvectorizer
170/16: spamham_countvectorizer.toarray()
161/1:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
161/2: df = pd.read_csv('Facebook_Ads_2.csv')
161/3: df = pd.read_csv('Facebook_Ads_2.csv', , encoding='ISO-8859-1')
161/4: df = pd.read_csv('Facebook_Ads_2.csv', encoding='ISO-8859-1')
161/5: df.head(5)
161/6: df.tail(5)
161/7: df.describe()
161/8: df.info()
161/9: df_click = df[df.Clicked == 1]
161/10: df_no_click = df[df.Clicked == 0]
161/11: print('Total = ', len(df))
161/12:
print('Total = ', len(df))
print('Number of customers who clicked on Ad = ', len(df_click))
161/13:
print('Total = ', len(df))
print('Number of customers who clicked on Ad = ', len(df_click))
print('Percentage Clicked = ', (len(df_click)/len(df)))
161/14:
print('Total = ', len(df))
print('Number of customers who clicked on Ad = ', len(df_click))
print('Percentage Clicked = ', "%.2f" % (len(df_click)/len(df)))
161/15:
print('Total = ', len(df))
print('Number of customers who clicked on Ad = ', len(df_click))
print('Percentage Clicked = ', '%.3f' % (len(df_click)/len(df)))
161/16:
print('Total = ', len(df))
print('Number of customers who clicked on Ad = ', len(df_click))
print('Percentage Clicked = ', '%.3f' % (len(df_click)/len(df)), '%')
161/17:
print('Total = ', len(df))
print('Number of customers who clicked on Ad = ', len(df_click))
print('Percentage Clicked = ', '%.3f' % (len(df_click)/len(df)), '%')
print('Did not Click = ', len(df_no_click))
161/18:
print('Total =', len(df))
print('Number of customers who clicked on Ad =', len(df_click))
print('Percentage Clicked =', '%.3f' % (len(df_click)/len(df)), '%')
print('Did not Click =', len(df_no_click))
print('Percentage did not click =', '%.3f' % (len(df_no_click)/len(df)))
161/19:
print('Total =', len(df))
print('Number of customers who clicked on Ad =', len(df_click))
print('Percentage Clicked =', '%.3f' % (len(df_click)/len(df)), '%')
print('Did not Click =', len(df_no_click))
print('Percentage did not click =', '%.3f' % (len(df_no_click)/len(df)), '%')
161/20:
print('Total =', len(df))
print('Number of customers who clicked on Ad =', len(df_click))
print('Percentage Clicked =', '%.3f' % (len(df_click)/len(df)) * 100, '%')
print('Did not Click =', len(df_no_click))
print('Percentage did not click =', '%.3f' % (len(df_no_click)/len(df)) * 100, '%')
161/21:
print('Total =', len(df))
print('Number of customers who clicked on Ad =', len(df_click))
print('Percentage Clicked =', '%.3f' % (len(df_click)/len(df) * 100), '%')
print('Did not Click =', len(df_no_click))
print('Percentage did not click =', '%.3f' % (len(df_no_click)/len(df)) * 100, '%')
161/22:
print('Total =', len(df))
print('Number of customers who clicked on Ad =', len(df_click))
print('Percentage Clicked =', '%.3f' % (len(df_click)/len(df) * 100), '%')
print('Did not Click =', len(df_no_click))
print('Percentage did not click =', '%.3f' % (len(df_no_click)/len(df) * 100), '%')
161/23: df.columns
161/24: sns.scatterplot(x='Time Spent on Site', y='Salary', data=df)
161/25: sns.scatterplot(x='Time Spent on Site', y='Salary', data=df, hue='Clicked')
161/26: sns.boxplot(x='Clicked', y='Salary', data=df)
161/27: sns.boxplot(x='Time Spent on Site', y='Salary', data=df)
161/28: sns.boxplot(x='Clicked', y='Time Spent on Site', data=df)
161/29: df.describe
161/30: df.describe()
161/31: df.Salary.hist(bins=50)
161/32: df['Time Spent on Site'].hist(bins = 30)
161/33: df['Time Spent on Site'].hist(bins = 20)
161/34: df
161/35:
#Let's drop the emails, country and names (we can make use of the country later!)
df_short = df.drop(['emails', 'Country', 'Names'], axis=1)
161/36: df_short
161/37:
#Let's drop the target coloumn before we do train test split
X = df_short.drop(['Clicked'], axis=1)
y = df_short.Clicked
161/38:
# Feature Scaling
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
X_scaled = ss.fit_transform(X)
161/39: X_scaled
161/40:
#Let's drop the target coloumn before we do train test split
X = df_short.drop(['Clicked'], axis=1).values
y = df_short.Clicked
161/41:
# Feature Scaling
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
X_scaled = ss.fit_transform(X)
161/42: X_scaled
161/43:
#Let's drop the target coloumn before we do train test split
X = df_short.drop(['Clicked'], axis=1)
y = df_short.Clicked
161/44:
# Feature Scaling
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
X_scaled = ss.fit_transform(X)
161/45: X_scaled
161/46: X_scaled.describe()
161/47: pd.DataFrame(X_scaled)
161/48: pd.DataFrame(X_scaled).describe()
161/49: from sklearn.model_selection import train_test_split
161/50: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 0)
161/51: X_train
161/52: X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = .2, random_state = 0)
161/53: X_train
161/54: y_train
161/55:
#Let's drop the target coloumn before we do train test split
X = df_short.drop(['Clicked'], axis=1)
y = df_short.Clicked.values
161/56: X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = .2, random_state = 0)
161/57: X_train
161/58: y_train
161/59:
from sklearn.linear_model import LogisticRegression
lg = LogisticRegression()
lg.fit(X_scaled, y_train)
161/60: from sklearn.model_selection import train_test_split
161/61: X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = .2, random_state = 0)
161/62: X_train
161/63: y_train
161/64:
from sklearn.linear_model import LogisticRegression
lg = LogisticRegression()
lg.fit(X_scaled, y_train)
161/65: X_train.shape
161/66: len(y_train)
161/67:
from sklearn.linear_model import LogisticRegression
lg = LogisticRegression()
lg.fit(X_train, y_train)
161/68:
y_predict = lg.predict(X_test)
print(y_predict)
161/69: from sklearn.metrics import classification_report, confusion_matrix
161/70: sns.heatmap(confusion_matrix(y_test, y_predict), annot=True)
161/71: y_pred_train = lg.predict(X_train)
161/72: sns.heatmap(confusion_matrix(y_train, y_pred_train), annot=True)
161/73: sns.heatmap(confusion_matrix(y_test, y_predict), annot=True)
161/74: classification_report(y_test, y_predict)
161/75: print(classification_report(y_test, y_predict))
172/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
172/2: df = pd.read_csv('Bank_Customer_retirement.csv')
172/3: df
172/4: df.columns
172/5: df.shape
172/6: df.head(5)
172/7: df.tail(5)
172/8: sns.pairplot(df)
172/9: sns.pairplot(df[['Age', '401K Savings']], hue='Retire')
172/10: sns.pairplot(df[['Age', '401K Savings']], hue=df['Retire'])
172/11: sns.pairplot(df[['Age', '401K Savings']], hue='Retire', data=df)
172/12: sns.pairplot(data=df, hue='Retire', vars=['Age', '401K Savings'])
172/13: sns.countplot(data=df, x='Retire')
172/14: df.head(4)
172/15:

# Let's drop the target label coloumns
X = df.drop(['Customer ID', 'Retire'], axis=1)
172/16: X
172/17:

# Let's drop the target label coloumns
X = df.drop(['Customer ID', 'Retire'], axis=1)
y = df.Retire.values
172/18: y
172/19:

# Let's drop the target label coloumns
X = df.drop(['Customer ID', 'Retire'], axis=1)
y = df.Retire
172/20: X
172/21: y
172/22: from sklearn.model_selection import train_test_split
172/23: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 0)
172/24: X_train.shape
172/25: X_test.shape
172/26: y.shape
172/27: y_train.shape
172/28: y_test.shape
172/29: from sklearn.svm import SVC
172/30:
svc = SVC()
svc.fit(X_train, y_train)
172/31:
from sklearn.metrics import classification_report, confusion_matrix
y_pred = svc.predict(X_test)
172/32: sns.heatmap(confusion_matrix(y_test, y_pred), annot=True)
172/33: print(classification_report(y_test, y_pred))
172/34: X_scaled_train = (X_train - X_train.min)/(X_train.max() - X_train.min())
172/35: (X_train - X_train.min
172/36: X_scaled_train = (X_train - X_train.min())/(X_train.max() - X_train.min())
172/37: X_scaled_train
172/38: sns.scatterplot(x='Age', y='401K Savings', data = X_train, hue = y_train)
172/39: sns.scatterplot(x = X_train['Age'], y = X_train['401K Savings'], hue = y_train)
172/40: sns.scatterplot(x='Age', y='401K Savings', data = X_scaled_train, hue = y_train)
172/41: svc.fit(X_train, y_train)
172/42:
X_scaled_train = (X_train - X_train.min())/(X_train.max() - X_train.min())
X_scaled_test = (X_test - X_test.min())/(X_test.max() - X_test.min())
172/43: svc.fit(X_scaled_train, y_train)
172/44: y_pred_scaled = svc.predict(X_scaled_test)
172/45: sns.heatmap(confusion_matrix(y_test, y_pred_scaled), annot=True)
172/46: print(classification_report(y_test, y_pred_scaled))
172/47:
param_grid = {'C': [0.1, 1, 10, 100],
              'gamma': [1, 0.1, 0.01, 0.001]}
172/48: from sklearn.model_selection import GridSearchCV
172/49:
svc = SVC()
gscv = GridSearchCV(svc, param_grid=param_grid, cv=5)
172/50:
svc = SVC()
gscv = GridSearchCV(svc, param_grid=param_grid, cv=5, verbose=4)
172/51: gscv.fit(X_scaled_train, y_train)
172/52: gscv.best_params_
172/53: y_pred_scaled = gscv.predict(X_scaled_test)
172/54: sns.heatmap(classification_report(y_test, y_pred_scaled))
172/55: sns.heatmap(confusion_matrix(y_test, y_pred_scaled), annot=True)
172/56: print(classification_report(y_test, y_pred_scaled))
174/1:
# Let's view the head of the training dataset
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
df = pd.read_csv('Iris.csv')
df.head(5)
174/2:
# Let's view the last elements in the training dataset
df.tail(5)
174/3: sns.scatterplot(x='SepalLengthCm', y='SepalWidthCm', hue='Species', data=df)
174/4: sns.scatterplot(x='PetalLengthCm', y='PetalWidthCm', hue='Species', data=df)
174/5:
# Let's show the Violin plot 
plt.figure(figsize=(10, 10))

plt.subplot(2,2,1)
sns.violinplot(x='Species',y='PetalLengthCm',data=iris_df)

plt.subplot(2,2,2)
sns.violinplot(x='Species',y='PetalWidthCm',data=iris_df)

plt.subplot(2,2,3)
sns.violinplot(x='Species',y='SepalLengthCm',data=iris_df)

plt.subplot(2,2,4)
sns.violinplot(x='Species',y='SepalWidthCm',data=iris_df)
174/6:
# Let's show the Violin plot 
plt.figure(figsize=(10, 10))

plt.subplot(2,2,1)
sns.violinplot(x='Species',y='PetalLengthCm',data=df)

plt.subplot(2,2,2)
sns.violinplot(x='Species',y='PetalWidthCm',data=df)

plt.subplot(2,2,3)
sns.violinplot(x='Species',y='SepalLengthCm',data=df)

plt.subplot(2,2,4)
sns.violinplot(x='Species',y='SepalWidthCm',data=df)
174/7: sns.pairplot(df)
174/8: sns.pairplot(df, hue = 'Species', vars=['PetalLengthCm', 'PetalWidthCm', 'SepalLengthCm', 'SepalWidthCm'])
174/9: sns.heatmap(corr(df))
174/10: sns.heatmap(cor(df))
174/11: sns.heatmap(df.corr(), annot=True)
174/12:
plt.figure(figsize=(10,10))
sns.heatmap(df.corr(), annot=True)
174/13:
plt.figure(figsize=(10,6))
sns.heatmap(df.corr(), annot=True)
174/14:
# Let's drop the ID and Species (target label) coloumns
X = df.drop(['Species'], axis=1)
X
174/15:
y = df.Species
y
174/16:
# Import train_test_split from scikit library
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 0)
174/17: X_train.shape
174/18:
# Import train_test_split from scikit library
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, random_state = 0)
174/19: X_train.shape
174/20:
# Import train_test_split from scikit library
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .15, random_state = 0)
174/21: X_train.shape
174/22:
# Import train_test_split from scikit library
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 0)
174/23: X_train.shape
174/24:
# Import train_test_split from scikit library
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, random_state = 0)
174/25: X_train.shape
174/26:
# Import train_test_split from scikit library
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 0)
174/27: X_train.shape
174/28:
# Import train_test_split from scikit library
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .4, random_state = 0)
174/29: X_train.shape
174/30:
# Import train_test_split from scikit library
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .35, random_state = 0)
174/31: X_train.shape
174/32: X_train.shape
174/33: X_train.head(5)
174/34: y_train
174/35:
# Fitting K-NN to the Training set
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors = 5, metric='minkowski', p=2)
knn.fit(X_train, y_train)
174/36: y_pred = knn.predict(X_test)
174/37: from sklearn.metrics import classification_report, confusion_matrix
174/38: sns.heatmap(confusion_matrix(y_test, y_pred), annot=True)
174/39: print(classification_report(y_test, y_pred))
176/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
176/2: df = pd.read_csv('creditcard.csv')
176/3: df
176/4: df.head(10)
176/5: df.describe()
176/6: df.info
176/7: df.info()
176/8: non_fraud = creditcard_df[creditcard_df['Class']==0]
176/9: df_non_fraud = df[df['Class']==0]
176/10: df_fraud = df[df['Class']==1]
176/11: print('fraud transactions percentage =', '%.4f' % (len(df_fraud)/len(df)))
176/12: print('fraud transactions percentage =', '%.4f' % (len(df_fraud)/len(df)), '%')
176/13: print('fraud transactions percentage =', '%.4f' % (len(df_fraud)/len(df) * 100), '%')
176/14: sns.countplot(x='Class', data = df)
176/15:
plt.figure(figsize=(10, 10))
corr = df.corr()
sns.heatmap(corr, annot=True)
176/16:
plt.figure(figsize=(20, 20))
corr = df.corr()
sns.heatmap(corr, annot=True)
176/17:
plt.figure(figsize=(25, 20))
corr = df.corr()
sns.heatmap(corr, annot=True)
176/18:
# kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable.
i = 1

fig, ax = plt.subplots(8,4,figsize=(18,30))
for column_header in column_headers:    
    plt.subplot(8,4,i)
    sns.kdeplot(fraud[column_header], bw = 0.4, label = "Fraud", shade=True, color="r", linestyle="--")
    sns.kdeplot(non_fraud[column_header], bw = 0.4, label = "Non Fraud", shade=True, color= "y", linestyle=":")
    plt.title(column_header, fontsize=12)
    i = i + 1
plt.show();
176/19: column_headers = –≤–∞.columns.values
176/20: column_headers = df.columns.values
176/21:
# kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable.
i = 1

fig, ax = plt.subplots(8,4,figsize=(18,30))
for column_header in column_headers:    
    plt.subplot(8,4,i)
    sns.kdeplot(fraud[column_header], bw = 0.4, label = "Fraud", shade=True, color="r", linestyle="--")
    sns.kdeplot(non_fraud[column_header], bw = 0.4, label = "Non Fraud", shade=True, color= "y", linestyle=":")
    plt.title(column_header, fontsize=12)
    i = i + 1
plt.show();
176/22:
# kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable.
i = 1

fig, ax = plt.subplots(8,4,figsize=(18,30))
for column_header in column_headers:    
    plt.subplot(8,4,i)
    sns.kdeplot(df_fraud[column_header], bw = 0.4, label = "Fraud", shade=True, color="r", linestyle="--")
    sns.kdeplot(df_non_fraud[column_header], bw = 0.4, label = "Non Fraud", shade=True, color= "y", linestyle=":")
    plt.title(column_header, fontsize=12)
    i = i + 1
plt.show();
176/23:
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
creditcard_df['Amount_Norm'] = sc.fit_transform(–≤–∞['Amount'].values.reshape(-1,1))
176/24:
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
creditcard_df['Amount_Norm'] = sc.fit_transform(df['Amount'].values.reshape(-1,1))
176/25:
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
df['Amount_Norm'] = sc.fit_transform(df['Amount'].values.reshape(-1,1))
176/26: df
176/27:
creditcard_df = creditcard_df.drop(['Amount'], axis = 1)
# Let's drop the target label coloumns
X = creditcard_df.drop(['Class'],axis=1)
y = creditcard_df['Class']
176/28:
creditcard_df = df.drop(['Amount'], axis = 1)
# Let's drop the target label coloumns
X = creditcard_df.drop(['Class'],axis=1)
y = creditcard_df['Class']
176/29: # Let's drop the target label coloumns
176/30: X
176/31:
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_sample = 0)
176/32:
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 0)
176/33:
print(X_train.shape)
print(X_test.shape)
print(len(y_train))
print(y_test)
176/34:
print(X_train.shape)
print(X_test.shape)
print(len(y_train))
print(len(y_test))
176/35:
from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(X_train, y_train)
176/36: from sklearn.metrics impirt classification_report, confusion_matrix
176/37: from sklearn.metrics import classification_report, confusion_matrix
176/38: y_predict = gnb.predict(X_test)
176/39: sns.heatmap(confusion_matrix(y_test, y_predict), annot=True)
176/40: print(classification_report(y_test, y_predict))
178/1:
def my_f(a):
    print(a)
178/2: my_f('3')
178/3: q = my_f('d')
178/4: q
178/5: print(q)
178/6: 5 != 6
178/7:
while True:
    print('1')
178/8:
while
    print('1')
178/9:
while:
    print('1')
178/10:
def mu_p(a, b):
    for char in a:
        for num in b:
            print(a, b)
    return
178/11: mu_p(['a', 'b'], [1, 2])
178/12:
def mu_p(a, b):
    for char in a:
        for num in b:
            print(char, num)
    return
178/13: mu_p(['a', 'b'], [1, 2])
178/14: len([2,3,4,5])
178/15: len([2,3,4,5]) //2
178/16: len([2,3,4,5, 10]) //2
178/17: len([2,3,4,5, 10])
178/18: len([2,3,4,5, 10, 11]) //2
178/19: y = 'jj; kkk;kk'
178/20: z = y.split(';')
178/21: z
178/22: len(z)
178/23: import numpy as np
178/24:
t = np.array([
    [1,3],
    [2,4]
])
178/25: print(t.max(axis=1))
178/26: print(t.max(axis=0))
178/27: print(t.max(axis=1))
179/1:
# to handle datasets
import pandas as pd
import numpy as np

# for plotting
import matplotlib.pyplot as plt
import seaborn as sns

# for the yeo-johnson transformation
import scipy.stats as stats

# to display all the columns of the dataframe in the notebook
pd.pandas.set_option('display.max_columns', None)
179/2:
# load dataset
data = pd.read_csv('train.csv')

# rows and columns of the data
print(data.shape)

# visualise the dataset
data.head()
179/3:
# load dataset
data = pd.read_csv('train.csv')

# rows and columns of the data
print(data.shape)

# visualise the dataset
data.head()
179/4:
# drop id, it is just a number given to identify each house
data.drop('Id', axis=1, inplace=True)

data.shape
179/5:
# histogran to evaluate target distribution

data['SalePrice'].hist(bins=50, density=True)
plt.ylabel('Number of houses')
plt.xlabel('Sale Price')
plt.show()
179/6:
# let's transform the target using the logarithm

np.log(data['SalePrice']).hist(bins=50, density=True)
plt.ylabel('Number of houses')
plt.xlabel('Log of Sale Price')
plt.show()
179/7: data
179/8: data.LotFrontage.dtype
179/9: data.LotFrontage.dtype == 'O'
179/10:
# let's identify the categorical variables
# we will capture those of type *object*

cat_vars = [var for var in data.columns if data[var].dtype == 'O']

# MSSubClass is also categorical by definition, despite its numeric values
# (you can find the definitions of the variables in the data_description.txt
# file available on Kaggle, in the same website where you downloaded the data)

# lets add MSSubClass to the list of categorical variables
cat_vars = cat_vars + ['MSSubClass']

# number of categorical variables
len(cat_vars)
179/11:
# cast all variables as categorical
data[cat_vars] = data[cat_vars].astype('O')
179/12: cat_vars
179/13: data.Alley.dtype == 'O'
179/14: data.LotFrontage.dtype == 'O'
179/15: data.LotFrontage.dtype == 'F'
179/16: data.Alley.dtype
179/17: data.dtype
179/18: data.dtypes
179/19:
# plot

data[vars_with_na].isnull().mean().sort_values(
    ascending=False).plot.bar(figsize=(10, 4))
plt.ylabel('Percentage of missing data')
plt.axhline(y=0.90, color='r', linestyle='-')
plt.axhline(y=0.80, color='g', linestyle='-')

plt.show()
179/20:
# now let's identify the numerical variables

num_vars = [
    var for var in data.columns if var not in cat_vars and var != 'SalePrice'
]

# number of numerical variables
len(num_vars)
179/21:
# make a list of the variables that contain missing values
vars_with_na = [var for var in data.columns if data[var].isnull().sum() > 0]

# determine percentage of missing values (expressed as decimals)
# and display the result ordered by % of missin data

data[vars_with_na].isnull().mean().sort_values(ascending=False)
179/22:
# plot

data[vars_with_na].isnull().mean().sort_values(
    ascending=False).plot.bar(figsize=(10, 4))
plt.ylabel('Percentage of missing data')
plt.axhline(y=0.90, color='r', linestyle='-')
plt.axhline(y=0.80, color='g', linestyle='-')

plt.show()
179/23:
# now we can determine which variables, from those with missing data,
# are numerical and which are categorical

cat_na = [var for var in cat_vars if var in vars_with_na]
num_na = [var for var in num_vars if var in vars_with_na]

print('Number of categorical variables with na: ', len(cat_na))
print('Number of numerical variables with na: ', len(num_na))
179/24: num_na
179/25: cat_na
179/26:
def analyse_na_value(df, var):

    # copy of the dataframe, so that we do not override the original data
    # see the link for more details about pandas.copy()
    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.copy.html
    df = df.copy()

    # let's make an interim variable that indicates 1 if the
    # observation was missing or 0 otherwise
    df[var] = np.where(df[var].isnull(), 1, 0)

    # let's compare the median SalePrice in the observations where data is missing
    # vs the observations where data is available

    # determine the median price in the groups 1 and 0,
    # and the standard deviation of the sale price,
    # and we capture the results in a temporary dataset
    tmp = df.groupby(var)['SalePrice'].agg(['mean', 'std'])

    # plot into a bar graph
    tmp.plot(kind="barh", y="mean", legend=False,
             xerr="std", title="Sale Price", color='green')

    plt.show()
179/27:
# let's run the function on each variable with missing data

for var in vars_with_na:
    analyse_na_value(data, var)
179/28:
#  let's male a list of discrete variables
discrete_vars = [var for var in num_vars if len(
    data[var].unique()) < 20 and var not in year_vars]


print('Number of discrete variables: ', len(discrete_vars))
179/29:
# let's visualise the discrete variables

data[discrete_vars].head()
179/30:
# now we can determine which variables, from those with missing data,
# are numerical and which are categorical

cat_na = [var for var in cat_vars if var in vars_with_na]
num_na = [var for var in num_vars if var in vars_with_na]

print('Number of categorical variables with na: ', len(cat_na))
print('Number of numerical variables with na: ', len(num_na))
179/31: num_na
179/32: cat_na
179/33:
def analyse_na_value(df, var):

    # copy of the dataframe, so that we do not override the original data
    # see the link for more details about pandas.copy()
    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.copy.html
    df = df.copy()

    # let's make an interim variable that indicates 1 if the
    # observation was missing or 0 otherwise
    df[var] = np.where(df[var].isnull(), 1, 0)

    # let's compare the median SalePrice in the observations where data is missing
    # vs the observations where data is available

    # determine the median price in the groups 1 and 0,
    # and the standard deviation of the sale price,
    # and we capture the results in a temporary dataset
    tmp = df.groupby(var)['SalePrice'].agg(['mean', 'std'])

    # plot into a bar graph
    tmp.plot(kind="barh", y="mean", legend=False,
             xerr="std", title="Sale Price", color='green')

    plt.show()
179/34:
# let's run the function on each variable with missing data

for var in vars_with_na:
    analyse_na_value(data, var)
179/35:
print('Number of numerical variables: ', len(num_vars))

# visualise the numerical variables
data[num_vars].head()
179/36:
# list of variables that contain year information

year_vars = [var for var in num_vars if 'Yr' in var or 'Year' in var]

year_vars
179/37:
# let's explore the values of these temporal variables

for var in year_vars:
    print(var, data[var].unique())
    print()
179/38:
# plot median sale price vs year in which it was sold

data.groupby('YrSold')['SalePrice'].median().plot()
plt.ylabel('Median House Price')
179/39:
# plot median sale price vs year in which it was built

data.groupby('YearBuilt')['SalePrice'].median().plot()
plt.ylabel('Median House Price')
179/40:
def analyse_year_vars(df, var):
    
    df = df.copy()
    
    # capture difference between a year variable and year
    # in which the house was sold
    df[var] = df['YrSold'] - df[var]
    
    df.groupby('YrSold')[var].median().plot()
    plt.ylabel('Time from ' + var)
    plt.show()
    
    
for var in year_vars:
    if var !='YrSold':
        analyse_year_vars(data, var)
179/41:
def analyse_year_vars(df, var):
    
    df = df.copy()
    
    # capture difference between a year variable and year
    # in which the house was sold
    df[var] = df['YrSold'] - df[var]
    
    plt.scatter(df[var], df['SalePrice'])
    plt.ylabel('SalePrice')
    plt.xlabel(var)
    plt.show()
    
    
for var in year_vars:
    if var !='YrSold':
        analyse_year_vars(data, var)
179/42:
#  let's male a list of discrete variables
discrete_vars = [var for var in num_vars if len(
    data[var].unique()) < 20 and var not in year_vars]


print('Number of discrete variables: ', len(discrete_vars))
179/43:
# let's visualise the discrete variables

data[discrete_vars].head()
179/44:
for var in discrete_vars:
    # make boxplot with Catplot
    sns.catplot(x=var, y='SalePrice', data=data, kind="box", height=4, aspect=1.5)
    # add data points to boxplot with stripplot
    sns.stripplot(x=var, y='SalePrice', data=data, jitter=0.1, alpha=0.3, color='k')
    plt.show()
179/45:
# make list of continuous variables
cont_vars = [
    var for var in num_vars if var not in discrete_vars+year_vars]

print('Number of continuous variables: ', len(cont_vars))
179/46:
# let's visualise the continuous variables

data[cont_vars].head()
179/47:
# lets plot histograms for all continuous variables

data[cont_vars].hist(bins=30, figsize=(15,15))
plt.show()
179/48:
# first make a list with the super skewed variables
# for later

skewed = [
    'BsmtFinSF2', 'LowQualFinSF', 'EnclosedPorch',
    '3SsnPorch', 'ScreenPorch', 'MiscVal'
]
179/49:
# capture the remaining continuous variables

cont_vars = [
    'LotFrontage',
    'LotArea',
    'MasVnrArea',
    'BsmtFinSF1',
    'BsmtUnfSF',
    'TotalBsmtSF',
    '1stFlrSF',
    '2ndFlrSF',
    'GrLivArea',
    'GarageArea',
    'WoodDeckSF',
    'OpenPorchSF',
]
179/50:
# Let's go ahead and analyse the distributions of the variables
# after applying a yeo-johnson transformation

# temporary copy of the data
tmp = data.copy()

for var in cont_vars:

    # transform the variable - yeo-johsnon
    tmp[var], param = stats.yeojohnson(data[var])

    
# plot the histograms of the transformed variables
tmp[cont_vars].hist(bins=30, figsize=(15,15))
plt.show()
179/51: from scipy import stats
179/52:
# Let's go ahead and analyse the distributions of the variables
# after applying a yeo-johnson transformation

# temporary copy of the data
tmp = data.copy()

for var in cont_vars:

    # transform the variable - yeo-johsnon
    tmp[var], param = stats.yeojohnson(data[var])

    
# plot the histograms of the transformed variables
tmp[cont_vars].hist(bins=30, figsize=(15,15))
plt.show()
179/53:
# Let's go ahead and analyse the distributions of the variables
# after applying a yeo-johnson transformation

# temporary copy of the data
tmp = data.copy()

for var in cont_vars:

    # transform the variable - yeo-johsnon
    x = np.array(data[var])
    tmp[var], param = stats.yeojohnson(x)

    
# plot the histograms of the transformed variables
tmp[cont_vars].hist(bins=30, figsize=(15,15))
plt.show()
179/54: data[var]
179/55: data[var].describe()
179/56:
# Let's go ahead and analyse the distributions of the variables
# after applying a yeo-johnson transformation

# temporary copy of the data
tmp = data.copy()

for var in cont_vars:

    # transform the variable - yeo-johsnon
    x = np.array(data[var])
    tmp[var], param = stats.yeojohnson(x, lambda=1)

    
# plot the histograms of the transformed variables
tmp[cont_vars].hist(bins=30, figsize=(15,15))
plt.show()
179/57:
# Let's go ahead and analyse the distributions of the variables
# after applying a yeo-johnson transformation

# temporary copy of the data
tmp = data.copy()

for var in cont_vars:

    # transform the variable - yeo-johsnon
    x = np.array(data[var])
    tmp[var], param = stats.yeojohnson(x=x, lambda=1)

    
# plot the histograms of the transformed variables
tmp[cont_vars].hist(bins=30, figsize=(15,15))
plt.show()
179/58:
# Let's go ahead and analyse the distributions of the variables
# after applying a yeo-johnson transformation

# temporary copy of the data
tmp = data.copy()

for var in cont_vars:

    # transform the variable - yeo-johsnon
    x = np.array(data[var])
    tmp[var], param = stats.yeojohnson(x=x, lambda==1)

    
# plot the histograms of the transformed variables
tmp[cont_vars].hist(bins=30, figsize=(15,15))
plt.show()
179/59:
# Let's go ahead and analyse the distributions of the variables
# after applying a yeo-johnson transformation

# temporary copy of the data
tmp = data.copy()

for var in cont_vars:

    # transform the variable - yeo-johsnon
    x = np.array(data[var])
    tmp[var], param = stats.yeojohnson(x=x, lambda=1)

    
# plot the histograms of the transformed variables
tmp[cont_vars].hist(bins=30, figsize=(15,15))
plt.show()
179/60:
# Let's go ahead and analyse the distributions of the variables
# after applying a yeo-johnson transformation

# temporary copy of the data
tmp = data.copy()

for var in cont_vars:

    # transform the variable - yeo-johsnon
    x = np.array(data[var])
    tmp[var], param = stats.yeojohnson(x=x, lambda = 1)

    
# plot the histograms of the transformed variables
tmp[cont_vars].hist(bins=30, figsize=(15,15))
plt.show()
179/61:
# Let's go ahead and analyse the distributions of the variables
# after applying a yeo-johnson transformation

# temporary copy of the data
tmp = data.copy()

for var in cont_vars:

    # transform the variable - yeo-johsnon
    x = np.array(data[var])
    tmp[var], param = stats.yeojohnson(x=x, lambda = 1)

    
# plot the histograms of the transformed variables
tmp[cont_vars].hist(bins=30, figsize=(15,15))
plt.show()
179/62:
# Let's go ahead and analyse the distributions of the variables
# after applying a yeo-johnson transformation

# temporary copy of the data
tmp = data.copy()

for var in cont_vars:

    # transform the variable - yeo-johsnon
    x = np.array(data[var])
    tmp[var], param = stats.yeojohnson(x=x, lmbda = 1)

    
# plot the histograms of the transformed variables
tmp[cont_vars].hist(bins=30, figsize=(15,15))
plt.show()
179/63:
# Let's go ahead and analyse the distributions of the variables
# after applying a yeo-johnson transformation

# temporary copy of the data
tmp = data.copy()

for var in cont_vars:

    # transform the variable - yeo-johsnon
    x = np.array(data[var])
    tmp[var], param = stats.yeojohnson(x, 1)

    
# plot the histograms of the transformed variables
tmp[cont_vars].hist(bins=30, figsize=(15,15))
plt.show()
179/64:
# Let's go ahead and analyse the distributions of the variables
# after applying a yeo-johnson transformation

# temporary copy of the data
tmp = data.copy()

for var in cont_vars:

    # transform the variable - yeo-johsnon
    x = np.array(data[var])
    tmp[var], param = stats.yeojohnson(x, lmbda=1)

    
# plot the histograms of the transformed variables
tmp[cont_vars].hist(bins=30, figsize=(15,15))
plt.show()
179/65:
# Let's go ahead and analyse the distributions of the variables
# after applying a yeo-johnson transformation

# temporary copy of the data
tmp = data.copy()

for var in cont_vars:

    # transform the variable - yeo-johsnon
    x = np.array(data[var])
    tmp[var], param = stats.yeojohnson(x)

    
# plot the histograms of the transformed variables
tmp[cont_vars].hist(bins=30, figsize=(15,15))
plt.show()
179/66:
# Let's go ahead and analyse the distributions of the variables
# after applying a yeo-johnson transformation

# temporary copy of the data
tmp = data.copy()

for var in cont_vars:

    # transform the variable - yeo-johsnon
    x = np.array(data[var])
    tmp[var], param = stats.iBoxCox(x)

    
# plot the histograms of the transformed variables
tmp[cont_vars].hist(bins=30, figsize=(15,15))
plt.show()
179/67:
# Let's go ahead and analyse the distributions of the variables
# after applying a yeo-johnson transformation

# temporary copy of the data
tmp = data.copy()

for var in cont_vars:

    # transform the variable - yeo-johsnon
    x = np.array(data[var])
    tmp[var], param = stats.boxCox(x)

    
# plot the histograms of the transformed variables
tmp[cont_vars].hist(bins=30, figsize=(15,15))
plt.show()
179/68:
# Let's go ahead and analyse the distributions of the variables
# after applying a yeo-johnson transformation

# temporary copy of the data
tmp = data.copy()

for var in cont_vars:

    # transform the variable - yeo-johsnon
    x = np.array(data[var])
    tmp[var], param = stats.boxcox(x)

    
# plot the histograms of the transformed variables
tmp[cont_vars].hist(bins=30, figsize=(15,15))
plt.show()
179/69: cont_vars
179/70:
for var in cont_vars:
    print(data[var].describe())
179/71:
# Let's go ahead and analyse the distributions of the variables
# after applying a yeo-johnson transformation

# temporary copy of the data
tmp = data.copy()

for var in cont_vars:

    # transform the variable - yeo-johsnon
    x = np.array(data[var]) + 0.001
    tmp[var], param = stats.boxcox(x)

    
# plot the histograms of the transformed variables
tmp[cont_vars].hist(bins=30, figsize=(15,15))
plt.show()
179/72:
# Let's go ahead and analyse the distributions of the variables
# after applying a yeo-johnson transformation

# temporary copy of the data
tmp = data.copy()

for var in cont_vars:

    # transform the variable - yeo-johsnon
    x = np.array(data[var]) + 0.001
    tmp[var], param = stats.yeojohnson(x)

    
# plot the histograms of the transformed variables
tmp[cont_vars].hist(bins=30, figsize=(15,15))
plt.show()
179/73:
# Let's go ahead and analyse the distributions of the variables
# after applying a yeo-johnson transformation

# temporary copy of the data
tmp = data.copy()

for var in cont_vars:

    # transform the variable - yeo-johsnon
    x = np.array(data[var]) + 0.00001
    tmp[var], param = stats.yeojohnson(x)

    
# plot the histograms of the transformed variables
tmp[cont_vars].hist(bins=30, figsize=(15,15))
plt.show()
179/74:
# let's plot the original or transformed variables
# vs sale price, and see if there is a relationship

for var in cont_vars:
    
    plt.figure(figsize=(12,4))
    
    # plot the original variable vs sale price    
    plt.subplot(1, 2, 1)
    plt.scatter(data[var], np.log(data['SalePrice']))
    plt.ylabel('Sale Price')
    plt.xlabel('Original ' + var)

    # plot transformed variable vs sale price
    plt.subplot(1, 2, 2)
    plt.scatter(tmp[var], np.log(tmp['SalePrice']))
    plt.ylabel('Sale Price')
    plt.xlabel('Transformed ' + var)
                
    plt.show()
179/75:
# Let's go ahead and analyse the distributions of these variables
# after applying a logarithmic transformation

tmp = data.copy()

for var in ["LotFrontage", "1stFlrSF", "GrLivArea"]:

    # transform the variable with logarithm
    tmp[var] = np.log(data[var])
    
tmp[["LotFrontage", "1stFlrSF", "GrLivArea"]].hist(bins=30)
plt.show()
179/76:
# let's plot the original or transformed variables
# vs sale price, and see if there is a relationship

for var in ["LotFrontage", "1stFlrSF", "GrLivArea"]:
    
    plt.figure(figsize=(12,4))
    
    # plot the original variable vs sale price    
    plt.subplot(1, 2, 1)
    plt.scatter(data[var], np.log(data['SalePrice']))
    plt.ylabel('Sale Price')
    plt.xlabel('Original ' + var)

    # plot transformed variable vs sale price
    plt.subplot(1, 2, 2)
    plt.scatter(tmp[var], np.log(tmp['SalePrice']))
    plt.ylabel('Sale Price')
    plt.xlabel('Transformed ' + var)
                
    plt.show()
179/77: skewed
179/78:
for var in skewed:
    
    tmp = data.copy()
    
    # map the variable values into 0 and 1
    tmp[var] = np.where(data[var]==0, 0, 1)
    
    # determine mean sale price in the mapped values
    tmp = tmp.groupby(var)['SalePrice'].agg(['mean', 'std'])

    # plot into a bar graph
    tmp.plot(kind="barh", y="mean", legend=False,
             xerr="std", title="Sale Price", color='green')

    plt.show()
179/79: print('Number of categorical variables: ', len(cat_vars))
179/80:
# let's visualise the values of the categorical variables
data[cat_vars].head()
179/81:
# we count unique categories with pandas unique() 
# and then plot them in descending order

data[cat_vars].nunique().sort_values(ascending=False).plot.bar(figsize=(12,5))
179/82:
# re-map strings to numbers, which determine quality

qual_mappings = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5, 'Missing': 0, 'NA': 0}

qual_vars = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond',
             'HeatingQC', 'KitchenQual', 'FireplaceQu',
             'GarageQual', 'GarageCond',
            ]

for var in qual_vars:
    data[var] = data[var].map(qual_mappings)
179/83:
exposure_mappings = {'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4, 'Missing': 0, 'NA': 0}

var = 'BsmtExposure'

data[var] = data[var].map(exposure_mappings)
179/84:
finish_mappings = {'Missing': 0, 'NA': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6}

finish_vars = ['BsmtFinType1', 'BsmtFinType2']

for var in finish_vars:
    data[var] = data[var].map(finish_mappings)
179/85:
garage_mappings = {'Missing': 0, 'NA': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3}

var = 'GarageFinish'

data[var] = data[var].map(garage_mappings)
179/86:
fence_mappings = {'Missing': 0, 'NA': 0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv': 4}

var = 'Fence'

data[var] = data[var].map(fence_mappings)
179/87:
# capture all quality variables

qual_vars  = qual_vars + finish_vars + ['BsmtExposure','GarageFinish','Fence']
179/88:
# now let's plot the house mean sale price based on the quality of the 
# various attributes

for var in qual_vars:
    # make boxplot with Catplot
    sns.catplot(x=var, y='SalePrice', data=data, kind="box", height=4, aspect=1.5)
    # add data points to boxplot with stripplot
    sns.stripplot(x=var, y='SalePrice', data=data, jitter=0.1, alpha=0.3, color='k')
    plt.show()
179/89:
# capture the remaining categorical variables
# (those that we did not re-map)

cat_others = [
    var for var in cat_vars if var not in qual_vars
]

len(cat_others)
179/90:
def analyse_rare_labels(df, var, rare_perc):
    df = df.copy()

    # determine the % of observations per category
    tmp = df.groupby(var)['SalePrice'].count() / len(df)

    # return categories that are rare
    return tmp[tmp < rare_perc]

# print categories that are present in less than
# 1 % of the observations

for var in cat_others:
    print(analyse_rare_labels(data, var, 0.01))
    print()
179/91:
for var in cat_others:
    # make boxplot with Catplot
    sns.catplot(x=var, y='SalePrice', data=data, kind="box", height=4, aspect=1.5)
    # add data points to boxplot with stripplot
    sns.stripplot(x=var, y='SalePrice', data=data, jitter=0.1, alpha=0.3, color='k')
    plt.show()
182/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import scipy.stats as stats
pd.pandas.set_option('display.max_columns', None)
182/2:
data = pd.read_csv('train.csv')
print(data.shape)
print(data.head(5))
182/3:
data = pd.read_csv('train.csv')
print(data.shape)
data.head(5)
182/4:
data.drop('Id', axis=1, inplace=True)
data.shape
182/5: data
182/6: data.SalePrice.hist(bins=30)
182/7: data.SalePrice.hist(bins=30, density=True)
182/8:
data.SalePrice.hist(bins=50, density=True)
plt.xlabel('sale Price')
plt.ylabel('Number of houses')
plt.show()
182/9:
np.log(data.SalePrice).hist(bins=50, density=True)
plt.xlabel('sale Price')
plt.ylabel('Number of houses')
plt.show()
182/10: data.dtypes
182/11:
# let's identify the categorical variables
# we will capture those of type *object*

cat_vars = [var for var in data.columns if data[var].dtype == 'O']

# MSSubClass is also categorical by definition, despite its numeric values
# (you can find the definitions of the variables in the data_description.txt
# file available on Kaggle, in the same website where you downloaded the data)

# lets add MSSubClass to the list of categorical variables


# number of categorical variables
182/12: cat_vars
182/13: len(cat_vars)
182/14:
# let's identify the categorical variables
# we will capture those of type *object*

cat_vars = [var for var in data.columns if data[var].dtype == 'O']

# MSSubClass is also categorical by definition, despite its numeric values
# (you can find the definitions of the variables in the data_description.txt
# file available on Kaggle, in the same website where you downloaded the data)

# lets add MSSubClass to the list of categorical variables
cat_vars += ['MSSubClass']

# number of categorical variables
print('len of cat list: ', len(cat_vars))
182/15:
# cast all variables as categorical
data[cat_vars] = data[cat_vars].astype('O')
182/16:
# now let's identify the numerical variables
num_vars = [var for var in data.columns if var not in cat_vars and var != 'SalesPrice']
# number of numerical variables
len(num_vars)
182/17:
# now let's identify the numerical variables
num_vars = [var for var in data.columns if var not in cat_vars and var != 'SalePrice']
# number of numerical variables
len(num_vars)
182/18:
# make a list of the variables that contain missing values

vars_with_na = [var for var in data.columns if data[var].isnull().sum() > 0]

# determine percentage of missing values (expressed as decimals)
# and display the result ordered by % of missin data
data[vars_with_na].isnull().mean().sort_values(ascending = False)
182/19:
# plot
data[vars_with_na].isnull().mean().plot.bar()
182/20:
# plot
data[vars_with_na].isnull().mean().sort_values(ascending=False).plot.bar()
182/21:
# plot
data[vars_with_na].isnull().mean().sort_values(ascending=False).plot.bar(figsize = (10, 6))
182/22:
# plot
data[vars_with_na].isnull().mean().sort_values(ascending=False).plot.bar(figsize = (12, 6))
182/23:
# plot
data[vars_with_na].isnull().mean().sort_values(ascending=False).plot.bar(figsize = (12, 4))
182/24:
# plot
data[vars_with_na].isnull().mean().sort_values(ascending=False).plot.bar(figsize=(12, 4))
plt.ylabel('Percentage of missing data')
plt.axhline(y=.8, color='g', linestyle='-')
plt.axhline(y=.9, color='r', linestyle=':')
182/25:
cat_na = [var for var in vars_with_na if var in cat_vars]
num_na = [var for var in vars_with_na if var in num_vars]
print(len(cat_vars))
print(len(num_vars))
182/26: vars_with_na
182/27: len(vars_with_na)
182/28:
cat_na = [var for var in cat_vars if var in vars_with_na]
num_na = [var for var in num_vars if var in vars_with_na]
print(len(cat_vars))
print(len(num_vars))
182/29:
cat_na = [var for var in vars_with_na if var in cat_vars]
num_na = [var for var in vars_with_na if var in num_vars]
print(len(cat_na))
print(len(num_na))
182/30:
cat_na = [var for var in vars_with_na if var in cat_vars]
num_na = [var for var in vars_with_na if var in num_vars]
print('Number of categorical variables with na', len(cat_na))
print('Number of numerical variables with na', len(num_na))
182/31:
cat_na = [var for var in vars_with_na if var in cat_vars]
num_na = [var for var in vars_with_na if var in num_vars]
print('Number of categorical variables with na:', len(cat_na))
print('Number of numerical variables with na:', len(num_na))
182/32: num_na
182/33: cat_na
182/34:
def analyse_na_value(df, var):
    df = df.copy()
    df[var] = np.where(df[var].isnull(), 1, 0)
    tmp = df.groupby(var)['SalePrice'].agg(['mean', 'std'])
    tmp.plot(kind='barh', y='mean', legend=False, xerr='std', title='Sale Price')
    
    plt.show()
182/35:
# let's run the function on each variable with missing data

for var in vars_with_na:
    analyse_na_value(data, var)
182/36: num_vars
182/37: len(num_vars)
182/38:
print('Number of numerical variables: ', len(num_vars))

# visualise the numerical variables
print(len(num_vars))
data[num_vars].head()
182/39: year_vars = [var for var in data.columns if data.columns.contains('Ye', 'Yr')]
182/40: year_vars = [var for var in data.columns if data.columns.contains('Ye'|'Yr')]
182/41: year_vars = [var for var in data.columns if data.columns.str.contains('Ye'|'Yr')]
182/42: year_vars = [var for var in data.columns if data.columns.str.contains('Ye|Yr')]
182/43: data.columns.str.contains('Ye|Yr')
182/44: year_vars = [var for var in data.columns if data.columns.str.contains('Ye|Yr') = True]
182/45: year_vars = [var for var in data.columns if data.columns.str.contains('Ye|Yr') == True]
182/46: year_vars = [var for var in data.columns if data[var].str.contains('Ye|Yr')]
182/47: data.columns
182/48: data.LotShape
182/49: data.LotShape.columns
182/50: data.LotShape.value_countses
182/51: data.LotShape.values
182/52: data.LotShape
182/53: year_vars = [var for var in data.columns if var.str.contains('Ye|Yr')]
182/54: LotShape.str.contains('Ye|Yr')
182/55: year_vars = [var for var in data.columns if var.contains('Ye|Yr')]
182/56: year_vars = [var for var in data.columns if 'Yr' in var or 'Year' in var]
182/57:
# list of variables that contain year information
year_vars
182/58:
# let's explore the values of these temporal variables

for var in year_vars:
    print(var, data[var].unique())
    print()
182/59: data.groupby('YrSold')['SalePrice'].median().plot()
182/60:
data.groupby('YrSold')['SalePrice'].median().plot()
plt.ylabel('Median House Price')
182/61: data.groupby('YearBuilt').median().plot()
182/62: data.groupby('YearBuilt')['SalePrice'].median().plot()
182/63:
data.groupby('YearBuilt')['SalePrice'].median().plot()
plt.ylabel('Median House Price')
182/64:
def analyse_year_vars(df, var):
    df_years = data['YrSold'] - df[var]
    df_years.plot()

    
    
for var in year_vars:
    if var !='YrSold':
        analyse_year_vars(data, var)
182/65:
def analyse_year_vars(df, var):
    df_years = data['YrSold'] - df[var]
    df_years.plot()
    plt.show()

    
    
for var in year_vars:
    if var !='YrSold':
        analyse_year_vars(data, var)
182/66:
def analyse_year_vars(df, var):
    df_years = data['YrSold'] - df[var]
    plt.plot(x='YrSold', var, data=df)
    plt.show()

    
    
for var in year_vars:
    if var !='YrSold':
        analyse_year_vars(data, var)
182/67:
def analyse_year_vars(df, var):
    df_years = data['YrSold'] - df[var]
    plt.plot(x='YrSold', y=var, data=df)
    plt.show()

    
    
for var in year_vars:
    if var !='YrSold':
        analyse_year_vars(data, var)
182/68:
def analyse_year_vars(df, var):
    df = df.copy()
    df[var] = df['YrSold'] - df[var]
    df[var].groupby('YrSold')['SalePrice'].median().plot()
    plt.show()

    
    
for var in year_vars:
    if var !='YrSold':
        analyse_year_vars(data, var)
182/69:
def analyse_year_vars(df, var):
    df = df.copy()
    df[var] = df['YrSold'] - df[var]
    df.groupby('YrSold')['SalePrice'].median().plot()
    plt.show()

    
    
for var in year_vars:
    if var !='YrSold':
        analyse_year_vars(data, var)
182/70:
def analyse_year_vars(df, var):
    df = df.copy()
    df[var] = df['YrSold'] - df[var]
    df.groupby('YrSold')[var].median().plot()
    plt.show()

    
    
for var in year_vars:
    if var !='YrSold':
        analyse_year_vars(data, var)
182/71:
def analyse_year_vars(df, var):
    df = df.copy()
    df[var] = df['YrSold'] - df[var]
    df.groupby('YrSold')[var].median().plot()
    plt.ylabel(var)
    plt.show()

    
    
for var in year_vars:
    if var !='YrSold':
        analyse_year_vars(data, var)
182/72:
def analyse_year_vars(df, var):
    plt.scatter(var, 'SalePrice')
    
for var in year_vars:
    if var !='YrSold':
        analyse_year_vars(data, var)
182/73:
def analyse_year_vars(df, var):
    df = df.copy()
    plt.scatter(var, 'SalePrice', data=df)
    plt.show()
    
for var in year_vars:
    if var !='YrSold':
        analyse_year_vars(data, var)
182/74:
def analyse_year_vars(df, var):
    df = df.copy()
    plt.scatter(var, 'SalePrice', data=df)
    plt.xlabel(var)
    plt.show()
    
for var in year_vars:
    if var !='YrSold':
        analyse_year_vars(data, var)
182/75:
def analyse_year_vars(df, var):
    df = df.copy()
    df[var] = data['YrSold'] - df[var]
    plt.scatter(var, 'SalePrice', data=df)
    plt.xlabel(var)
    plt.show()
    
for var in year_vars:
    if var !='YrSold':
        analyse_year_vars(data, var)
182/76:
def analyse_year_vars(df, var):
    
    
for var in year_vars:
    if var !='YrSold':
        analyse_year_vars(data, var)
182/77:
def analyse_year_vars(df, var):
    
    
for var in year_vars:
    if var !='YrSold':
        analyse_year_vars(data, var)
182/78: data.nunique()
182/79: descrete_vars = [var for var in data.columns if data[var].nunique() < 20 and var not in year_vars]
182/80: descrete_vars
182/81: len(descrete_vars)
182/82: descrete_vars = [var for var in num_vars if data[var].nunique() < 20 and var not in year_vars]
182/83: len(descrete_vars)
182/84:
descrete_vars = [var for var in num_vars if data[var].nunique() < 20 and var not in year_vars]
print('Number of discrete variables: ', len(descrete_vars))
179/92:
#  let's male a list of discrete variables
discrete_vars = [var for var in num_vars if len(
    data[var].unique()) < 20 and var not in year_vars]


print('Number of discrete variables: ', len(discrete_vars))
182/85: data[discrete_vars].head()
182/86:
discrete_vars = [var for var in num_vars if data[var].nunique() < 20 and var not in year_vars]
print('Number of discrete variables: ', len(descrete_vars))
182/87:
discrete_vars = [var for var in num_vars if data[var].nunique() < 20 and var not in year_vars]
print('Number of discrete variables: ', len(discrete_vars))
182/88: data[discrete_vars].head()
182/89:
for var in discrete_vars:
    # make boxplot with Catplot
    sns.catplot(x=var, y='SalePrice', data=data)
    # add data points to boxplot with stripplot

    plt.show()
182/90:
for var in discrete_vars:
    # make boxplot with Catplot
    sns.catplot(x=var, y='SalePrice', data=data, kind='box')
    # add data points to boxplot with stripplot

    plt.show()
182/91:
for var in discrete_vars:
    # make boxplot with Catplot
    sns.catplot(x=var, y='SalePrice', data=data, kind='swarm')
    # add data points to boxplot with stripplot

    plt.show()
182/92:
for var in discrete_vars:
    # make boxplot with Catplot
    sns.catplot(x=var, y='SalePrice', data=data, kind='box')
    # add data points to boxplot with stripplot

    plt.show()
182/93:
for var in discrete_vars:
    # make boxplot with Catplot
    sns.catplot(x=var, y='SalePrice', data=data, kind='box', height=4)
    # add data points to boxplot with stripplot

    plt.show()
182/94:
for var in discrete_vars:
    # make boxplot with Catplot
    sns.catplot(x=var, y='SalePrice', data=data, kind='box', height=2)
    # add data points to boxplot with stripplot

    plt.show()
182/95:
for var in discrete_vars:
    # make boxplot with Catplot
    sns.catplot(x=var, y='SalePrice', data=data, kind='box', height=5)
    # add data points to boxplot with stripplot

    plt.show()
182/96:
for var in discrete_vars:
    # make boxplot with Catplot
    sns.catplot(x=var, y='SalePrice', data=data, kind='box', height=5)
    # add data points to boxplot with stripplot

    sns.stripplot(x=var, y='SalePrice', data=data)
    plt.show()
182/97:
for var in discrete_vars:
    # make boxplot with Catplot
    sns.catplot(x=var, y='SalePrice', data=data, kind='box', height=5)
    # add data points to boxplot with stripplot

    sns.stripplot(x=var, y='SalePrice', data=data, jitter=.1, alpha=.3)
    plt.show()
182/98: cont_values = [var for var in num_vars if var not in year_vars and discrete_vars ]
182/99: cont_vars = [var for var in num_vars if var not in year_vars and discrete_vars ]
182/100:
# make list of continuous variables

print('Number of continuous variables: ', len(cont_vars))
182/101: cont_vars = [var for var in num_vars if var not in year_vars + discrete_vars ]
182/102:
# make list of continuous variables

print('Number of continuous variables: ', len(cont_vars))
182/103: year_vars + discrete_vars
182/104:
cont_vars = [var for var in num_vars if var not in year_vars + discrete_vars ]
# make list of continuous variables

print('Number of continuous variables: ', len(cont_vars))
182/105:
# let's visualise the continuous variables

data[cont_vars].head()
182/106:
# lets plot histograms for all continuous variables
data[cont_vars].hist(bins=30)
182/107:
# lets plot histograms for all continuous variables
data[cont_vars].hist(bins=30, figsize=(15, 15))
plt.show()
182/108:
# first make a list with the super skewed variables
skewed = [
    'BsmtFinSF2', 'LowQualFinSF', 'EnclosedPorch',
    '3SsnPorch', 'ScreenPorch', 'MiscVal'
]
182/109:
# capture the remaining continuous variables
cont_vars
182/110:
# capture the remaining continuous variables
cont_vars - skewed
182/111:
# capture the remaining continuous variables
list(set(cont_vars) - set(skewed))
182/112:
# capture the remaining continuous variables
cont_vars = list(set(cont_vars) - set(skewed))
182/113: from scipy import stats
182/114:
for var in cont_vars:
    print(data[var].describe())
182/115: cont_vars
182/116:
# Let's go ahead and analyse the distributions of the variables
# after applying a yeo-johnson transformation

# temporary copy of the data
tmp = data.copy()

for var in cont_vars:
    x = np.array(data[var]) + 0.0000001
    tmp[var], param = stats.yeojohnson(x)
    
# plot the histograms of the transformed variables
tmp[cont_vars].hist(bins=30, figsize(15, 15))
plt.show()
182/117:
# Let's go ahead and analyse the distributions of the variables
# after applying a yeo-johnson transformation

# temporary copy of the data
tmp = data.copy()

for var in cont_vars:
    x = np.array(data[var]) + 0.0000001
    tmp[var], param = stats.yeojohnson(x)
    
# plot the histograms of the transformed variables
tmp[cont_vars].hist(bins=30, figsize=(15, 15))
plt.show()
182/118:
# let's plot the original or transformed variables
# vs sale price, and see if there is a relationship

for var in cont_vars:
    plt.subplot(2,1,1)
    plt.scatter(x=var, y='SalePrice')
182/119:
# let's plot the original or transformed variables
# vs sale price, and see if there is a relationship

for var in cont_vars:
    plt.subplot(2,1,1)
    plt.scatter(x=var, y='SalePrice')
    
    plt.show()
182/120:
# let's plot the original or transformed variables
# vs sale price, and see if there is a relationship

for var in cont_vars:
    plt.subplot(2,1,1)
    plt.scatter(x=var, y='SalePrice', data=data)
    
    plt.show()
182/121:
# let's plot the original or transformed variables
# vs sale price, and see if there is a relationship

for var in cont_vars:
    plt.figsize(10, 5)
    plt.subplot(2,1,1)
    plt.scatter(x=var, y='SalePrice', data=data)
    
    plt.show()
182/122:
# let's plot the original or transformed variables
# vs sale price, and see if there is a relationship

for var in cont_vars:
    plt.subplot(2,1,1)
    plt.scatter(x=var, y='SalePrice', data=data)
    
    plt.show()
182/123:
# let's plot the original or transformed variables
# vs sale price, and see if there is a relationship

for var in cont_vars:
    plt.figure(figsize(12,4))
    plt.subplot(2,1,1)
    plt.scatter(x=var, y='SalePrice', data=data)
    plt.subplot(2,1,2)
    plt.scatter(x=var, y='SalePrice', data=tmp)
    
    plt.show()
182/124:
# let's plot the original or transformed variables
# vs sale price, and see if there is a relationship

for var in cont_vars:
    plt.figure(figsize=(12,4))
    plt.subplot(2,1,1)
    plt.scatter(x=var, y='SalePrice', data=data)
    plt.subplot(2,1,2)
    plt.scatter(x=var, y='SalePrice', data=tmp)
    
    plt.show()
182/125:
# let's plot the original or transformed variables
# vs sale price, and see if there is a relationship

for var in cont_vars:
    plt.figure(figsize=(12,4))
    
    plt.subplot(2,1,1)
    plt.scatter(x=data[var], y=data['SalePrice'])
    plt.subplot(2,1,2)
    plt.scatter(x=tmp[var], y=tmp['SalePrice'])
    
    plt.show()
182/126:
# let's plot the original or transformed variables
# vs sale price, and see if there is a relationship

for var in cont_vars:
    plt.figure(figsize=(6,4))
    
    plt.subplot(2,1,1)
    plt.scatter(x=data[var], y=data['SalePrice'])
    plt.subplot(2,1,2)
    plt.scatter(x=tmp[var], y=tmp['SalePrice'])
    
    plt.show()
182/127:
# let's plot the original or transformed variables
# vs sale price, and see if there is a relationship

for var in cont_vars:
    plt.figure(figsize=(5,5))
    
    plt.subplot(2,1,1)
    plt.scatter(x=data[var], y=data['SalePrice'])
    plt.subplot(2,1,2)
    plt.scatter(x=tmp[var], y=tmp['SalePrice'])
    
    plt.show()
182/128:
# let's plot the original or transformed variables
# vs sale price, and see if there is a relationship

for var in cont_vars:
    plt.figure(figsize=(5,5))
    
    plt.subplot(1,2,1)
    plt.scatter(x=data[var], y=data['SalePrice'])
    plt.subplot(1,2,2)
    plt.scatter(x=tmp[var], y=tmp['SalePrice'])
    
    plt.show()
182/129:
# let's plot the original or transformed variables
# vs sale price, and see if there is a relationship

for var in cont_vars:
    plt.figure(figsize=(10,5))
    
    plt.subplot(1,2,1)
    plt.scatter(x=data[var], y=data['SalePrice'])
    plt.subplot(1,2,2)
    plt.scatter(x=tmp[var], y=tmp['SalePrice'])
    
    plt.show()
182/130:
# let's plot the original or transformed variables
# vs sale price, and see if there is a relationship

for var in cont_vars:
    plt.figure(figsize=(10,4))
    
    plt.subplot(1,2,1)
    plt.scatter(x=data[var], y=data['SalePrice'])
    plt.xlabel(var)
    plt.subplot(1,2,2)
    plt.scatter(x=tmp[var], y=tmp['SalePrice'])
    plt.xlabel('Transform' + var)
    
    plt.show()
182/131:
# let's plot the original or transformed variables
# vs sale price, and see if there is a relationship

for var in cont_vars:
    plt.figure(figsize=(10,4))
    
    plt.subplot(1,2,1)
    plt.scatter(x=data[var], y=np.log(data['SalePrice']))
    plt.xlabel(var)
    plt.subplot(1,2,2)
    plt.scatter(x=tmp[var], y=np.log(tmp['SalePrice']))
    plt.xlabel('Transform' + var)
    
    plt.show()
182/132: skewed
182/133:
tmp = data.copy()

for var in ["LotFrontage", "1stFlrSF", "GrLivArea"]:
    tmp[var] = np.log(data[var])
    
tmp[["LotFrontage", "1stFlrSF", "GrLivArea"]]
182/134:
tmp = data.copy()

for var in ["LotFrontage", "1stFlrSF", "GrLivArea"]:
    tmp[var] = np.log(data[var])
    
tmp[["LotFrontage", "1stFlrSF", "GrLivArea"]].hist(bins=30)
182/135:
# let's plot the original or transformed variables
# vs sale price, and see if there is a relationship

for var in ["LotFrontage", "1stFlrSF", "GrLivArea"]:
    plt.figure(figsize=(10, 5))
    plt.subplot(1,2,1)
    plt.scatter(x=data[var], y=np.log(data.SalePrice))
    plt.xlabel(var)
    plt.subplot(1,2,2)
    plt.scatter(x=tmp[var], y=np.log(tmp.SalePrice))
    plt.xlabel('Transformed' + var)
                
    plt.show()
182/136: skewed
182/137:
for var in skewed:
    tmp = data.copy()
    tmp[var] = np.where(data[var] == 0, 1)
    tmp.groupby(var)['SalePrise'].agg(['mean', 'std'])
    
    tmp.plot(kind='barh', y='mean', xerr='std')
    plt.show()
182/138:
for var in skewed:
    tmp = data.copy()
    tmp[var] = np.where(data[var] == 0, 0, 1)
    tmp.groupby(var)['SalePrise'].agg(['mean', 'std'])
    
    tmp.plot(kind='barh', y='mean', xerr='std')
    plt.show()
182/139:
for var in skewed:
    tmp = data.copy()
    tmp[var] = np.where(data[var] == 0, 0, 1)
    tmp = tmp.groupby(var)['SalePrise'].agg(['mean', 'std'])
    
    tmp.plot(kind='barh', y='mean', xerr='std')
    plt.show()
182/140:
for var in skewed:
    tmp = data.copy()
    tmp[var] = np.where(data[var] == 0, 0, 1)
    tmp = tmp.groupby(var)['SalePrice'].agg(['mean', 'std'])
    
    tmp.plot(kind='barh', y='mean', xerr='std')
    plt.show()
182/141:
for var in skewed:
    tmp = data.copy()
    tmp[var] = np.where(data[var] == 0, 0, 1)
    tmp = tmp.groupby(var)['SalePrice'].agg(['mean', 'std'])
    
    tmp.plot(kind='barh', y='mean', xerr='std', legend=False)
    plt.show()
182/142: print('Number of categorical variables: ', len(cat_vars))
182/143:
# let's visualise the values of the categorical variables
data[cat_vars].head()
182/144: data[cat_vars].nunique().sort_values(ascending = False).plot.bar(figsize=(12,6))
182/145:
data[cat_vars].nunique().sort_values(ascending = False).plot.bar(figsize=(12,6))
plt.axhline(y=20)
182/146:
exposure_mappings = {'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4, 'Missing': 0, 'NA': 0}

var = 'BsmtExposure'

data[var] = data[var].map(exposure_mappings)
182/147:
# re-map strings to numbers, which determine quality

qual_mappings = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5, 'Missing': 0, 'NA': 0}

qual_vars = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond',
             'HeatingQC', 'KitchenQual', 'FireplaceQu',
             'GarageQual', 'GarageCond',
            ]

for var in qual_vars:
    data[var] = data[var].map(qual_mappings)
182/148:
finish_mappings = {'Missing': 0, 'NA': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6}

finish_vars = ['BsmtFinType1', 'BsmtFinType2']

for var in finish_vars:
    data[var] = data[var].map(finish_mappings)
182/149:
garage_mappings = {'Missing': 0, 'NA': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3}

var = 'GarageFinish'

data[var] = data[var].map(garage_mappings)
182/150:
fence_mappings = {'Missing': 0, 'NA': 0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv': 4}

var = 'Fence'

data[var] = data[var].map(fence_mappings)
182/151:
# capture all quality variables

qual_vars  = qual_vars + finish_vars + ['BsmtExposure','GarageFinish','Fence']
182/152:
# now let's plot the house mean sale price based on the quality of the 
# various attributes

for var in qual_vars:
    # make boxplot with Catplot
    sns.catplot(x=var)

    # add data points to boxplot with stripplot

    plt.show()
182/153: qual_vars
182/154:
# now let's plot the house mean sale price based on the quality of the 
# various attributes

for var in qual_vars:
    # make boxplot with Catplot
    sns.catplot(x=var, y='SalePrice', data=data, kind='box')

    # add data points to boxplot with stripplot
    sns.stripplot(x=var, y='SalePrice', data=data)

    plt.show()
182/155:
# capture the remaining categorical variables
# (those that we did not re-map)

cat_others = [
    var for var in cat_vars if var not in qual_vars
]

len(cat_others)
182/156:
def analyse_rare_labels(df, var, rare_perc):
    df = df.copy()

    # determine the % of observations per category
    tmp = df.groupby(var)['SalePrice'].count()/len(df)

    # return categories that are rare
    return tmp[tmp < rare_perc]


# print categories that are present in less than
# 1 % of the observations

for var in cat_others:
    print(analyse_rare_labels(data, var, 0.01))
    print()
189/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import scipy.stats as ststa

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

import jobplot
pd.pandas.set_option('display.max_columns', None)
189/2:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import scipy.stats as ststa

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

import joblib
pd.pandas.set_option('display.max_columns', None)
189/3:
data = pd.read_csv('train.csv')
print(data.shape)
189/4:
# Let's separate into train and test set
# Remember to set the seed (random_state for this sklearn function)

X_train, X_test, y_train, y_test = train_test_split(
    data.drop(['Id', 'SalePrice'], axis=1), # predictive variables
    data['SalePrice'], # target
    test_size=0.1, # portion of dataset to allocate to test set
    random_state=0, # we are setting the seed here
)

X_train.shape, X_test.shape
189/5:
y_train = np.log(y_train)
y_test = np.log(y_test)
189/6:
# let's identify the categorical variables
# we will capture those of type object

cat_vars = [var for var in data.columns if data[var].dtype == 'O']

# MSSubClass is also categorical by definition, despite its numeric values
# (you can find the definitions of the variables in the data_description.txt
# file available on Kaggle, in the same website where you downloaded the data)

# lets add MSSubClass to the list of categorical variables
cat_vars = cat_vars + ['MSSubClass']

# cast all variables as categorical
X_train[cat_vars] = X_train[cat_vars].astype('O')
X_test[cat_vars] = X_test[cat_vars].astype('O')

# number of categorical variables
len(cat_vars)
189/7:
cat_vars_with_na = [var for var in cat_vars if X_train[var].isnull().sum() > 0]

X_train[cat_vars_with_na].isnull().mean().sort_values(ascending=False)
189/8:
with_string_missing = [var for var in cat_vars_with_na if X_train[var].isnull().mean() > .1]
with_freq_missing = [var for var in cat_vars_with_na if X_train[var].isnull().mean() <= .1]
189/9: with_string_missing
189/10:
with_string_missing = [var for var in cat_vars_with_na if X_train[var].isnull().mean() > .1]
with_freq_category = [var for var in cat_vars_with_na if X_train[var].isnull().mean() <= .1]
189/11: with_string_missing
189/12:
X_train[with_string_missing] = X_train[with_string_missing].fillna('Missing')
X_test[with_string_missing] = X_test[with_string_missing].fillna('Missing')
189/13:
for var in with_freq_category:
    mode = X_train[var].mode()
    print(mode)
189/14:
for var in with_freq_category:
    mode = X_train[var].mode()[0]
    print(mode)
189/15:
for var in with_freq_category:
    mode = X_train[var].mode()[0]
    print(var, ':', mode)
    
    X_train[var].fillna(mode, inplace=True)
    X_test[var].fillna(mode, inplace=True)
189/16: X_train.isnull().mean()
189/17:
# make a list of the categorical variables that contain missing values

cat_vars_with_na = [
    var for var in cat_vars
    if X_train[var].isnull().sum() > 0
]

# print percentage of missing values per variable
X_train[cat_vars_with_na ].isnull().mean().sort_values(ascending=False)
189/18:

X_train[cat_vars_with_na].isnull().sum()
189/19:
for var in with_freq_category:
    mode = X_train[var].mode()[0]
    print(var, ':', mode)
    
    X_train[var].fillna(mode, inplace=True)
    X_test[var].fillna(mode, inplace=True)
189/20:

X_train[cat_vars_with_na].isnull().sum()
189/21:

X_train[cat_vars_with_na]
189/22: cat_vars_with_na
189/23:
# now let's identify the numerical variables

num_vars = [
    var for var in X_train.columns if var not in cat_vars and var != 'SalePrice'
]

# number of numerical variables
len(num_vars)
189/24:
# make a list with the numerical variables that contain missing values
vars_with_na = [
    var for var in num_vars
    if X_train[var].isnull().sum() > 0
]

# print percentage of missing values per variable
X_train[vars_with_na].isnull().mean()
189/25:
# replace missing values as we described above

for var in vars_with_na:

    # calculate the mean using the train set
    mean_val = X_train[var].mean()
    
    print(var, mean_val)

    # add binary missing indicator (in train and test)
    X_train[var + '_na'] = np.where(X_train[var].isnull(), 1, 0)
    X_test[var + '_na'] = np.where(X_test[var].isnull(), 1, 0)

    # replace missing values by the mean
    # (in train and test)
    X_train[var].fillna(mean_val, inplace=True)
    X_test[var].fillna(mean_val, inplace=True)

# check that we have no more missing values in the engineered variables
X_train[vars_with_na].isnull().sum()
189/26:
# check that test set does not contain null values in the engineered variables

[var for var in vars_with_na if X_test[var].isnull().sum() > 0]
189/27: X_train[['LotFrontage_na', 'MasVnrArea_na', 'GarageYrBlt_na']].head()
189/28:
def elapsed_years(df, var):
    # capture difference between the year variable
    # and the year in which the house was sold
    df[var] = df['YrSold'] - df[var]
    return df
189/29:
for var in ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']:
    X_train = elapsed_years(X_train, var)
    X_test = elapsed_years(X_test, var)
189/30:
# now we drop YrSold
X_train.drop(['YrSold'], axis=1, inplace=True)
X_test.drop(['YrSold'], axis=1, inplace=True)
189/31:
for var in ["LotFrontage", "1stFlrSF", "GrLivArea"]:
    X_train[var] = np.log(X_train[var])
    X_test[var] = np.log(X_test[var])
189/32: [var for var in ["LotFrontage", "1stFlrSF", "GrLivArea"] if X_test[var].isnull().sum() > 0]
189/33: [var for var in ["LotFrontage", "1stFlrSF", "GrLivArea"] if X_train[var].isnull().sum() > 0]
189/34:
# the yeo-johnson transformation learns the best exponent to transform the variable
# it needs to learn it from the train set: 
X_train['LotArea'], param = stats.yeojohnson(X_train['LotArea'])

# and then apply the transformation to the test set with the same
# parameter: see who this time we pass param as argument to the 
# yeo-johnson
X_test['LotArea'] = stats.yeojohnson(X_test['LotArea'], lmbda=param)

print(param)
189/35:
# the yeo-johnson transformation learns the best exponent to transform the variable
# it needs to learn it from the train set: 
X_train['LotArea'], param = stats.yeojohnson(X_train['LotArea'])

# and then apply the transformation to the test set with the same
# parameter: see who this time we pass param as argument to the 
# yeo-johnson
X_test['LotArea'] = stats.yeojohnson(X_test['LotArea'] + 0.000001, lmbda=param)

print(param)
189/36:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import scipy.stats as stats

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

import joblib
pd.pandas.set_option('display.max_columns', None)
189/37:
# the yeo-johnson transformation learns the best exponent to transform the variable
# it needs to learn it from the train set: 
X_train['LotArea'], param = stats.yeojohnson(X_train['LotArea'])

# and then apply the transformation to the test set with the same
# parameter: see who this time we pass param as argument to the 
# yeo-johnson
X_test['LotArea'] = stats.yeojohnson(X_test['LotArea'] + 0.000001, lmbda=param)

print(param)
189/38:
# the yeo-johnson transformation learns the best exponent to transform the variable
# it needs to learn it from the train set: 
X_train['LotArea'], param = stats.yeojohnson(X_train['LotArea'] + 0.0000001)

# and then apply the transformation to the test set with the same
# parameter: see who this time we pass param as argument to the 
# yeo-johnson
X_test['LotArea'] = stats.yeojohnson(X_test['LotArea'], lmbda=param)

print(param)
189/39:
skewed = [
    'BsmtFinSF2', 'LowQualFinSF', 'EnclosedPorch',
    '3SsnPorch', 'ScreenPorch', 'MiscVal'
]

for var in skewed:
    
    # map the variable values into 0 and 1
    X_train[var] = np.where(X_train[var]==0, 0, 1)
    X_test[var] = np.where(X_test[var]==0, 0, 1)
189/40:
# re-map strings to numbers, which determine quality

qual_mappings = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5, 'Missing': 0, 'NA': 0}

qual_vars = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond',
             'HeatingQC', 'KitchenQual', 'FireplaceQu',
             'GarageQual', 'GarageCond',
            ]

for var in qual_vars:
    X_train[var] = X_train[var].map(qual_mappings)
    X_test[var] = X_test[var].map(qual_mappings)
189/41:
exposure_mappings = {'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4}

var = 'BsmtExposure'

X_train[var] = X_train[var].map(exposure_mappings)
X_test[var] = X_test[var].map(exposure_mappings)
189/42:
finish_mappings = {'Missing': 0, 'NA': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6}

finish_vars = ['BsmtFinType1', 'BsmtFinType2']

for var in finish_vars:
    X_train[var] = X_train[var].map(finish_mappings)
    X_test[var] = X_test[var].map(finish_mappings)
189/43:
garage_mappings = {'Missing': 0, 'NA': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3}

var = 'GarageFinish'

X_train[var] = X_train[var].map(garage_mappings)
X_test[var] = X_test[var].map(garage_mappings)
189/44:
fence_mappings = {'Missing': 0, 'NA': 0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv': 4}

var = 'Fence'

X_train[var] = X_train[var].map(fence_mappings)
X_test[var] = X_test[var].map(fence_mappings)
189/45: [var for var in X_train.columns if X_train[var].isnull().sum() > 0]
189/46:
# capture all quality variables

qual_vars  = qual_vars + finish_vars + ['BsmtExposure','GarageFinish','Fence']

# capture the remaining categorical variables
# (those that we did not re-map)

cat_others = [
    var for var in cat_vars if var not in qual_vars
]

len(cat_others)
189/47:
def find_frequent_labels(df, var, rare_perc):
    
    # function finds the labels that are shared by more than
    # a certain % of the houses in the dataset

    df = df.copy()

    tmp = df.groupby(var)[var].count() / len(df)

    return tmp[tmp > rare_perc].index


for var in cat_others:
    
    # find the frequent categories
    frequent_ls = find_frequent_labels(X_train, var, 0.01)
    
    print(var, frequent_ls)
    print()
    
    # replace rare categories by the string "Rare"
    X_train[var] = np.where(X_train[var].isin(
        frequent_ls), X_train[var], 'Rare')
    
    X_test[var] = np.where(X_test[var].isin(
        frequent_ls), X_test[var], 'Rare')
189/48: data.groupby('MSZoning Index')
189/49: data
189/50: data.groupby('Street Index')
189/51: data.groupby('Alley')
189/52: data.groupby('Alley').count()
189/53: data.groupby('Alley')['Alley'].count()
189/54: data.groupby('Alley')['Alley'].count()/len(data)
189/55: data.groupby('Street')['Street'].count()/len(data)
190/1:
# to handle datasets
import pandas as pd
import numpy as np

# for plotting
import matplotlib.pyplot as plt

# to build the models
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectFromModel

# to visualise al the columns in the dataframe
pd.pandas.set_option('display.max_columns', None)
190/2:
# to handle datasets
import pandas as pd
import numpy as np

# for plotting
import matplotlib.pyplot as plt

# to build the models
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectFromModel

# to visualise al the columns in the dataframe
pd.pandas.set_option('display.max_columns', None)
190/3:
# load the train and test set with the engineered variables

# we built and saved these datasets in the previous lecture.
# If you haven't done so, go ahead and check the previous notebook
# to find out how to create these datasets

X_train = pd.read_csv('xtrain.csv')
X_test = pd.read_csv('xtest.csv')

X_train.head()
180/1:
# this function will assign discrete values to the strings of the variables,
# so that the smaller value corresponds to the category that shows the smaller
# mean house sale price

def replace_categories(train, test, y_train, var, target):
    
    tmp = pd.concat([X_train, y_train], axis=1)
    
    # order the categories in a variable from that with the lowest
    # house sale price, to that with the highest
    ordered_labels = tmp.groupby([var])[target].mean().sort_values().index

    # create a dictionary of ordered categories to integer values
    ordinal_label = {k: i for i, k in enumerate(ordered_labels, 0)}
    
    print(var, ordinal_label)
    print()

    # use the dictionary to replace the categorical strings by integers
    train[var] = train[var].map(ordinal_label)
    test[var] = test[var].map(ordinal_label)
191/1:
# to handle datasets
import pandas as pd
import numpy as np

# for plotting
import matplotlib.pyplot as plt

# for the yeo-johnson transformation
import scipy.stats as stats

# to divide train and test set
from sklearn.model_selection import train_test_split

# feature scaling
from sklearn.preprocessing import MinMaxScaler

# to save the trained scaler class
import joblib

# to visualise al the columns in the dataframe
pd.pandas.set_option('display.max_columns', None)
191/2:
# load dataset
data = pd.read_csv('train.csv')

# rows and columns of the data
print(data.shape)

# visualise the dataset
data.head()
191/3:
# Let's separate into train and test set
# Remember to set the seed (random_state for this sklearn function)

X_train, X_test, y_train, y_test = train_test_split(
    data.drop(['Id', 'SalePrice'], axis=1), # predictive variables
    data['SalePrice'], # target
    test_size=0.1, # portion of dataset to allocate to test set
    random_state=0, # we are setting the seed here
)

X_train.shape, X_test.shape
191/4:
y_train = np.log(y_train)
y_test = np.log(y_test)
191/5:
# let's identify the categorical variables
# we will capture those of type object

cat_vars = [var for var in data.columns if data[var].dtype == 'O']

# MSSubClass is also categorical by definition, despite its numeric values
# (you can find the definitions of the variables in the data_description.txt
# file available on Kaggle, in the same website where you downloaded the data)

# lets add MSSubClass to the list of categorical variables
cat_vars = cat_vars + ['MSSubClass']

# cast all variables as categorical
X_train[cat_vars] = X_train[cat_vars].astype('O')
X_test[cat_vars] = X_test[cat_vars].astype('O')

# number of categorical variables
len(cat_vars)
191/6:
# make a list of the categorical variables that contain missing values

cat_vars_with_na = [
    var for var in cat_vars
    if X_train[var].isnull().sum() > 0
]

# print percentage of missing values per variable
X_train[cat_vars_with_na ].isnull().mean().sort_values(ascending=False)
191/7:
# variables to impute with the string missing
with_string_missing = [
    var for var in cat_vars_with_na if X_train[var].isnull().mean() > 0.1]

# variables to impute with the most frequent category
with_frequent_category = [
    var for var in cat_vars_with_na if X_train[var].isnull().mean() < 0.1]
191/8: with_string_missing
191/9:
# replace missing values with new label: "Missing"

X_train[with_string_missing] = X_train[with_string_missing].fillna('Missing')
X_test[with_string_missing] = X_test[with_string_missing].fillna('Missing')
191/10:
for var in with_frequent_category:
    
    # there can be more than 1 mode in a variable
    # we take the first one with [0]    
    mode = X_train[var].mode()[0]
    
    print(var, mode)
    
    X_train[var].fillna(mode, inplace=True)
    X_test[var].fillna(mode, inplace=True)
191/11:
# check that we have no missing information in the engineered variables

X_train[cat_vars_with_na].isnull().sum()
191/12:
# check that test set does not contain null values in the engineered variables

[var for var in cat_vars_with_na if X_test[var].isnull().sum() > 0]
191/13:
# now let's identify the numerical variables

num_vars = [
    var for var in X_train.columns if var not in cat_vars and var != 'SalePrice'
]

# number of numerical variables
len(num_vars)
191/14:
# make a list with the numerical variables that contain missing values
vars_with_na = [
    var for var in num_vars
    if X_train[var].isnull().sum() > 0
]

# print percentage of missing values per variable
X_train[vars_with_na].isnull().mean()
191/15:
# replace missing values as we described above

for var in vars_with_na:

    # calculate the mean using the train set
    mean_val = X_train[var].mean()
    
    print(var, mean_val)

    # add binary missing indicator (in train and test)
    X_train[var + '_na'] = np.where(X_train[var].isnull(), 1, 0)
    X_test[var + '_na'] = np.where(X_test[var].isnull(), 1, 0)

    # replace missing values by the mean
    # (in train and test)
    X_train[var].fillna(mean_val, inplace=True)
    X_test[var].fillna(mean_val, inplace=True)

# check that we have no more missing values in the engineered variables
X_train[vars_with_na].isnull().sum()
191/16:
# check that test set does not contain null values in the engineered variables

[var for var in vars_with_na if X_test[var].isnull().sum() > 0]
191/17:
# check the binary missing indicator variables

X_train[['LotFrontage_na', 'MasVnrArea_na', 'GarageYrBlt_na']].head()
191/18:
def elapsed_years(df, var):
    # capture difference between the year variable
    # and the year in which the house was sold
    df[var] = df['YrSold'] - df[var]
    return df
191/19:
for var in ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']:
    X_train = elapsed_years(X_train, var)
    X_test = elapsed_years(X_test, var)
191/20:
# now we drop YrSold
X_train.drop(['YrSold'], axis=1, inplace=True)
X_test.drop(['YrSold'], axis=1, inplace=True)
191/21:
for var in ["LotFrontage", "1stFlrSF", "GrLivArea"]:
    X_train[var] = np.log(X_train[var])
    X_test[var] = np.log(X_test[var])
191/22:
# check that test set does not contain null values in the engineered variables
[var for var in ["LotFrontage", "1stFlrSF", "GrLivArea"] if X_test[var].isnull().sum() > 0]
191/23:
# same for train set
[var for var in ["LotFrontage", "1stFlrSF", "GrLivArea"] if X_train[var].isnull().sum() > 0]
191/24:
# the yeo-johnson transformation learns the best exponent to transform the variable
# it needs to learn it from the train set: 
X_train['LotArea'], param = stats.yeojohnson(X_train['LotArea'])

# and then apply the transformation to the test set with the same
# parameter: see who this time we pass param as argument to the 
# yeo-johnson
X_test['LotArea'] = stats.yeojohnson(X_test['LotArea'], lmbda=param)

print(param)
191/25:
# the yeo-johnson transformation learns the best exponent to transform the variable
# it needs to learn it from the train set: 
X_train['LotArea'], param = stats.yeojohnson(np.abs(X_train['LotArea']))

# and then apply the transformation to the test set with the same
# parameter: see who this time we pass param as argument to the 
# yeo-johnson
X_test['LotArea'] = stats.yeojohnson(X_test['LotArea'], lmbda=param)

print(param)
191/26:
# the yeo-johnson transformation learns the best exponent to transform the variable
# it needs to learn it from the train set: 
X_train['LotArea'], param = stats.yeojohnson(X_train['LotArea'] + 0.00001)

# and then apply the transformation to the test set with the same
# parameter: see who this time we pass param as argument to the 
# yeo-johnson
X_test['LotArea'] = stats.yeojohnson(X_test['LotArea'], lmbda=param)

print(param)
191/27:
# check absence of na in the train set
[var for var in X_train.columns if X_train[var].isnull().sum() > 0]
191/28:
# check absence of na in the test set
[var for var in X_train.columns if X_test[var].isnull().sum() > 0]
191/29:
skewed = [
    'BsmtFinSF2', 'LowQualFinSF', 'EnclosedPorch',
    '3SsnPorch', 'ScreenPorch', 'MiscVal'
]

for var in skewed:
    
    # map the variable values into 0 and 1
    X_train[var] = np.where(X_train[var]==0, 0, 1)
    X_test[var] = np.where(X_test[var]==0, 0, 1)
191/30:
# re-map strings to numbers, which determine quality

qual_mappings = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5, 'Missing': 0, 'NA': 0}

qual_vars = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond',
             'HeatingQC', 'KitchenQual', 'FireplaceQu',
             'GarageQual', 'GarageCond',
            ]

for var in qual_vars:
    X_train[var] = X_train[var].map(qual_mappings)
    X_test[var] = X_test[var].map(qual_mappings)
191/31:
exposure_mappings = {'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4}

var = 'BsmtExposure'

X_train[var] = X_train[var].map(exposure_mappings)
X_test[var] = X_test[var].map(exposure_mappings)
191/32:
finish_mappings = {'Missing': 0, 'NA': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6}

finish_vars = ['BsmtFinType1', 'BsmtFinType2']

for var in finish_vars:
    X_train[var] = X_train[var].map(finish_mappings)
    X_test[var] = X_test[var].map(finish_mappings)
191/33:
garage_mappings = {'Missing': 0, 'NA': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3}

var = 'GarageFinish'

X_train[var] = X_train[var].map(garage_mappings)
X_test[var] = X_test[var].map(garage_mappings)
191/34:
fence_mappings = {'Missing': 0, 'NA': 0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv': 4}

var = 'Fence'

X_train[var] = X_train[var].map(fence_mappings)
X_test[var] = X_test[var].map(fence_mappings)
191/35:
# check absence of na in the train set
[var for var in X_train.columns if X_train[var].isnull().sum() > 0]
191/36:
# capture all quality variables

qual_vars  = qual_vars + finish_vars + ['BsmtExposure','GarageFinish','Fence']

# capture the remaining categorical variables
# (those that we did not re-map)

cat_others = [
    var for var in cat_vars if var not in qual_vars
]

len(cat_others)
191/37:
def find_frequent_labels(df, var, rare_perc):
    
    # function finds the labels that are shared by more than
    # a certain % of the houses in the dataset

    df = df.copy()

    tmp = df.groupby(var)[var].count() / len(df)

    return tmp[tmp > rare_perc].index


for var in cat_others:
    
    # find the frequent categories
    frequent_ls = find_frequent_labels(X_train, var, 0.01)
    
    print(var, frequent_ls)
    print()
    
    # replace rare categories by the string "Rare"
    X_train[var] = np.where(X_train[var].isin(
        frequent_ls), X_train[var], 'Rare')
    
    X_test[var] = np.where(X_test[var].isin(
        frequent_ls), X_test[var], 'Rare')
191/38:
# this function will assign discrete values to the strings of the variables,
# so that the smaller value corresponds to the category that shows the smaller
# mean house sale price

def replace_categories(train, test, y_train, var, target):
    
    tmp = pd.concat([X_train, y_train], axis=1)
    
    # order the categories in a variable from that with the lowest
    # house sale price, to that with the highest
    ordered_labels = tmp.groupby([var])[target].mean().sort_values().index

    # create a dictionary of ordered categories to integer values
    ordinal_label = {k: i for i, k in enumerate(ordered_labels, 0)}
    
    print(var, ordinal_label)
    print()

    # use the dictionary to replace the categorical strings by integers
    train[var] = train[var].map(ordinal_label)
    test[var] = test[var].map(ordinal_label)
191/39:
for var in cat_others:
    replace_categories(X_train, X_test, y_train, var, 'SalePrice')
191/40:
# check absence of na in the train set
[var for var in X_train.columns if X_train[var].isnull().sum() > 0]
191/41:
# check absence of na in the test set
[var for var in X_test.columns if X_test[var].isnull().sum() > 0]
191/42:
# let me show you what I mean by monotonic relationship
# between labels and target

def analyse_vars(train, y_train, var):
    
    # function plots median house sale price per encoded
    # category
    
    tmp = pd.concat([X_train, np.log(y_train)], axis=1)
    
    tmp.groupby(var)['SalePrice'].median().plot.bar()
    plt.title(var)
    plt.ylim(2.2, 2.6)
    plt.ylabel('SalePrice')
    plt.show()
    
for var in cat_others:
    analyse_vars(X_train, y_train, var)
191/43:
# create scaler
scaler = MinMaxScaler()

#  fit  the scaler to the train set
scaler.fit(X_train) 

# transform the train and test set

# sklearn returns numpy arrays, so we wrap the
# array with a pandas dataframe

X_train = pd.DataFrame(
    scaler.transform(X_train),
    columns=X_train.columns
)

X_test = pd.DataFrame(
    scaler.transform(X_test),
    columns=X_train.columns
)
191/44: X_train.head()
191/45:
# let's now save the train and test sets for the next notebook!

X_train.to_csv('xtrain.csv', index=False)
X_test.to_csv('xtest.csv', index=False)

y_train.to_csv('ytrain.csv', index=False)
y_test.to_csv('ytest.csv', index=False)
191/46:
# now let's save the scaler

joblib.dump(scaler, 'minmax_scaler.joblib')
190/4:
# load the train and test set with the engineered variables

# we built and saved these datasets in the previous lecture.
# If you haven't done so, go ahead and check the previous notebook
# to find out how to create these datasets

X_train = pd.read_csv('xtrain.csv')
X_test = pd.read_csv('xtest.csv')

X_train.head()
190/5:
# load the target (remember that the target is log transformed)
y_train = pd.read_csv('ytrain.csv')
y_test = pd.read_csv('ytest.csv')

y_train.head()
190/6:
# We will do the model fitting and feature selection
# altogether in a few lines of code

# first, we specify the Lasso Regression model, and we
# select a suitable alpha (equivalent of penalty).
# The bigger the alpha the less features that will be selected.

# Then we use the selectFromModel object from sklearn, which
# will select automatically the features which coefficients are non-zero

# remember to set the seed, the random state in this function
sel_ = SelectFromModel(Lasso(alpha=0.001, random_state=0))

# train Lasso model and select features
sel_.fit(X_train, y_train)
190/7:
# load the target (remember that the target is log transformed)
y_train = pd.read_csv('ytrain.csv')
y_test = pd.read_csv('ytest.csv')

y_train.head()
190/8:
# We will do the model fitting and feature selection
# altogether in a few lines of code

# first, we specify the Lasso Regression model, and we
# select a suitable alpha (equivalent of penalty).
# The bigger the alpha the less features that will be selected.

# Then we use the selectFromModel object from sklearn, which
# will select automatically the features which coefficients are non-zero

# remember to set the seed, the random state in this function
sel_ = SelectFromModel(Lasso(alpha=0.001, random_state=0))

# train Lasso model and select features
sel_.fit(X_train, y_train)
190/9: X_train.shape
190/10: y_train
191/47: X_train.shape
190/11:
# load the target (remember that the target is log transformed)
y_train = pd.read_csv('ytrain.csv', header=False)
y_test = pd.read_csv('ytest.csv')

y_train.head()
190/12:
# load the target (remember that the target is log transformed)
y_train = pd.read_csv('ytrain.csv', header=None)
y_test = pd.read_csv('ytest.csv')

y_train.head()
190/13: X_train.shape
190/14: y_train
190/15:
# We will do the model fitting and feature selection
# altogether in a few lines of code

# first, we specify the Lasso Regression model, and we
# select a suitable alpha (equivalent of penalty).
# The bigger the alpha the less features that will be selected.

# Then we use the selectFromModel object from sklearn, which
# will select automatically the features which coefficients are non-zero

# remember to set the seed, the random state in this function
sel_ = SelectFromModel(Lasso(alpha=0.001, random_state=0))

# train Lasso model and select features
sel_.fit(X_train, y_train)
190/16: sel_.get_support().sum()
190/17:
# We will do the model fitting and feature selection
# altogether in a few lines of code

# first, we specify the Lasso Regression model, and we
# select a suitable alpha (equivalent of penalty).
# The bigger the alpha the less features that will be selected.

# Then we use the selectFromModel object from sklearn, which
# will select automatically the features which coefficients are non-zero

# remember to set the seed, the random state in this function
sel_ = SelectFromModel(Lasso(alpha=0.0001, random_state=0))

# train Lasso model and select features
sel_.fit(X_train, y_train)
190/18: sel_.get_support().sum()
190/19:
# We will do the model fitting and feature selection
# altogether in a few lines of code

# first, we specify the Lasso Regression model, and we
# select a suitable alpha (equivalent of penalty).
# The bigger the alpha the less features that will be selected.

# Then we use the selectFromModel object from sklearn, which
# will select automatically the features which coefficients are non-zero

# remember to set the seed, the random state in this function
sel_ = SelectFromModel(Lasso(alpha=0.01, random_state=0))

# train Lasso model and select features
sel_.fit(X_train, y_train)
190/20: sel_.get_support().sum()
190/21:
# We will do the model fitting and feature selection
# altogether in a few lines of code

# first, we specify the Lasso Regression model, and we
# select a suitable alpha (equivalent of penalty).
# The bigger the alpha the less features that will be selected.

# Then we use the selectFromModel object from sklearn, which
# will select automatically the features which coefficients are non-zero

# remember to set the seed, the random state in this function
sel_ = SelectFromModel(Lasso(alpha=0.001, random_state=0))

# train Lasso model and select features
sel_.fit(X_train, y_train)
190/22: sel_.get_support().sum()
190/23: sel_.get_support()
190/24: sel_.get_support().sum()
190/25:
# let's visualise those features that were selected.
# (selected features marked with True)

sel_.get_support()
190/26:
# let's print the number of total and selected features

# this is how we can make a list of the selected features
selected_feats = X_train.columns[(sel_.get_support())]

# let's print some stats
print('total features: {}'.format((X_train.shape[1])))
print('selected features: {}'.format(len(selected_feats)))
print('features with coefficients shrank to zero: {}'.format(
    np.sum(sel_.estimator_.coef_ == 0)))
190/27:
# print the selected features
selected_feats
190/28: pd.Series(selected_feats).to_csv('selected_features.csv', index=False)
192/1:
# to handle datasets
import pandas as pd
import numpy as np

# for plotting
import matplotlib.pyplot as plt

# to save the model
import joblib

# to build the model
from sklearn.linear_model import Lasso

# to evaluate the model
from sklearn.metrics import mean_squared_error, r2_score

# to visualise al the columns in the dataframe
pd.pandas.set_option('display.max_columns', None)
192/2:
# load the train and test set with the engineered variables

# we built and saved these datasets in a previous notebook.
# If you haven't done so, go ahead and check the previous notebooks (step 2)
# to find out how to create these datasets

X_train = pd.read_csv('xtrain.csv')
X_test = pd.read_csv('xtest.csv')

X_train.head()
192/3:
# load the target (remember that the target is log transformed)
y_train = pd.read_csv('ytrain.csv')
y_test = pd.read_csv('ytest.csv')

y_train.head()
192/4:
# load the pre-selected features
# ==============================

# we selected the features in the previous notebook (step 3)

# if you haven't done so, go ahead and visit the previous notebook
# to find out how to select the features

features = pd.read_csv('selected_features.csv')
features = features['0'].to_list() 

# display final feature set
features
192/5:
# load the target (remember that the target is log transformed)
y_train = pd.read_csv('ytrain.csv', header=None)
y_test = pd.read_csv('ytest.csv', header=None)

y_train.head()
192/6:
# load the pre-selected features
# ==============================

# we selected the features in the previous notebook (step 3)

# if you haven't done so, go ahead and visit the previous notebook
# to find out how to select the features

features = pd.read_csv('selected_features.csv')
features = features['0'].to_list() 

# display final feature set
features
192/7: features
192/8:
# load the pre-selected features
# ==============================

# we selected the features in the previous notebook (step 3)

# if you haven't done so, go ahead and visit the previous notebook
# to find out how to select the features

features = pd.read_csv('selected_features.csv')
#features = features['0'].to_list() 

# display final feature set
features
192/9:
# load the pre-selected features
# ==============================

# we selected the features in the previous notebook (step 3)

# if you haven't done so, go ahead and visit the previous notebook
# to find out how to select the features

features = pd.read_csv('selected_features.csv', header=None)
#features = features['0'].to_list() 

# display final feature set
features
192/10:
# load the pre-selected features
# ==============================

# we selected the features in the previous notebook (step 3)

# if you haven't done so, go ahead and visit the previous notebook
# to find out how to select the features

features = pd.read_csv('selected_features.csv', header=None)
features = features['0'].to_list() 

# display final feature set
features
192/11: features.columns
192/12:
# load the pre-selected features
# ==============================

# we selected the features in the previous notebook (step 3)

# if you haven't done so, go ahead and visit the previous notebook
# to find out how to select the features

features = pd.read_csv('selected_features.csv', header=None)
features = features[0].to_list() 

# display final feature set
features
192/13:
# set up the model
# remember to set the random_state / seed

lin_model = Lasso(alpha=0.001, random_state=0)

# train the model

lin_model.fit(X_train, y_train)
192/14:
# evaluate the model:
# ====================

# remember that we log transformed the output (SalePrice)
# in our feature engineering notebook (step 2).

# In order to get the true performance of the Lasso
# we need to transform both the target and the predictions
# back to the original house prices values.

# We will evaluate performance using the mean squared error and
# the root of the mean squared error and r2

# make predictions for train set
pred = lin_model.predict(X_train)

# determine mse, rmse and r2
print('train mse: {}'.format(int(
    mean_squared_error(np.exp(y_train), np.exp(pred)))))
print('train rmse: {}'.format(int(
    mean_squared_error(np.exp(y_train), np.exp(pred), squared=False))))
print('train r2: {}'.format(
    r2_score(np.exp(y_train), np.exp(pred))))
print()

# make predictions for test set
pred = lin_model.predict(X_test)

# determine mse, rmse and r2
print('test mse: {}'.format(int(
    mean_squared_error(np.exp(y_test), np.exp(pred)))))
print('test rmse: {}'.format(int(
    mean_squared_error(np.exp(y_test), np.exp(pred), squared=False))))
print('test r2: {}'.format(
    r2_score(np.exp(y_test), np.exp(pred))))
print()

print('Average house price: ', int(np.exp(y_train).median()))
192/15:
# evaluate the model:
# ====================

# remember that we log transformed the output (SalePrice)
# in our feature engineering notebook (step 2).

# In order to get the true performance of the Lasso
# we need to transform both the target and the predictions
# back to the original house prices values.

# We will evaluate performance using the mean squared error and
# the root of the mean squared error and r2

# make predictions for train set
pred = lin_model.predict(X_train)

# determine mse, rmse and r2
print('train mse: {}'.format(int(
    mean_squared_error(np.exp(y_train), np.exp(pred)))))
print('train rmse: {}'.format(int(
    mean_squared_error(np.exp(y_train), np.exp(pred)))))
print('train r2: {}'.format(
    r2_score(np.exp(y_train), np.exp(pred))))
print()

# make predictions for test set
pred = lin_model.predict(X_test)

# determine mse, rmse and r2
print('test mse: {}'.format(int(
    mean_squared_error(np.exp(y_test), np.exp(pred)))))
print('test rmse: {}'.format(int(
    mean_squared_error(np.exp(y_test), np.exp(pred), squared=False))))
print('test r2: {}'.format(
    r2_score(np.exp(y_test), np.exp(pred))))
print()

print('Average house price: ', int(np.exp(y_train).median()))
192/16:
# evaluate the model:
# ====================

# remember that we log transformed the output (SalePrice)
# in our feature engineering notebook (step 2).

# In order to get the true performance of the Lasso
# we need to transform both the target and the predictions
# back to the original house prices values.

# We will evaluate performance using the mean squared error and
# the root of the mean squared error and r2

# make predictions for train set
pred = lin_model.predict(X_train)

# determine mse, rmse and r2
print('train mse: {}'.format(int(
    mean_squared_error(np.exp(y_train), np.exp(pred)))))
print('train rmse: {}'.format(int(
    mean_squared_error(np.exp(y_train), np.exp(pred)))))
print('train r2: {}'.format(
    r2_score(np.exp(y_train), np.exp(pred))))
print()

# make predictions for test set
pred = lin_model.predict(X_test)

# determine mse, rmse and r2
print('test mse: {}'.format(int(
    mean_squared_error(np.exp(y_test), np.exp(pred)))))
print('test rmse: {}'.format(int(
    mean_squared_error(np.exp(y_test), np.exp(pred)))))
print('test r2: {}'.format(
    r2_score(np.exp(y_test), np.exp(pred))))
print()

print('Average house price: ', int(np.exp(y_train).median()))
192/17:
# evaluate the model:
# ====================

# remember that we log transformed the output (SalePrice)
# in our feature engineering notebook (step 2).

# In order to get the true performance of the Lasso
# we need to transform both the target and the predictions
# back to the original house prices values.

# We will evaluate performance using the mean squared error and
# the root of the mean squared error and r2

# make predictions for train set
pred = lin_model.predict(X_train)

# determine mse, rmse and r2
print('train mse: {}'.format(int(
    mean_squared_error(np.exp(y_train), np.exp(pred), squared=False))))
print('train rmse: {}'.format(int(
    mean_squared_error(np.exp(y_train), np.exp(pred)))))
print('train r2: {}'.format(
    r2_score(np.exp(y_train), np.exp(pred))))
print()

# make predictions for test set
pred = lin_model.predict(X_test)

# determine mse, rmse and r2
print('test mse: {}'.format(int(
    mean_squared_error(np.exp(y_test), np.exp(pred)))))
print('test rmse: {}'.format(int(
    mean_squared_error(np.exp(y_test), np.exp(pred)))))
print('test r2: {}'.format(
    r2_score(np.exp(y_test), np.exp(pred))))
print()

print('Average house price: ', int(np.exp(y_train).median()))
192/18:
# evaluate the model:
# ====================

# remember that we log transformed the output (SalePrice)
# in our feature engineering notebook (step 2).

# In order to get the true performance of the Lasso
# we need to transform both the target and the predictions
# back to the original house prices values.

# We will evaluate performance using the mean squared error and
# the root of the mean squared error and r2

# make predictions for train set
pred = lin_model.predict(X_train)

# determine mse, rmse and r2
print('train mse: {}'.format(int(
    mean_squared_error(np.exp(y_train), np.exp(pred)))))
print('train rmse: {}'.format(int(
    mean_squared_error(np.exp(y_train), np.exp(pred)))))
print('train r2: {}'.format(
    r2_score(np.exp(y_train), np.exp(pred))))
print()

# make predictions for test set
pred = lin_model.predict(X_test)

# determine mse, rmse and r2
print('test mse: {}'.format(int(
    mean_squared_error(np.exp(y_test), np.exp(pred)))))
print('test rmse: {}'.format(int(
    mean_squared_error(np.exp(y_test), np.exp(pred)))))
print('test r2: {}'.format(
    r2_score(np.exp(y_test), np.exp(pred))))
print()

print('Average house price: ', int(np.exp(y_train).median()))
192/19:
# let's evaluate our predictions respect to the real sale price
plt.scatter(y_test, lin_model.predict(X_test))
plt.xlabel('True House Price')
plt.ylabel('Predicted House Price')
plt.title('Evaluation of Lasso Predictions')
192/20: y_test.reset_index(drop=True)
192/21:
# let's evaluate the distribution of the errors: 
# they should be fairly normally distributed

y_test.reset_index(drop=True, inplace=True)

preds = pd.Series(lin_model.predict(X_test))

preds
192/22:
# let's evaluate the distribution of the errors: 
# they should be fairly normally distributed

errors = y_test['SalePrice'] - preds
errors.hist(bins=30)
plt.show()
192/23:
# let's evaluate the distribution of the errors: 
# they should be fairly normally distributed

errors = y_test - preds
errors.hist(bins=30)
plt.show()
192/24: errors
192/25: preds
179/93:
for var in cat_others:
    # make boxplot with Catplot
    sns.catplot(x=var, y='SalePrice', data=data, kind="box", height=4, aspect=1.5)
    # add data points to boxplot with stripplot
    sns.stripplot(x=var, y='SalePrice', data=data, jitter=0.1, alpha=0.3, color='k')
    plt.show()
192/26: y_test
192/27:
# let's evaluate the distribution of the errors: 
# they should be fairly normally distributed

errors = y_test[0] - preds
errors.hist(bins=30)
plt.show()
192/28: lin_model.coef_
192/29: lin_model.coef_.reval()
192/30: lin_model.coef_.ravel()
192/31: lin_model.coef_
192/32:
# Finally, just for fun, let's look at the feature importance

importance = pd.Series(np.abs(lin_model.coef_))
importance.index = features
importance.sort_values(inplace=True, ascending=False)
importance.plot.bar(figsize=(18,6))
plt.ylabel('Lasso Coefficients')
plt.title('Feature Importance')
192/33: len(lin_model.coef_)
192/34: len(lin_model.coef_.ravel())
192/35:
# Finally, just for fun, let's look at the feature importance

importance = pd.Series(np.abs(lin_model.coef_.ravel()))
importance.index = features
importance.sort_values(inplace=True, ascending=False)
importance.plot.bar(figsize=(18,6))
plt.ylabel('Lasso Coefficients')
plt.title('Feature Importance')
192/36: len(lin_model.coef_)
192/37: len(lin_model.coef_.ravel())
192/38:
# Finally, just for fun, let's look at the feature importance

importance = pd.Series(np.abs(lin_model.coef_.ravel()))
importance.index = features
importance.sort_values(inplace=True, ascending=False)
importance.plot.bar(figsize=(18,6))
plt.ylabel('Lasso Coefficients')
plt.title('Feature Importance')
192/39: lin_model.coef_
192/40: len(lin_model.coef_.ravel() > 0)
192/41: len(lin_model.coef_ > 0)
192/42: lin_model.coef_
192/43:
# evaluate the model:
# ====================

# remember that we log transformed the output (SalePrice)
# in our feature engineering notebook (step 2).

# In order to get the true performance of the Lasso
# we need to transform both the target and the predictions
# back to the original house prices values.

# We will evaluate performance using the mean squared error and
# the root of the mean squared error and r2

# make predictions for train set
pred = lin_model.predict(X_train)

# determine mse, rmse and r2
print('train mse: {}'.format(int(
    mean_squared_error(np.exp(y_train), np.exp(pred)))))
print('train rmse: {}'.format(int(
    mean_squared_error(np.exp(y_train), np.exp(pred)))))
print('train r2: {}'.format(
    r2_score(np.exp(y_train), np.exp(pred))))
print()

# make predictions for test set
pred = lin_model.predict(X_test)

# determine mse, rmse and r2
print('test mse: {}'.format(int(
    mean_squared_error(np.exp(y_test), np.exp(pred)))))
print('test rmse: {}'.format(int(
    mean_squared_error(np.exp(y_test), np.exp(pred)))))
print('test r2: {}'.format(
    r2_score(np.exp(y_test), np.exp(pred))))
print()

print('Average house price: ', int(np.exp(y_train).median()))
192/44:
# set up the model
# remember to set the random_state / seed

lin_model = Lasso(alpha=0.001, random_state=0)

# train the model

lin_model.fit(X_train[features], y_train)
192/45:
# evaluate the model:
# ====================

# remember that we log transformed the output (SalePrice)
# in our feature engineering notebook (step 2).

# In order to get the true performance of the Lasso
# we need to transform both the target and the predictions
# back to the original house prices values.

# We will evaluate performance using the mean squared error and
# the root of the mean squared error and r2

# make predictions for train set
pred = lin_model.predict(X_train)

# determine mse, rmse and r2
print('train mse: {}'.format(int(
    mean_squared_error(np.exp(y_train), np.exp(pred)))))
print('train rmse: {}'.format(int(
    mean_squared_error(np.exp(y_train), np.exp(pred)))))
print('train r2: {}'.format(
    r2_score(np.exp(y_train), np.exp(pred))))
print()

# make predictions for test set
pred = lin_model.predict(X_test)

# determine mse, rmse and r2
print('test mse: {}'.format(int(
    mean_squared_error(np.exp(y_test), np.exp(pred)))))
print('test rmse: {}'.format(int(
    mean_squared_error(np.exp(y_test), np.exp(pred)))))
print('test r2: {}'.format(
    r2_score(np.exp(y_test), np.exp(pred))))
print()

print('Average house price: ', int(np.exp(y_train).median()))
192/46:
# evaluate the model:
# ====================

# remember that we log transformed the output (SalePrice)
# in our feature engineering notebook (step 2).

# In order to get the true performance of the Lasso
# we need to transform both the target and the predictions
# back to the original house prices values.

# We will evaluate performance using the mean squared error and
# the root of the mean squared error and r2

# make predictions for train set
pred = lin_model.predict(X_train[features])

# determine mse, rmse and r2
print('train mse: {}'.format(int(
    mean_squared_error(np.exp(y_train), np.exp(pred)))))
print('train rmse: {}'.format(int(
    mean_squared_error(np.exp(y_train), np.exp(pred)))))
print('train r2: {}'.format(
    r2_score(np.exp(y_train), np.exp(pred))))
print()

# make predictions for test set
pred = lin_model.predict(X_test[features])

# determine mse, rmse and r2
print('test mse: {}'.format(int(
    mean_squared_error(np.exp(y_test), np.exp(pred)))))
print('test rmse: {}'.format(int(
    mean_squared_error(np.exp(y_test), np.exp(pred)))))
print('test r2: {}'.format(
    r2_score(np.exp(y_test), np.exp(pred))))
print()

print('Average house price: ', int(np.exp(y_train).median()))
192/47:
# let's evaluate our predictions respect to the real sale price
plt.scatter(y_test, lin_model.predict(X_test))
plt.xlabel('True House Price')
plt.ylabel('Predicted House Price')
plt.title('Evaluation of Lasso Predictions')
192/48:
# let's evaluate our predictions respect to the real sale price
plt.scatter(y_test, lin_model.predict(X_test[features]))
plt.xlabel('True House Price')
plt.ylabel('Predicted House Price')
plt.title('Evaluation of Lasso Predictions')
192/49: y_test.reset_index(drop=True)
192/50:
# let's evaluate the distribution of the errors: 
# they should be fairly normally distributed

y_test.reset_index(drop=True, inplace=True)

preds = pd.Series(lin_model.predict(X_test))

preds
192/51:
# let's evaluate the distribution of the errors: 
# they should be fairly normally distributed

y_test.reset_index(drop=True, inplace=True)

preds = pd.Series(lin_model.predict(X_test[features]))

preds
192/52: y_test
192/53:
# let's evaluate the distribution of the errors: 
# they should be fairly normally distributed

errors = y_test[0] - preds
errors.hist(bins=30)
plt.show()
192/54: len(lin_model.coef_)
192/55: lin_model.coef_
192/56: len(lin_model.coef_ > 0)
192/57:
# Finally, just for fun, let's look at the feature importance

importance = pd.Series(np.abs(lin_model.coef_.ravel()))
importance.index = features
importance.sort_values(inplace=True, ascending=False)
importance.plot.bar(figsi
                    ze=(18,6))
plt.ylabel('Lasso Coefficients')
plt.title('Feature Importance')
192/58:
# Finally, just for fun, let's look at the feature importance

importance = pd.Series(np.abs(lin_model.coef_.ravel()))
importance.index = features
importance.sort_values(inplace=True, ascending=False)
importance.plot.bar(figsize=(18,6))
plt.ylabel('Lasso Coefficients')
plt.title('Feature Importance')
192/59:
# we are happy to our model, so we save it to be able
# to score new data

joblib.dump(lin_model, 'linear_regression.joblib')
197/1: person = 'Nastya'
197/2: print('my name is {}'.format(person))
197/3:
person = 'Nastya'
person_2 = 'Mar'
197/4: print('my name is {}{}'.format(person, person_2))
197/5: print('my name is {} {}'.format(person, person_2))
197/6: print(f'my name is {person}'')
197/7: print(f'my name is {person}')
197/8: print(f'my name is {person, person_2}')
197/9: print(f'my name is {person}')
197/10: print(f'my name is {person} {person_2}')
197/11: library = [('Author', 'Topic', 'Pages'), ('Twain', 'Rafting', 601), ('Feynman', 'Physics', 95), ('Hamilton', 'Mythology', 144)]
197/12: library
197/13:
for book in library:
    print(f"Author is {book[0]}")
197/14:
for book, topic, pages in library:
    print(f"{author} {topic} {pages}")
197/15:
for book, topic, pages in library:
    print(f'{author} {topic} {pages}')
197/16:
for author, topic, pages in library:
    print(f'{author} {topic} {pages}')
197/17:
for author, topic, pages in library:
    print(f'{author:{10}} {topic:{30}} {pages:{10}}')
197/18:
for author, topic, pages in library:
    print(f'{author:{10}} {topic:{30}} {pages:>{10}}')
197/19:
for author, topic, pages in library:
    print(f'{author:{10}} {topic:{30}} {pages:->{10}}')
197/20:
for author, topic, pages in library:
    print(f'{author:{10}} {topic:{30}} {pages:0>{10}}')
197/21:
for author, topic, pages in library:
    print(f'{author:{10}} {topic:{30}} {pages:>{10}}')
197/22: from datetime import datetime
197/23: today = datetime(year=2022, month=4, day=22)
197/24: print(f"{today}")
197/25: today
197/26: print(f"{today:%B}")
197/27: print(f"{today:%B %d, %Y}")
197/28:
%%writefile test.txt
Hello, this is a quick test file
This is the second line of the file.
197/29: myfile = open('whoops.txt')
197/30: pwd
197/31: ls - la
197/32: myfile = open(test.txt)
197/33: myfile = open('test.txt')
197/34: myfile
197/35: myfile.read()
197/36: myfile.read()
197/37: myfile.seek(0)
197/38: myfile.read()
197/39: myfile.seek(1)
197/40: myfile.read()
197/41: myfile.seek(0)
197/42: content = myfile.read()
197/43: print(content)
197/44: myfile.close()
197/45: myfile
197/46: myfile = open(test.txt)
197/47: myfile = open('test.txt')
197/48: myfile.readlines()
197/49: myfile.seek(0)
197/50: myfile.readlines()
197/51: myfile.seek(0)
197/52: mylines = myfile.readlines()
197/53:
for line in mylines:
    print(line)
197/54:
for line in mylines:
    print(line.split()[0])
197/55:
for line in mylines:
    print(line.split()[0:4])
197/56: myfile = open('test.txt', 'w+')
197/57: myfile.read()
197/58: myfile.write('MY NEW TEXT')
197/59: myfile.seek(0)
197/60: myfile.read()
197/61: myfile.close)
197/62: myfile.close())
197/63: myfile.close()
197/64: myfile = open('whoop.txt', 'a+')
197/65: myfile.write('MY FIRST LINE IN A+ OPENNING')
197/66: myfile.close()
197/67: newfile = open('whoop.txt')
197/68: newfile.read()
197/69: newfile.write('try to write')
197/70: newfile.close()
197/71: myfile = open('whoop.txt', mpde='a+')
197/72: myfile = open('whoop.txt', mode='a+')
197/73: myfile.write('NeW LiNe A+')
197/74: myfile.seek(0)
197/75: myfile.read()
197/76: myfile.close()
197/77:
with open('whoops.txt', 'r') as mynewfile:
    myvar = mynewfile.readlines()
197/78:
with open('whoop.txt', 'r') as mynewfile:
    myvar = mynewfile.readlines()
197/79: myvar
198/1:
# note the capitalization
import PyPDF2
197/80: !conda import PyPDF2
197/81: !conda install PyPDF2
200/1: import PyPDF2
200/2: !ls -la
200/3: @ls -la
200/4: !ls -la
200/5: !!ls -la
200/6: ls -la
202/1:
# note the capitalization
import PyPDF2
202/2:
# Notice we read it as a binary with 'rb'
f = open('US_Declaration.pdf','rb')
202/3: pdf_reader = PyPDF2.PdfFileReader(f)
200/7: !ls
200/8: !conda env list
200/9: import sys
200/10: !ls
200/11: !pwd
200/12: pwd
200/13: ls
200/14: ls -la
200/15: 'ls -la'
200/16: !'ls -la'
202/4: pwd
200/17: f = open('../UPDATED_NLP_COURSE/US_Declaration.pdf','rb')
200/18: f = open('..\\UPDATED_NLP_COURSE/US_Declaration.pdf','rb')
200/19: f = open(':\\Users\\Me\\Desktop\\Udemy\\UPDATED_NLP_COURSE\\UPDATED_NLP_COURSE\\US_Declaration.pdf','rb')
200/20: f = open(':../UPDATED_NLP_COURSE/US_Declaration.pdf','rb')
200/21: f = open('../UPDATED_NLP_COURSE/US_Declaration.pdf','rb')
200/22: f = open('..\\UPDATED_NLP_COURSE/US_Declaration.pdf','rb')
200/23: f = open('..\\UPDATED_NLP_COURSE\\US_Declaration.pdf','rb')
200/24: pwd..
200/25: pwd
200/26: f = open('..\UPDATED_NLP_COURSE\US_Declaration.pdf','rb')
200/27: f = open('..\UPDATED_NLP_COURSE\00-Python-Text-Basics\US_Declaration.pdf','rb')
200/28: f = open('..\\UPDATED_NLP_COURSE\\00-Python-Text-Basics\US_Declaration.pdf','rb')
200/29: f = open('..\\UPDATED_NLP_COURSE\\00-Python-Text-Basics\\US_Declaration.pdf','rb')
200/30: f
200/31: pdf_reader = PyPDF2.PdfFileReader(f)
200/32: pdf_reader
200/33: pdf_reader.numPages
200/34: pdf_reader.getOutlines
200/35: pdf_reader.getOutlines()
200/36: page_one = pdf_reader.getPage(0)
200/37: page_one.extractText()
200/38: print(page_one.extractText())
200/39: myfile.close()
200/40: f.close()
200/41: f = open('..\\UPDATED_NLP_COURSE\\00-Python-Text-Basics\\US_Declaration.pdf','rb')
200/42: pdf_reader = PyPDF2.PdfFileReader(f)
200/43: first_page = pdf_reader.getPage(0)
200/44: first_write = PyPDF2.PdfFileWriter()
200/45: pdf_writer.addPage(first_page)
200/46: first_write.addPage(first_page)
200/47: pdf_output = open('MY_NEW.pdf')
200/48: pdf_output = open('MY_NEW.pdf', 'wb')
200/49: first_write.write(pdf_output)
200/50: f.close()
200/51: pdf_output.close()
200/52:
b_new = open('MY_NEW.pdf', 'rb')
pdf_reader = PyPDF2.PdfFileReader(b_new)
200/53: pdf_reader.numPages
200/54: f = open('..\\UPDATED_NLP_COURSE\\00-Python-Text-Basics\\US_Declaration.pdf','rb')
200/55:
f = open('..\\UPDATED_NLP_COURSE\\00-Python-Text-Basics\\US_Declaration.pdf','rb')
pdf_text = [0]
pdf_reader = PyPDF2.PdfFileReader(f)

for p in range(pdf_reader.numPages):
    page = pdf_reader.getPage(p)
    pdf_text.append(page.extractText())
    
f.close()
200/56: pdf_text
200/57: pdf_text.size()
200/58: pdf_text.size
200/59: pdf_text
200/60: len(pdf_text)
200/61: import re
200/62: text = "The phone number is 207-37-27"
200/63: 'phone' in text
200/64: pattern = 'phone'
200/65: re.search(pattern, text)
200/66: my_match = re.search(pattern, text)
200/67: my_match.span()
200/68: my_match.start
200/69: my_match.start()
200/70: text = 'My phone is phone'
200/71: all_matches = re.findall('phone', text)
200/72: len(all_matches)
200/73:
for i in re.finditer('phone', text):
    print(i.span())
200/74: text = "The phone number is 207-37-27"
200/75: pattern = r'\d\d\d-\d\d-\d\d'
200/76: phone_num = re.search(pattern, text)
200/77: phone_num
200/78: pattern = r'\d{3}-\d\d-\d\d'
200/79: phone_num = re.search(pattern, text)
200/80: phone_num
200/81: pattern = r'\d{2,}-\d\d-\d\d'
200/82: phone_num = re.search(pattern, text)
200/83: phone_num
200/84: pattern.group()
200/85: phone_num.group()
200/86: pattern = r'\(d{2,})-(\d\d)-\d\d'
200/87: phone_num.group()
200/88: phone_num.group(0)
200/89: phone_num = re.search(pattern, text)
200/90: phone_num.group(0)
200/91: pattern = r'\(d{2,})-(\d\d)-(\d\d)'
200/92: phone_num = re.search(pattern, text)
200/93: pattern = r'(\d{2,})-(\d\d)-(\d\d)'
200/94: phone_num = re.search(pattern, text)
200/95: phone_num.group(0)
200/96: phone_num.group(0)
200/97: phone_num.group(1)
200/98: phone_num.group(2)
200/99: re.search(r"man")
200/100: re.search(r"man|woman", 'This is a man')
200/101: re.findall(r".at", "The cat hat sat")
200/102: re.findall(r".at", "The cat hat sat flat")
200/103: re.findall(r"%at", "The cat hat sat flat")
200/104: re.findall(r"..at", "The cat hat sat flat")
200/105: re.findall(r"?at", "The cat hat sat flat")
200/106: re.findall(r"..at", "The cat hat sat flat")
200/107: re.findall(r"\d$", 'ends 2')
200/108: re.findall(r"\d$", 'ends 233')
200/109: re.findall(r"^\d", 'ends 233')
200/110: re.findall(r"^\d", '2 ends 233')
200/111: re.findall(r"[^\d]+", 'There are 5 moods for 1 person')
200/112: re.findall(r"[^!.?]+", 'There! are, 5 moods for 1 person')
200/113: list_text = re.findall(r"[^!.?]+", 'There! are, 5 moods for 1 person')
200/114: ' '.join(list_text)
200/115: text = 'Only find the hyphen-words. There is a long-dish'
200/116: re.findall(r'[\w]+-[\w]+')
200/117: re.findall(r'[\w]+-[\w]+', text)
200/118: text = 'Only find the hyphen-words. There is a long-dish 33-33'
200/119: re.findall(r'[\d]+-[\d]+', text)
200/120: re.findall(r'[\d]+-[\w]+', text)
200/121: text = 'Only find the hyphen-words. There is a long-dish 33-w3'
200/122: re.findall(r'[\d]+-[\w]+', text)
200/123: re.findall(r'[\w]+-[\w]+', text)
203/1:
abbr = 'NLP'
full_text = 'Natural Language Processing'

# Enter your code here:
print('{abbr} stands for {full_text}')
203/2:
abbr = 'NLP'
full_text = 'Natural Language Processing'

# Enter your code here:
print(f'{abbr} stands for {full_text}')
203/3:
abbr = 'NLP'
full_text = 'Natural Language Processing'

# Enter your code here:
print(f'{abbr:10} stands for {full_text}')
203/4:
abbr = 'NLP'
full_text = 'Natural Language Processing'

# Enter your code here:
print(f'{abbr:.10} stands for {full_text}')
203/5:
abbr = 'NLP'
full_text = 'Natural Language Processing'

# Enter your code here:
print(f'{abbr:.>10} stands for {full_text}')
203/6:
%%writefile contacts.txt
First_Name Last_Name, Title, Extension, Email
203/7:
with open('contacts.txt') as c:
        fields.read(c)
203/8:
with open('contacts.txt') as c:
        fields = c.read()
203/9: fields
203/10:
# Write your code here:
with open('contacts.txt') as c:
    fields = c.read()

    
# Run fields to see the contents of contacts.txt:
fields
203/11:
# Perform import
import PyPDF2

# Open the file as a binary object
f = open('Business_Proposal.pdf', 'rb')
# Use PyPDF2 to read the text of the file
pdf_read = PyPDF2.PdfFileReader(f)


# Get the text from page 2 (CHALLENGE: Do this in one step!)
page_two_text = pdf_read.getPage(2)



# Close the file
f.close()

# Print the contents of page_two_text
print(page_two_text)
203/12:
# Perform import
import PyPDF2

# Open the file as a binary object
f = open('Business_Proposal.pdf', 'rb')
# Use PyPDF2 to read the text of the file
pdf_read = PyPDF2.PdfFileReader(f)


# Get the text from page 2 (CHALLENGE: Do this in one step!)
page_two_text = pdf_read.getPage(1).extractText()



# Close the file
f.close()

# Print the contents of page_two_text
print(page_two_text)
203/13:
# Perform import
import PyPDF2

# Open the file as a binary object
f = open('Business_Proposal.pdf', 'rb')
# Use PyPDF2 to read the text of the file
pdf_read = PyPDF2.PdfFileReader(f)
print(pdf_read.numPages)

# Get the text from page 2 (CHALLENGE: Do this in one step!)
page_two_text = pdf_read.getPage(1).extractText()



# Close the file
f.close()

# Print the contents of page_two_text
print(page_two_text)
203/14:
# Simple Solution:
with open('contacts.txt', 'a+') as c:
    c.write(page_two_text)
    c.seek(0)
    print(c.read())
203/15:
# CHALLENGE Solution (re-run the %%writefile cell above to obtain an unmodified contacts.txt file):

with open('contacts.txt', 'a+') as c:
    c.write(page_two_text[8:])
    c.seek(0)
    print(c.read())
203/16:
# CHALLENGE Solution (re-run the %%writefile cell above to obtain an unmodified contacts.txt file):

with open('contacts.txt', 'w+') as c:
    c.write(page_two_text[8:])
    c.seek(0)
    print(c.read())
203/17:
# CHALLENGE Solution (re-run the %%writefile cell above to obtain an unmodified contacts.txt file):

with open('contacts.txt', 'a+') as c:
    c.write(page_two_text[8:])
    c.seek(0)
    print(c.read())
203/18:
# CHALLENGE Solution (re-run the %%writefile cell above to obtain an unmodified contacts.txt file):

with open('contacts.txt', 'a+') as c:
    c.write(page_two_text[8:])
    c.seek(0)
    print(c.read())
203/19: page_two_text
203/20:
%%writefile contacts.txt
First_Name Last_Name, Title, Extension, Email
203/21:
# CHALLENGE Solution (re-run the %%writefile cell above to obtain an unmodified contacts.txt file):

with open('contacts.txt', 'a+') as c:
    c.write(page_two_text[8:])
    c.seek(0)
    print(c.read())
203/22:
import re

# Enter your regex pattern here. This may take several tries!
pattern = r'[\w]+@[\w]'

re.findall(pattern, page_two_text)
203/23:
import re

# Enter your regex pattern here. This may take several tries!
pattern = r'[\w]+@[\w]+'

re.findall(pattern, page_two_text)
203/24:
import re

# Enter your regex pattern here. This may take several tries!
pattern = r'[\w]+@[\w]+.com'

re.findall(pattern, page_two_text)
203/25:
import re

# Enter your regex pattern here. This may take several tries!
pattern = r'\w+@\w+.com'

re.findall(pattern, page_two_text)
203/26:
import re

# Enter your regex pattern here. This may take several tries!
pattern = r'\w+@\w+.\w{3}'

re.findall(pattern, page_two_text)
203/27:
import re

# Enter your regex pattern here. This may take several tries!
pattern = r'(\w+)@(\w+).(\w{3})'

re.findall(pattern, page_two_text)
204/1:
import re

# Enter your regex pattern here. This may take several tries!
pattern = r'\w+@\w+.\w{3}'

pat_list = re.findall(pattern, page_two_text)
203/28:
import re

# Enter your regex pattern here. This may take several tries!
pattern = r'(\w+)@(\w+).(\w{3})'

re.findall(pattern, page_two_text)
203/29:
import re

# Enter your regex pattern here. This may take several tries!
pattern = r'[\w]+@[\w]+.com'

re.findall(pattern, page_two_text)
203/30:
import re

# Enter your regex pattern here. This may take several tries!
pattern = r'[\w]+@[\w]+.com'

pat_list = re.findall(pattern, page_two_text)
203/31: pat_list
203/32:
import re

# Enter your regex pattern here. This may take several tries!
pattern = r'[\w]+@[\w]+.com'

pat_list = re.findall(pattern, page_two_text)[0]
203/33: pat_list
203/34:
import re

# Enter your regex pattern here. This may take several tries!
pattern = r'[\w]+@[\w]+.com'

pat_list = re.findall(pattern, page_two_text)
203/35:
import re

# Enter your regex pattern here. This may take several tries!
pattern = r'[\w]+@[\w]+.\w{3}'

pat_list = re.findall(pattern, page_two_text)
203/36: pat_list
203/37:
pattern = r'(\w+)@(\w+).(\w{3})'

pat_list = re.findall(pattern, page_two_text)
203/38: pat_list
203/39: pat_list[0]
203/40: pat_list
203/41: pat_list[9]
203/42: pat_list[0]
203/43: pat_list[0].dtype
203/44: pat_list[0]
203/45:
pattern = r'[\w+]@\w+.\w{3}'

pat_list = re.findall(pattern, page_two_text)
203/46: pat_list[0]
203/47:
pattern = r'[\w+]@[\w+].[\w{3}]'

pat_list = re.findall(pattern, page_two_text)
203/48: pat_list[0]
203/49:
pattern = r'\w+@\w+.\w{3}'

pat_list = re.findall(pattern, page_two_text)
203/50: pat_list[0]
203/51: pat_list
203/52:
pattern = r'[\w+]@\w+.\w{3}'

pat_list = re.findall(pattern, page_two_text)
203/53: pat_list
204/2:
import re

# Enter your regex pattern here. This may take several tries!
pattern = r'(\w+)@(\w+).(\w{3})'

pat_list = re.findall(pattern, page_two_text)
203/54:
import re

# Enter your regex pattern here. This may take several tries!
pattern = r'[\w]+@[\w]+.\w{3}'

pat_list = re.findall(pattern, page_two_text)
203/55: pat_list
203/56: pat_list
203/57: pat_list[0]
203/58: pat_list[0][0]
203/59:
import re

# Enter your regex pattern here. This may take several tries!
pattern = r'[\w]+@[\w]+.\w{3}'

pat_list = re.findall(pattern, page_two_text)
203/60: pat_list
203/61:
import re

# Enter your regex pattern here. This may take several tries!
pattern = r'[\w]+@[\w]+.\w{3}'

pat_list = re.findall(pattern[0], page_two_text)
203/62:
import re

# Enter your regex pattern here. This may take several tries!
pattern = r'[\w]+@[\w]+.\w{3}'

pat_list = re.findall(pattern, page_two_text)
203/63: pat_list
203/64:
pattern = r'\w+@\w+.\w{3}'

pat_list = re.findall(pattern, page_two_text)
203/65: pat_list
206/1:
# Import spaCy and load the language library
import spacy
nlp = spacy.load('en_core_web_sm')

# Create a Doc object
doc = nlp(u'Tesla is looking at buying U.S. startup for $6 million')

# Print each token separately
for token in doc:
    print(token.text, token.pos_, token.dep_)
206/2:
# Import spaCy and load the language library
import spacy
nlp = spacy.load('en_core_web_sm')

# Create a Doc object
doc = nlp(u'Tesla is looking at buying U.S. startup for $6 million')

# Print each token separately
for token in doc:
    print(token.text, token.pos, token.dep_)
206/3:
# Import spaCy and load the language library
import spacy
nlp = spacy.load('en_core_web_sm')

# Create a Doc object
doc = nlp(u'Tesla is looking at buying U.S. startup for $6 million')

# Print each token separately
for token in doc:
    print(token.text, token.pos_, token.dep_)
206/4:
# Import spaCy and load the language library
import spacy
nlp = spacy.load('en_core_web_sm')

# Create a Doc object
doc = nlp(u'Tesla is looking at buying U.S. startup for $6 million')

# Print each token separately
for token in doc:
    print(token.text, token.pos_, token.dep_, token.ent_id_)
206/5: nlp.pipeline
206/6:
doc2 = nlp(u"Tesla isn't looking into startups anymore.")

for token in doc2:
    print(token.text, token.pos_, token.dep_)
206/7:
doc2 = nlp(u"Tesla isn't      looking into startups anymore.")

for token in doc2:
    print(token.text, token.pos_, token.dep_)
206/8: doc2
206/9: doc2[0]
206/10: doc2[0].pos
206/11: doc2[0].pos_
206/12:
doc3 = nlp(u'Although commmonly attributed to John Lennon from his song "Beautiful Boy", \
the phrase "Life is what happens to us while we are making other plans" was written by \
cartoonist Allen Saunders and published in Reader\'s Digest in 1957, when Lennon was 17.')
206/13:
life_quote = doc3[16:30]
print(life_quote)
207/1:
# Import spaCy and load the language library
import spacy
nlp = spacy.load('en_core_web_sm')
207/2:
# Create a string that includes opening and closing quotation marks
mystring = '"We\'re moving to L.A.!"'
print(mystring)
207/3:
# Create a Doc object and explore tokens
doc = nlp(mystring)

for token in doc:
    print(token.text, end=' | ')
207/4:
doc4 = nlp(u"Let's visit St. Louis in the U.S. next year.")

for t in doc4:
    print(t)
207/5: len(doc)
207/6: len(doc4)
207/7:
doc5 = nlp(u'It is better to give than to receive.')

# Retrieve the third token:
doc5[2]
207/8:
# Retrieve three tokens from the middle:
doc5[2:5]
207/9:
# Retrieve the last four tokens:
doc5[-4:]
207/10:
doc6 = nlp(u'My dinner was horrible.')
doc7 = nlp(u'Your dinner was delicious.')
207/11:
# Try to change "My dinner was horrible" to "My dinner was delicious"
doc6[3] = doc7[3]
207/12:
doc8 = nlp(u'Apple to build a Hong Kong factory for $6 million')

for token in doc8:
    print(token.text, end=' | ')

print('\n----')

for ent in doc8.ents:
    print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
207/13:
for entity in doc8.ents:
    print(entity)
    print(entity.label_)
    print(str(spacy.explain(entity.label_)))
    print('\n')
207/14:
doc9 = nlp(u"Autonomous cars shift insurance liability toward manufacturers.")

for chunk in doc9.noun_chunks:
    print(chunk.text)
207/15:
from spacy import displacy

doc = nlp(u'Apple is going to build a U.K. factory for $6 million.')
displacy.render(doc, style='dep', jupyter=True, options={'distance': 110})
207/16:
from spacy import displacy

doc = nlp(u'Apple is going to build a U.K. factory for $6 million.')
displacy.render(doc, style='dep', jupyter=True, options={'distance': 60})
207/17:
from spacy import displacy

doc = nlp(u'Apple is going to build a U.K. factory for $6 million.')
displacy.render(doc, style='dep', jupyter=True, options={'distance': 80})
207/18:
doc = nlp(u'Over the last quarter Apple sold nearly 20 thousand iPods for a profit of $6 million.')
displacy.render(doc, style='ent', jupyter=True)
207/19:
doc = nlp(u'This is a sentence.')
displacy.serve(doc, style='dep')
208/1:
# Import spaCy and load the language library
import spacy
nlp = spacy.load('en_core_web_sm')

# Create a Doc object
doc = nlp(u'Tesla is looking at buying U.S. startup for $6 million')

# Print each token separately
for token in doc:
    print(token.text, token.pos_, token.dep_, token.ent_id_)
208/2: nlp.pipeline
208/3: nlp.pipe_names
208/4:
doc2 = nlp(u"Tesla isn't      looking into startups anymore.")

for token in doc2:
    print(token.text, token.pos_, token.dep_)
208/5: doc2
208/6: doc2[0].pos_
208/7: type(doc2)
208/8: doc2[0].pos_
208/9: doc2[0].dep_
208/10: spacy.explain('PROPN')
208/11: spacy.explain('nsubj')
208/12:
# Lemmas (the base form of the word):
print(doc2[4].text)
print(doc2[4].lemma_)
208/13:
# Simple Parts-of-Speech & Detailed Tags:
print(doc2[4].pos_)
print(doc2[4].tag_ + ' / ' + spacy.explain(doc2[4].tag_))
208/14:
# Word Shapes:
print(doc2[0].text+': '+doc2[0].shape_)
print(doc[5].text+' : '+doc[5].shape_)
208/15:
# Boolean Values:
print(doc2[0].is_alpha)
print(doc2[0].is_stop)
208/16:
doc3 = nlp(u'Although commmonly attributed to John Lennon from his song "Beautiful Boy", \
the phrase "Life is what happens to us while we are making other plans" was written by \
cartoonist Allen Saunders and published in Reader\'s Digest in 1957, when Lennon was 17.')
208/17:
life_quote = doc3[16:30]
print(life_quote)
208/18: type(life_quote)
208/19: doc4 = nlp(u'This is the first sentence. This is another sentence. This is the last sentence.')
208/20:
for sent in doc4.sents:
    print(sent)
208/21: doc4[6].is_sent_start
209/1:
# Import spaCy and load the language library
import spacy
nlp = spacy.load('en_core_web_sm')
209/2:
# Create a string that includes opening and closing quotation marks
mystring = '"We\'re moving to L.A.!"'
print(mystring)
209/3:
# Create a Doc object and explore tokens
doc = nlp(mystring)

for token in doc:
    print(token.text, end=' | ')
209/4:
doc2 = nlp(u"We're here to help! Send snail-mail, email support@oursite.com or visit us at http://www.oursite.com!")

for t in doc2:
    print(t)
209/5:
doc3 = nlp(u'A 5km NYC cab ride costs $10.30')

for t in doc3:
    print(t)
209/6:
doc4 = nlp(u"Let's visit St. Louis in the U.S. next year.")

for t in doc4:
    print(t)
209/7: len(doc4)
209/8: len(doc.vocab)
209/9:
doc5 = nlp(u'It is better to give than to receive.')

# Retrieve the third token:
doc5[2]
209/10:
# Retrieve three tokens from the middle:
doc5[2:5]
209/11:
# Retrieve the last four tokens:
doc5[-4:]
209/12:
doc6 = nlp(u'My dinner was horrible.')
doc7 = nlp(u'Your dinner was delicious.')
209/13:
# Try to change "My dinner was horrible" to "My dinner was delicious"
doc6[3] = doc7[3]
209/14:
doc8 = nlp(u'Apple to build a Hong Kong factory for $6 million')

for token in doc8:
    print(token.text, end=' | ')

print('\n----')

for ent in doc8.ents:
    print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
209/15:
for entity in doc8.ents:
    print(entity)
    print(entity.label_)
    print(str(spacy.explain(entity.label_)))
    print('\n')
209/16: len(doc8.ents)
209/17:
doc9 = nlp(u"Autonomous cars shift insurance liability toward manufacturers.")

for chunk in doc9.noun_chunks:
    print(chunk.text)
209/18:
doc10 = nlp(u"Red cars do not carry higher insurance rates.")

for chunk in doc10.noun_chunks:
    print(chunk.text)
209/19:
doc11 = nlp(u"He was a one-eyed, one-horned, flying, purple people-eater.")

for chunk in doc11.noun_chunks:
    print(chunk.text)
209/20:
from spacy import displacy

doc = nlp(u'Apple is going to build a U.K. factory for $6 million.')
displacy.render(doc, style='dep', jupyter=True, options={'distance': 80})
209/21:
doc = nlp(u'Over the last quarter Apple sold nearly 20 thousand iPods for a profit of $6 million.')
displacy.render(doc, style='ent', jupyter=True)
209/22:
doc = nlp(u'This is a sentence.')
displacy.serve(doc, style='dep')
210/1:
# Import the toolkit and the full Porter Stemmer library
import nltk

from nltk.stem.porter import *
210/2: p_stemmer = PorterStemmer()
210/3: words = ['run','runner','running','ran','runs','easily','fairly']
210/4:
# Import the toolkit and the full Porter Stemmer library
import nltk

from nltk.stem.porter import *
210/5: p_stemmer = PorterStemmer()
210/6: words = ['run','runner','running','ran','runs','easily','fairly']
210/7:
for word in words:
    print(word+' --> '+p_stemmer.stem(word))
210/8:
from nltk.stem.snowball import SnowballStemmer

# The Snowball Stemmer requires that you pass a language parameter
s_stemmer = SnowballStemmer(language='english')
210/9:
words = ['run','runner','running','ran','runs','easily','fairly']
# words = ['generous','generation','generously','generate']
210/10:
for word in words:
    print(word+' --> '+s_stemmer.stem(word))
211/1:
# Perform standard imports:
import spacy
nlp = spacy.load('en_core_web_sm')
211/2:
doc1 = nlp(u"I am a runner running in a race because I love to run since I ran today")

for token in doc1:
    print(token.text, '\t', token.pos_, '\t', token.lemma, '\t', token.lemma_)
211/3:
def show_lemmas(text):
    for token in text:
        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')
211/4:
doc2 = nlp(u"I saw eighteen mice today!")

show_lemmas(doc2)
212/1:
# Perform standard imports:
import spacy
nlp = spacy.load('en_core_web_sm')
212/2:
# Print the set of spaCy's default stop words (remember that sets are unordered):
print(nlp.Defaults.stop_words)
212/3: nlp.vocab['myself'].is_stop
212/4: nlp.vocab['mystery'].is_stop
212/5:
# Add the word to the set of stop words. Use lowercase!
nlp.Defaults.stop_words.add('btw')

# Set the stop_word tag on the lexeme
nlp.vocab['btw'].is_stop = True
212/6: len(nlp.Defaults.stop_words)
212/7: nlp.vocab['btw'].is_stop
213/1:
# Perform standard imports
import spacy
nlp = spacy.load('en_core_web_sm')
213/2:
# Import the Matcher library
from spacy.matcher import Matcher
matcher = Matcher(nlp.vocab)
213/3:
pattern1 = [{'LOWER': 'solarpower'}]
pattern2 = [{'LOWER': 'solar'}, {'LOWER': 'power'}]
pattern3 = [{'LOWER': 'solar'}, {'IS_PUNCT': True}, {'LOWER': 'power'}]

matcher.add('SolarPower', None, pattern1, pattern2, pattern3)
213/4:
pattern1 = [{'LOWER': 'solarpower'}]
pattern2 = [{'LOWER': 'solar'}, {'LOWER': 'power'}]
pattern3 = [{'LOWER': 'solar'}, {'IS_PUNCT': True}, {'LOWER': 'power'}]

matcher.add('SolarPower', None, pattern1, pattern2, pattern3)
213/5:
doc = nlp(u'The Solar Power industry continues to grow as demand \
for solarpower increases. Solar-power cars are gaining popularity.')
213/6:
found_matches = matcher(doc)
print(found_matches)
213/7:
for match_id, start, end in found_matches:
    string_id = nlp.vocab.strings[match_id]  # get string representation
    span = doc[start:end]                    # get the matched span
    print(match_id, string_id, start, end, span.text)
213/8:
# Redefine the patterns:
pattern1 = [{'LOWER': 'solarpower'}]
pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'power'}]

# Remove the old patterns to avoid duplication:
matcher.remove('SolarPower')

# Add the new set of patterns to the 'SolarPower' matcher:
matcher.add('SolarPower', None, pattern1, pattern2)
213/9:
found_matches = matcher(doc)
print(found_matches)
213/10:
pattern1 = [{'LOWER': 'solarpower'}]
pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LEMMA': 'power'}] # CHANGE THIS PATTERN

# Remove the old patterns to avoid duplication:
matcher.remove('SolarPower')

# Add the new set of patterns to the 'SolarPower' matcher:
matcher.add('SolarPower', None, pattern1, pattern2)
213/11: doc2 = nlp(u'Solar-powered energy runs solar-powered cars.')
213/12:
found_matches = matcher(doc2)
print(found_matches)
213/13:
# Perform standard imports, reset nlp
import spacy
nlp = spacy.load('en_core_web_sm')
213/14:
# Import the PhraseMatcher library
from spacy.matcher import PhraseMatcher
matcher = PhraseMatcher(nlp.vocab)
213/15:
with open('../TextFiles/reaganomics.txt', encoding='utf8') as f:
    doc3 = nlp(f.read())
213/16:
with open('../TextFiles/reaganomics.txt', encoding='cp1252') as f:
    doc3 = nlp(f.read())
213/17:
# First, create a list of match phrases:
phrase_list = ['voodoo economics', 'supply-side economics', 'trickle-down economics', 'free-market economics']

# Next, convert each phrase to a Doc object:
phrase_patterns = [nlp(text) for text in phrase_list]

# Pass each Doc object into matcher (note the use of the asterisk!):
matcher.add('VoodooEconomics', None, *phrase_patterns)

# Build a list of matches:
matches = matcher(doc3)
213/18:
# First, create a list of match phrases:
phrase_list = ['voodoo economics', 'supply-side economics', 'trickle-down economics', 'free-market economics']

# Next, convert each phrase to a Doc object:
phrase_patterns = [nlp(text) for text in phrase_list]

# Pass each Doc object into matcher (note the use of the asterisk!):
matcher.add('VoodooEconomics', None, *phrase_patterns)

# Build a list of matches:
matches = matcher(doc3)
213/19:
# (match_id, start, end)
matches
214/1:
# RUN THIS CELL to perform standard imports:
import spacy
nlp = spacy.load('en_core_web_sm')
214/2:
# Enter your code here:
with open('../TextFiles/owlcreek.txt', encoding='cp1252') as f:
    doc = nlp(f.read())
214/3:

doc[:36]
214/4:
# Run this cell to verify it worked:

doc[:36]
214/5: len(doc)
214/6: doc[2:22]
214/7: doc[-10:]
214/8: doc[-100:]
214/9:
for sent in doc.sent:
    print(sent)
214/10:
for sent in doc.sents:
    print(sent)
214/11: doc.sents
214/12: len(doc.sents)
214/13: doc.sents()
214/14: [sent in doc.sents]
214/15: sent_list = [sent for sent in doc.sents]
214/16: sent_list
214/17: len(sent_list)
214/18: sent_list[1]
214/19:
for token in sent_5:
    print(token.text)
214/20: sent_5 = sent_list[1]
214/21:
for token in sent_5:
    print(token.text)
214/22:
for token in sent_5:
    print(token.text, token.POS, token.dep, token.lemma)
214/23:
for token in sent_5:
    print(token.text, token.POS_, token.dep, token.lemma)
214/24:
for token in sent_5:
    print(token.text, token.pos_, token.dep, token.lemma)
214/25:
for token in sent_5:
    print(token.text, token.pos_, token.dep_, token.lemma)
214/26:
for token in sent_5:
    print(token.text, token.pos_, token.dep_, token.lemma_)
214/27:
for token in sent_5:
    print(token.text, '\t', token.pos_, '\t', token.dep_,'\t', token.lemma_)
214/28:
for token in sent_5:
    print(f'{token.text:{:15}, token.pos_, token.dep_, token.lemma_})
214/29:
for token in sent_5:
    print(f'{token.text:{:15}, token.pos_, token.dep_, token.lemma_}')
214/30:
for token in sent_5:
    print(token.text, '\t', token.pos_, '\t', token.dep_,'\t', token.lemma_)
214/31:
for token in sent_5:
    print(f'{token.text:{15}, token.pos_, token.dep_, token.lemma_}')
214/32:
for token in sent_5:
    print(f'{token.text:{15} token.pos_ token.dep_ token.lemma_}')
214/33:
for token in sent_5:
    print(f'{token.text:{15}}')
214/34:
for token in sent_5:
    print(f'{token.text:{15} token.pos_:{10}}')
214/35:
for token in sent_5:
    print(f'{token.text:{15}} {token.pos_:{10}}')
214/36:
for token in sent_5:
    print(f'{token.text:{15}} {token.pos_:{10}} {token.dep_:{10}} {token.lemma_:{10}}')
214/37:
for token in sent_5:
    print(f'{token.text:>{15}} {token.pos_:{10}} {token.dep_:{10}} {token.lemma_:{10}}')
214/38:
for token in sent_5:
    print(f'{token.text:>.{15}} {token.pos_:{10}} {token.dep_:{10}} {token.lemma_:{10}}')
214/39:
for token in sent_5:
    print(f'{token.text:.>{15}} {token.pos_:{10}} {token.dep_:{10}} {token.lemma_:{10}}')
214/40:
for token in sent_5:
    print(f'{token.text:{15}} {token.pos_:{10}} {token.dep_:{10}} {token.lemma_:{10}}')
214/41:
for token in sent_5:
    print(f'{token.text:{10}} {token.pos_:{10}} {token.dep_:{10}} {token.lemma_:{10}}')
214/42:
for token in sent_5:
    print(f'{token.text:{10}} {token.pos_:{10}} {token.dep_:{10}} {token.lemma_:.>{10}}')
214/43:
# Import the Matcher library:

from spacy.matcher import Matcher
matcher = Matcher(nlp.vocab)
214/44:
# Create a pattern and add it to matcher:
pattern = [{'LOWER': 'swimming'}, {'IS_SPACE': True, 'OP': '*'}, {'LOWER': 'vigorously'}]
214/45: matcher.add('swimming vigorously', None, pattern)
214/46: found_matches = matcher(sent_5)
214/47:
found_matches = matcher(doc)
print(found_matches)
214/48: found_matches = matcher(doc)
214/49:
# Create a list of matches called "found_matches" and print the list:

found_matches
214/50: print(doc[1274])
214/51: print(doc[1274-4])
214/52: print(doc[1274-4: 1274+4])
214/53: print(doc[1274-10: 1274+4])
214/54: print(doc[1274-9: 1274+4])
214/55: print(doc[1274-9: 1274+14])
214/56: print(doc[1274-9: 1274+16])
214/57: found_matches
214/58: found_matches[0]
214/59: found_matches[0][1]
214/60:
for sent in doc.sents:
    if found_matches[0][1] < sent.end:
        print(sent)
214/61:
for sent in doc.sents:
    if found_matches[0][1] < sent.end:
        print(sent)
        break
214/62:
for sent in doc.sents:
    if found_matches[1][1] < sent.end:
        print(sent)
        break
216/1:
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split

from sklearn.feature_selection import VarianceThreshold
216/2:
# load our first dataset

# (feel free to write some code to explore the dataset and become
# familiar with it ahead of this demo)

data = pd.read_csv('../dataset_1.csv')
data.shape
216/3: data.head(3)
216/4: data.describe()
216/5:
const_f = [
    feat for feat in X_train.columns if X_train[feat].std() == 0
]
216/6:
# separate train and test (again, as we transformed the previous ones)

X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),
    data['target'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
216/7:
# short and easy: find constant features

# in this dataset, all features are numeric,
# so this bit of code will suffice:

constant_features = [
    feat for feat in X_train.columns if X_train[feat].std() == 0
]

len(constant_features)
216/8:
const_f = [
    feat for feat in X_train.columns if X_train[feat].std() == 0
]
216/9: len(const_f)
217/1:
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split

from sklearn.feature_selection import VarianceThreshold
217/2:
# load dataset

# (feel free to write some code to explore the dataset and become
# familiar with it ahead of this demo)

data = pd.read_csv('../dataset_1.csv')
data.shape
217/3:
# separate dataset into train and test

X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1), # drop the target
    data['target'], # just the target
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
217/4:
# using the code from the previous lecture
# I remove 34 constant features

constant_features = [
    feat for feat in X_train.columns if X_train[feat].std() == 0
]

X_train.drop(labels=constant_features, axis=1, inplace=True)
X_test.drop(labels=constant_features, axis=1, inplace=True)

X_train.shape, X_test.shape
217/5:
# separate train and test
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),
    data['target'],
    test_size=0.3,
    random_state=0)

# remove constant features
# using the code from the previous lecture

constant_features = [
    feat for feat in X_train.columns if X_train[feat].std() == 0
]

X_train.drop(labels=constant_features, axis=1, inplace=True)
X_test.drop(labels=constant_features, axis=1, inplace=True)

X_train.shape, X_test.shape
217/6: X_train
217/7: X_train[var_1]
217/8: X_train['var_1']
217/9: X_train['var_1'].value_counts()
217/10: X_train['var_1'].value_counts() / len(X_train)
217/11: (X_train['var_1'].value_counts() / len(X_train)).sort_values(ascending = False)
217/12: (X_train['var_1'].value_counts() / len(X_train)).sort_values(ascending = False)[0]
218/1:
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
218/2:
# load dataset

data = pd.read_csv('../dataset_1.csv')
data.shape
218/3:
# check the presence of missing data.
# (there are no missing data in this dataset)

[col for col in data.columns if data[col].isnull().sum() > 0]
221/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
221/2:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
223/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
223/2: !conda install seaborn
224/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
225/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
225/2:
# load dataset

data = pd.read_csv('../dataset_2.csv', nrows=50000)
data.shape
225/3: data.head()
225/4:
# separate train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),
    data['target'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
225/5:
# visualise correlated features:

# I will build a correlation matrix, which examines the 
# correlation of all features (that is, for all possible feature combinations)
# and then visualise the correlation matrix using a heatmap

# the default correlation method of pandas.corr is pearson
# I include it anyways for the demo
corrmat = X_train.corr(method='pearson')

# we can make a heatmap with the package seaborn
# and customise the colours of searborn's heatmap
cmap = sns.diverging_palette(220, 20, as_cmap=True)

# some more parameters for the figure
fig, ax = plt.subplots()
fig.set_size_inches(11,11)

# and now plot the correlation matrix
sns.heatmap(corrmat, cmap=cmap)
225/6:
# separate train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),
    data['target'],
    test_size=0.3,
    random_state=0)
225/7:
# build a dataframe with the correlation between features
# remember that the absolute value of the correlation
# coefficient is important and not the sign

corrmat = X_train.corr()
corrmat = corrmat.abs().unstack() # absolute value of corr coef
corrmat = corrmat.sort_values(ascending=False)
corrmat = corrmat[corrmat >= 0.8]
corrmat = corrmat[corrmat < 1]
corrmat = pd.DataFrame(corrmat).reset_index()
corrmat.columns = ['feature1', 'feature2', 'corr']
corrmat.head()
225/8:
corrmat = X_train.corr()
corrmat = corrmat.abs().unstack()
225/9: corrmat
225/10:
corrmat = X_train.corr()
corrmat = corrmat.abs().unstack().sort_values(ascending=False)
225/11: corrmat
225/12: X_train.corr()
225/13:
corrmat = X_train.corr()
corrmat = corrmat.abs().unstack().sort_values(ascending=False)
225/14: corrmat
225/15: corrmat.shape
225/16: corrmat
225/17: corrmat[6]
225/18: corrmat
225/19: corrmat[2]
225/20: corrmat
225/21: corrmat.index
225/22: corrmat
225/23: corrmat[corrmat > .4]
225/24: corrmat[(corrmat > .4) & (corrmat < 1)]
225/25: corrmat[(corrmat > .4) & (corrmat < 1)].reset_index()
225/26:
# build a dataframe with the correlation between features
# remember that the absolute value of the correlation
# coefficient is important and not the sign

corrmat = X_train.corr()
corrmat = corrmat.abs().unstack().sort_values(ascending=False) # absolute value of corr coef
corrmat = corrmat[(corrmat >= 0.8) & (corrmat < 1)]
corrmat = pd.DataFrame(corrmat).reset_index()
corrmat.columns = ['feature1', 'feature2', 'corr']
corrmat.head()
225/27:
# we can now investigate further features within one group.
# let's for example select group 1

group = correlated_groups[1]
group
225/28:
# build a dataframe with the correlation between features
# remember that the absolute value of the correlation
# coefficient is important and not the sign

corrmat = X_train.corr()
corrmat = corrmat.abs().unstack().sort_values(ascending=False) # absolute value of corr coef
corrmat = corrmat[(corrmat >= 0.8) & (corrmat < 1)]
corrmat = pd.DataFrame(corrmat).reset_index()
corrmat.columns = ['feature1', 'feature2', 'corr']
corrmat.head()
225/29:
# find groups of correlated features

grouped_feature_ls = []
correlated_groups = []

for feature in corrmat.feature1.unique():
    
    if feature not in grouped_feature_ls:

        # find all features correlated to a single feature
        correlated_block = corrmat[corrmat.feature1 == feature]
        grouped_feature_ls = grouped_feature_ls + list(
            correlated_block.feature2.unique()) + [feature]

        # append the block of features to the list
        correlated_groups.append(correlated_block)

print('found {} correlated groups'.format(len(correlated_groups)))
print('out of {} total features'.format(X_train.shape[1]))
225/30:
# now we can print out each group. We see that some groups contain
# only 2 correlated features, some other groups present several features 
# that are correlated among themselves.

for group in correlated_groups:
    print(group)
    print()
225/31:
# we can now investigate further features within one group.
# let's for example select group 1

group = correlated_groups[1]
group
225/32:
from sklearn.ensemble import RandomForestClassifier

# add all features of the group to a list
features = list(group['feature2'].unique())+['var_22']

# train a random forest 
rf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=4)
rf.fit(X_train[features].fillna(0), y_train)
225/33: group
225/34: group['feature2'].unique()
225/35: features
225/36: corrmat
225/37: corrmat.groupby('feature1')
225/38: corrmat.groupby('feature1')[0]
225/39: corrmat.groupby('feature1')(1)
225/40: corrmat.groupby('feature1')
225/41: corrmat.groupby('feature1')[1]
225/42: corrmat.groupby('feature1')[0]
225/43: gc = corrmat.groupby('feature1')
225/44: gc.groups()
225/45: gc.groups
225/46: gc.groups.keys()
225/47: list(g.groups)
225/48: list(gc.groups)
225/49: gc['var_100']
225/50: gc.get_group('var_100')
225/51: gc = corrmat.groupby('feature1').count()
225/52: corrmat.groupby('feature1').count()
225/53: corrmat.groupby('feature1').count().reset_index()
225/54: corrmat.groupby('feature1').count().reset_index().filter(corr > 1)
225/55: corrmat.groupby('feature1').count().reset_index().filter('corr' > 1)
225/56: corrmat.groupby('feature1').count().reset_index().where('corr' > 1)
225/57: corrmat.groupby('feature1').count().reset_index()[corrmat.corr > 1]
225/58: corrmat.groupby('feature1').count().reset_index()[corrmat['corr'] > 1]
225/59: corrmat.groupby('feature1').count().reset_index()
225/60: corrmat.groupby('feature1')['corr'].count().reset_index()
225/61: corrmat.groupby('feature1')['corr'].transform('count') > 1
225/62: corrmat[corrmat.groupby('feature1')['corr'].transform('count') > 1]
225/63: corrmat.groupby('feature1')['corr'].count()
225/64: corrmat.groupby('feature1')['corr'].count().reset_index()
225/65:
from sklearn.ensemble import RandomForestClassifier

# add all features of the group to a list
features = list(group['feature2'].unique())+['var_32']

# train a random forest 
rf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=4)
rf.fit(X_train[features].fillna(0), y_train)
225/66: features
225/67: group['feature2'].unique()
225/68: group
225/69:
# get the feature importance attributed by the 
# random forest model (more on this in coming lectures)

importance = pd.concat(
    [pd.Series(features),
     pd.Series(rf.feature_importances_)], axis=1)

importance.columns = ['feature', 'importance']

# sort features by importance, most important first
importance.sort_values(by='importance', ascending=False)
226/1:
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import roc_auc_score
226/2:
# load the dataset

data = pd.read_csv('../dataset_1.csv')
data.shape
226/3:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),
    data['target'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
226/4:
# I keep a copy of the dataset with all the variables
# to measure the performance of machine learning models
# at the end of the notebook

X_train_original = X_train.copy()
X_test_original = X_test.copy()
226/5:
# remove constant features
constant_features = [
    feat for feat in X_train.columns if X_train[feat].std() == 0
]

X_train.drop(labels=constant_features, axis=1, inplace=True)
X_test.drop(labels=constant_features, axis=1, inplace=True)

X_train.shape, X_test.shape
226/6:
# remove quasi-constant features
sel = VarianceThreshold(
    threshold=0.01)  # 0.1 indicates 99% of observations approximately

sel.fit(X_train)  # fit finds the features with low variance

sum(sel.get_support()) # how many not quasi-constant?
226/7: features_to_keep = X_train.columns[sel.get_support()]
226/8:
# we can then remove the features like this
X_train = sel.transform(X_train)
X_test = sel.transform(X_test)

X_train.shape, X_test.shape
226/9:
# sklearn transformations lead to numpy arrays
# here I transform the arrays back to dataframes
# please be mindful of getting the columns assigned
# correctly

X_train= pd.DataFrame(X_train)
X_train.columns = features_to_keep

X_test= pd.DataFrame(X_test)
X_test.columns = features_to_keep
226/10:
# check for duplicated features in the training set
duplicated_feat = []
for i in range(0, len(X_train.columns)):
    if i % 10 == 0:  # this helps me understand how the loop is going
        print(i)

    col_1 = X_train.columns[i]

    for col_2 in X_train.columns[i + 1:]:
        if X_train[col_1].equals(X_train[col_2]):
            duplicated_feat.append(col_2)
            
len(duplicated_feat)
226/11:
# remove duplicated features
X_train.drop(labels=duplicated_feat, axis=1, inplace=True)
X_test.drop(labels=duplicated_feat, axis=1, inplace=True)

X_train.shape, X_test.shape
226/12:
# I keep a copy of the dataset except constant and duplicated variables
# to measure the performance of machine learning models
# at the end of the notebook

X_train_basic_filter = X_train.copy()
X_test_basic_filter = X_test.copy()
226/13:
# find and remove correlated features

def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns
    
    corr_matrix = dataset.corr()
    
    for i in range(len(corr_matrix.columns)):
    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr

corr_features = correlation(X_train, 0.8)
print('correlated features: ', len(set(corr_features)) )
226/14:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
226/15:
# create a function to build random forests and compare performance in train and test set

def run_randomForests(X_train, X_test, y_train, y_test):
    rf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=4)
    rf.fit(X_train, y_train)
    print('Train set')
    pred = rf.predict_proba(X_train)
    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))
    print('Test set')
    pred = rf.predict_proba(X_test)
    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))
226/16:
# original dataset (all variables)
run_randomForests(X_train_original,
                  X_test_original,
                  y_train, y_test)
226/17:
# filter methods - basic
run_randomForests(X_train_basic_filter,
                  X_test_basic_filter,
                  y_train, y_test)
226/18:
# filter methods - correlation
run_randomForests(X_train,
                  X_test,
                  y_train, y_test)
226/19:
# create a function to build logistic regression and compare performance in train and test set

def run_logistic(X_train, X_test, y_train, y_test):
    # function to train and test the performance of logistic regression
    logit = LogisticRegression(random_state=44, max_iter=500)
    logit.fit(X_train, y_train)
    print('Train set')
    pred = logit.predict_proba(X_train)
    print('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))
    print('Test set')
    pred = logit.predict_proba(X_test)
    print('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))
226/20:
# original
# for logistic regression features need to be in the same scale

# original
scaler = StandardScaler().fit(X_train_original)

run_logistic(scaler.transform(X_train_original),
             scaler.transform(X_test_original), y_train, y_test)
226/21:
# filter methods - basic
scaler = StandardScaler().fit(X_train_basic_filter)

run_logistic(scaler.transform(X_train_basic_filter),
             scaler.transform(X_test_basic_filter),
                  y_train, y_test)
226/22:
# filter methods - correlation
scaler = StandardScaler().fit(X_train)

run_logistic(scaler.transform(X_train),
             scaler.transform(X_test),
                  y_train, y_test)
227/1:
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

# to obtain the mutual information values
from sklearn.feature_selection import mutual_info_classif, mutual_info_regression

# to select the features
from sklearn.feature_selection import SelectKBest, SelectPercentile
227/2:
# load dataset

data = pd.read_csv('../dataset_2.csv')
data.shape
227/3: data.head()
227/4:
# separate train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),
    data['target'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
227/5:
# calculate the mutual information between the variables and the target

# the smaller the value of the mi, the less information we can infer from
# the feature about the target

mi = mutual_info_classif(X_train, y_train)
mi
227/6:
# calculate the mutual information between the variables and the target

# the smaller the value of the mi, the less information we can infer from
# the feature about the target

mi = mutual_info_classif(X_train, y_train)
mi
227/7:
# 1) let's capture the above array in a pandas series
# 2)add the variable names in the index
# 3) sort the features based on their mutual information value
# 4) and make a var plot

mi = pd.Series(mi)
mi.index = X_train.columns
mi.sort_values(ascending=False).plot.bar(figsize=(20, 6))
plt.ylabel('Mutual Information')
227/8:
# here we will select the top 10 features
# based on their mutual information value

# select features
sel_ = SelectKBest(mutual_info_classif, k=10).fit(X_train, y_train)

# display features
X_train.columns[sel_.get_support()]
227/9:
# to remove the rest of the features:

X_train = sel_.transform(X_train)
X_test = sel_.transform(X_test)
227/10: X_train.shape
227/11:
# load dataset
data = pd.read_csv('../houseprice.csv')
data.shape
227/12:
# load dataset
data = pd.read_csv('../dataset_1.csv')
data.shape
227/13:
# In practice, feature selection should be done after data pre-processing,
# so ideally, all the categorical variables are encoded into numbers,
# and then you can assess how deterministic they are of the target

# here for simplicity I will use only numerical variables
# select numerical columns:

numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
numerical_vars = list(data.select_dtypes(include=numerics).columns)
data = data[numerical_vars]
data.shape
227/14:
# load dataset
data = pd.read_csv('dataset_1.csv')
data.shape
227/15:
# load dataset
data = pd.read_csv('dataset_1.csv')
data.shape
227/16:
# load dataset
data = pd.read_csv('../dataset_1.csv')
data.shape
227/17:
# load dataset
data = pd.read_csv('../dataset_2.csv')
data.shape
227/18:
# In practice, feature selection should be done after data pre-processing,
# so ideally, all the categorical variables are encoded into numbers,
# and then you can assess how deterministic they are of the target

# here for simplicity I will use only numerical variables
# select numerical columns:

numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
numerical_vars = list(data.select_dtypes(include=numerics).columns)
data = data[numerical_vars]
data.shape
228/1:
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold, f_classif, SelectKBest

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import roc_auc_score
228/2:
# load the Santander customer satisfaction dataset from Kaggle

data = pd.read_csv('../dataset_1.csv')
data.shape
228/3:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),
    data['target'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
228/4:
# I keep a copy of the dataset with all the variables
# to compare the performance of machine learning models
# at the end of the notebook

X_train_original = X_train.copy()
X_test_original = X_test.copy()
228/5:
constant_features = [
    feat for feat in X_train.columns if X_train[feat].std() == 0
]

X_train.drop(labels=constant_features, axis=1, inplace=True)
X_test.drop(labels=constant_features, axis=1, inplace=True)

X_train.shape, X_test.shape
228/6:
sel = VarianceThreshold(threshold=0.01)

sel.fit(X_train) # finds the features with low variance

sum(sel.get_support()) # how many not quasi-constant?
228/7: features_to_keep = X_train.columns[sel.get_support()]
228/8:
# remove the features

X_train = sel.transform(X_train)
X_test = sel.transform(X_test)

X_train.shape, X_test.shape
228/9:
# sklearn transformations lead to numpy arrays
# here I transform the arrays back to dataframes

X_train= pd.DataFrame(X_train)
X_train.columns = features_to_keep

X_test= pd.DataFrame(X_test)
X_test.columns = features_to_keep
228/10:
# check for duplicated features in the training set

duplicated_feat = []
for i in range(0, len(X_train.columns)):
    if i % 10 == 0:  # this helps me understand how the loop is going
        print(i)

    col_1 = X_train.columns[i]

    for col_2 in X_train.columns[i + 1:]:
        if X_train[col_1].equals(X_train[col_2]):
            duplicated_feat.append(col_2)
            
len(duplicated_feat)
228/11:
# remove duplicated features
X_train.drop(labels=duplicated_feat, axis=1, inplace=True)
X_test.drop(labels=duplicated_feat, axis=1, inplace=True)

X_train.shape, X_test.shape
228/12:
# I keep a copy of the dataset except constant and duplicated variables
# to measure the performance of machine learning models
# at the end of the notebook

X_train_basic_filter = X_train.copy()
X_test_basic_filter = X_test.copy()
228/13:
# find and remove correlated features
def correlation(dataset, threshold):
    
    col_corr = set()  # Set of all the names of correlated columns
    corr_matrix = dataset.corr()
    
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            # we are interested in absolute coeff value
            if abs(corr_matrix.iloc[i, j]) > threshold:
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    
    return col_corr


corr_features = correlation(X_train, 0.8)
print('correlated features: ', len(set(corr_features)))
228/14:
# remove correlated features
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
228/15:
# keep a copy of the dataset at  this stage
X_train_corr = X_train.copy()
X_test_corr = X_test.copy()
229/1:
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split

# to determine the chi2 value
from sklearn.feature_selection import chi2

# to select the features
from sklearn.feature_selection import SelectKBest
229/2:
# load dataset

data = pd.read_csv('../titanic.csv')
data.shape
229/3:
# load dataset

data = pd.read_csv('../titanic.csv')
data.shape
229/4: data
229/5:
# load dataset

data = pd.read_csv('../titanic_2.csv')
data.shape
229/6: data
229/7: data.head()
229/8:
# the categorical variables in the titanic are pclass, sex and embarked

# first I will encode the labels of the categories into numbers

# for Sex / Gender
data['sex'] = np.where(data['sex'] == 'male', 1, 0)

# for Embarked
ordinal_label = {k: i for i, k in enumerate(data['embarked'].unique(), 0)}
data['embarked'] = data['embarked'].map(ordinal_label)

# pclass is already ordinal
229/9: data.columns
229/10: data.columns.lowcase()
229/11: data.columns = map(str.lower, data.columns)
229/12: data.head()
229/13:
# the categorical variables in the titanic are pclass, sex and embarked

# first I will encode the labels of the categories into numbers

# for Sex / Gender
data['sex'] = np.where(data['sex'] == 'male', 1, 0)

# for Embarked
ordinal_label = {k: i for i, k in enumerate(data['embarked'].unique(), 0)}
data['embarked'] = data['embarked'].map(ordinal_label)

# pclass is already ordinal
229/14:
# separate train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    data[['pclass', 'sex', 'embarked']],
    data['survived'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
229/15:
# calculate the chi2 p_value between each of the variables
# and the target

# chi2 returns 2 arrays, one contains the F-Scores which are then
# evaluated against the chi2 distribution to obtain the pvalue.
# The pvalues are in the second array

f_score = chi2(X_train.fillna(0), y_train)

# the 2 arrays of values
f_score
229/16: f_score[1]
229/17:
# 1) let's capture the p_values (in the second array, remember python indexes at 0) in a pandas Series
# 2) add the variable names in the index
# 3) order the variables based on their fscore

pvalues = pd.Series(f_score[1])
pvalues.index = X_train.columns
pvalues.sort_values(ascending=True)
229/18:
sel_ = SelectKBest(chi2, k=2).fit(X_train, y_train)

# display features
X_train.columns[sel_.get_support()]
229/19: sel_.get_feature_names_out
229/20: sel_.get_params
229/21: sel_.get_params()
229/22: sel_.get_feature_names_out()
230/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

# to determine the p-values with anova
from sklearn.feature_selection import f_classif, f_regression

# to select features
from sklearn.feature_selection import SelectKBest, SelectPercentile
230/2:
# load dataset
data = pd.read_csv('../dataset_2.csv')
data.shape
230/3: data.head()
230/4:
# separate train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),
    data['target'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
230/5:
# calculate the univariate statistical measure between
# each of the variables and the target

# similarly to chi2, the output is one array with f-scores
# and one array with the pvalues

univariate = f_classif(X_train, y_train)

univariate
230/6:
# 1) let's capture the pvalues in a pandas series
# 2) add the variable names in the index
# 3) sort the features based on their anova pvalues
# 4) and make a var plot

univariate = pd.Series(univariate[1])
univariate.index = X_train.columns
univariate.sort_values(ascending=False).plot.bar(figsize=(20, 6))
230/7:
# select the top 10 features
sel_ = SelectKBest(f_classif, k=10).fit(X_train, y_train)

# display selected feature names
X_train.columns[sel_.get_support()]
230/8:
# remove unwanted features from the dataset

X_train = sel_.transform(X_train)

X_train.shape
230/9:
# load dataset
data = pd.read_csv('../houseprice.csv')
data.shape
231/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.metrics import roc_auc_score, mean_squared_error
231/2:
# load dataset
data = pd.read_csv('../dataset_2.csv')
data.shape
231/3: data.head()
231/4:
# separate train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),
    data['target'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
231/5:
# determine roc-auc for each feature

# here we store the roc-auc values
roc_values = []

# iterate over each feature in the dataset
for feature in X_train.columns:

    # train a decision tree classifier
    clf = DecisionTreeClassifier()
    clf.fit(X_train[feature].fillna(0).to_frame(), y_train)

    # obtain the predictions
    y_scored = clf.predict_proba(X_test[feature].to_frame())

    # calculate and store the roc-auc
    roc_values.append(roc_auc_score(y_test, y_scored[:, 1]))
    
# display the result
roc_values[0:10]
231/6:
# now let's:

# 1) capture the roc-auc values in a pandas series
# 2) add the variable names in the index
# 3) sort the features based on the roc-auc
# 4) and make a var plot

roc_values = pd.Series(roc_values)
roc_values.index = X_train.columns
roc_values.sort_values(ascending=False).plot.bar(figsize=(20, 5))
plt.ylabel('roc-auc')
231/7:
# determine roc-auc for each feature

# here we store the roc-auc values
roc_values = []

# iterate over each feature in the dataset
for feature in X_train.columns:

    # train a decision tree classifier
    clf = DecisionTreeClassifier()
    print(feature)
    clf.fit(X_train[feature].fillna(0).to_frame(), y_train)

    # obtain the predictions
    y_scored = clf.predict_proba(X_test[feature].to_frame())

    # calculate and store the roc-auc
    roc_values.append(roc_auc_score(y_test, y_scored[:, 1]))
    
# display the result
roc_values[0:10]
232/1:
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split

from sklearn.metrics import roc_auc_score
232/2:
# load the titanic dataset
data = pd.read_csv('../titanic.csv')
data.shape
232/3: data.head()
232/4:
# load the titanic dataset
data = pd.read_csv('../titanic_2.csv')
data.shape
232/5: data.head()
232/6: data.columns = map(str.lower, data.columns)
232/7: data.head()
232/8:
# Variable preprocessing:

# then I will narrow down the different cabins by selecting only the
# first letter, which represents the deck in which the cabin was located

# captures first letter of string (the letter of the cabin)
data['cabin'] = data['cabin'].str[0]
data['cabin'].unique()
232/9:
# separate train and test sets

# I will only use the categorical variables and the target

X_train, X_test, y_train, y_test = train_test_split(
    data[['pclass', 'sex', 'embarked', 'cabin', 'survived']],
    data['survived'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
232/10:
# function that determines the target mean per category

def mean_encoding(df_train, df_test, categorical_vars):
    
    # temporary copy of the original dataframes
    df_train_temp = df_train.copy()
    df_test_temp = df_test.copy()
    
    # iterate over each variable
    for col in categorical_vars:
        
        # make a dictionary of categories, target-mean pairs
        target_mean_dict = df_train.groupby([col])['survived'].mean().to_dict()
        
        # replace the categories by the mean of the target
        df_train_temp[col] = df_train[col].map(target_mean_dict)
        df_test_temp[col] = df_test[col].map(target_mean_dict)
    
    # drop the target from the daatset
    df_train_temp.drop(['survived'], axis=1, inplace=True)
    df_test_temp.drop(['survived'], axis=1, inplace=True)
    
    # return  remapped datasets
    return df_train_temp, df_test_temp
232/11:
categorical_vars = ['pclass', 'sex', 'embarked', 'cabin']

X_train_enc, X_test_enc = mean_encoding(X_train, X_test, categorical_vars)

X_train_enc.head()
232/12: X_test_enc.head()
232/13:
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split

from sklearn.metrics import roc_auc_score
232/14:
# load the titanic dataset
data = pd.read_csv('../titanic_2.csv')
data.shape
232/15: data.head()
232/16: data.columns = map(str.lower, data.columns)
232/17:
# Variable preprocessing:

# then I will narrow down the different cabins by selecting only the
# first letter, which represents the deck in which the cabin was located

# captures first letter of string (the letter of the cabin)
data['cabin'] = data['cabin'].str[0]
data['cabin'].unique()
232/18:
# separate train and test sets

# I will only use the categorical variables and the target

X_train, X_test, y_train, y_test = train_test_split(
    data[['pclass', 'sex', 'embarked', 'cabin', 'survived']],
    data['survived'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
232/19:
# function that determines the target mean per category

def mean_encoding(df_train, df_test, categorical_vars):
    
    # temporary copy of the original dataframes
    df_train_temp = df_train.copy()
    df_test_temp = df_test.copy()
    
    # iterate over each variable
    for col in categorical_vars:
        
        # make a dictionary of categories, target-mean pairs
        target_mean_dict = df_train.groupby([col])['survived'].mean().to_dict()
        
        # replace the categories by the mean of the target
        df_train_temp[col] = df_train[col].map(target_mean_dict)
        df_test_temp[col] = df_test[col].map(target_mean_dict)
    
    # drop the target from the daatset
    df_train_temp.drop(['survived'], axis=1, inplace=True)
    df_test_temp.drop(['survived'], axis=1, inplace=True)
    
    # return  remapped datasets
    return df_train_temp, df_test_temp
232/20:
categorical_vars = ['pclass', 'sex', 'embarked', 'cabin']

X_train_enc, X_test_enc = mean_encoding(X_train, X_test, categorical_vars)

X_train_enc.head()
232/21: X_test_enc.head()
232/22: data['cabin'].isnull().sum()
232/23: data['cabin'].isnull().mean()
232/24: data['cabin'].fillna('A', inplace=True)
232/25:
# separate train and test sets

# I will only use the categorical variables and the target

X_train, X_test, y_train, y_test = train_test_split(
    data[['pclass', 'sex', 'embarked', 'cabin', 'survived']],
    data['survived'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
232/26:
# function that determines the target mean per category

def mean_encoding(df_train, df_test, categorical_vars):
    
    # temporary copy of the original dataframes
    df_train_temp = df_train.copy()
    df_test_temp = df_test.copy()
    
    # iterate over each variable
    for col in categorical_vars:
        
        # make a dictionary of categories, target-mean pairs
        target_mean_dict = df_train.groupby([col])['survived'].mean().to_dict()
        
        # replace the categories by the mean of the target
        df_train_temp[col] = df_train[col].map(target_mean_dict)
        df_test_temp[col] = df_test[col].map(target_mean_dict)
    
    # drop the target from the daatset
    df_train_temp.drop(['survived'], axis=1, inplace=True)
    df_test_temp.drop(['survived'], axis=1, inplace=True)
    
    # return  remapped datasets
    return df_train_temp, df_test_temp
232/27:
categorical_vars = ['pclass', 'sex', 'embarked', 'cabin']

X_train_enc, X_test_enc = mean_encoding(X_train, X_test, categorical_vars)

X_train_enc.head()
232/28: X_test_enc.head()
232/29:
# now, we calculate a roc-auc value, using the encoded variables
# as predictions

roc_values = []

for feature in categorical_vars:
    
    roc_values.append(roc_auc_score(y_test, X_test_enc[feature]))
232/30:
# I make a series for easy visualisation

m1 = pd.Series(roc_values)
m1.index = categorical_vars
m1.sort_values(ascending=False)
232/31:
# separate train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    data[['age', 'fare', 'survived']],
    data['survived'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
232/32:
# fill missing values

X_train = X_train.fillna(0)
X_test = X_test.fillna(0)
232/33:
# Let's divide Age in 10 bins. We use the qcut (quantile cut)
# function from pandas indicating that 11 cutting points (q),
# thus 10 bins.

# retbins= True indicates that I want to capture the limits of
# each interval (so I can then use them to cut the test set)

X_train['age_binned'], intervals = pd.qcut(
    X_train['age'],
    q = 5,
    labels=False,
    retbins=True,
    precision=3,
    duplicates='drop',
)

X_train[['age_binned', 'age']].head(10)
232/34:
# count the number of distinct bins

X_train['age_binned'].nunique()
232/35:
# display the bins

X_train['age_binned'].unique()
232/36:
# now I use the interval limits calculated in the previous cell to
# bin the testing set

X_test['age_binned'] = pd.cut(x = X_test['age'], bins=intervals, labels=False)

X_test[['age_binned', 'age']].head(10)
235/1:
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split

from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.metrics import roc_auc_score, r2_score

from mlxtend.feature_selection import SequentialFeatureSelector as SFS
236/1:
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split

from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.metrics import roc_auc_score, r2_score

from mlxtend.feature_selection import SequentialFeatureSelector as SFS
236/2:
# load dataset

data = pd.read_csv('../dataset_2.csv')
data.shape
236/3: data.head()
236/4:
# separate train and test sets

X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),
    data['target'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
236/5:
# remove correlated features to reduce the feature space

def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr

corr_features = correlation(X_train, 0.8)
print('correlated features: ', len(set(corr_features)) )
236/6:
# remove correlated features
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
236/7:
# within the SFS we indicate:

# 1) the algorithm we want to create, in this case RandomForests
# (note that I use few trees to speed things up)

# 2) the stopping criteria: want to select 10 features 

# 3) wheter to perform step forward or step backward

# 4) the evaluation metric: in this case the roc_auc
# 5) the cross-validation

# this is going to take a while, do not despair

sfs = SFS(RandomForestClassifier(n_estimators=10, n_jobs=4, random_state=0), 
           k_features=10, # the more features we want, the longer it will take to run
           forward=True, 
           floating=False, # see the docs for more details in this parameter
           verbose=2, # this indicates how much to print out intermediate steps
           scoring='roc_auc',
           cv=2)

sfs = sfs.fit(np.array(X_train), y_train)
236/8: sfs.get_metric_dict
236/9: sfs.get_metric_dict()
236/10:
selected_feat = X_train.columns[list(sfs.k_feature_idx_)]
selected_feat
236/11:
# function to train random forests and evaluate the performance

def run_randomForests(X_train, X_test, y_train, y_test):
    
    rf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=4)
    rf.fit(X_train, y_train)

    print('Train set')
    pred = rf.predict_proba(X_train)
    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))
    
    print('Test set')
    pred = rf.predict_proba(X_test)
    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))
236/12:
# evaluate performance of algorithm built
# using selected features

run_randomForests(X_train[selected_feat],
                  X_test[selected_feat],
                  y_train, y_test)
236/13:
# and for comparison, we train random forests using
# all features (except the correlated ones, which we removed already)

run_randomForests(X_train,
                  X_test,
                  y_train, y_test)
236/14:
# load dataset
data = pd.read_csv('../train_house.csv')
data.shape
236/15:
# In practice, feature selection should be done after data pre-processing,
# so ideally, all the categorical variables are encoded into numbers,
# and then you can assess how deterministic they are of the target

# here for simplicity I will use only numerical variables
# select numerical columns:

numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
numerical_vars = list(data.select_dtypes(include=numerics).columns)
data = data[numerical_vars]
data.shape
236/16:
# separate train and test sets

X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['SalePrice'], axis=1),
    data['SalePrice'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
236/17:
# find and remove correlated features

def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr

corr_features = correlation(X_train, 0.8)
print('correlated features: ', len(set(corr_features)) )
236/18:
# removed correlated features
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
236/19:
X_train.fillna(0, inplace=True)
X_test.fillna(0, inplace=True)
236/20:
# step forward feature selection

sfs = SFS(RandomForestRegressor(n_estimators=10, n_jobs=4, random_state=10), 
           k_features=20, 
           forward=True, 
           floating=False, 
           verbose=2,
           scoring='r2',
           cv=2)

sfs = sfs.fit(np.array(X_train), y_train)
237/1:
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split

from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.metrics import roc_auc_score, r2_score

from mlxtend.feature_selection import SequentialFeatureSelector as SFS
237/2:
# load dataset

data = pd.read_csv('../dataset_2.csv')
data.shape
237/3: data.head()
237/4:
# separate train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),
    data['target'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
237/5:
# remove correlated features to reduce the feature space

def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr

corr_features = correlation(X_train, 0.8)
print('correlated features: ', len(set(corr_features)) )
237/6:
# removed correlated  features
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
238/1:
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split

from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.metrics import roc_auc_score, r2_score

from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS
238/2:
# load dataset

# few rows to speed things up

data = pd.read_csv('../dataset_2.csv', nrows=10000)
data.shape
238/3: data.head()
238/4:
# separate train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),
    data['target'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
238/5:
# remove correlated features to reduce the feature space

def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr

# note that we reduce the correlation threshold
# to remove more features
corr_features = correlation(X_train, 0.6)
print('correlated features: ', len(set(corr_features)) )
238/6:
# removed correlated features
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
238/7:
##################################

# in order to shorter search time for the demonstration
# i will ask the algorithm to try all possible 1 and 2
# feature combinations

# if you have access to a multicore or distributed computer
# system you can try more greedy searches

###################################

# within the EFS we indicate:

# 1) the algorithm we want to create, in this case RandomForests
# (note that I use few trees to speed things up)

# 2) the number of minimum features we want our model to have

# 3) the number of maximum features we want our model to have

# with 2 and 3 we regulate the number of possible feature combinations to
# be evaluated by the model.

# 4) the evaluation metric: in this case the roc_auc
# 5) the cross-validation

# this is going to take a while, do not despair

efs = EFS(RandomForestClassifier(n_estimators=5,
                                 n_jobs=4,
                                 random_state=0,
                                 max_depth=2),
          min_features=1,
          max_features=2,
          scoring='roc_auc',
          print_progress=True,
          cv=2)

# search features
efs = efs.fit(np.array(X_train), y_train)
238/8: efs.best_idx_
238/9:
selected_feat = X_train.columns[list(efs.best_idx_)]
selected_feat
239/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectFromModel
239/2:
# load dataset
data = pd.read_csv('../dataset_2.csv')
data.shape
239/3: data.head()
239/4:
# separate train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),
    data['target'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
239/5:
# we will scale the variables, so we fit a scaler

scaler = StandardScaler()
scaler.fit(X_train)
239/6:
# here I will do the model fitting and feature selection
# altogether in 2 lines of code

# first I specify the Logistic Regression model, here I
# select the Ridge Penalty (l2)(it is the default parameter in sklearn)

# remember that here I want to evaluate the coefficient magnitud
# itself and not whether lasso shrinks coefficients to zero

# ideally, I want to avoid regularisation at all, so the coefficients
# are not affected (modified) by the penalty of the regularisation

# In order to do this in sklearn, I set the parameter C really high
# which is basically like fitting a non-regularised logistic regression

# Then I use the selectFromModel object from sklearn
# to automatically select the features

# set C to 1000, to avoid regularisation
sel_ = SelectFromModel(
    LogisticRegression(C=1000, penalty='l2', max_iter=300, random_state=10))

sel_.fit(scaler.transform(X_train), y_train)
239/7:
# this command let's me visualise those features that were kept.

# sklearn will select those features which coefficients are greater
# than the mean of all the coefficients.

# it compares absolute values of coefficients. More on this in a second.

sel_.get_support()
239/8:
# let's add the variable names and order it for clearer visualisation
# and then let's sum the number of selected features

selected_feat = X_train.columns[(sel_.get_support())]
len(selected_feat)
239/9:
# with the parameter coef_ we access the coefficients of the variables
# for the linear regression (for all the 108 variables)

sel_.estimator_.coef_
239/10:
# this command let's me visualise those features that were kept.

# sklearn will select those features which coefficients are greater
# than the mean of all the coefficients.

# it compares absolute values of coefficients. More on this in a second.

sel_.get_support()
239/11:
# let's add the variable names and order it for clearer visualisation
# and then let's sum the number of selected features

selected_feat = X_train.columns[(sel_.get_support())]
len(selected_feat)
239/12:
# with the parameter coef_ we access the coefficients of the variables
# for the linear regression (for all the 108 variables)

sel_.estimator_.coef_
239/13:
# the feature importance is informed by the absolute value of
# the coefficient, and not the sign.
# therefore, let's recalculate the mean using the absolute values instead

np.abs(sel_.estimator_.coef_).mean()
239/14:
# and now let's plot the histogram of absolute coefficients

pd.Series(np.abs(sel_.estimator_.coef_).ravel()).hist(bins=20)
plt.xlabel('Coefficients')
plt.ylabel('Number of variables')
plt.show()
240/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import SelectFromModel
240/2:
# load dataset
data = pd.read_csv('../houseprice.csv')
data.shape
240/3:
# load dataset
data = pd.read_csv('../train_house.csv')
data.shape
240/4:
# In practice, feature selection should be done after data pre-processing,
# so ideally, all the categorical variables are encoded into numbers,
# and then you can assess how deterministic they are of the target

# here for simplicity I will use only numerical variables
# select numerical columns:

numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
numerical_vars = list(data.select_dtypes(include=numerics).columns)
data = data[numerical_vars]
data.shape
240/5:
# separate train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['SalePrice'], axis=1),
    data['SalePrice'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
240/6:
X_train = X_train.fillna(0)
X_test = X_test.fillna(0)
240/7:
# the features in the house dataset are in different scales
# so we train a scaler to scale them

scaler = StandardScaler()
scaler.fit(X_train)
240/8:
# we train a Linear regression model and select
# features with higher coefficients.

# the LinearRegression object from sklearn is a non-regularised
# linear method. It fits by matrix multiplication and not 
# gradient descent.

# therefore we don't need to specify penalty and other parameters

sel_ = SelectFromModel(LinearRegression())

sel_.fit(scaler.transform(X_train), y_train)
240/9:
# let's count the number of variables selected
selected_feat = X_train.columns[(sel_.get_support())]

len(selected_feat)
240/10:
# and now let's plot the histogram of absolute coefficients

pd.Series(np.abs(sel_.estimator_.coef_).ravel()).hist(bins=10)
plt.xlabel('Coefficients')
plt.ylabel('Number of variables')
plt.show()
240/11:
# and now, let's compare the  amount of selected features
# with the amount of features which coefficient is above the
# mean coefficient, to make sure we understand the output of
# sklearn

print('total features: {}'.format((X_train.shape[1])))

print('selected features: {}'.format(len(selected_feat)))

print(
    'features with coefficients greater than the mean coefficient: {}'.format(
        np.sum(
            np.abs(sel_.estimator_.coef_) > np.abs(
                sel_.estimator_.coef_).mean())))
241/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
241/2:
# load dataset
data = pd.read_csv('../dataset_2.csv')
data.shape
241/3: data.head()
241/4:
# separate train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),
    data['target'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
241/5:
scaler = StandardScaler()
scaler.fit(X_train)

# we train 4 different models, decreasing the amount
# of regularisation (that is, increasing C)

# here we will store the coefficients of the variables
# fitted for each different model
coefs_df = []

# we train 4 different models with regularization
penalties = [0.00005, 0.0005, 0.005, 0.05, 0.5]

for c in penalties:   
    
    logit = LogisticRegression(C=c, penalty='l2', random_state=10, max_iter=300)
    logit.fit(scaler.transform(X_train), y_train)
    
    # store the coefficients of the variables in a list
    coefs_df.append(pd.Series(logit.coef_.ravel()))
241/6:
# now I create a dataframe with the coefficients for all
# the variables for the 4 different logistic regression models

coefs = pd.concat(coefs_df, axis=1)
coefs.columns = penalties
coefs.index = X_train.columns
coefs.head()
241/7:
# apply log scale to the penalties (simplifies comparison)

coefs.columns = np.log(penalties)
coefs.head()
241/8:
# plot the change in coefficients with the penalty
coefs.T.plot(figsize=(15,10), legend=False)
plt.xlabel('Penalty value')
plt.ylabel('Coefficient')
plt.title('Coefficient value vs penalty. Each line corresponds to one variable')
241/9:
# now I will plot only the first 10 features for better
# visualisation

temp = coefs.head(10)
temp = temp.T
temp.plot(figsize=(12,8))
plt.xlabel('Penalty value')
plt.ylabel('Coefficient')
plt.title('Coefficient value vs penalty')
241/10:
# plot another 10 features for visualisation
temp = coefs.tail(10)
temp = temp.T
temp.plot(figsize=(12,8))
plt.xlabel('Penalty value')
plt.ylabel('Coefficient')
plt.title('Coefficient value vs penalty')
242/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectFromModel
from sklearn.metrics import roc_auc_score
242/2:
# load the Santander customer satisfaction dataset from Kaggle

data = pd.read_csv('../dataset_1.csv')
data.shape
242/3:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),
    data['target'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
242/4:
# I keep a copy of the dataset with all the variables
# to compare the performance of machine learning models
# at the end of the notebook

X_train_original = X_train.copy()
X_test_original = X_test.copy()
242/5:
constant_features = [
    feat for feat in X_train.columns if X_train[feat].std() == 0
]

X_train.drop(labels=constant_features, axis=1, inplace=True)
X_test.drop(labels=constant_features, axis=1, inplace=True)

X_train.shape, X_test.shape
242/6:
# find features with low variance
sel = VarianceThreshold(threshold=0.01)
sel.fit(X_train)  

# how many not quasi-constant?
sum(sel.get_support())
242/7: features_to_keep = X_train.columns[sel.get_support()]
242/8:
# remove the features
X_train = sel.transform(X_train)
X_test = sel.transform(X_test)

X_train.shape, X_test.shape
242/9:
# sklearn transformations lead to numpy arrays
# here we transform the arrays back to dataframes

X_train= pd.DataFrame(X_train)
X_train.columns = features_to_keep

X_test= pd.DataFrame(X_test)
X_test.columns = features_to_keep
242/10:
duplicated_feat = []
for i in range(0, len(X_train.columns)):
    if i % 10 == 0:  # this helps me understand how the loop is going
        print(i)

    col_1 = X_train.columns[i]

    for col_2 in X_train.columns[i + 1:]:
        if X_train[col_1].equals(X_train[col_2]):
            duplicated_feat.append(col_2)
            
len(duplicated_feat)
242/11:
# remove duplicated features
X_train.drop(labels=duplicated_feat, axis=1, inplace=True)
X_test.drop(labels=duplicated_feat, axis=1, inplace=True)

X_train.shape, X_test.shape
242/12:
# I keep a copy of the dataset except constant, quasi-constant and duplicated variables

X_train_basic_filter = X_train.copy()
X_test_basic_filter = X_test.copy()
242/13:
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr

corr_features = correlation(X_train, 0.8)
print('correlated features: ', len(set(corr_features)) )
242/14:
# remove correlated features
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
242/15:
# keep a copy of the dataset without correlated features
X_train_corr = X_train.copy()
X_test_corr = X_test.copy()
242/16:
scaler = StandardScaler()
scaler.fit(X_train)
242/17:
# we use regularisation by setting a low value of C

sel_ = SelectFromModel(
    LogisticRegression(C=0.0005, random_state=10, max_iter=1000, penalty='l2'))

sel_.fit(scaler.transform(X_train), y_train)

# select features where coefficient is above the mean
# coefficient value and parse again as dataframe
# (remember that the output of sklearn is a
# numpy array)

X_train_coef = pd.DataFrame(sel_.transform(X_train))
X_test_coef = pd.DataFrame(sel_.transform(X_test))

# add the columns name
X_train_coef.columns = X_train.columns[(sel_.get_support())]
X_test_coef.columns = X_train.columns[(sel_.get_support())]
242/18: X_train_coef.shape, X_test_coef.shape
242/19:
# create a function to train a logistic regression 
# and compare its performance in the train and test sets

def run_logistic(X_train, X_test, y_train, y_test):
    
    scaler = StandardScaler().fit(X_train)
    
    logit = LogisticRegression(C=0.0005, random_state=10, max_iter=10000, penalty='l2')
    logit.fit(scaler.transform(X_train), y_train)
    
    print('Train set')
    pred = logit.predict_proba(scaler.transform(X_train))
    print('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))
    
    print('Test set')
    pred = logit.predict_proba(scaler.transform(X_test))
    print('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))
242/20:
# original dataset - all variables
run_logistic(X_train_original,
             X_test_original,
             y_train,
             y_test)
242/21:
# filter methods - basic
run_logistic(X_train_basic_filter,
             X_test_basic_filter,
             y_train,
             y_test)
242/22:
# filter methods - correlation
run_logistic(X_train_corr,
             X_test_corr,
             y_train,
             y_test)
242/23:
# embedded methods - Logistic regression coefficients
run_logistic(X_train_coef,
             X_test_coef,
             y_train,
             y_test)
242/24:
# we use regularisation by setting a low value of C

sel_ = SelectFromModel(
    LogisticRegression(C=1000, random_state=10, max_iter=1000, penalty='l2'))

sel_.fit(scaler.transform(X_train), y_train)

# select features where coefficient is above the mean
# coefficient value and parse again as dataframe
# (remember that the output of sklearn is a
# numpy array)

X_train_coef = pd.DataFrame(sel_.transform(X_train))
X_test_coef = pd.DataFrame(sel_.transform(X_test))

# add the columns name
X_train_coef.columns = X_train.columns[(sel_.get_support())]
X_test_coef.columns = X_train.columns[(sel_.get_support())]
242/25: X_train_coef.shape, X_test_coef.shape
242/26:
# embedded methods - Logistic regression coefficients
run_logistic(X_train_coef,
             X_test_coef,
             y_train,
             y_test)
242/27:
# we use regularisation by setting a low value of C

sel_ = SelectFromModel(
    LogisticRegression(C=0.0001, random_state=10, max_iter=1000, penalty='l2'))

sel_.fit(scaler.transform(X_train), y_train)

# select features where coefficient is above the mean
# coefficient value and parse again as dataframe
# (remember that the output of sklearn is a
# numpy array)

X_train_coef = pd.DataFrame(sel_.transform(X_train))
X_test_coef = pd.DataFrame(sel_.transform(X_test))

# add the columns name
X_train_coef.columns = X_train.columns[(sel_.get_support())]
X_test_coef.columns = X_train.columns[(sel_.get_support())]
242/28: X_train_coef.shape, X_test_coef.shape
242/29:
# embedded methods - Logistic regression coefficients
run_logistic(X_train_coef,
             X_test_coef,
             y_train,
             y_test)
243/1:
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split

from sklearn.linear_model import Lasso, LogisticRegression
from sklearn.feature_selection import SelectFromModel
from sklearn.preprocessing import StandardScaler
243/2:
# load dataset
data = pd.read_csv('../dataset_2.csv')
data.shape
243/3: data.head()
243/4:
# separate train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),
    data['target'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
243/5:
# linear models benefit from feature scaling

scaler = StandardScaler()
scaler.fit(X_train)
243/6:
# here I will do the model fitting and feature selection
# altogether in one line of code

# first I specify the Logistic Regression model, and I
# make sure I select the Lasso (l1) penalty.

# Then I use the selectFromModel class from sklearn, which
# will select the features which coefficients are non-zero

sel_ = SelectFromModel(
    LogisticRegression(C=0.5, penalty='l1', solver='liblinear', random_state=10))

sel_.fit(scaler.transform(X_train), y_train)
243/7:
# this command let's me visualise the index of the
# features that were selected

sel_.get_support()
243/8:
# Now I make a list with the selected features
selected_feat = X_train.columns[(sel_.get_support())]

print('total features: {}'.format((X_train.shape[1])))
print('selected features: {}'.format(len(selected_feat)))
print('features with coefficients shrank to zero: {}'.format(
    np.sum(sel_.estimator_.coef_ == 0)))
243/9:
# the number of features which coefficient was shrank to zero:
np.sum(sel_.estimator_.coef_ == 0)
243/10:
# we can identify the removed features like this:

removed_feats = X_train.columns[(sel_.estimator_.coef_ == 0).ravel().tolist()]
removed_feats
243/11:
# we can then remove the features from the training and testing set
# like this:

X_train_selected = sel_.transform(X_train)
X_test_selected = sel_.transform(X_test)

X_train_selected.shape, X_test_selected.shape
243/12:
# For comparison, I will fit a logistic regression with a
# Ridge regularisation, and evaluate the coefficients

l1_logit = LogisticRegression(C=0.5, penalty='l2', max_iter=300, random_state=10)
l1_logit.fit(scaler.transform(X_train), y_train)

# I count the number of coefficients with zero values
# and it is zero, as expected
np.sum(l1_logit.coef_ == 0)
243/13:
# For comparison, I will fit a logistic regression with a
# Ridge regularisation, and evaluate the coefficients

l1_logit = LogisticRegression(C=0.005, penalty='l2', max_iter=300, random_state=10)
l1_logit.fit(scaler.transform(X_train), y_train)

# I count the number of coefficients with zero values
# and it is zero, as expected
np.sum(l1_logit.coef_ == 0)
243/14:
# For comparison, I will fit a logistic regression with a
# Ridge regularisation, and evaluate the coefficients

l1_logit = LogisticRegression(C=0.0005, penalty='l2', max_iter=300, random_state=10)
l1_logit.fit(scaler.transform(X_train), y_train)

# I count the number of coefficients with zero values
# and it is zero, as expected
np.sum(l1_logit.coef_ == 0)
243/15:
# For comparison, I will fit a logistic regression with a
# Ridge regularisation, and evaluate the coefficients

l1_logit = LogisticRegression(C=10000, penalty='l2', max_iter=300, random_state=10)
l1_logit.fit(scaler.transform(X_train), y_train)

# I count the number of coefficients with zero values
# and it is zero, as expected
np.sum(l1_logit.coef_ == 0)
243/16:
sel_log_ = SelectFromModel(l1_logit)
sel_log_.get_support()
243/17:
sel_log_ = SelectFromModel(l1_logit)
sel_log_.fit(scaler.transform(X_train), y_train)
sel_log_.get_support()
243/18:
sel_log_ = SelectFromModel(l1_logit)
sel_log_.fit(scaler.transform(X_train), y_train)
sel_log_.get_support().sum()
243/19:
# For comparison, I will fit a logistic regression with a
# Ridge regularisation, and evaluate the coefficients

l1_logit = LogisticRegression(C=0.0001, penalty='l2', max_iter=300, random_state=10)
l1_logit.fit(scaler.transform(X_train), y_train)

# I count the number of coefficients with zero values
# and it is zero, as expected
np.sum(l1_logit.coef_ == 0)
243/20:
sel_log_ = SelectFromModel(l1_logit)
sel_log_.fit(scaler.transform(X_train), y_train)
sel_log_.get_support().sum()
243/21:
# load dataset

data = pd.read_csv('../train_house.csv')
data.shape
243/22:
# In practice, feature selection should be done after data pre-processing,
# so ideally, all the categorical variables are encoded into numbers,
# and then you can assess how deterministic they are of the target

# here for simplicity I will use only numerical variables
# select numerical columns:

numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
numerical_vars = list(data.select_dtypes(include=numerics).columns)
data = data[numerical_vars]
data.shape
243/23:
# separate train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['SalePrice'], axis=1),
    data['SalePrice'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
243/24:
X_train.fillna(0, inplace=True)
X_test.fillna(0, inplace=True)
243/25:
# the features in the house dataset are in very
# different scales, so it helps the regression to scale
# them

scaler = StandardScaler()
scaler.fit(X_train)
243/26:
# here, again I will train a Lasso Linear regression and select
# the non zero features in one line.

# bear in mind that the linear regression object from sklearn does
# not allow for regularisation. So If you want to make a regularised
# linear regression you need to import specifically "Lasso"

# alpha is the penalisation, so I set it high
# to force the algorithm to shrink some coefficients

sel_ = SelectFromModel(Lasso(alpha=100, random_state=10))
sel_.fit(scaler.transform(X_train), y_train)
243/27: sel_.get_support()
243/28:
# here, again I will train a Lasso Linear regression and select
# the non zero features in one line.

# bear in mind that the linear regression object from sklearn does
# not allow for regularisation. So If you want to make a regularised
# linear regression you need to import specifically "Lasso"

# alpha is the penalisation, so I set it high
# to force the algorithm to shrink some coefficients

sel_ = SelectFromModel(Lasso(alpha=1000, random_state=10))
sel_.fit(scaler.transform(X_train), y_train)
243/29: sel_.get_support()
243/30:
# make a list with the selected features and print the outputs
selected_feat = X_train.columns[(sel_.get_support())]

print('total features: {}'.format((X_train.shape[1])))
print('selected features: {}'.format(len(selected_feat)))
print('features with coefficients shrank to zero: {}'.format(
    np.sum(sel_.estimator_.coef_ == 0)))
244/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

from sklearn.feature_selection import SelectFromModel

from sklearn.metrics import roc_auc_score
244/2:
# load the dataset

data = pd.read_csv('../dataset_1.csv')
data.shape
244/3:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),
    data['target'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
244/4:
# I keep a copy of the dataset with all the variables
# to measure the performance of the machine learning models
# at the end of the notebook

X_train_original = X_train.copy()
X_test_original = X_test.copy()
244/5:
constant_features = [
    feat for feat in X_train.columns if X_train[feat].std() == 0
]

X_train.drop(labels=constant_features, axis=1, inplace=True)
X_test.drop(labels=constant_features, axis=1, inplace=True)

X_train.shape, X_test.shape
244/6:
sel = VarianceThreshold(
    threshold=0.01)  # 0.1 indicates 99% of observations approximately

sel.fit(X_train)  # fit finds the features with low variance

sum(sel.get_support()) # how many not quasi-constant?
244/7: features_to_keep = X_train.columns[sel.get_support()]
244/8:
# remove features

X_train = sel.transform(X_train)
X_test = sel.transform(X_test)

X_train.shape, X_test.shape
244/9:
# I transform the NumPy arrays to dataframes

X_train= pd.DataFrame(X_train)
X_train.columns = features_to_keep

X_test= pd.DataFrame(X_test)
X_test.columns = features_to_keep
244/10:
duplicated_feat = []

for i in range(0, len(X_train.columns)):
    if i % 10 == 0:  # this helps me understand how the loop is going
        print(i)

    col_1 = X_train.columns[i]

    for col_2 in X_train.columns[i + 1:]:
        if X_train[col_1].equals(X_train[col_2]):
            duplicated_feat.append(col_2)
            
len(duplicated_feat)
244/11:
# remove duplicated features

X_train.drop(labels=duplicated_feat, axis=1, inplace=True)
X_test.drop(labels=duplicated_feat, axis=1, inplace=True)

X_train.shape, X_test.shape
244/12:
# I keep a copy of the dataset without constant, quasi-constant and duplicated variables
# to measure the performance of machine learning models
# at the end of the notebook

X_train_basic_filter = X_train.copy()
X_test_basic_filter = X_test.copy()
244/13:
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            # we are interested in absolute coeff value
            if abs(corr_matrix.iloc[i, j]) > threshold:
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr


corr_features = correlation(X_train, 0.8)
print('correlated features: ', len(set(corr_features)))
244/14:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
244/15:
# keep a copy of the dataset without
# correlated features

X_train_corr = X_train.copy()
X_test_corr = X_test.copy()
244/16:
scaler = StandardScaler()
scaler.fit(X_train)
244/17:
# fit a Lasso and selet features, make sure to select l1

sel_ = SelectFromModel(
    LogisticRegression(C=0.5,
                       penalty='l1',
                       solver='liblinear',
                       random_state=10))

sel_.fit(scaler.transform(X_train), y_train)

# remove features with zero coefficient from dataset
# and parse again as dataframe

X_train_lasso = pd.DataFrame(sel_.transform(X_train))
X_test_lasso = pd.DataFrame(sel_.transform(X_test))

# add the columns name
X_train_lasso.columns = X_train.columns[(sel_.get_support())]
X_test_lasso.columns = X_train.columns[(sel_.get_support())]
244/18: X_train_lasso.shape, X_test_lasso.shape
244/19:
# create a function to train logistic regression
# and compare performance in train and test set


def run_logistic(X_train, X_test, y_train, y_test):

    # with a scaler
    scaler = StandardScaler().fit(X_train)

    logit = LogisticRegression(random_state=44, max_iter=500)
    logit.fit(scaler.transform(X_train), y_train)

    print('Train set')
    pred = logit.predict_proba(scaler.transform(X_train))
    print('Logistic Regression roc-auc: {}'.format(
        roc_auc_score(y_train, pred[:, 1])))

    print('Test set')
    pred = logit.predict_proba(scaler.transform(X_test))
    print('Logistic Regression roc-auc: {}'.format(
        roc_auc_score(y_test, pred[:, 1])))
244/20:
# original

run_logistic(X_train_original,
             X_test_original,
             y_train,
             y_test)
244/21:
# filter methods - basic

run_logistic(X_train_basic_filter,
             X_test_basic_filter,
             y_train,
             y_test)
244/22:
# filter methods - correlation

run_logistic(X_train_corr,
             X_test_corr,
             y_train,
             y_test)
244/23:
# embedded methods - Lasso

run_logistic(X_train_lasso,
             X_test_lasso,
             y_train,
             y_test)
245/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.feature_selection import SelectFromModel
245/2:
# load dataset
data = pd.read_csv('../dataset_2.csv')
data.shape
245/3: data.head()
245/4:
# separate train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),
    data['target'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
245/5:
# we fit Random Forests and select features in 2 lines of code

# first I specify the Random Forest instance and its parameters

# Then I use the selectFromModel class from sklearn
# to automatically select the features

# SelectFrom model will select those features which importance
# is greater than the mean importance of all the features
# by default, but you can alter this threshold if you want to

sel_ = SelectFromModel(RandomForestClassifier(n_estimators=10, random_state=10))

sel_.fit(X_train, y_train)
245/6:
# this command let's me visualise those features that were selected.

# sklearn will select those features which importance values
# are greater than the mean of all the coefficients.

sel_.get_support()
245/7:
# let's make a list and count the selected features

selected_feat = X_train.columns[(sel_.get_support())]

len(selected_feat)
245/8: selected_feat
245/9:
# and now let's plot the distribution of importances

pd.Series(sel_.estimator_.feature_importances_.ravel()).hist(bins=20)
plt.xlabel('Feature importance')
plt.ylabel('Number of Features')
plt.show()
245/10:
# and now, let's compare the  amount of selected features
# with the amount of features which importance is above the
# mean of all features, to make sure we understand the output of
# SelectFromModel

print('total features: {}'.format((X_train.shape[1])))

print('selected features: {}'.format(len(selected_feat)))

print(
    'features with importance greater than the mean importance of all features: {}'.format(
        np.sum(sel_.estimator_.feature_importances_ >
               sel_.estimator_.feature_importances_.mean())))
245/11:
# load dataset
data = pd.read_csv('../houseprice.csv')
data.shape
245/12:
# load dataset
data = pd.read_csv('../train_house.csv')
data.shape
245/13:
# In practice, feature selection should be done after data pre-processing,
# so ideally, all the categorical variables are encoded into numbers,
# and then you can assess how deterministic they are of the target

# here for simplicity I will use only numerical variables
# select numerical columns:

numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
numerical_vars = list(data.select_dtypes(include=numerics).columns)
data = data[numerical_vars]
data.shape
245/14:
# separate train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['SalePrice'], axis=1),
    data['SalePrice'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
245/15:
X_train = X_train.fillna(0)
X_test = X_test.fillna(0)
245/16:
# we train a random forest for regression and select features
# in 2 lines of code

# SelectFrom model will select those features which importance
# is greater than the mean importance of all the features
# by default, but you can alter this threshold if you want to

sel_ = SelectFromModel(RandomForestRegressor(n_estimators=100, random_state=10))
sel_.fit(X_train, y_train)
245/17:
# let's make a list and count the selected features
selected_feat = X_train.columns[(sel_.get_support())]
len(selected_feat)
245/18:
# and now, let's compare the  amount of selected features
# with the amount of features which importance is above the
# mean importance, to make sure we understand the output of
# sklearn

print('total features: {}'.format((X_train.shape[1])))

print('selected features: {}'.format(len(selected_feat)))

print(
    'features with coefficients greater than the mean coefficient: {}'.format(
        np.sum(sel_.estimator_.feature_importances_ >
               sel_.estimator_.feature_importances_.mean())))
246/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE
from sklearn.metrics import roc_auc_score
246/2:
# load dataset
data = pd.read_csv('../dataset_2.csv')
data.shape
246/3: data.head()
246/4:
# separate train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),
    data['target'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
246/5:
# we do model training and feature selection in 2 lines of code

# first I specify the Random Forest and its parameters

# Then RFE from sklearn to remove features recursively

# RFE will remove one feature at each iteration => the least  important.
# then it will build another random forest and repeat
# till a criteria is met.

# in sklearn the criteria to stop is an arbitrary number
# of features to select, that we need to decide before hand
# not the best solution, but a solution

sel_ = RFE(RandomForestClassifier(n_estimators=10, random_state=10), n_features_to_select=27)
sel_.fit(X_train, y_train)
246/6:
# selected features

selected_feat = X_train.columns[(sel_.get_support())]
len(selected_feat)
246/7:
# let's display the list of features
selected_feat
246/8:
# these are the features selected in the previous
# notebook where we used SelectFromModel from sklearn
# without doing it recursively

previous_lecture_selected_features = [
    'var_1', 'var_2', 'var_6', 'var_9', 'var_13', 'var_15', 'var_16', 'var_17',
    'var_20', 'var_21', 'var_30', 'var_34', 'var_37', 'var_55', 'var_60',
    'var_67', 'var_69', 'var_70', 'var_71', 'var_82', 'var_87', 'var_88',
    'var_95', 'var_96', 'var_99', 'var_103', 'var_108'
]
248/1:
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split

from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score
248/2:
# load dataset
data = pd.read_csv('../dataset_2.csv')
data.shape
248/3: data.head()
248/4:
# separate train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),
    data['target'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
248/5:
# for this method, it is necessary to reset the indeces of the returned 
# datasets

X_train.reset_index(drop=True, inplace=True)
X_test.reset_index(drop=True, inplace=True)
248/6:
# The first step to determine feature importance by feature shuffling
# is to build the machine learning model for which we want to 
# select features

# In this case, I will build Random Forests, but remember that 
# you can use this procedure with any other machine learning algorithm

# I build few and shallow trees to avoid overfitting
rf = RandomForestClassifier(
    n_estimators=50, max_depth=2, random_state=2909, n_jobs=4)

rf.fit(X_train, y_train)

# print roc-auc in train and testing sets
print('train auc score: ',
      roc_auc_score(y_train, (rf.predict_proba(X_train.fillna(0)))[:, 1]))
print('test auc score: ',
      roc_auc_score(y_test, (rf.predict_proba(X_test.fillna(0)))[:, 1]))
248/7: y_train
248/8: rf.predict_proba(X_train.fillna(0))
248/9: X_train.sample(frac=1, random_state=10)
248/10: X_train.shape
248/11: X_train.sample(frac=2, random_state=10)
248/12: X_train.sample(frac=.4, random_state=10)
248/13: X_train.sample(frac=.4, random_state=10).shape
248/14:
# in this cell, I will shuffle one by one, each feature of the dataset

# then I use the dataset with the shuffled variable to make predictions
# with the random forests I trained in the previous cell

# overall train roc-auc: using all the features
train_roc = roc_auc_score(y_train, (rf.predict_proba(X_train))[:, 1])

# list to capture the performance shift
performance_shift = []

# selection  logic
for feature in X_train.columns:

    X_train_c = X_train.copy()

    # shuffle individual feature
    X_train_c[feature] = X_train_c[feature].sample(
        frac=1, random_state=10).reset_index(drop=True)
    X_train_c.shape

    # make prediction with shuffled feature and calculate roc-auc
    shuff_roc = roc_auc_score(y_train, rf.predict_proba(X_train_c)[:, 1])
    
    drift = train_roc - shuff_roc

    # save the drop in roc-auc
    performance_shift.append(drift)
248/15:
# le't have a look at our list of performances
performance_shift
248/16:
# in this cell, I will shuffle one by one, each feature of the dataset

# then I use the dataset with the shuffled variable to make predictions
# with the random forests I trained in the previous cell

# overall train roc-auc: using all the features
train_roc = roc_auc_score(y_train, (rf.predict_proba(X_train))[:, 1])

# list to capture the performance shift
performance_shift = []

# selection  logic
for feature in X_train.columns:

    X_train_c = X_train.copy()

    # shuffle individual feature
    X_train_c[feature] = X_train_c[feature].sample(
        frac=1, random_state=10).reset_index(drop=True)
    print(X_train_c.shape)

    # make prediction with shuffled feature and calculate roc-auc
    shuff_roc = roc_auc_score(y_train, rf.predict_proba(X_train_c)[:, 1])
    
    drift = train_roc - shuff_roc

    # save the drop in roc-auc
    performance_shift.append(drift)
248/17:
# le't have a look at our list of performances
performance_shift
248/18:
# in this cell, I will shuffle one by one, each feature of the dataset

# then I use the dataset with the shuffled variable to make predictions
# with the random forests I trained in the previous cell

# overall train roc-auc: using all the features
train_roc = roc_auc_score(y_train, (rf.predict_proba(X_train))[:, 1])

# list to capture the performance shift
performance_shift = []

# selection  logic
for feature in X_train.columns:

    X_train_c = X_train.copy()

    # shuffle individual feature
    X_train_c[feature] = X_train_c[feature].sample(
        frac=1, random_state=10).reset_index(drop=True)
    print(X_train_c[feature].shape)

    # make prediction with shuffled feature and calculate roc-auc
    shuff_roc = roc_auc_score(y_train, rf.predict_proba(X_train_c)[:, 1])
    
    drift = train_roc - shuff_roc

    # save the drop in roc-auc
    performance_shift.append(drift)
248/19:
# Now I will transform the list into a pandas Series
# for easy manipulation

feature_importance = pd.Series(performance_shift)

# add variable names in the index
feature_importance.index = X_train.columns

feature_importance.head()
248/20:
# Now I will sort the dataframe according to the drop in performance
# caused by feature shuffling

feature_importance.sort_values(ascending=False)
248/21:
# visualise the top 10 features that caused the major drop
# in the roc-auc (aka model performance)

feature_importance.sort_values(ascending=False).head(10)
248/22:
# original number of features (rows in this case)
feature_importance.shape[0]
248/23:
# number of features that cause a drop in performance
# when shuffled

feature_importance[feature_importance>0].shape[0]
248/24:
# print the important features

feature_importance[feature_importance>0].index
248/25:
# Now let's build a random forests only with the selected features

# capture the selected features
selected_features = feature_importance[feature_importance > 0].index

# train a new random forests using only the selected features
rf = RandomForestClassifier(n_estimators=50,
                            max_depth=2,
                            random_state=2909,
                            n_jobs=4)

rf.fit(X_train[selected_features], y_train)

# print roc-auc in train and testing sets
print(
    'train auc score: ',
    roc_auc_score(y_train, (rf.predict_proba(X_train[selected_features]))[:,
                                                                          1]))
print(
    'test auc score: ',
    roc_auc_score(y_test, (rf.predict_proba(X_test[selected_features]))[:, 1]))
248/26: selected_features
248/27: len(selected_features)
248/28:
# Now let's build a random forests only with the selected features

# capture the selected features
selected_features = feature_importance[feature_importance > 0].index

# train a new random forests using only the selected features
rf = RandomForestClassifier(n_estimators=50,
                            max_depth=2,
                            random_state=2909,
                            n_jobs=4)

rf.fit(X_train[selected_features], y_train)

# print roc-auc in train and testing sets
print(
    'train auc score: ',
    roc_auc_score(y_train, (rf.predict_proba(X_train[selected_features]))[:,
                                                                          1]))
print(
    'test auc score: ',
    roc_auc_score(y_test, (rf.predict_proba(X_test[selected_features]))[:, 1]))

print(
    'train auc score: ',
    roc_auc_score(y_train, (rf.predict_proba(X_train))[:,
                                                                          1]))
print(
    'test auc score: ',
    roc_auc_score(y_test, (rf.predict_proba(X_test))[:, 1]))
248/29:
# Now let's build a random forests only with the selected features

# capture the selected features
selected_features = feature_importance[feature_importance > 0].index

# train a new random forests using only the selected features
rf = RandomForestClassifier(n_estimators=50,
                            max_depth=2,
                            random_state=2909,
                            n_jobs=4)

rf.fit(X_train[selected_features], y_train)

# print roc-auc in train and testing sets
print(
    'train auc score: ',
    roc_auc_score(y_train, (rf.predict_proba(X_train[selected_features]))[:,
                                                                          1]))
print(
    'test auc score: ',
    roc_auc_score(y_test, (rf.predict_proba(X_test[selected_features]))[:, 1]))
248/30:
# load dataset
data = pd.read_csv('../train_house.csv')
data.shape
248/31:
# In practice, feature selection should be done after data pre-processing,
# so ideally, all the categorical variables are encoded into numbers,
# and then you can assess how deterministic they are of the target

# here for simplicity I will use only numerical variables
# select numerical columns:

numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
numerical_vars = list(data.select_dtypes(include=numerics).columns)
data = data[numerical_vars]
data.shape
248/32:
# separate train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['Id', 'SalePrice'], axis=1),
    data['SalePrice'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
248/33:
# for this method, it is necessary to reset the indeces of the returned 
# datasets

X_train.reset_index(drop=True, inplace=True)
X_test.reset_index(drop=True, inplace=True)
248/34:
X_train = X_train.fillna(0)
X_test = X_test.fillna(0)
248/35:
# load dataset
data = pd.read_csv('../train_house.csv')
data.shape
248/36:
# In practice, feature selection should be done after data pre-processing,
# so ideally, all the categorical variables are encoded into numbers,
# and then you can assess how deterministic they are of the target

# here for simplicity I will use only numerical variables
# select numerical columns:

numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
numerical_vars = list(data.select_dtypes(include=numerics).columns)
data = data[numerical_vars]
data.shape
248/37:
# separate train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['Id', 'SalePrice'], axis=1),
    data['SalePrice'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
248/38: X_train.head()
248/39:
# for this method, it is necessary to reset the indeces of the returned 
# datasets

X_train.reset_index(drop=True, inplace=True)
X_test.reset_index(drop=True, inplace=True)
248/40: X_train.shape
248/41: X_train
248/42:
X_train = X_train.fillna(0)
X_test = X_test.fillna(0)
248/43:
# The first step to determine feature importance by feature shuffling
# is to build the machine learning model for which we want to
# select features

# In this case, I will build Random Forests, but remember that
# you can use this procedure for any other machine learning algorithm

# I build few and shallow trees to avoid overfitting
rf = RandomForestRegressor(n_estimators=100,
                           max_depth=3,
                           random_state=2909,
                           n_jobs=4)

rf.fit(X_train, y_train)

# print performance metrics
print('train rmse: ', mean_squared_error(y_train, rf.predict(X_train), squared=False))
print('train r2: ', r2_score(y_train, (rf.predict(X_train))))
print()
print('test rmse: ', mean_squared_error(y_test, rf.predict(X_test), squared=False))
print('test r2: ', r2_score(y_test, rf.predict(X_test)))
248/44:
# in this cell, I will shuffle one by one, each feature of the dataset
# and then use the dataset with the shuffled variable to make predictions
# using the random forests I trained in the previous cell

# overall train rmse: using all the features
train_rmse = mean_squared_error(y_train, rf.predict(X_train), squared=False)

# list to capture the performance shift
performance_shift = []

# for each feature:
for feature in X_train.columns:
    
    X_train_c = X_train.copy()

    # shuffle individual feature
    X_train_c[feature] = X_train_c[feature].sample(frac=1, random_state=11).reset_index(
        drop=True)

    # make prediction with shuffled feature and calculate roc-auc
    shuff_rmse = mean_squared_error(y_train, rf.predict(X_train_c), squared=False)
    
    drift = train_rmse - shuff_rmse 

    # store the drop in roc-auc
    performance_shift.append(drift)
248/45:
# Now I will transform the list into a pandas Series
# for easy manipulation

feature_importance = pd.Series(performance_shift)

# add variable names in the index
feature_importance.index = X_train.columns

feature_importance.head()
248/46: feature_importance
248/47: feature_importance.sort_values(ascending=False).head(10)
248/48: feature_importance.sort_values(ascending=False).head(-10)
248/49: feature_importance.sort_values(ascending=False).head(10)
248/50: feature_importance.sort_values(ascending=True).head(10)
248/51:
# Note here that when looking at the rmse, the smaller the better.

# as we do original_rmse - shuffled_data_rmse

# if the feature was important, the shuffled data would increase the rsme

# thus, we are looking for negative values here

# number of features that cause a drop in performance
# when shuffled

feature_importance[feature_importance<0].shape[0]
248/52:
# and the variable names

feature_importance[feature_importance<0].index
248/53:
# Now let's compare the performance of a random forest
# built only using the selected features

# slice the data
feat = feature_importance[feature_importance<0].index

X_train = X_train[feat]
X_test = X_test[feat]
248/54: X_train.shape, X_train.shape
248/55:
# build and evaluate the model

rf = RandomForestRegressor(n_estimators=100,
                           max_depth=3,
                           random_state=2909,
                           n_jobs=4)

rf.fit(X_train, y_train)

# print performance metrics
print('train rmse: ', mean_squared_error(y_train, rf.predict(X_train), squared=False))
print('train r2: ', r2_score(y_train, (rf.predict(X_train))))
print()
print('test rmse: ', mean_squared_error(y_test, rf.predict(X_test), squared=False))
print('test r2: ', r2_score(y_test, rf.predict(X_test)))
248/56:
# number of features that cause a drop in performance
# when shuffled

feature_importance[feature_importance<0].shape[0]
248/57:
# print the important features

feature_importance[feature_importance>0].index
248/58:
# print the important features

feature_importance[feature_importance<0].index
248/59:
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split

from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score
248/60:
# load dataset
data = pd.read_csv('../dataset_2.csv')
data.shape
248/61: data.head()
248/62:
# separate train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),
    data['target'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
248/63:
# for this method, it is necessary to reset the indeces of the returned 
# datasets

X_train.reset_index(drop=True, inplace=True)
X_test.reset_index(drop=True, inplace=True)
248/64:
# The first step to determine feature importance by feature shuffling
# is to build the machine learning model for which we want to 
# select features

# In this case, I will build Random Forests, but remember that 
# you can use this procedure with any other machine learning algorithm

# I build few and shallow trees to avoid overfitting
rf = RandomForestClassifier(
    n_estimators=50, max_depth=2, random_state=2909, n_jobs=4)

rf.fit(X_train, y_train)

# print roc-auc in train and testing sets
print('train auc score: ',
      roc_auc_score(y_train, (rf.predict_proba(X_train.fillna(0)))[:, 1]))
print('test auc score: ',
      roc_auc_score(y_test, (rf.predict_proba(X_test.fillna(0)))[:, 1]))
248/65: X_train.shape
248/66: X_train.sample(frac=.4, random_state=10).shape
248/67:
# in this cell, I will shuffle one by one, each feature of the dataset

# then I use the dataset with the shuffled variable to make predictions
# with the random forests I trained in the previous cell

# overall train roc-auc: using all the features
train_roc = roc_auc_score(y_train, (rf.predict_proba(X_train))[:, 1])

# list to capture the performance shift
performance_shift = []

# selection  logic
for feature in X_train.columns:

    X_train_c = X_train.copy()

    # shuffle individual feature
    X_train_c[feature] = X_train_c[feature].sample(
        frac=1, random_state=10).reset_index(drop=True)
    print(X_train_c[feature].shape)

    # make prediction with shuffled feature and calculate roc-auc
    shuff_roc = roc_auc_score(y_train, rf.predict_proba(X_train_c)[:, 1])
    
    drift = train_roc - shuff_roc

    # save the drop in roc-auc
    performance_shift.append(drift)
248/68:
# le't have a look at our list of performances
performance_shift
248/69:
# Now I will transform the list into a pandas Series
# for easy manipulation

feature_importance = pd.Series(performance_shift)

# add variable names in the index
feature_importance.index = X_train.columns

feature_importance.head()
248/70:
# Now I will sort the dataframe according to the drop in performance
# caused by feature shuffling

feature_importance.sort_values(ascending=False)
248/71:
# visualise the top 10 features that caused the major drop
# in the roc-auc (aka model performance)

feature_importance.sort_values(ascending=False).head(10)
248/72:
# original number of features (rows in this case)
feature_importance.shape[0]
248/73:
# number of features that cause a drop in performance
# when shuffled

feature_importance[feature_importance<0].shape[0]
248/74:
# number of features that cause a drop in performance
# when shuffled

feature_importance[feature_importance<0].shape[0]
248/75:
# print the important features

feature_importance[feature_importance<0].index
248/76:
# Now let's build a random forests only with the selected features

# capture the selected features
selected_features = feature_importance[feature_importance < 0].index

# train a new random forests using only the selected features
rf = RandomForestClassifier(n_estimators=50,
                            max_depth=2,
                            random_state=2909,
                            n_jobs=4)

rf.fit(X_train[selected_features], y_train)

# print roc-auc in train and testing sets
print(
    'train auc score: ',
    roc_auc_score(y_train, (rf.predict_proba(X_train[selected_features]))[:,
                                                                          1]))
print(
    'test auc score: ',
    roc_auc_score(y_test, (rf.predict_proba(X_test[selected_features]))[:, 1]))
248/77:
# Now let's build a random forests only with the selected features

# capture the selected features
selected_features = feature_importance[feature_importance > 0].index

# train a new random forests using only the selected features
rf = RandomForestClassifier(n_estimators=50,
                            max_depth=2,
                            random_state=2909,
                            n_jobs=4)

rf.fit(X_train[selected_features], y_train)

# print roc-auc in train and testing sets
print(
    'train auc score: ',
    roc_auc_score(y_train, (rf.predict_proba(X_train[selected_features]))[:,
                                                                          1]))
print(
    'test auc score: ',
    roc_auc_score(y_test, (rf.predict_proba(X_test[selected_features]))[:, 1]))
248/78:
# Now let's build a random forests only with the selected features

# capture the selected features
selected_features = feature_importance[feature_importance > 0].index

# train a new random forests using only the selected features
rf = RandomForestClassifier(n_estimators=50,
                            max_depth=2,
                            random_state=2909,
                            n_jobs=4)

rf.fit(X_train[selected_features], y_train)

# print roc-auc in train and testing sets
print(
    'train auc score: ',
    roc_auc_score(y_train, (rf.predict_proba(X_train[selected_features]))[:,
                                                                          1]))
print(
    'test auc score: ',
    roc_auc_score(y_test, (rf.predict_proba(X_test[selected_features]))[:, 1]))
248/79:
# number of features that cause a drop in performance
# when shuffled

feature_importance[feature_importance>00].shape[0]
248/80:
# print the important features

feature_importance[feature_importance>0].index
249/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor
from sklearn.metrics import roc_auc_score, r2_score
249/2:
# load dataset
data = pd.read_csv('../dataset_1.csv')
data.shape
249/3: data.head()
249/4:
# separate train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),
    data['target'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
249/5:
# to speed things up we remove constant, quasi-constand and duplicated features

quasi_constant_feat = []

# iterate over every feature
for feature in X_train.columns:

    # find the predominant value, that is the value that is shared
    # by most observations
    predominant = (X_train[feature].value_counts() / np.float(
        len(X_train))).sort_values(ascending=False).values[0]

    # evaluate the predominant feature: do more than 99% of the observations
    # show 1 value?
    if predominant > 0.998:
        
        # if yes, add the variable to the list
        quasi_constant_feat.append(feature)

X_train.drop(labels=quasi_constant_feat, axis=1, inplace=True)
X_test.drop(labels=quasi_constant_feat, axis=1, inplace=True)

X_train.shape, X_test.shape
249/6:
duplicated_feat = []
for i in range(0, len(X_train.columns)):
    if i % 10 == 0:  # this helps me understand how the loop is going
        print(i)

    col_1 = X_train.columns[i]

    for col_2 in X_train.columns[i + 1:]:
        if X_train[col_1].equals(X_train[col_2]):
            duplicated_feat.append(col_2)
            
len(duplicated_feat)
249/7:
# remove duplicated features
X_train.drop(labels=duplicated_feat, axis=1, inplace=True)
X_test.drop(labels=duplicated_feat, axis=1, inplace=True)

X_train.shape, X_test.shape
249/8:
# the first step of this procedure consists in building
# a machine learning algorithm using all the available features
# and then determine the importance of the features according
# to the algorithm

# build initial model using all the features
model_full = GradientBoostingClassifier(n_estimators=10, max_depth=4, random_state=10)

model_full.fit(X_train, y_train)

# calculate the roc-auc in the test set
y_pred_test = model_full.predict_proba(X_test)[:, 1]
roc_full = roc_auc_score(y_test, y_pred_test)

print('Test ROC AUC=%f' % (roc_full))
249/9:
# the first step of this procedure consists in building
# a machine learning algorithm using all the available features
# and then determine the importance of the features according
# to the algorithm

# build initial model using all the features
model_full = GradientBoostingClassifier(n_estimators=10, max_depth=4, random_state=10)

model_full.fit(X_train, y_train)

# calculate the roc-auc in the test set
y_pred_test = model_full.predict_proba(X_test)[:, 1]
roc_full = roc_auc_score(y_test, y_pred_test)

print('Test ROC AUC=%f' % (roc_full))
print(f'Test ROC AUC={roc_full}')
249/10:
# the first step of this procedure consists in building
# a machine learning algorithm using all the available features
# and then determine the importance of the features according
# to the algorithm

# build initial model using all the features
model_full = GradientBoostingClassifier(n_estimators=10, max_depth=4, random_state=10)

model_full.fit(X_train, y_train)

# calculate the roc-auc in the test set
y_pred_test = model_full.predict_proba(X_test)[:, 1]
roc_full = roc_auc_score(y_test, y_pred_test)

print('Test ROC AUC=%f' % (roc_full))
print(f'Test ROC AUC={roc_full:.d%}')
249/11:
# the first step of this procedure consists in building
# a machine learning algorithm using all the available features
# and then determine the importance of the features according
# to the algorithm

# build initial model using all the features
model_full = GradientBoostingClassifier(n_estimators=10, max_depth=4, random_state=10)

model_full.fit(X_train, y_train)

# calculate the roc-auc in the test set
y_pred_test = model_full.predict_proba(X_test)[:, 1]
roc_full = roc_auc_score(y_test, y_pred_test)

print('Test ROC AUC=%f' % (roc_full))
print(f'Test ROC AUC={roc_full:.15f}')
249/12:
# the first step of this procedure consists in building
# a machine learning algorithm using all the available features
# and then determine the importance of the features according
# to the algorithm

# build initial model using all the features
model_full = GradientBoostingClassifier(n_estimators=10, max_depth=4, random_state=10)

model_full.fit(X_train, y_train)

# calculate the roc-auc in the test set
y_pred_test = model_full.predict_proba(X_test)[:, 1]
roc_full = roc_auc_score(y_test, y_pred_test)

print('Test ROC AUC=%f' % (roc_full))
print(f'Test ROC AUC={roc_full:.2f}')
249/13: print(f'Test ROC AUC={roc_full:>{10}.d%}
249/14: print(f'Test ROC AUC={roc_full:>{10}.d%})
249/15: print(f'Test ROC AUC={roc_full:>{10}.2f})
249/16: print(f'Test ROC AUC={roc_full:.2f}')
249/17: print(f'Test ROC AUC={roc_full:>{10}.2f}')
249/18: print(f'Test ROC AUC={roc_full:>.{10}.2f}')
249/19: print(f'Test ROC AUC={roc_full:.>{10}.2f}')
249/20:
# the second step consist of deriving the importance of 
# each feature and ranking them from the least to the most
# important

# get feature name and importance
features = pd.Series(model_full.feature_importances_)
features.index = X_train.columns

# sort the features by importance
features.sort_values(ascending=True, inplace=True)

# plot
features.plot.bar(figsize=(20,6))
plt.xlabel('Features')
plt.ylabel('Importance')
plt.show()
249/21:
# make list of ordered features
features = list(features.index)
features
257/1:
# Import spaCy and load the language library
import spacy
nlp = spacy.load('en_core_web_sm')

# Create a Doc object
doc = nlp(u'Tesla is looking at buying U.S. startup for $6 million')

# Print each token separately
for token in doc:
    print(token.text, token.pos_, token.dep_, token.ent_id_)
259/1:
# Perform standard imports
import spacy
nlp = spacy.load('en_core_web_sm')
259/2:
# Create a simple Doc object
doc = nlp(u"The quick brown fox jumped over the lazy dog's back.")
259/3:
# Print the full text:
print(doc.text)
259/4:
# Print the fifth word and associated tags:
print(doc[4].text, doc[4].pos_, doc[4].tag_, spacy.explain(doc[4].tag_))
259/5:
for token in doc:
    print(f'{token.text:{10}} {token.pos_:{8}} {token.tag_:{6}} {spacy.explain(token.tag_)}')
259/6:
doc = nlp(u'I read books on NLP.')
r = doc[1]

print(f'{r.text:{10}} {r.pos_:{8}} {r.tag_:{6}} {spacy.explain(r.tag_)}')
259/7:
doc = nlp(u'I read a book on NLP.')
r = doc[1]

print(f'{r.text:{10}} {r.pos_:{8}} {r.tag_:{6}} {spacy.explain(r.tag_)}')
259/8:
doc = nlp(u"The quick brown fox jumped over the lazy dog's back.")

# Count the frequencies of different coarse-grained POS tags:
POS_counts = doc.count_by(spacy.attrs.POS)
POS_counts
259/9: doc.vocab[83].text
259/10:
for k,v in sorted(POS_counts.items()):
    print(f'{k}. {doc.vocab[k].text:{5}}: {v}')
259/11:
# Count the different fine-grained tags:
TAG_counts = doc.count_by(spacy.attrs.TAG)

for k,v in sorted(TAG_counts.items()):
    print(f'{k}. {doc.vocab[k].text:{4}}: {v}')
259/12:
# Count the different dependencies:
DEP_counts = doc.count_by(spacy.attrs.DEP)

for k,v in sorted(DEP_counts.items()):
    print(f'{k}. {doc.vocab[k].text:{4}}: {v}')
260/1:
# Perform standard imports
import spacy
nlp = spacy.load('en_core_web_sm')

# Import the displaCy library
from spacy import displacy
260/2:
# Create a simple Doc object
doc = nlp(u"The quick brown fox jumped over the lazy dog's back.")
260/3:
# Render the dependency parse immediately inside Jupyter:
displacy.render(doc, style='dep', jupyter=True, options={'distance': 110})
260/4: displacy.render(doc, style='dep', jupyter=True, options={'distance': 4})
260/5: displacy.render(doc, style='dep', jupyter=True, options={'distance': 40, 'compact': True, 'color': 'yellow'})
260/6:
displacy.render(doc, style='dep', jupyter=True, options={'distance': 40, 'compact': True, 
                                                         'color': 'yellow'
                                                         'bg': 'red'})
260/7:
displacy.render(doc, style='dep', jupyter=True, options={'distance': 40, 'compact': True, 
                                                         'color': 'yellow',
                                                         'bg': 'red'})
260/8:
displacy.render(doc, style='dep', jupyter=True, options={'distance': 40, 'compact': True, 
                                                         'color': 'yellow',
                                                         'bg': 'blue'})
260/9:
displacy.render(doc, style='dep', jupyter=True, options={'distance': 40, 'compact': False, 
                                                         'color': 'yellow',
                                                         'bg': 'blue'})
260/10:
displacy.render(doc, style='dep', jupyter=True, options={'distance': 40, 'compact': True, 
                                                         'color': 'yellow',
                                                         'bg': 'blue'})
260/11:
displacy.render(doc, style='dep', jupyter=True, options={'distance': 40, 'compact': False, 
                                                         'color': 'yellow',
                                                         'bg': 'blue'})
260/12:
displacy.render(doc, style='dep', jupyter=True, options={'distance': 90, 'compact': False, 
                                                         'color': 'yellow',
                                                         'bg': 'blue'})
260/13:
displacy.render(doc, style='dep', jupyter=True, options={'distance': 90, 'compact': False, 
                                                         'color': 'yellow',
                                                         'bg': 'lightblue'})
260/14:
displacy.render(doc, style='dep', jupyter=True, options={'distance': 90, 'compact': False, 
                                                         'color': 'yellow',
                                                         'bg': 'lightblue',
                                                         'font': 'red'})
260/15:
displacy.render(doc, style='dep', jupyter=True, options={'distance': 90, 'compact': False, 
                                                         'color': 'yellow',
                                                         'bg': 'lightblue',
                                                         'font': 'Times'})
260/16:
# Render the dependency parse immediately inside Jupyter:
displacy.render(doc, style='dep', jupyter=True, options={'distance': 110})
260/17:
doc2 = nlp(u"This is a sentence. This is another, possibly longer sentence.")

# Create spans from Doc.sents:
spans = list(doc2.sents)

displacy.serve(spans, style='dep', options={'distance': 110})
261/1:
# Perform standard imports
import spacy
nlp = spacy.load('en_core_web_sm')
261/2:
# Write a function to display basic entity info:
def show_ents(doc):
    if doc.ents:
        for ent in doc.ents:
            print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
    else:
        print('No named entities found.')
261/3:
doc = nlp(u'May I go to Washington, DC next May to see the Washington Monument?')

show_ents(doc)
261/4:
doc = nlp(u'Can I please borrow 500 dollars from you to buy some Microsoft stock?')

for ent in doc.ents:
    print(ent.text, ent.start, ent.end, ent.start_char, ent.end_char, ent.label_)
261/5:
doc = nlp(u'Tesla to build a U.K. factory for $6 million')

show_ents(doc)
261/6:
from spacy.tokens import Span

# Get the hash value of the ORG entity label
ORG = doc.vocab.strings[u'ORG']  

# Create a Span for the new entity
new_ent = Span(doc, 0, 1, label=ORG)

# Add the entity to the existing Doc object
doc.ents = list(doc.ents) + [new_ent]
261/7: show_ents(doc)
261/8:
doc = nlp(u'Our company plans to introduce a new vacuum cleaner. '
          u'If successful, the vacuum cleaner will be our first product.')

show_ents(doc)
261/9:
# Import PhraseMatcher and create a matcher object:
from spacy.matcher import PhraseMatcher
matcher = PhraseMatcher(nlp.vocab)
261/10:
# Create the desired phrase patterns:
phrase_list = ['vacuum cleaner', 'vacuum-cleaner']
phrase_patterns = [nlp(text) for text in phrase_list]
261/11:
# Apply the patterns to our matcher object:
matcher.add('newproduct', None, *phrase_patterns)

# Apply the matcher to our Doc object:
matches = matcher(doc)

# See what matches occur:
matches
261/12: new_ents
261/13:
# Here we create Spans from each match, and create named entities from them:
from spacy.tokens import Span

PROD = doc.vocab.strings[u'PRODUCT']

new_ents = [Span(doc, match[1],match[2],label=PROD) for match in matches]

doc.ents = list(doc.ents) + new_ents
261/14: new_ents
261/15: phrase_patterns
261/16: show_ents(doc)
261/17:
doc = nlp(u'Originally priced at $29.50, the sweater was marked down to five dollars.')

show_ents(doc)
261/18: len([ent for ent in doc.ents if ent.label_=='MONEY'])
261/19: spacy.__version__
261/20:
doc = nlp(u'Originally priced at $29.50,\nthe sweater was marked down to five dollars.')

show_ents(doc)
261/21:
# Quick function to remove ents formed on whitespace:
def remove_whitespace_entities(doc):
    doc.ents = [e for e in doc.ents if not e.text.isspace()]
    return doc

# Insert this into the pipeline AFTER the ner component:
nlp.add_pipe(remove_whitespace_entities, after='ner')
262/1:
# Perform standard imports
import spacy
nlp = spacy.load('en_core_web_sm')

# Import the displaCy library
from spacy import displacy
262/2:
doc = nlp(u'Over the last quarter Apple sold nearly 20 thousand iPods for a profit of $6 million. '
         u'By contrast, Sony sold only 7 thousand Walkman music players.')

displacy.render(doc, style='ent', jupyter=True)
262/3:
for sent in doc.sents:
    displacy.rander(nlp(sent.text), style='ent', jupyter=True)
262/4:
for sent in doc.sents:
    displacy.render(nlp(sent.text), style='ent', jupyter=True)
262/5:
options = {'ents': ['ORG', 'PRODUCT', 'ORG']}

displacy.render(doc, style='ent', jupyter=True, options=options)
262/6:
colors = {'ORG': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'PRODUCT': 'radial-gradient(yellow, blue)'}

options = {'ents': ['ORG', 'PRODUCT'], 'colors':colors}

displacy.render(doc, style='ent', jupyter=True, options=options)
262/7:
colors = {'ORG': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'PRODUCT': 'linear-gradient(to right, rgba(255,0,0,0), rgba(255,0,0,1))'}

options = {'ents': ['ORG', 'PRODUCT'], 'colors':colors}

displacy.render(doc, style='ent', jupyter=True, options=options)
263/1:
# Perform standard imports
import spacy
nlp = spacy.load('en_core_web_sm')
263/2:
# From Spacy Basics:
doc = nlp(u'This is the first sentence. This is another sentence. This is the last sentence.')

for sent in doc.sents:
    print(sent)
263/3:
doc_sents = [sent for sent in doc.sents]
doc_sents
263/4:
# Now you can access individual sentences:
print(doc_sents[1])
263/5: type(doc_sents[1])
263/6: print(doc_sents[1].start, doc_sents[1].end)
263/7:
# Parsing the segmentation start tokens happens during the nlp pipeline
doc2 = nlp(u'This is a sentence. This is a sentence. This is a sentence.')

for token in doc2:
    print(token.is_sent_start, ' '+token.text)
263/8:
# SPACY'S DEFAULT BEHAVIOR
doc3 = nlp(u'"Management is doing things right; leadership is doing the right things." -Peter Drucker')

for sent in doc3.sents:
    print(sent)
263/9:
# SPACY'S DEFAULT BEHAVIOR
doc3 = nlp(u'"Management is doing things right; leadership is doing the right things." -Peter Drucker')

for sent in doc3.sents:
    print(sent)
    print('\n')
263/10:
# Re-run the Doc object creation:
doc4 = nlp(u'"Management is doing things right; leadership is doing the right things." -Peter Drucker')

for sent in doc4.sents:
    print(sent)
263/11:
# ADD A NEW RULE TO THE PIPELINE
def set_custom_boundaries(doc):
    for token in doc[:-1]:
        if token.text == ';':
            doc[token.i+1].is_sent_start = True
    return doc

nlp.add_pipe(set_custom_boundaries, before='parser')

nlp.pipe_names
263/12:
# Re-run the Doc object creation:
doc4 = nlp(u'"Management is doing things right; leadership is doing the right things." -Peter Drucker')

for sent in doc4.sents:
    print(sent)
263/13:
# And yet the new rule doesn't apply to the older Doc object:
for sent in doc3.sents:
    print(sent)
263/14:
# SPACY'S DEFAULT BEHAVIOR
doc3 = nlp(u'"Management is doing things right; leadership is doing the right things." -Peter Drucker')

for sent in doc3.sents:
    print(sent)
    print('\n')
263/15:
# Find the token we want to change:
doc3[7]
263/16:
# Try to change the .is_sent_start attribute:
doc3[7].is_sent_start = True
263/17:
nlp = spacy.load('en_core_web_sm')  # reset to the original

mystring = u"This is a sentence. This is another.\n\nThis is a \nthird sentence."

# SPACY DEFAULT BEHAVIOR:
doc = nlp(mystring)

for sent in doc.sents:
    print([token.text for token in sent])
263/18:
nlp = spacy.load('en_core_web_sm')  # reset to the original

mystring = u"This is a sentence. This is another.\n\nThis is a \nthird sentence."

# SPACY DEFAULT BEHAVIOR:
doc = nlp(mystring)

for sent in doc.sents:
    print(sent)
    print([token.text for token in sent])
263/19:
nlp = spacy.load('en_core_web_sm')  # reset to the original

mystring = u"This is a sentence. This is another.\n\nThis is a \nthird sentence."

# SPACY DEFAULT BEHAVIOR:
doc = nlp(mystring)

for sent in doc.sents:
    print(sent)
  #  print([token.text for token in sent])
263/20:
# CHANGING THE RULES
from spacy.pipeline import SentenceSegmenter

def split_on_newlines(doc):
    start = 0
    seen_newline = False
    for word in doc:
        if seen_newline:
            yield doc[start:word.i]
            start = word.i
            seen_newline = False
        elif word.text.startswith('\n'): # handles multiple occurrences
            seen_newline = True
    yield doc[start:]      # handles the last group of tokens


sbd = SentenceSegmenter(nlp.vocab, strategy=split_on_newlines)
nlp.add_pipe(sbd)
263/21:
doc = nlp(mystring)
for sent in doc.sents:
    print([token.text for token in sent])
263/22:
doc = nlp(mystring)
for sent in doc.sents:
    #print([token.text for token in sent])
    print(sent)
263/23:
# CHANGING THE RULES
from spacy.pipeline import SentenceSegmenter

def split_on_newlines(doc):
    start = 0
    seen_newline = False
    for word in doc:
        if seen_newline:
            yield doc[start:word.i]
            start = word.i
            seen_newline = False
        elif word.text.startswith('e'): # handles multiple occurrences
            seen_newline = True
    yield doc[start:]      # handles the last group of tokens


sbd = SentenceSegmenter(nlp.vocab, strategy=split_on_newlines)
nlp.add_pipe(sbd)
264/1:
# RUN THIS CELL to perform standard imports:
import spacy
nlp = spacy.load('en_core_web_sm')
from spacy import displacy
264/2:
with open('../TextFiles/peterrabbit.txt') as f:
    doc = nlp(f.read())
264/3: doc
264/4: doc
264/5: doc
264/6: doc[2]
264/7: list(doc.sents)[2]
264/8:
for tokens in list(doc.sents)[2]:
    print(tokens)
264/9:
for tokens in list(doc.sents)[2]:
    print(tokens, tokens.pos_)
264/10:
for tokens in list(doc.sents)[2]:
    print(tokens, tokens.pos_, tokens.tag_)
264/11:
for tokens in list(doc.sents)[2]:
    print(tokens, tokens.pos_, tokens.tag_, tokens.tag)
264/12:
for tokens in list(doc.sents)[2]:
    print(tokens, tokens.pos_, tokens.tag_, tokens.text)
264/13:
for tokens in list(doc.sents)[2]:
    print(tokens, tokens.pos_, tokens.tag_, tokens.dep_)
264/14:
for tokens in list(doc.sents)[2]:
    print(tokens, tokens.pos_, tokens.tag_, tokens.dep)
264/15:
for tokens in list(doc.sents)[2]:
    print(tokens, tokens.pos_, tokens.tag_, tokens.pos)
264/16:
for tokens in list(doc.sents)[2]:
    print(tokens, tokens.pos_, tokens.tag_, spacy.explain(tokens.tag_))
264/17:
for tokens in list(doc.sents)[2]:
    print(f'{tokens: {10}}, {tokens.pos_:{10}}, tokens.tag_, spacy.explain(tokens.tag_)')
264/18:
for tokens in list(doc.sents)[2]:
    print(f'{tokens: {10}} {tokens.pos_:{10}} tokens.tag_, spacy.explain(tokens.tag_)')
264/19:
for tokens in list(doc.sents)[2]:
    print(f'{tokens: {10}} {tokens.pos_:{10}} {tokens.tag_} {spacy.explain(tokens.tag_)}')
264/20:
for tokens in list(doc.sents)[2]:
    print(f'{tokens: {10}} 
            {tokens.pos_:{10}} 
            {tokens.tag_} 
            {spacy.explain(tokens.tag_)}
          ')
264/21:
for tokens in list(doc.sents)[2]:
    print(f'{tokens:{10}} 
            {tokens.pos_:{10}} 
            {tokens.tag_} 
            {spacy.explain(tokens.tag_)}
          ')
264/22:
for tokens in list(doc.sents)[2]:
    print(f'{tokens:{10}}
        {tokens.pos_:{10}} 
            {tokens.tag_} 
            {spacy.explain(tokens.tag_)}
          ')
264/23:
for tokens in list(doc.sents)[2]:
    print(f'{tokens:{10}}')
264/24:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{10}}')
264/25:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{10}}
            {tokens.pos_:{10}}')
264/26:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{12}} {tokens.pos_:{6}}
264/27:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{12}} {tokens.pos_:{6}}')
264/28:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text}
          ')
264/29:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text}')
264/30:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text}\
            ')
264/31:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{10}}\
            ')
264/32:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{10}}\
            {tokens.pos_:{10}}
            ')
264/33:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{10}}\
            {tokens.pos_:{10}}\
            ')
264/34:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{6}}\
            {tokens.pos_:{6}}\
            ')
264/35:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{6}}\
            {tokens.pos_:{8}}\
            ')
264/36:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{6}}\
            {tokens.pos_:{10}}\
            ')
264/37:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{6}}\
            {tokens.pos_:{15}}\
            ')
264/38:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{10}}\
            {tokens.pos_:{15}}\
            ')
264/39:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{10}}\
            {tokens.pos_:{15}}\
            {tokens.tag_:{10}}\
            {spacy.explain(tokens.tag_):{10}}\
            ')
264/40:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{\t}}\
            {tokens.pos_:{15}}\
            {tokens.tag_:{10}}\
            {spacy.explain(tokens.tag_):{10}}\
            ')
264/41:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{'\t'}}\
            {tokens.pos_:{15}}\
            {tokens.tag_:{10}}\
            {spacy.explain(tokens.tag_):{10}}\
            ')
264/42:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{"\t"}}\
            {tokens.pos_:{15}}\
            {tokens.tag_:{10}}\
            {spacy.explain(tokens.tag_):{10}}\
            ')
264/43:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{10}}\
            {tokens.pos_:{15}}\
            {tokens.tag_:{10}}\
            {spacy.explain(tokens.tag_):{10}}\
            ')
264/44:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{8}}\
            {tokens.pos_:{15}}\
            {tokens.tag_:{10}}\
            {spacy.explain(tokens.tag_):{10}}\
            ')
264/45:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{8}}\
            {tokens.pos_:{8}}\
            {tokens.tag_:{10}}\
            {spacy.explain(tokens.tag_):{10}}\
            ')
264/46:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{8}}\
            {tokens.pos_:{8}}\
            {tokens.tag_:{8}}\
            {spacy.explain(tokens.tag_):{10}}\
            ')
264/47:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{8}}\
            {tokens.pos_:{8}}\
            {tokens.tag_:{8}}\
            {spacy.explain(tokens.tag_):{8}}\
            ')
264/48:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{8}}\
            {tokens.pos_:{8}}\
            {tokens.tag_:{8}}\
            {spacy.explain(tokens.tag_):{8}}\
            ')
264/49:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{8}}\
            {tokens.pos_:{8}}\
            {tokens.tag_:{8}}\
            {spacy.explain(tokens.tag_):{8}}')
264/50:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{8}}\
            {tokens.pos_:{8}}\
            {tokens.tag_:{8}}\
            {spacy.explain(tokens.tag_).text:{8}}')
264/51:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{8}}\
            {tokens.pos_:{8}}\
            {tokens.tag_:{8}}\
            {spacy.explain(tokens.tag_):{8}}')
264/52:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{8}}\
            {tokens.pos_:{8}}\
            {tokens.tag_:{8}}\
            {spacy.explain(tokens.tag_)}')
264/53:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:"\t"}\
            {tokens.pos_:{8}}\
            {tokens.tag_:{8}}\
            {spacy.explain(tokens.tag_)}')
264/54:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{8}}\
            {tokens.pos_:{8}}\
            {tokens.tag_:{8}}\
            {spacy.explain(tokens.tag_)}')
264/55:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{2}}\
            {tokens.pos_:{8}}\
            {tokens.tag_:{8}}\
            {spacy.explain(tokens.tag_)}')
264/56:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{2}}\
            {tokens.pos_:{2}}\
            {tokens.tag_:{8}}\
            {spacy.explain(tokens.tag_)}')
264/57:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{12}}\
            {tokens.pos_:{2}}\
            {tokens.tag_:{8}}\
            {spacy.explain(tokens.tag_)}')
264/58:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{10}}\
            {tokens.pos_:{2}}\
            {tokens.tag_:{8}}\
            {spacy.explain(tokens.tag_)}')
264/59:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{10}}\
            {tokens.pos_:{6}}\
            {tokens.tag_:{8}}\
            {spacy.explain(tokens.tag_)}')
264/60:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{10}}\
            {tokens.pos_:{6}}\
            {tokens.tag_:{5}}\
            {spacy.explain(tokens.tag_)}')
264/61:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{4}}\
            {tokens.pos_:{6}}\
            {tokens.tag_:{5}}\
            {spacy.explain(tokens.tag_)}')
264/62:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{7}}\
            {tokens.pos_:{6}}\
            {tokens.tag_:{5}}\
            {spacy.explain(tokens.tag_)}')
264/63: POS_counts = doc.count_by(spacy.attrs.POS)
264/64: POS_counts
264/65:
for k, v in POS_counts:
    print(k, v)
264/66:
for k, v in enumerate(POS_counts):
    print(k, v)
264/67:
for k, v in POS_counts.items():
    print(f'{}, v')
264/68: POS_counts.items()
264/69: POS_counts.items
264/70: POS_counts.items()
264/71:
for k, v in sorted(POS_counts.items()):
    print(f'{k}, {v}')
264/72:
for k, v in POS_counts.items():
    print(f'{k}, {v}')
264/73:
for k, v in sorted(POS_counts.items()):
    print(f'{k.}, {v}')
264/74:
for k, v in sorted(POS_counts.items()):
    print(f'{k}., {v}')
264/75:
for k, v in sorted(POS_counts.items()):
    print(f'{k:{8}}., {v}')
264/76:
for k, v in sorted(POS_counts.items()):
    print(f'{k}., {v}')
264/77:
for k, v in sorted(POS_counts.items()):
    print(f'{k}. {v}')
264/78:
for k, v in sorted(POS_counts.items()):
    print(f'{k}. {spacy.explain(k)} {v}')
264/79: doc.vocab
264/80: doc.vocab()
264/81:
for k, v in sorted(POS_counts.items()):
    print(f'{k}. {doc.vocab[k].text} {v}')
264/82:
for k, v in sorted(POS_counts.items()):
    print(f'{k}. {doc.vocab[k].text:{10}} {v}')
264/83:
for k, v in sorted(POS_counts.items()):
    print(f'{k}. {doc.vocab[k].text:{5}} {v}')
264/84:
for k, v in sorted(POS_counts.items()):
    print(f'{k}. {doc.vocab[k].text:{7}} : {v}')
264/85:
for k, v in sorted(POS_counts.items()):
    print(f'{k}. {doc.vocab[k].text:{8}} : {v}')
264/86:
for k, v in sorted(POS_counts.items()):
    print(f'{k:{3}}. {doc.vocab[k].text:{8}} : {v}')
264/87:
for k, v in sorted(POS_counts.items()):
    print(f'{k:<{3}}. {doc.vocab[k].text:{8}} : {v}')
264/88:
for k, v in sorted(POS_counts.items()):
    print(f'{k:>{3}}. {doc.vocab[k].text:{8}} : {v}')
264/89: doc.vocab[91]
264/90: POS_counts[91]
264/91: POS_counts
264/92: len(POS_counts)
264/93: sum(POS_counts)
264/94: len(doc)
264/95: POS_counts[91]/len(doc)
264/96: print(f'{POS_counts[91]/len(doc)}:—é2–∞')
264/97: print(f'{POS_counts[91]/len(doc): .2f}')
264/98: print(f'{POS_counts[91]/len(doc)*100: .2f}')
264/99: print(f'{POS_counts[91]/len(doc)*100: .2f}%')
264/100: print(f'176/1258 = {POS_counts[91]/len(doc)*100: .2f}%')
264/101: print(f'176/1258 = {POS_counts[91]/len(doc)*100:.2f}%')
264/102: doc.sents
264/103: list(doc.sents)[2]
264/104: displacy.render(list(doc.sents)[2], jupyter=True)
264/105: displacy.render(list(doc.sents)[2], style='ent', jupyter=True)
264/106: displacy.render(list(doc.sents)[2], style='dep', jupyter=True)
264/107: displacy.render(list(doc.sents)[2], style='dep', jupyter=True, options={'distance': 110})
264/108: displacy.render(list(doc.sents)[2], style='dep', jupyter=True, options={'distance': 50})
264/109: displacy.render(list(doc.sents)[2], style='dep', jupyter=True, options={'distance': 70})
264/110: list_of_sents = doc.sents
264/111: list_of_sents = list(doc.sents)
264/112: list_of_sents[0]
264/113:
colors = {'ORG': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'PRODUCT': 'linear-gradient(to right, rgba(255,0,0,0), rgba(255,0,0,1))'}

options = {'ents': ['ORG', 'PRODUCT'], 'colors':colors}

displacy.render(list_of_sents[0], style='ent', jupyter=True, options=options)
269/1:
import numpy as np
import pandas as pd

df = pd.read_csv('../TextFiles/smsspamcollection.tsv', sep='\t')
df.head()
269/2: len(df)
269/3: df.isnull().sum()
269/4: df['label'].unique()
269/5: df['label'].value_counts()
269/6: df['length'].describe()
269/7:
import matplotlib.pyplot as plt
%matplotlib inline

#plt.xscale('log')
bins = 1.15**(np.arange(0,50))
plt.hist(df[df['label']=='ham']['length'],bins=bins,alpha=0.8)
plt.hist(df[df['label']=='spam']['length'],bins=bins,alpha=0.8)
plt.legend(('ham','spam'))
plt.show()
269/8:
import matplotlib.pyplot as plt
%matplotlib inline

#plt.xscale('log')
bins = 1.15**(np.arange(0,50))
plt.hist(df[df['label']=='ham']['length'],bins=bins,alpha=0.8)
plt.hist(df[df['label']=='spam']['length'],bins=bins,alpha=0.8)
plt.legend(('ham','spam'))
plt.show()
269/9:
import matplotlib.pyplot as plt
%matplotlib inline

plt.xscale('log')
bins = 1.15**(np.arange(0,50))
plt.hist(df[df['label']=='ham']['length'],bins=bins,alpha=0.8)
plt.hist(df[df['label']=='spam']['length'],bins=bins,alpha=0.8)
plt.legend(('ham','spam'))
plt.show()
269/10: df['punct'].describe()
269/11:
plt.xscale('log')
bins = 1.5**(np.arange(0,15))
plt.hist(df[df['label']=='ham']['punct'],bins=bins,alpha=0.8)
plt.hist(df[df['label']=='spam']['punct'],bins=bins,alpha=0.8)
plt.legend(('ham','spam'))
plt.show()
269/12:
# Create Feature and Label sets
X = df[['length','punct']]  # note the double set of brackets
y = df['label']
269/13:
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

print('Training Data Shape:', X_train.shape)
print('Testing Data Shape: ', X_test.shape)
269/14:
from sklearn.linear_model import LogisticRegression

lr_model = LogisticRegression(solver='lbfgs')

lr_model.fit(X_train, y_train)
269/15:
from sklearn import metrics

# Create a prediction set:
predictions = lr_model.predict(X_test)

# Print a confusion matrix
print(metrics.confusion_matrix(y_test,predictions))
269/16: pd.DataFrame(metrics.confusion_matrix(y_test,predictions)
269/17: pd.DataFrame(metrics.confusion_matrix(y_test,predictions))
269/18:
pd.DataFrame(metrics.confusion_matrix(y_test,predictions), \
             index=['ham', 'spam'])
269/19:
pd.DataFrame(metrics.confusion_matrix(y_test,predictions), \
             index=['ham', 'spam'],
             columns=['ham', 'spam'])
269/20:
pd.DataFrame(metrics.confusion_matrix(y_test,predictions), \
             index=['ham', 'spam'],\
             columns=['ham', 'spam'])
269/21: print(metrics.classification_report(y_test,predictions))
269/22: print(metrics.accuracy_score(y_test,predictions))
269/23:
from sklearn.svm import SVC
svc_model = SVC(gamma='auto')
svc_model.fit(X_train,y_train)
269/24:
predictions = svc_model.predict(X_test)
print(metrics.confusion_matrix(y_test,predictions))
269/25: print(metrics.classification_report(y_test,predictions))
269/26: print(metrics.accuracy_score(y_test,predictions))
270/1:
%%writefile 1.txt
This is a story about cats
our feline pets
Cats are furry animals
270/2:
%%writefile 2.txt
This story is about surfing
Catching waves is fun
Surfing is a popular water sport
270/3:
vocab = {}
i = 1

with open('1.txt') as f:
    x = f.read().lower().split()

for word in x:
    if word in vocab:
        continue
    else:
        vocab[word]=i
        i+=1

print(vocab)
270/4:
with open('2.txt') as f:
    x = f.read().lower().split()

for word in x:
    if word in vocab:
        continue
    else:
        vocab[word]=i
        i+=1

print(vocab)
270/5:
# Create an empty vector with space for each word in the vocabulary:
one = ['1.txt']+[0]*len(vocab)
one
270/6:
# map the frequencies of each word in 1.txt to our vector:
with open('1.txt') as f:
    x = f.read().lower().split()
    
for word in x:
    one[vocab[word]]+=1
    
one
270/7:
# Do the same for the second document:
two = ['2.txt']+[0]*len(vocab)

with open('2.txt') as f:
    x = f.read().lower().split()
    
for word in x:
    two[vocab[word]]+=1
270/8:
# Compare the two vectors:
print(f'{one}\n{two}')
270/9:
# Perform imports and load the dataset:
import numpy as np
import pandas as pd

df = pd.read_csv('../TextFiles/smsspamcollection.tsv', sep='\t')
df.head()
270/10: df.isnull().sum()
270/11: df['label'].value_counts()
270/12: df['label'].value_counts()/len(df)
270/13:
from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()

X_train_counts = count_vect.fit_transform(X_train)
X_train_counts.shape
270/14:
from sklearn.model_selection import train_test_split

X = df['message']  # this time we want to look at the text
y = df['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
270/15:
from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()

X_train_counts = count_vect.fit_transform(X_train)
X_train_counts.shape
270/16:
from sklearn.feature_extraction.text import TfidfTransformer
tfidf_transformer = TfidfTransformer()

X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
X_train_tfidf.shape
270/17:
from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()

X_train_counts = count_vect.fit_transform(X_train)
X_train_counts.shape
270/18:
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()

X_train_tfidf = vectorizer.fit_transform(X_train) # remember to use the original X_train set
X_train_tfidf.shape
270/19: X_train_tfidf.head(3)
270/20: X_train_tfidf
270/21: X_train_tfidf[0]
270/22:
from sklearn.svm import LinearSVC
clf = LinearSVC()
clf.fit(X_train_tfidf,y_train)
270/23:
from sklearn.pipeline import Pipeline
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.svm import LinearSVC

text_clf = Pipeline([('tfidf', TfidfVectorizer()),
                     ('clf', LinearSVC()),
])

# Feed the training data through the pipeline
text_clf.fit(X_train, y_train)
270/24:
# Form a prediction set
predictions = text_clf.predict(X_test)
270/25:
# Report the confusion matrix
from sklearn import metrics
print(metrics.confusion_matrix(y_test,predictions))
271/1:
import numpy as np
import pandas as pd

df = pd.read_csv('../TextFiles/moviereviews.tsv', sep='\t')
df.head()
271/2: len(df)
271/3: print(df[0])
271/4: print(df['review'][0])
271/5:
# Check for the existence of NaN values in a cell:
df.isnull().sum()
271/6: df.itertuples()
271/7: my_str = ' '
271/8: my_str.isspace()
271/9: my_str = ''
271/10: my_str.isspace()
271/11:
blanks = []
for i, lb, rw in df.itertuples():
    if rv.isspace():
        blanks.append(i)
271/12:
blanks = []
for i, lb, rw in df.itertuples():
    if rw.isspace():
        blanks.append(i)
271/13:
# Check for the existence of NaN values in a cell:
df.isnull().sum()
271/14:
blanks = []
for i, lb, rw in df.itertuples():
    print(rw)
    if rw.isspace():
        blanks.append(i)
271/15:
blanks = []
for i, lb, rv in df.itertuples():
    print(rw)
    if rv.isspace():
        blanks.append(i)
271/16:
blanks = []
for i, lb, rv in df.itertuples():
    print(rv)
    if rv.isspace():
        blanks.append(i)
271/17:
blanks = []
for i, lb, rv in df.itertuples():
    if rv.isspace():
        blanks.append(i)
271/18: df.dtypes
271/19:
blanks = []
for i, lb, rv in df.itertuples():
    if str(rv).isspace():
        blanks.append(i)
271/20: blanks
271/21: df.drop(blanks, inplace=True)
271/22: df.shape
271/23:
from sklearn.model_selection import train_test_split

X = df['review']
y = df['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
271/24:
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC

# Na√Øve Bayes:
text_clf_nb = Pipeline([('tfidf', TfidfVectorizer()),
                     ('clf', MultinomialNB()),
])

# Linear SVC:
text_clf_lsvc = Pipeline([('tfidf', TfidfVectorizer()),
                     ('clf', LinearSVC()),
])
271/25: text_clf_nb.fit(X_train, y_train)
271/26: tf = TfidfVectorizer()
271/27: tf.fit(X_train)
271/28: X_train
271/29: X_train.isnull().sum()
271/30: df.isnull().mean()
271/31:
df.drop(blanks, inplace=True)

len(df)
271/32:
import numpy as np
import pandas as pd

df = pd.read_csv('../TextFiles/moviereviews.tsv', sep='\t')
df.head()
271/33: len(df)
271/34: print(df['review'][0])
271/35:
from IPython.display import Markdown, display
display(Markdown('> '+df['review'][0]))
271/36:
# Check for the existence of NaN values in a cell:
df.isnull().sum()
271/37: my_str = ''
271/38: my_str.isspace()
271/39: df.itertuples()
271/40:
blanks = []
for i, lb, rv in df.itertuples():
    if str(rv).isspace():
        blanks.append(i)
271/41: blanks
271/42: df.drop(blanks, inplace=True)
271/43: df.shape
271/44:


len(df)
271/45:
# Check for the existence of NaN values in a cell:
df.isnull().sum()
271/46:
# Check for the existence of NaN values in a cell:
df.isnull().sum()
271/47: df.shape
271/48:
import numpy as np
import pandas as pd

df = pd.read_csv('../TextFiles/moviereviews.tsv', sep='\t')
df.head()
271/49: len(df)
271/50: print(df['review'][0])
271/51:
from IPython.display import Markdown, display
display(Markdown('> '+df['review'][0]))
271/52:
# Check for the existence of NaN values in a cell:
df.isnull().sum()
271/53: my_str = ''
271/54: my_str.isspace()
271/55: df.itertuples()
271/56:
blanks = []
for i, lb, rv in df.itertuples():
    if str(rv).isspace():
        blanks.append(i)
271/57: blanks
271/58: df.shape
271/59: df[1993]
271/60: df.iloc[1993]
271/61: df.drop(blanks, inplace=True)
271/62: df.iloc[1993]
271/63: df.shape
271/64: df['label'].value_counts()
271/65: df.isnull().mean()
271/66: df.dropna(inplace=True)
271/67: df.isnull().mean()
271/68:
from sklearn.model_selection import train_test_split

X = df['review']
y = df['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
271/69:
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC

# Na√Øve Bayes:
text_clf_nb = Pipeline([('tfidf', TfidfVectorizer()),
                     ('clf', MultinomialNB()),
])

# Linear SVC:
text_clf_lsvc = Pipeline([('tfidf', TfidfVectorizer()),
                     ('clf', LinearSVC()),
])
271/70: tf = TfidfVectorizer()
271/71: X_train.isnull().sum()
271/72: text_clf_nb.fit(X_train, y_train)
271/73:
# Form a prediction set
predictions = text_clf_nb.predict(X_test)
271/74:
# Report the confusion matrix
from sklearn import metrics
print(metrics.confusion_matrix(y_test,predictions))
271/75:
# Print a classification report
print(metrics.classification_report(y_test,predictions))
271/76:
# Print the overall accuracy
print(metrics.accuracy_score(y_test,predictions))
271/77: text_clf_lsvc.fit(X_train, y_train)
271/78:
# Form a prediction set
predictions = text_clf_lsvc.predict(X_test)
271/79:
# Report the confusion matrix
from sklearn import metrics
print(metrics.confusion_matrix(y_test,predictions))
271/80:
# Print a classification report
print(metrics.classification_report(y_test,predictions))
271/81:
# Print the overall accuracy
print(metrics.accuracy_score(y_test,predictions))
271/82:
stopwords = ['a', 'about', 'an', 'and', 'are', 'as', 'at', 'be', 'been', 'but', 'by', 'can', \
             'even', 'ever', 'for', 'from', 'get', 'had', 'has', 'have', 'he', 'her', 'hers', 'his', \
             'how', 'i', 'if', 'in', 'into', 'is', 'it', 'its', 'just', 'me', 'my', 'of', 'on', 'or', \
             'see', 'seen', 'she', 'so', 'than', 'that', 'the', 'their', 'there', 'they', 'this', \
             'to', 'was', 'we', 'were', 'what', 'when', 'which', 'who', 'will', 'with', 'you']
271/83:
# YOU DO NOT NEED TO RUN THIS CELL UNLESS YOU HAVE
# RECENTLY OPENED THIS NOTEBOOK OR RESTARTED THE KERNEL:

import numpy as np
import pandas as pd

df = pd.read_csv('../TextFiles/moviereviews.tsv', sep='\t')
df.dropna(inplace=True)
blanks = []
for i,lb,rv in df.itertuples():
    if type(rv)==str:
        if rv.isspace():
            blanks.append(i)
df.drop(blanks, inplace=True)
from sklearn.model_selection import train_test_split
X = df['review']
y = df['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn import metrics
271/84:
# RUN THIS CELL TO ADD STOPWORDS TO THE LINEAR SVC PIPELINE:
text_clf_lsvc2 = Pipeline([('tfidf', TfidfVectorizer(stop_words=stopwords)),
                     ('clf', LinearSVC()),
])
text_clf_lsvc2.fit(X_train, y_train)
271/85:
predictions = text_clf_lsvc2.predict(X_test)
print(metrics.confusion_matrix(y_test,predictions))
271/86: print(metrics.classification_report(y_test,predictions))
271/87: print(metrics.accuracy_score(y_test,predictions))
271/88:
# YOU DO NOT NEED TO RUN THIS CELL UNLESS YOU HAVE
# RECENTLY OPENED THIS NOTEBOOK OR RESTARTED THE KERNEL:

import numpy as np
import pandas as pd

df = pd.read_csv('../TextFiles/moviereviews.tsv', sep='\t')
df.dropna(inplace=True)
blanks = []
for i,lb,rv in df.itertuples():
    if type(rv)==str:
        if rv.isspace():
            blanks.append(i)
df.drop(blanks, inplace=True)
from sklearn.model_selection import train_test_split
X = df['review']
y = df['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn import metrics

# Na√Øve Bayes Model:
text_clf_nb = Pipeline([('tfidf', TfidfVectorizer()),
                     ('clf', MultinomialNB()),
])

# Linear SVC Model:
text_clf_lsvc = Pipeline([('tfidf', TfidfVectorizer()),
                     ('clf', LinearSVC()),
])

# Train both models on the moviereviews.tsv training set:
text_clf_nb.fit(X_train, y_train)
text_clf_lsvc.fit(X_train, y_train)
271/89:
myreview = "A movie I really wanted to love was terrible. \
I'm sure the producers had the best intentions, but the execution was lacking."
271/90: print(text_clf_nb.predict([myreview]))  # be sure to put "myreview" inside square brackets
271/91: print(text_clf_lsvc.predict([myreview]))
272/1:
import pandas as pd
import numpy as np

df = pd.read_csv('../TextFiles/moviereviews2.tsv')
272/2:
import pandas as pd
import numpy as np

df = pd.read_csv('../TextFiles/moviereviews2.tsv', sep='\t')
272/3:
# Check for NaN values:
df.isnull().maen()
272/4:
# Check for NaN values:
df.isnull().mean()
272/5: df.head(3)
272/6:
# Check for whitespace strings (it's OK if there aren't any!):

space_lines = []

for i, lab, rv in df.itertuples():
    if rv.isspace():
        space_lines.append(space_lines)
272/7:
# Check for whitespace strings (it's OK if there aren't any!):

space_lines = []

for i, lab, rv in df.itertuples():
    if str(rv).isspace():
        space_lines.append(i)
272/8: space_lines
272/9:
df.shape
df.dropna(inplace=True)
272/10: df.label[1]
272/11: df.label.value_counts()
272/12: from sklearn.model_selection import train_test_split
272/13:
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(df, test_size=.33, random_state = 42)
272/14: df.head(3)
272/15:
from sklearn.model_selection import train_test_split
X = df['review']
y = df['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33, random_state = 42)
272/16:
from sklearn import pipeline
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix
272/17:
from sklearn import pipeline
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix

text_clf_svc = pipeline([('tfidf', TfidfVectorizer()),
                         ('svc', LinearSVC())])

text_clf_svc.fit(X_train, y_train)
272/18:
from sklearn import pipeline
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix

text_clf_svc = pipeline([('tfidf', TfidfVectorizer()),\
                         ('svc', LinearSVC())])

text_clf_svc.fit(X_train, y_train)
272/19:
from sklearn import pipeline
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix

text_clf_svc = pipeline([('tfidf', TfidfVectorizer()),\
                         ('clf', LinearSVC())])

text_clf_svc.fit(X_train, y_train)
272/20:
from sklearn import Pipeline
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix

text_clf_svc = Pipeline([('tfidf', TfidfVectorizer()),
                         ('clf', LinearSVC())])

text_clf_svc.fit(X_train, y_train)
272/21:
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix

text_clf_svc = Pipeline([('tfidf', TfidfVectorizer()),
                         ('clf', LinearSVC())])

text_clf_svc.fit(X_train, y_train)
272/22:
# Form a prediction set
y_predict = text_clf_svc.predict(X_test)
272/23:
# Report the confusion matrix
print(confusion_matrix(y_test, y_predict))
272/24:
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix, classification_report

text_clf_svc = Pipeline([('tfidf', TfidfVectorizer()),
                         ('clf', LinearSVC())
                        ])

text_clf_svc.fit(X_train, y_train)
272/25:
# Form a prediction set
y_predict = text_clf_svc.predict(X_test)
272/26:
# Report the confusion matrix
print(confusion_matrix(y_test, y_predict))
272/27:
# Print a classification report
print(classification_report(y_test, y_predict))
272/28:
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

text_clf_svc = Pipeline([('tfidf', TfidfVectorizer()),
                         ('clf', LinearSVC())
                        ])

text_clf_svc.fit(X_train, y_train)
272/29:
# Print the overall accuracy
print(accuracy_score(y_test, y_predict))
275/1: id_name_verified
275/2: import pandas as pd
275/3: id_name_verified = pd.DataFrame([[1, "John", True]], columns=['Id', 'Login', 'Verified'])
275/4: id_name_verified
275/5: id_password = np.array([[1, 987340123], [2, 187031122]])
275/6:
import pandas as pd
import numpy as np
275/7: id_name_verified
275/8: id_password = np.array([[1, 987340123], [2, 187031122]])
275/9: id_password
275/10: id_password[:, 0]
275/11: pd.DataFrame(id_password)
275/12: pd.DataFrame(id_password, columns=['index', 'Password'])
275/13: df = pd.DataFrame(id_password, columns=['index', 'Password'])
275/14: id_name_verified
275/15: id_name_verified.merge(df, left_on='Id', right_on='index')
275/16: id_name_verified.merge(df, left_on='Id', right_on='index').drop(['Verified', 'index'], axes=1, inplace=True)
275/17: id_name_verified.merge(df, left_on='Id', right_on='index').drop(columns = ['Verified', 'index'], inplace=True)
275/18: id_name_verified
275/19: id_name_verified = id_name_verified.merge(df, left_on='Id', right_on='index').drop(columns = ['Verified', 'index'], inplace=True)
275/20: id_name_verified
275/21: id_name_verified = pd.DataFrame([[1, "John", True]], columns=['Id', 'Login', 'Verified'])
275/22: id_password = np.array([[1, 987340123], [2, 187031122]])
275/23: id_password[:, 0]
275/24: df = pd.DataFrame(id_password, columns=['index', 'Password'])
275/25: id_name_verified
275/26: id_name_verified = id_name_verified.merge(df, left_on='Id', right_on='index').drop(columns = ['Verified', 'index'], inplace=True)
275/27: id_name_verified
275/28: id_name_verified = pd.DataFrame([[1, "John", True]], columns=['Id', 'Login', 'Verified'])
275/29: id_password = np.array([[1, 987340123], [2, 187031122]])
275/30: id_password[:, 0]
275/31: df = pd.DataFrame(id_password, columns=['index', 'Password'])
275/32: id_name_verified
275/33: id_name_verified = id_name_verified.merge(df, left_on='Id', right_on='index')\
275/34: id_name_verified
275/35: id_name_verified.drop(columns = ['Verified', 'index'], inplace=True)
275/36: id_name_verified
275/37:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    df = pd.DataFrame(id_password, columns=['index', 'Password'])
    id_name_verified = id_name_verified.merge(df, left_on='Id', right_on='index')
    id_name_verified.drop(columns = ['Verified', 'index'], inplace=True)
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/38:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    df = pd.DataFrame(id_password, columns=['index', 'Password'])
    print(df)
    id_name_verified = id_name_verified.merge(df, left_on='Id', right_on='index')
    id_name_verified.drop(columns = ['Verified', 'index'], inplace=True)
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/39:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    df = pd.DataFrame(id_password, columns=['index', 'Password'])
    id_name_verified = id_name_verified.merge(df, left_on='Id', right_on='index')
    print(id_name_verified)
    id_name_verified.drop(columns = ['Verified', 'index'], inplace=True)
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/40:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    df = pd.DataFrame(id_password, columns=['index', 'Password'])
    id_name_verified = id_name_verified.merge(df, left_on='Id', right_on='index')
    id_name_verified = id_name_verified.drop(columns = ['Verified', 'index'])
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/41:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    global id_name_verified
    df = pd.DataFrame(id_password, columns=['index', 'Password'])
    id_name_verified = id_name_verified.merge(df, left_on='Id', right_on='index')
    id_name_verified = id_name_verified.drop(columns = ['Verified', 'index'])
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/42:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    global id_name_verified
    df = pd.DataFrame(id_password, columns=['index', 'Password'])
    id_name_verified = id_name_verified.drop(columns = ['Verified'], inplace=True)
    
    id_name_verified = id_name_verified.merge(df, left_on='Id', right_on='index')
    id_name_verified = id_name_verified.drop(columns = ['Verified', 'index'])
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/43:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    global id_name_verified
    df = pd.DataFrame(id_password, columns=['index', 'Password'])
    id_name_verified = id_name_verified.drop(columns = ['Verified'], inplace=True)
    
    id_name_verified = id_name_verified.merge(df, left_on='Id', right_on='index')
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/44:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    global id_name_verified
    df = pd.DataFrame(id_password, columns=['index', 'Password'])
    id_name_verified = id_name_verified.drop(columns = ['Verified'], inplace=True)  
    id_name_verified = id_name_verified.merge(df, left_on='Id', right_on='index')
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/45:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    global id_name_verified
    df = pd.DataFrame(id_password, columns=['Id', 'Password'])
    id_name_verified = id_name_verified.drop(columns = ['Verified'], inplace=True)  
    id_name_verified = id_name_verified.merge(df, on='Id')
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/46:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    global id_name_verified
    df = pd.DataFrame(id_password, columns=['Id', 'Password'])
    id_name_verified = id_name_verified.drop(columns = ['Verified'], inplace=True)  
    id_name_verified = id_name_verified.merge(df, on=['Id'])
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/47:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    global id_name_verified
    df = pd.DataFrame(id_password, columns=['Id', 'Password'])
    id_name_verified = id_name_verified.drop(columns = ['Verified'], inplace=True)  
    id_name_verified =id_name_verified.merge(password, on=['Id'])
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/48:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
#    global id_name_verified
    df = pd.DataFrame(id_password, columns=['Id', 'Password'])
    id_name_verified = id_name_verified.drop(columns = ['Verified'], inplace=True)  
    id_name_verified =id_name_verified.merge(password, on=['Id'])
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/49:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
#    global id_name_verified
    df = pd.DataFrame(id_password, columns=['Id', 'Password'])
    id_name_verified = id_name_verified.drop(columns = ['Verified'], inplace=True)  
    id_name_verified =id_name_verified.merge(df, on=['Id'])
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/50:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
#    global id_name_verified
    df = pd.DataFrame(id_password, columns=['Id', 'Password'])
    id_name_verified = id_name_verified.drop(columns = ['Verified'], inplace=True)  
    id_name_verified =id_name_verified.merge(df, on=['Id'])
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/51:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    global id_name_verified
    df = pd.DataFrame(id_password, columns=['Id', 'Password'])
    id_name_verified = id_name_verified.drop(columns = ['Verified'], inplace=True)  
    id_name_verified =id_name_verified.merge(df, on=['Id'])
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/52:
import pandas as pd
import numpy as np

def login_table( id_password):
    global id_name_verified
    id_name_verified.drop(columns="Verified",inplace=True)
    password = pd.DataFrame(id_password)
    password.columns = ["Id", "Password"]
    id_name_verified =id_name_verified.merge(password, on=['Id'])

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table( id_password)
print(id_name_verified)
275/53:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    global id_name_verified
    df = pd.DataFrame(id_password, columns=['Id', 'Password'])
    id_name_verified.drop(columns = ['Verified'], inplace=True)  
    id_name_verified =id_name_verified.merge(df, on=['Id'])
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/54:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    global id_name_verified
    df = pd.DataFrame(id_password, columns=['Id', 'Password'])
    id_name_verified.drop(columns = ['Verified'], inplace=True)  
    id_name_verified.merge(df, on=['Id'])
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/55:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    global id_name_verified
    df = pd.DataFrame(id_password, columns=['Id', 'Password'])
    id_name_verified.drop(columns = ['Verified'], inplace=True)  
    id_name_verified = id_name_verified.merge(df, on=['Id'])
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/56:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    global id_name_verified
    id_name_verified.drop(columns = ['Verified'], inplace=True)  
    df = pd.DataFrame(id_password, columns=['Id', 'Password'])
    id_name_verified = id_name_verified.merge(df, on=['Id'])
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/57:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
 #   global id_name_verified
    id_name_verified.drop(columns = ['Verified'], inplace=True)  
    df = pd.DataFrame(id_password, columns=['Id', 'Password'])
    id_name_verified = id_name_verified.merge(df, on=['Id'])
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/58:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
 #   global id_name_verified
    id_name_verified.drop(columns = ['Verified'], inplace=True)  
    df = pd.DataFrame(id_password, columns=['Id', 'Password'])
    print(df)
    id_name_verified = id_name_verified.merge(df, on=['Id'])
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/59:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
 #   global id_name_verified
    id_name_verified.drop(columns = ['Verified'], inplace=True)  
    df = pd.DataFrame(id_password, columns=['Id', 'Password'])
    print(df)
    id_name_verified = id_name_verified.merge(df, on=['Id'])
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/60:
import pandas as pd
import numpy as np

def login_table( id_password):
    global id_name_verified
    id_name_verified.drop(columns="Verified",inplace=True)
    password = pd.DataFrame(id_password)
    password.columns = ["Id", "Password"]
    print(password)
    id_name_verified =id_name_verified.merge(password, on=['Id'])

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table( id_password)
print(id_name_verified)
275/61:
import pandas as pd
import numpy as np

def login_table( id_password):
    global id_name_verified
    id_name_verified.drop(columns="Verified",inplace=True)
    password = pd.DataFrame(id_password, columns=['Id', 'Password'])
    print(password)
    id_name_verified =id_name_verified.merge(password, on=['Id'])

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table( id_password)
print(id_name_verified)
275/62:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
 #   global id_name_verified
    id_name_verified.drop(columns = ['Verified'], inplace=True)  
    df = pd.DataFrame(id_password, columns=['Id', 'Password'])
    print(df)
    id_name_verified = id_name_verified.merge(df, on=['Id'])
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/63:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
 #   global id_name_verified
    id_name_verified.drop(columns = ['Verified'], inplace=True)  
    df = pd.DataFrame(id_password, columns=['Id', 'Password'])
    print(df)
    id_name_verified = id_name_verified.merge(df, on=['Id'])
    print(id_name_verified)
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/64:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
 #   global id_name_verified
    id_name_verified.drop(columns = ['Verified'], inplace=True)  
    df = pd.DataFrame(id_password, columns=['Id', 'Password'])
    print(df)
    id_name_verified = id_name_verified.join(df, how="inner")
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/65:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
 #   global id_name_verified
    id_name_verified.drop(columns = ['Verified'], inplace=True)  
    df = pd.DataFrame(id_password, columns=['Id', 'Password'])
    print(df)
    id_name_verified = id_name_verified.join(df, on='Id')
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/66:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
 #   global id_name_verified
    id_name_verified.drop(columns = ['Verified'], inplace=True)  
    df = pd.DataFrame(id_password, columns=['Id', 'Password'])
    print(df)
    id_name_verified = id_name_verified.join(df, on='Id', how='left')
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/67:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
 #   global id_name_verified
    id_name_verified.drop(columns = ['Verified'], inplace=True)  
    df = pd.DataFrame(id_password, columns=['Id', 'Password'])
    print(df)
    id_name_verified = id_name_verified.join(df.set_index('Id'), on='Id', how='left')
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/68:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
 #   global id_name_verified
    id_name_verified.drop(columns = ['Verified'], inplace=True)  
    for row in id_name_verified.itertuples(index=True):
        id_name_verified.at[row.Index,'Password'] = id_password[row.Index,1]
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/69:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
 #   global id_name_verified
    id_name_verified.drop(columns = ['Verified'], inplace=True)  
    for row in id_name_verified.itertuples(index=True):
        id_name_verified.at[row.Index,'Password'] = int(id_password[row.Index,1])
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/70:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
 #   global id_name_verified
    id_name_verified.drop(columns = ['Verified'], inplace=True)  
    for row in id_name_verified.itertuples(index=True):
        id_name_verified.at[row.Index,'Password'] = int(id_password[row.Index,1])
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified.dtypes)
275/71:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
 #   global id_name_verified
    id_name_verified.drop(columns = ['Verified'], inplace=True)  
    for row in id_name_verified.itertuples(index=True):
        id_name_verified.at[row.Index,'Password'] = int(id_password[row.Index,1])
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/72:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
 #   global id_name_verified
    id_name_verified.drop(columns = ['Verified'], inplace=True)  
    id_name_verified['Password'] = pd.Series(dtype='int')
    for row in id_name_verified.itertuples(index=True):
        id_name_verified.at[row.Index,'Password'] = int(id_password[row.Index,1])
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/73:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
 #   global id_name_verified
    id_name_verified.drop(columns = ['Verified'], inplace=True)  
    id_name_verified['Password'] = pd.Series(dtype='int')
    print(id_name_verified)
    for row in id_name_verified.itertuples(index=True):
        id_name_verified.at[row.Index,'Password'] = int(id_password[row.Index,1])
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/74:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
 #   global id_name_verified
    id_name_verified.drop(columns = ['Verified'], inplace=True)  
    id_name_verified['Password'] = pd.Series(dtype='int')
    for row in id_name_verified.itertuples(index=True):
        id_name_verified.at[row.Index,'Password'] = id_password[row.Index,1].astype(np.int32)
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/75:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
 #   global id_name_verified
    id_name_verified.drop(columns = ['Verified'], inplace=True)  
    id_name_verified['Password'] = pd.Series(dtype='int')
    for row in id_name_verified.itertuples(index=True):
        id_name_verified.at[row.Index,'Password'] = id_password[row.Index,1].astype(np.int32)
    id_name_verified['Password'] = id_name_verified['Password'].astype('int')
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/76:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
 #   global id_name_verified
    id_name_verified.drop(columns = ['Verified'], inplace=True)  
    for row in id_name_verified.itertuples(index=True):
        id_name_verified.at[row.Index,'Password'] = id_password[row.Index,1].astype(np.int32)
    id_name_verified['Password'] = id_name_verified['Password'].astype('int')
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/77:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
 #   global id_name_verified
    id_name_verified.drop(columns = ['Verified'], inplace=True)  
    for row in id_name_verified.itertuples(index=True):
        id_name_verified.at[row.Index,'Password'] = id_password[row.Index,1]
    id_name_verified['Password'] = id_name_verified['Password'].astype('int')
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/78:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
 #   global id_name_verified
    id_name_verified.drop(columns = ['Verified'], inplace=True) 
    print(id_name_verified)
    for row in id_name_verified.itertuples(index=True):
        id_name_verified.at[row.Index,'Password'] = id_password[row.Index,1]
    id_name_verified['Password'] = id_name_verified['Password'].astype('int')
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/79:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
 #   global id_name_verified
    id_name_verified.drop(columns = ['Verified'], inplace=True) 
    print(id_name_verified)
    for i, Id, Login in id_name_verified.itertuples(index=True):
        id_name_verified.at[i,'Password'] = id_password[row.Index,1]
    id_name_verified['Password'] = id_name_verified['Password'].astype('int')
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/80:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
 #   global id_name_verified
    id_name_verified.drop(columns = ['Verified'], inplace=True) 
    print(id_name_verified)
    for i, Id, Login in id_name_verified.itertuples(index=True):
        print(i)
        id_name_verified.at[i,'Password'] = id_password[row.Index,1]
    id_name_verified['Password'] = id_name_verified['Password'].astype('int')
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/81:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
 #   global id_name_verified
    id_name_verified.drop(columns = ['Verified'], inplace=True) 
    print(id_name_verified)
    for i, Id, Login in id_name_verified.itertuples(index=True):
        print(i)
        id_name_verified.at[i,'Password'] = id_password[i,1]
    id_name_verified['Password'] = id_name_verified['Password'].astype('int')
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/82:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
 #   global id_name_verified
    id_name_verified.drop(columns = ['Verified'], inplace=True) 
    print(id_name_verified)
    for i, Id, Login in id_name_verified.itertuples(index=True):
        id_name_verified.at[i,'Password'] = id_password[i,1]
    id_name_verified['Password'] = id_name_verified['Password'].astype('int')
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/83:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
 #   global id_name_verified
    id_name_verified.drop(columns = ['Verified'], inplace=True) 
    for i, Id, Login in id_name_verified.itertuples():
        id_name_verified.at[i,'Password'] = id_password[i,1]
    id_name_verified['Password'] = id_name_verified['Password'].astype('int')
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/84:
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn import metrics

def train_and_predict(train_input_features, train_outputs, prediction_features):
    """
    :param train_input_features: (numpy.array) A two-dimensional NumPy array where each element
                        is an array that contains: sepal length, sepal width, petal length, and petal width   
    :param train_outputs: (numpy.array) A one-dimensional NumPy array where each element
                        is a number representing the species of iris which is described in
                        the same row of train_input_features. 0 represents Iris setosa,
                        1 represents Iris versicolor, and 2 represents Iris virginica.
    :param prediction_features: (numpy.array) A two-dimensional NumPy array where each element
                        is an array that contains: sepal length, sepal width, petal length, and petal width
    :returns: (list) The function should return an iterable (like list or numpy.ndarray) of the predicted 
                        iris species, one for each item in prediction_features
    """   
    pass

iris = datasets.load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,
                                                    test_size=0.3, random_state=0)

y_pred = train_and_predict(X_train, y_train, X_test)
if y_pred is not None:
    print(metrics.accuracy_score(y_test, y_pred))
275/85: X_train
275/86: y_test
275/87: y_test.value_counts()
275/88: un, counts = np.unique(y_test, return_counts=True)
275/89: un
275/90: dict(zip(un, counts))
275/91:
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
275/92:
svm = SVC()
parametrs = {'C': [0.1, 0.5, 1, 10],
             'kernel': ['rbf', 'linear', 'poly'],
             'degree': [3, 4]
    
            }
275/93:
grid = GridSearchCV(svm, parametrs, cv=3)
grid.fit(X_train, y_train)
275/94:
grid = GridSearchCV(svm, parametrs, cv=3)
grid.fit(X_train, y_train)
grid.best_estimator_
275/95: grid.best_estimator_
275/96: best_model = grid.best_estimator_
275/97: best_model.predict(y_test)
275/98: best_model.predict(X_test)
275/99:
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, accuracy_score
275/100: y_predict = best_model.predict(X_test)
275/101: confusion_matrix(y_test, y_pred)
275/102: confusion_matrix(y_test, y_predict)
275/103: accuracy_score(y_test, y_predict)
275/104: from sklearn.tree import DecisionTreeClassifier
275/105:
dtc = DecisionTreeClassifier()
params = {'max_depth': [2,3,4,5,6]
         }
275/106: grid = GridSearchCV(dtc, params, cv=3)
275/107: grid.fir(X_train, y_train)
275/108: grid.fit(X_train, y_train)
275/109:
grid.fit(X_train, y_train)
best_dtc = grid.best_estimator_
275/110: best_dtc
275/111: best_dtc.predict(X_test)
275/112: y_predict_tree = best_dtc.predict(X_test)
275/113: print(confusion_matrix(y_test, y_predict_tree))
275/114: ##### DOG
275/115: df_dog = pd.read_csv('dog.csv')
275/116: df_dog = pd.read_csv('dog.csv')
275/117: df_dog
275/118: df_dog.sort_values('Classifier prediction')
275/119: ### CORRELATION
275/120:
pd.DataFrame.from_dict({
    'GOOG' : [
        742.66, 738.40, 738.22, 741.16,
        739.98, 747.28, 746.22, 741.80,
        745.33, 741.29, 742.83, 750.50
    ],
    'FB' : [
        108.40, 107.92, 109.64, 112.22,
        109.57, 113.82, 114.03, 112.24,
        114.68, 112.92, 113.28, 115.40
    ],
    'MSFT' : [
        55.40, 54.63, 54.98, 55.88,
        54.12, 59.16, 58.14, 55.97,
        61.20, 57.14, 56.62, 59.25
    ],
    'AAPL' : [
        106.00, 104.66, 104.87, 105.69,
        104.22, 110.16, 109.84, 108.86,
        110.14, 107.66, 108.08, 109.90
    ]
})
275/121:
df = pd.DataFrame.from_dict({
    'GOOG' : [
        742.66, 738.40, 738.22, 741.16,
        739.98, 747.28, 746.22, 741.80,
        745.33, 741.29, 742.83, 750.50
    ],
    'FB' : [
        108.40, 107.92, 109.64, 112.22,
        109.57, 113.82, 114.03, 112.24,
        114.68, 112.92, 113.28, 115.40
    ],
    'MSFT' : [
        55.40, 54.63, 54.98, 55.88,
        54.12, 59.16, 58.14, 55.97,
        61.20, 57.14, 56.62, 59.25
    ],
    'AAPL' : [
        106.00, 104.66, 104.87, 105.69,
        104.22, 110.16, 109.84, 108.86,
        110.14, 107.66, 108.08, 109.90
    ]
})
275/122: df
275/123: df.corr()
275/124: df.pct_change()
275/125: df.pct_change().corr()
275/126: df.pct_change().corr().idxmax()
275/127: df.pct_change().corr()
275/128: df_corr = df.pct_change().corr()
275/129: df_corr[df_corr < 1]
275/130: df_corr[df_corr < 1].idxmax()
275/131: df_corr[df_corr < 1]
275/132: df_corr[df_corr < 1].max()
275/133: df_corr[df_corr < 1].max().max()
275/134: df_corr[df_corr < 1].max().idxmax()
275/135: df_corr
275/136: df_corr['FB']
275/137: df_corr['FB'] < 1
275/138: df_corr[df_corr['FB'] < 1]
275/139: df_corr[df_corr['FB'] < 1]['FB']
275/140: df_corr[df_corr['FB'] < 1]['FB'].idxmax)
275/141: df_corr[df_corr['FB'] < 1]['FB'].idxmax()
275/142: first_f = df_corr[df_corr < 1].max().idxmax()
275/143: df_corr[df_corr[first_f] < 1][first_f].idxmax()
275/144: second_f = df_corr[df_corr[first_f] < 1][first_f].idxmax()
275/145: print(first_f, second_f)
275/146: ### MARKETINF
275/147:
import numpy as np
from sklearn import linear_model

def desired_marketing_expenditure(marketing_expenditure, units_sold, desired_units_sold):
    """
    :param marketing_expenditure: (list) A list of integers with the expenditure for each previous campaign.
    :param units_sold: (list) A list of integers with the number of units sold for each previous campaign.
    :param desired_units_sold: (integer) Target number of units to sell in the new campaign.
    :returns: (float) Required amount of money to be invested.
    """
    return None

#For example, with the parameters below, the function should return 250000.0
print(desired_marketing_expenditure(
    [300000, 200000, 400000, 300000, 100000],
    [60000, 50000, 90000, 80000, 30000],
    60000))
275/148: a = [300000, 200000, 400000, 300000, 100000]
275/149:
a = [300000, 200000, 400000, 300000, 100000]
b = [60000, 50000, 90000, 80000, 30000]
275/150: a.reshape(-1, 1)
275/151: np.reshape(a, (-1, 1))
275/152:
lin_mod = linear_model.LinearRegression()
lin_mof.fit(np.reshape(a, (-1, 1)), b)
275/153:
lin_mod = linear_model.LinearRegression()
lin_mod.fit(np.reshape(a, (-1, 1)), b)
275/154: lin_mod.predict(60000)
275/155: lin_mod.predict([60000])
275/156: lin_mod.predict([[60000]])
275/157:
lin_mod = linear_model.LinearRegression()
lin_mod.fit(np.reshape(b, (-1, 1)), a)
275/158: lin_mod.predict([[60000]])
275/159: lin_mod.coef_
275/160: (60000 - lin_mod.intercept_)/lin_mod.coef_
275/161:
lin_mod = linear_model.LinearRegression()
lin_mod.fit(np.reshape(b, (-1, 1)), a)
275/162: (60000 - lin_mod.intercept_)/lin_mod.coef_
275/163:
lin_mod = linear_model.LinearRegression()
lin_mod.fit(np.reshape(a, (-1, 1)), b)
275/164: (60000 - lin_mod.intercept_)/lin_mod.coef_
275/165:
lin_mod = linear_model.LinearRegression()
lin_mod.fit(np.reshape(b, (-1, 1)), a)
275/166: lin_mod.predict([[60000]])
275/167: ### ELECTIOM
275/168: ### ELECTION
275/169: df = pd.read_excel('electionpoll.csv')
275/170: df = pd.read_csv('electionpoll.csv')
275/171: df
275/172: df.mean()
275/173: df.median()
275/174: df.std()
275/175: df
275/176: import dt as dt
275/177: import dt as datetime
275/178: import datetime as dt
275/179: df['Date \ Party'].dt.month
275/180: df['Date \ Party'].dt.month
275/181: df
275/182: df.dtypes
275/183: pd.to_datetime(df['Date \ Party'])
275/184: df['Date \ Party'] = pd.to_datetime(df['Date \ Party'])
275/185: df['Date \ Party'].astype()
275/186: df.dtypes
275/187: df['Date \ Party'].dt.month
275/188: df[df['Date \ Party'].dt.month == 3]
275/189: df[df['Date \ Party'].dt.month == 3]["Workers' Party"].describe()
275/190: df.T
275/191: df.T.idxmax()
275/192: df.T.max()
275/193: df.T
275/194: df
275/195: df[1, :]
275/196: df[1:, :]
275/197: df.iloc[1:, :]
275/198: df.iloc[2:, :]
275/199: df.iloc[:, 1:]
275/200: df.iloc[:, 1:].T
275/201: df.iloc[:, 1:].T.max()
275/202: df.iloc[:, 1:].T.idxmax()
275/203: df.iloc[:, 1:]
275/204: df.iloc[:, 1:].max
275/205: df.iloc[:, 1:].max()
275/206: df.iloc[:, 1:].max().idxmax()
275/207: df.iloc[:, 1:].max()
275/208: df.iloc[:, 1:].T.max()
275/209: df.iloc[:, 1:].max().idx
275/210: df.iloc[:, 1:].max()
275/211: df.iloc[:, 1:].max().max()
275/212: df
275/213: df['Date \ Party'].dt.year
275/214: df[df['Date \ Party'].dt.year == 2019]
275/215: df[df['Date \ Party'].dt.year == 2019].max()
275/216: df[df['Date \ Party'].dt.year == 2019]
275/217: df[(df['Date \ Party'].dt.year == 2019) & (df == 37)]
275/218: df == 3
275/219: df[(df['Date \ Party'].dt.year == 2019) |(df == 37)]
275/220: df[(df['Date \ Party'].dt.year == 2019) ]
275/221: df[df == 37]
275/222: df.any(37)
275/223: df
275/224: df.max()
275/225: df[(df['Date \ Party'].dt.year == 2019) & (df['Civic Party' == 37]) ]
275/226: df[(df['Date \ Party'].dt.year == 2019) & (df['Civic Party'] == 37) ]
275/227: df.describe
275/228: df.describe()
275/229: df_describe = df.describe()
275/230: df_describe
275/231: df_describe.t
275/232: df_describe.T
275/233: df_describe = df.describe().T
275/234: df_describe
275/235: df_describe['diff'] = df_describe.max - df_describe.min
275/236: df_describe['diff'] = df_describe['max'] - df_describe['min']
275/237: df_describe
275/238: df_describe.sort_values('diff')
275/239:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    id_name_verified.drop(columns=['Verified'], inplace = True)
    print(id_name_verified)
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/240:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    id_name_verified.drop(columns=['Verified'], inplace = True)
    for i, Id, Login in id_name_verified.itertuples():
        print(i)
    print(id_name_verified)
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/241:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    id_name_verified.drop(columns=['Verified'], inplace = True)
    for i, Id, Login in id_name_verified.itertuples():
        print(i)
        print(id_password)
    print(id_name_verified)
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/242:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    id_name_verified.drop(columns=['Verified'], inplace = True)
    for i, Id, Login in id_name_verified.itertuples():
        print(i)
        print(id_password[i])
    print(id_name_verified)
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/243:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    id_name_verified.drop(columns=['Verified'], inplace = True)
    for i, Id, Login in id_name_verified.itertuples():
        print(i)
        print(id_password[i, 1])
    print(id_name_verified)
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/244:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    id_name_verified.drop(columns=['Verified'], inplace = True)
    for i, Id, Login in id_name_verified.itertuples():
        print(i)
        print(id_password[Id, 1])
    print(id_name_verified)
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/245:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    id_name_verified.drop(columns=['Verified'], inplace = True)
    for i, Id, Login in id_name_verified.itertuples():
        print(i)
        print(id_password[i, 1])
    print(id_name_verified)
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/246:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    id_name_verified.drop(columns=['Verified'], inplace = True)
    for i, Id, Login in id_name_verified.itertuples():
        print(Id)
        #id_password
        print(id_password[i, 1])
    print(id_name_verified)
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/247: id_password
275/248:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    id_name_verified.drop(columns=['Verified'], inplace = True)
    for i, Id, Login in id_name_verified.itertuples():
        print(Id)
        #id_password
        print(id_password[Id, 1])
    print(id_name_verified)
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/249: id_password
275/250: id_password[:,]
275/251: id_password[:,1]
275/252: id_password[:,0]
275/253: id_password
275/254:
for i, a in id_password.itertuples():
    print()
275/255:
for i, a in id_password:
    print()
275/256:
for i, a in id_password:
    print(i)
275/257:
for i, a in id_password:
    print(a)
275/258:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    id_name_verified.drop(columns=['Verified'], inplace = True)
    for i, Id, Login in id_name_verified.itertuples():
        for i, a in id_password:
            if (Id == i)
            print(Id)
            id_name_verified.at[i, 'Password'] == a
        #id_password
    print(id_name_verified)
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/259:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    id_name_verified.drop(columns=['Verified'], inplace = True)
    for i, Id, Login in id_name_verified.itertuples():
        for i, a in id_password:
            if (Id == i):
                id_name_verified.at[i, 'Password'] == a
        #id_password
    print(id_name_verified)
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/260:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    id_name_verified.drop(columns=['Verified'], inplace = True)
    id_name_verified['Password'] = []
    for i, Id, Login in id_name_verified.itertuples():
        for i, a in id_password:
            if (Id == i):
                id_name_verified.at[i, 'Password'] == a
        #id_password
    print(id_name_verified)
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/261:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    id_name_verified.drop(columns=['Verified'], inplace = True)
    id_name_verified['Password'] = []
    for i, Id, Login, pas in id_name_verified.itertuples():
        for i, a in id_password:
            if (Id == i):
                id_name_verified.at[i, 'Password'] == a
        #id_password
    print(id_name_verified)
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/262:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    id_name_verified.drop(columns=['Verified'], inplace = True)
    id_name_verified['Password'] = []
    print(id_name_verified)
    for i, Id, Login, pas in id_name_verified.itertuples():
        for i, a in id_password:
            if (Id == i):
                id_name_verified.at[i, 'Password'] == a
        #id_password
    print(id_name_verified)
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/263: id_name_verified
275/264:
id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/265:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    id_name_verified.drop(columns=['Verified'], inplace = True)
    id_name_verified['Password'] = []
    print(id_name_verified)
    for i, Id, Login, pas in id_name_verified.itertuples():
        for i, a in id_password:
            if (Id == i):
                id_name_verified.at[i, 'Password'] == a
        #id_password
    print(id_name_verified)
    pass
275/266:
id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/267:
id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
print(id_name_verified)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/268:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    id_name_verified.drop(columns=['Verified'], inplace = True)
    id_name_verified['Password'] = np.Series(int32)
    print(id_name_verified)
    for i, Id, Login, pas in id_name_verified.itertuples():
        for i, a in id_password:
            if (Id == i):
                id_name_verified.at[i, 'Password'] == a
        #id_password
    print(id_name_verified)
    pass
275/269:
id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
print(id_name_verified)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/270:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    id_name_verified.drop(columns=['Verified'], inplace = True)
    id_name_verified['Password'] = pd.Series(dtype='int')
    print(id_name_verified)
    for i, Id, Login, pas in id_name_verified.itertuples():
        for i, a in id_password:
            if (Id == i):
                id_name_verified.at[i, 'Password'] == a
        #id_password
    print(id_name_verified)
    pass
275/271:
id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
print(id_name_verified)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/272: id_name_verified
275/273: id_name_verified.loc[0, 'Password']
275/274: id_name_verified.loc[0, 'Password']
275/275: id_name_verified.loc[0, 'Password'] == 2
275/276:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    id_name_verified.drop(columns=['Verified'], inplace = True)
    id_name_verified['Password'] = pd.Series(dtype='int')
    print(id_name_verified)
    for i, Id, Login, pas in id_name_verified.itertuples():
        for i, a in id_password:
            if (Id == i):
                id_name_verified.loc[i, 'Password'] = a
        #id_password
    print(id_name_verified)
    pass
275/277:
id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
print(id_name_verified)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/278:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    id_name_verified.drop(columns=['Verified'], inplace = True)
   # id_name_verified['Password'] = pd.Series(dtype='int')
    print(id_name_verified)
    for i, Id, Login, pas in id_name_verified.itertuples():
        for i, a in id_password:
            if (Id == i):
                id_name_verified.loc[i, 'Password'] = a
        #id_password
    print(id_name_verified)
    pass
275/279:
id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
print(id_name_verified)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/280:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    id_name_verified.drop(columns=['Verified'], inplace = True)
    id_name_verified['Password'] = pd.Series(dtype='int')
    print(id_name_verified)
    for i, Id, Login, pas in id_name_verified.itertuples():
        for i, a in id_password:
            if (Id == i):
                id_name_verified.loc[i, 'Password'] = a
        #id_password
    print(id_name_verified)
    pass
275/281:
id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
print(id_name_verified)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/282:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    id_name_verified.drop(columns=['Verified'], inplace = True)
    id_name_verified['Password'] = pd.Series(dtype='int')
    print(id_name_verified)
    for i, Id, Login, pas in id_name_verified.itertuples():
        for i, a in id_password:
            if (Id == i):
                id_name_verified.loc[i, 'Password'] = a
        #id_password
    pass
275/283:
id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
print(id_name_verified)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/284:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    id_name_verified.drop(columns=['Verified'], inplace = True)
    id_name_verified['Password'] = pd.Series(dtype='int')
    for i, Id, Login, pas in id_name_verified.itertuples():
        for i, a in id_password:
            if (Id == i):
                id_name_verified.loc[i, 'Password'] = a
        #id_password
    pass
275/285:
id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
print(id_name_verified)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/286:
id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/287:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
    id_name_verified.drop(columns=['Verified'], inplace = True)
    id_name_verified['Password'] = pd.Series(dtype='int')
    for i, Id, Login, pas in id_name_verified.itertuples():
        for ind, val in id_password:
            if (Id == ind):
                id_name_verified.loc[i, 'Password'] = val
        #id_password
    pass
275/288:
id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/289:
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn import metrics

def train_and_predict(train_input_features, train_outputs, prediction_features):
    """
    :param train_input_features: (numpy.array) A two-dimensional NumPy array where each element
                        is an array that contains: sepal length, sepal width, petal length, and petal width   
    :param train_outputs: (numpy.array) A one-dimensional NumPy array where each element
                        is a number representing the species of iris which is described in
                        the same row of train_input_features. 0 represents Iris setosa,
                        1 represents Iris versicolor, and 2 represents Iris virginica.
    :param prediction_features: (numpy.array) A two-dimensional NumPy array where each element
                        is an array that contains: sepal length, sepal width, petal length, and petal width
    :returns: (list) The function should return an iterable (like list or numpy.ndarray) of the predicted 
                        iris species, one for each item in prediction_features
    """   
    pass

iris = datasets.load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,
                                                    test_size=0.3, random_state=0)

y_pred = train_and_predict(X_train, y_train, X_test)
if y_pred is not None:
    print(metrics.accuracy_score(y_test, y_pred))
275/290: y_test
275/291: i, counts = np.unique(y_test, return_counts=True)
275/292: dict(zip(i, counts))
275/293: dict(zip(i, counts)).sum()
275/294: dict(zip(i, counts))
275/295:
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.svm import SVC

def train_and_predict(train_input_features, train_outputs, prediction_features):
    """
    :param train_input_features: (numpy.array) A two-dimensional NumPy array where each element
                        is an array that contains: sepal length, sepal width, petal length, and petal width   
    :param train_outputs: (numpy.array) A one-dimensional NumPy array where each element
                        is a number representing the species of iris which is described in
                        the same row of train_input_features. 0 represents Iris setosa,
                        1 represents Iris versicolor, and 2 represents Iris virginica.
    :param prediction_features: (numpy.array) A two-dimensional NumPy array where each element
                        is an array that contains: sepal length, sepal width, petal length, and petal width
    :returns: (list) The function should return an iterable (like list or numpy.ndarray) of the predicted 
                        iris species, one for each item in prediction_features
    """   
    pass

iris = datasets.load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,
                                                    test_size=0.3, random_state=0)

y_pred = train_and_predict(X_train, y_train, X_test)
if y_pred is not None:
    print(metrics.accuracy_score(y_test, y_pred))
275/296:
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

def train_and_predict(train_input_features, train_outputs, prediction_features):
    """
    :param train_input_features: (numpy.array) A two-dimensional NumPy array where each element
                        is an array that contains: sepal length, sepal width, petal length, and petal width   
    :param train_outputs: (numpy.array) A one-dimensional NumPy array where each element
                        is a number representing the species of iris which is described in
                        the same row of train_input_features. 0 represents Iris setosa,
                        1 represents Iris versicolor, and 2 represents Iris virginica.
    :param prediction_features: (numpy.array) A two-dimensional NumPy array where each element
                        is an array that contains: sepal length, sepal width, petal length, and petal width
    :returns: (list) The function should return an iterable (like list or numpy.ndarray) of the predicted 
                        iris species, one for each item in prediction_features
    """   
    svm = SVC()
    params = {'C': [0.05, 0.1, 1, 2, 5],
              'kernel': ['rbf', 'linear', 'poly'],
              'degree': [3, 4, 5]        
             }
    grid = GridSearchCV(svm, params, cv = 3)
    grid.fit(X_train, y_train)
    best_svm_model = grid.best_estimator_
    y_pred = best_svm_model.predic(X_test)
    return y_pred

iris = datasets.load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,
                                                    test_size=0.3, random_state=0)

y_pred = train_and_predict(X_train, y_train, X_test)
if y_pred is not None:
    print(metrics.accuracy_score(y_test, y_pred))
275/297:
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

def train_and_predict(train_input_features, train_outputs, prediction_features):
    """
    :param train_input_features: (numpy.array) A two-dimensional NumPy array where each element
                        is an array that contains: sepal length, sepal width, petal length, and petal width   
    :param train_outputs: (numpy.array) A one-dimensional NumPy array where each element
                        is a number representing the species of iris which is described in
                        the same row of train_input_features. 0 represents Iris setosa,
                        1 represents Iris versicolor, and 2 represents Iris virginica.
    :param prediction_features: (numpy.array) A two-dimensional NumPy array where each element
                        is an array that contains: sepal length, sepal width, petal length, and petal width
    :returns: (list) The function should return an iterable (like list or numpy.ndarray) of the predicted 
                        iris species, one for each item in prediction_features
    """   
    svm = SVC()
    params = {'C': [0.05, 0.1, 1, 2, 5],
              'kernel': ['rbf', 'linear', 'poly'],
              'degree': [3, 4, 5]        
             }
    grid = GridSearchCV(svm, params, cv = 3)
    grid.fit(X_train, y_train)
    best_svm_model = grid.best_estimator_
    y_pred = best_svm_model.predict(X_test)
    return y_pred

iris = datasets.load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,
                                                    test_size=0.3, random_state=0)

y_pred = train_and_predict(X_train, y_train, X_test)
if y_pred is not None:
    print(metrics.accuracy_score(y_test, y_pred))
275/298:
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

def train_and_predict(train_input_features, train_outputs, prediction_features):
    """
    :param train_input_features: (numpy.array) A two-dimensional NumPy array where each element
                        is an array that contains: sepal length, sepal width, petal length, and petal width   
    :param train_outputs: (numpy.array) A one-dimensional NumPy array where each element
                        is a number representing the species of iris which is described in
                        the same row of train_input_features. 0 represents Iris setosa,
                        1 represents Iris versicolor, and 2 represents Iris virginica.
    :param prediction_features: (numpy.array) A two-dimensional NumPy array where each element
                        is an array that contains: sepal length, sepal width, petal length, and petal width
    :returns: (list) The function should return an iterable (like list or numpy.ndarray) of the predicted 
                        iris species, one for each item in prediction_features
    """   
    svm = SVC()
    params = {'C': [0.05, 0.1, 1, 2, 5],
              'kernel': ['rbf', 'linear', 'poly'],
              'degree': [3, 4, 5],
              'gamma': ['scale']
             }
    grid = GridSearchCV(svm, params, cv = 3)
    grid.fit(X_train, y_train)
    best_svm_model = grid.best_estimator_
    y_pred = best_svm_model.predict(X_test)
    return y_pred

iris = datasets.load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,
                                                    test_size=0.3, random_state=0)

y_pred = train_and_predict(X_train, y_train, X_test)
if y_pred is not None:
    print(metrics.accuracy_score(y_test, y_pred))
275/299:
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

def train_and_predict(train_input_features, train_outputs, prediction_features):
    """
    :param train_input_features: (numpy.array) A two-dimensional NumPy array where each element
                        is an array that contains: sepal length, sepal width, petal length, and petal width   
    :param train_outputs: (numpy.array) A one-dimensional NumPy array where each element
                        is a number representing the species of iris which is described in
                        the same row of train_input_features. 0 represents Iris setosa,
                        1 represents Iris versicolor, and 2 represents Iris virginica.
    :param prediction_features: (numpy.array) A two-dimensional NumPy array where each element
                        is an array that contains: sepal length, sepal width, petal length, and petal width
    :returns: (list) The function should return an iterable (like list or numpy.ndarray) of the predicted 
                        iris species, one for each item in prediction_features
    """   
    svm = RandomForestClassifier()
    params = {'C': [0.05, 0.1, 1, 2, 5],
              'kernel': ['rbf', 'linear', 'poly'],
              'degree': [3, 4, 5],
              'gamma': ['scale']
             }
    grid = GridSearchCV(svm, params, cv = 3)
    grid.fit(X_train, y_train)
    best_svm_model = grid.best_estimator_
    y_pred = best_svm_model.predict(X_test)
    return y_pred

iris = datasets.load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,
                                                    test_size=0.3, random_state=0)

y_pred = train_and_predict(X_train, y_train, X_test)
if y_pred is not None:
    print(metrics.accuracy_score(y_test, y_pred))
275/300:
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

def train_and_predict(train_input_features, train_outputs, prediction_features):
    """
    :param train_input_features: (numpy.array) A two-dimensional NumPy array where each element
                        is an array that contains: sepal length, sepal width, petal length, and petal width   
    :param train_outputs: (numpy.array) A one-dimensional NumPy array where each element
                        is a number representing the species of iris which is described in
                        the same row of train_input_features. 0 represents Iris setosa,
                        1 represents Iris versicolor, and 2 represents Iris virginica.
    :param prediction_features: (numpy.array) A two-dimensional NumPy array where each element
                        is an array that contains: sepal length, sepal width, petal length, and petal width
    :returns: (list) The function should return an iterable (like list or numpy.ndarray) of the predicted 
                        iris species, one for each item in prediction_features
    """   
    svm = RandomForestClassifier()
    params = {'n_estimators': [20, 40, 80, 100],
              'max_depth': [2, 3]
             }
    grid = GridSearchCV(svm, params, cv = 3)
    grid.fit(X_train, y_train)
    best_svm_model = grid.best_estimator_
    y_pred = best_svm_model.predict(X_test)
    return y_pred

iris = datasets.load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,
                                                    test_size=0.3, random_state=0)

y_pred = train_and_predict(X_train, y_train, X_test)
if y_pred is not None:
    print(metrics.accuracy_score(y_test, y_pred))
275/301:
def group_by_owners(files):
    return None

if __name__ == "__main__":    
    files = {
        'Input.txt': 'Randy',
        'Code.py': 'Stan',
        'Output.txt': 'Randy'
    }   
    print(group_by_owners(files))
275/302:
files = {
        'Input.txt': 'Randy',
        'Code.py': 'Stan',
        'Output.txt': 'Randy'
    }
275/303:
for i, v in files:
    print(i)
275/304:
for i, v in files.items():
    print(i)
275/305:
my_dic = {}
files = {
        'Input.txt': 'Randy',
        'Code.py': 'Stan',
        'Output.txt': 'Randy'
    }
275/306:
for i, v in files.items():
    print(i)
    my_dic[v] = my_dic[i]
275/307:
for i, v in files.items():
    print(v)
    my_dic[v] = my_dic[i]
275/308:
for i, v in files.items():
    print(v)
    my_dic[v] = —à
275/309:
for i, v in files.items():
    print(v)
    my_dic[v] = i
275/310: my_dic
275/311:
for i, v in files.items():
    print(v)
    
    my_dic[v] = my_dic.get(v) + [i]
275/312:
for i, v in files.items():
    print(v)
    
    my_dic[v] = my_dic.get(v, []) + [i]
275/313: files.get('Code.py', [])
275/314:
for i, v in files.items():
    print(v)
    
    my_dic[v] = my_dic.get(v,[]) + [i]
275/315:
my_dic= {}
for i, v in files.items():
    print(v)
    
    my_dic[v] = my_dic.get(v,[]) + [i]
275/316:
my_dic = {}
for i, v in files.items():
    my_dic[v] = my_dic.get(v,[]) + [i]
275/317: my_dic
275/318:
def unique_names(names1, names2):
    set(names1 + names2)
    return None

if __name__ == "__main__":
    names1 = ["Ava", "Emma", "Olivia"]
    names2 = ["Olivia", "Sophia", "Emma"]
    print(unique_names(names1, names2)) # should print Ava, Emma, Olivia, Sophia
275/319:
def unique_names(names1, names2):
    
    return set(names1 + names2)

if __name__ == "__main__":
    names1 = ["Ava", "Emma", "Olivia"]
    names2 = ["Olivia", "Sophia", "Emma"]
    print(unique_names(names1, names2)) # should print Ava, Emma, Olivia, Sophia
275/320:
def unique_names(names1, names2):
    
    return list(set(names1 + names2))

if __name__ == "__main__":
    names1 = ["Ava", "Emma", "Olivia"]
    names2 = ["Olivia", "Sophia", "Emma"]
    print(unique_names(names1, names2)) # should print Ava, Emma, Olivia, Sophia
275/321:
import numpy as np
from sklearn.manifold import TSNE
X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])
X_embedded = TSNE(n_components=2, learning_rate='auto',
                  init='random').fit_transform(X)
X_embedded.shape
275/322:
import numpy as np
from sklearn.manifold import TSNE
X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])
X_embedded = TSNE(n_components=2, learning_rate='auto',\
                  init='random').fit_transform(X)
X_embedded.shape
275/323:
import numpy as np
from sklearn.manifold import TSNE
X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])
X_embedded = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(X)
X_embedded.shape
275/324:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
 #   global id_name_verified
    id_name_verified.drop(columns = ['Verified'], inplace=True) 
    for i, Id, Login in id_name_verified.itertuples():
        id_name_verified.at[i,'Password'] = id_password[i,1]
    id_name_verified['Password'] = id_name_verified['Password'].astype('int')
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/325:
import pandas as pd
import numpy as np

def login_table(id_name_verified, id_password):
    """
    :param id_name_verified: (DataFrame) DataFrame with columns: Id, Login, Verified.   
    :param id_password: (numpy.array) Two-dimensional NumPy array where each element
                        is an array that contains: Id and Password
    :returns: (None) The function should modify id_name_verified DataFrame in-place. 
              It should not return anything.
    """   
 #   global id_name_verified
    id_name_verified.drop(columns = ['Verified'], inplace=True) 
    for i, Id, Login in id_name_verified.itertuples():
        id_name_verified.loc[i,'Password'] = id_password[i,1]
    id_name_verified['Password'] = id_name_verified['Password'].astype('int')
    pass

id_name_verified = pd.DataFrame([[1, "JohnDoe", True], [2, "AnnFranklin", False]], columns=["Id", "Login", "Verified"])
id_password = np.array([[1, 987340123], [2, 187031122]], np.int32)
login_table(id_name_verified, id_password)
print(id_name_verified)
275/326: df.head(5)
275/327: df.head(5).pct_change()
275/328: df.pct_change()
275/329:
df = pd.DataFrame.from_dict({
    'GOOG' : [
        742.66, 738.40, 738.22, 741.16,
        739.98, 747.28, 746.22, 741.80,
        745.33, 741.29, 742.83, 750.50
    ],
    'FB' : [
        108.40, 107.92, 109.64, 112.22,
        109.57, 113.82, 114.03, 112.24,
        114.68, 112.92, 113.28, 115.40
    ],
    'MSFT' : [
        55.40, 54.63, 54.98, 55.88,
        54.12, 59.16, 58.14, 55.97,
        61.20, 57.14, 56.62, 59.25
    ],
    'AAPL' : [
        106.00, 104.66, 104.87, 105.69,
        104.22, 110.16, 109.84, 108.86,
        110.14, 107.66, 108.08, 109.90
    ]
})
275/330: df.pct_change()
275/331: 1 - df / df.shift(1)
275/332: -(1 - df / df.shift(1))
275/333: df / df.shift(1)
275/334: df / df.shift(1) - 1
275/335: df.pct_change().max()
275/336: df.pct_change().max().idxmax()
275/337: df.pct_change()
275/338: df
275/339: .read_sql_query
276/1:
dic_encoder = {'1': 'abc',
               '2': 'def',
               '3': 'ghi',
               '4': 'jkn'           
              }
276/2: list(map(lambda x: x , 'aa'))
276/3: list(map(lambda x: x , 'aabb'))
276/4:
number = 24
for i, val in dic_encoder:
    print()
276/5:
number = 24
for i, val in dic_encoder:
    print(dic_encoder[i])
276/6:
number = 24
for i, val in dic_encoder.items():
    print(dic_encoder[i])
276/7:
number = 24
for i, val in dic_encoder.items():
    print(dic_encoder[i][1])
276/8:
number = 24
for i, val in dic_encoder.items():
    print(dic_encoder[i])
276/9:
number = 24
for i, val in dic_encoder.items():
    for chars in val:
        print(char)
276/10:
number = 24
for i, val in dic_encoder.items():
    for chars in val:
        print(chars)
276/11:
number = 24
for num_enum in str(number):
    for i, val in dic_encoder.items():
        for chars in dic_encoder[num_enum]:
        print(chars)
276/12:
number = 24
for num_enum in str(number):
    for i, val in dic_encoder.items():
        for chars in dic_encoder[num_enum]:
            print(chars)
276/13:
number = 24
for num_enum in str(number):
    for chars in dic_encoder[num_enum]:
        print(chars)
276/14: len(str(number))
276/15: str(number)
276/16: str(number)[2]
276/17:
number = 24
for num_enum in range(len(str(number))):
    for chars in dic_encoder[num_enum]:
        print(chars)
276/18:
number = 24
for num_enum in range(len(str(number))):
    print(num_enum)
    for chars in dic_encoder[num_enum]:
        print(chars)
276/19:
number = 24
for num_enum in range(len(str(number))):
    print(number[num_enum])
    for chars in dic_encoder[num_enum]:
        print(chars)
276/20:
number = 24
for num_enum in range(len(str(number))):
    print(str(number)[num_enum])
    for chars in dic_encoder[num_enum]:
        print(chars)
276/21:
number = 24
for num_enum in range(len(str(number))):
    if str(number)[num_enum + 1]:
        print(str(number)[num_enum])
        for chars in dic_encoder[num_enum]:
            print(chars)
    else:
        print('Error')
276/22:
number = 24
for num_enum in range(len(str(number))):
    own_number = str(number)[num_enum]
    if str(number)[num_enum + 1]:
        print(str(number)[num_enum])
        for chars in dic_encoder[own_number]:
            print(chars)
    else:
        print('Error')
276/23:
number = 24
for num_enum in range(len(str(number))):
    try:
        own_number = str(number)[num_enum]
        if str(number)[num_enum + 1]:
            print(str(number)[num_enum])
            for chars in dic_encoder[own_number]:
                print(chars)
    except IndexError:
        print('Out')
276/24:
number = 24
for num_enum in range(len(str(number))):
    try:
        own_number = str(number)[num_enum]
        str(number)[num_enum + 1]:
    except IndexError:
        print('Out')
276/25:
number = 24
for num_enum in range(len(str(number))):
    try:
        own_number = str(number)[num_enum]
        str(number)[num_enum + 1]
    except IndexError:
        print('Out')
276/26:
number = 24
for num_enum in range(len(str(number))):
    try:
        own_number = str(number)[num_enum]
        next_number = str(number)[num_enum + 1]
        for chars in dic_encoder[own_number]:
            print(chars)
    except IndexError:
        print('Out')
276/27:
number = 24
for num_enum in range(len(str(number))):
    try:
        own_number = str(number)[num_enum]
        next_number = str(number)[num_enum + 1]
        for chars in dic_encoder[own_number]:
            print(chars)
            print(list(map(lambda x: x , dic_encoder[next_number])))            
    except IndexError:
        print('Out')
276/28:
number = 24
for num_enum in range(len(str(number))):
    try:
        own_number = str(number)[num_enum]
        next_number = str(number)[num_enum + 1]
        for chars in dic_encoder[own_number]:
            print(chars)
            print(list(map(lambda x: list(x) , dic_encoder[next_number])))            
    except IndexError:
        print('Out')
276/29:
number = 24
for num_enum in range(len(str(number))):
    try:
        own_number = str(number)[num_enum]
        next_number = str(number)[num_enum + 1]
        for chars in dic_encoder[own_number]:
            print(chars)
            print(list(map(lambda x: list(x, x) , dic_encoder[next_number])))            
    except IndexError:
        print('Out')
276/30:
number = 24
for num_enum in range(len(str(number))):
    try:
        own_number = str(number)[num_enum]
        next_number = str(number)[num_enum + 1]
        for chars in dic_encoder[own_number]:
            print(chars)
            print(list(map(lambda x: [x,x] , dic_encoder[next_number])))            
    except IndexError:
        print('Out')
276/31:
number = 24
for num_enum in range(len(str(number))):
    try:
        own_number = str(number)[num_enum]
        next_number = str(number)[num_enum + 1]
        for chars in dic_encoder[own_number]:
            print(chars)
            print(list(map(lambda x: [chars, x] , dic_encoder[next_number])))            
    except IndexError:
        print('Out')
276/32:
number = 24
for num_enum in range(len(str(number))):
    try:
        own_number = str(number)[num_enum]
        next_number = str(number)[num_enum + 1]
        for chars in dic_encoder[own_number]:
            print(list(map(lambda x: [chars, x] , dic_encoder[next_number])))            
    except IndexError:
        print('Out')
276/33:
number = 24
for num_enum in range(len(str(number))):
    try:
        own_number = str(number)[num_enum]
        next_number = str(number)[num_enum + 1]
        for chars in dic_encoder[own_number]:
            print((map(lambda x: [chars, x] , dic_encoder[next_number])))            
    except IndexError:
        print('Out')
276/34:
number = 24
for num_enum in range(len(str(number))):
    final_list = []
    try:
        own_number = str(number)[num_enum]
        next_number = str(number)[num_enum + 1]
        for chars in dic_encoder[own_number]:
            print(list(map(lambda x: [chars, x] , dic_encoder[next_number])))            
    except IndexError:
        print('Out')
276/35:
number = 24
for num_enum in range(len(str(number))):
    final_list = []
    try:
        own_number = str(number)[num_enum]
        next_number = str(number)[num_enum + 1]
        for chars in dic_encoder[own_number]:
            final_list = final_list + list(map(lambda x: [chars, x] , dic_encoder[next_number]))
            print(list(map(lambda x: [chars, x] , dic_encoder[next_number])))            
    except IndexError:
        print('Out')
276/36: final_list
276/37:
number = 24
final_list = []
for num_enum in range(len(str(number))):
    try:
        own_number = str(number)[num_enum]
        next_number = str(number)[num_enum + 1]
        for chars in dic_encoder[own_number]:
            final_list = final_list + list(map(lambda x: [chars, x] , dic_encoder[next_number]))
            print(list(map(lambda x: [chars, x] , dic_encoder[next_number])))            
    except IndexError:
        print('Out')
276/38: final_list
276/39: final_list[0]
276/40: final_list
276/41:
number = 24
final_list = []
for num_enum in range(len(str(number))):
    try:
        own_number = str(number)[num_enum]
        next_number = str(number)[num_enum + 1]
        for chars in dic_encoder[own_number]:
            final_list = final_list + list(map(lambda x: [chars, x] , dic_encoder[next_number]))        
    except IndexError:
        print('Out')
276/42:
number = 12
final_list = []
for num_enum in range(len(str(number))):
    try:
        own_number = str(number)[num_enum]
        next_number = str(number)[num_enum + 1]
        for chars in dic_encoder[own_number]:
            final_list = final_list + list(map(lambda x: [chars, x] , dic_encoder[next_number]))        
    except IndexError:
        print('Out')
276/43: final_list
276/44:
dic_encoder = {'1': 'abc',
               '2': 'def3',
               '3': 'ghi',
               '4': 'jkn'           
              }
276/45: str(number)[2]
276/46:
number = 12
final_list = []
for num_enum in range(len(str(number))):
    try:
        own_number = str(number)[num_enum]
        next_number = str(number)[num_enum + 1]
        for chars in dic_encoder[own_number]:
            final_list = final_list + list(map(lambda x: [chars, x] , dic_encoder[next_number]))        
    except IndexError:
        print('Out')
276/47: final_list
276/48:
dic_encoder = {'1': 'abc',
               '2': 'def',
               '3': 'ghi',
               '4': 'jkn'           
              }
276/49: str(number)[2]
276/50:
number = 12
final_list = []
for num_enum in range(len(str(number))):
    try:
        own_number = str(number)[num_enum]
        next_number = str(number)[num_enum + 1]
        for chars in dic_encoder[own_number]:
            final_list = final_list + list(map(lambda x: [chars, x] , dic_encoder[next_number]))        
    except IndexError:
        print('Out')
276/51: final_list
276/52:
number = 1
final_list = []
for num_enum in range(len(str(number))):
    try:
        own_number = str(number)[num_enum]
        next_number = str(number)[num_enum + 1]
        for chars in dic_encoder[own_number]:
            final_list = final_list + list(map(lambda x: [chars, x] , dic_encoder[next_number]))        
    except IndexError:
        print('Out')
276/53: final_list
276/54:
number = ''
final_list = []
for num_enum in range(len(str(number))):
    try:
        own_number = str(number)[num_enum]
        next_number = str(number)[num_enum + 1]
        for chars in dic_encoder[own_number]:
            final_list = final_list + list(map(lambda x: [chars, x] , dic_encoder[next_number]))        
    except IndexError:
        print('Out')
276/55: final_list
276/56:
number = ''
final_list = []
for num_enum in range(len(str(number))):
    if len(str(number)) < 2:
        print('Sorry, to short number')
        break
    try:
        own_number = str(number)[num_enum]
        next_number = str(number)[num_enum + 1]
        for chars in dic_encoder[own_number]:
            final_list = final_list + list(map(lambda x: [chars, x] , dic_encoder[next_number]))        
    except IndexError:
        print('Out')
276/57: final_list
276/58:
number = 2
final_list = []
for num_enum in range(len(str(number))):
    if len(str(number)) < 2:
        print('Sorry, to short number')
        break
    try:
        own_number = str(number)[num_enum]
        next_number = str(number)[num_enum + 1]
        for chars in dic_encoder[own_number]:
            final_list = final_list + list(map(lambda x: [chars, x] , dic_encoder[next_number]))        
    except IndexError:
        print('Out')
276/59: final_list
276/60:
number = '' 
final_list = []
for num_enum in range(len(str(number))):
    if len(str(number)) < 2:
        print('Sorry, to short number')
        break
    try:
        own_number = str(number)[num_enum]
        next_number = str(number)[num_enum + 1]
        for chars in dic_encoder[own_number]:
            final_list = final_list + list(map(lambda x: [chars, x] , dic_encoder[next_number]))        
    except IndexError:
        print('Out')
276/61: final_list
276/62:
number = '' 
final_list = []
for num_enum in range(len(str(number))):
    print(len(str(number)))
    if len(str(number)) < 2:
        print('Sorry, to short number')
        break
    try:
        own_number = str(number)[num_enum]
        next_number = str(number)[num_enum + 1]
        for chars in dic_encoder[own_number]:
            final_list = final_list + list(map(lambda x: [chars, x] , dic_encoder[next_number]))        
    except IndexError:
        print('Out')
276/63: final_list
276/64: number
276/65: number.dtype
276/66:
number = '' 
final_list = []
for num_enum in range(len(str(number))):
    if not str(number):
        print('No string')
    print(len(str(number)))
    if len(str(number)) < 2:
        print('Sorry, to short number')
        break
    try:
        own_number = str(number)[num_enum]
        next_number = str(number)[num_enum + 1]
        for chars in dic_encoder[own_number]:
            final_list = final_list + list(map(lambda x: [chars, x] , dic_encoder[next_number]))        
    except IndexError:
        print('Out')
276/67: final_list
276/68:
if not str(number):
        print('No string')
276/69:
number = '' 
final_list = []
for num_enum in range(len(str(number))):
    if not str(number):
        print('No string')
    if len(str(number)) < 2:
        print('Sorry, to short number')
        break
    try:
        own_number = str(number)[num_enum]
        next_number = str(number)[num_enum + 1]
        for chars in dic_encoder[own_number]:
            final_list = final_list + list(map(lambda x: [chars, x] , dic_encoder[next_number]))        
    except IndexError:
        print('Out')
276/70: final_list
276/71:
number = '' 
final_list = []
for num_enum in range(len(str(number))):
    if not (str(number)):
        print('No string')
    if len(str(number)) < 2:
        print('Sorry, to short number')
        break
    try:
        own_number = str(number)[num_enum]
        next_number = str(number)[num_enum + 1]
        for chars in dic_encoder[own_number]:
            final_list = final_list + list(map(lambda x: [chars, x] , dic_encoder[next_number]))        
    except IndexError:
        print('Out')
276/72: len(str(number))
276/73:
number = '' 
final_list = []
for num_enum in range(len(str(number))):
    if len(str(number)) < 2:
        print('Sorry, to short number')
        break
    try:
        own_number = str(number)[num_enum]
        next_number = str(number)[num_enum + 1]
        for chars in dic_encoder[own_number]:
            final_list = final_list + list(map(lambda x: [chars, x] , dic_encoder[next_number]))        
    except IndexError:
        print('Out')
276/74: len(str(number))
276/75:
number = '' 
final_list = []
for num_enum in range(len(str(number))):
    print(len(str(number)))
    if len(str(number)) < 2:
        print('Sorry, to short number')
        break
    try:
        own_number = str(number)[num_enum]
        next_number = str(number)[num_enum + 1]
        for chars in dic_encoder[own_number]:
            final_list = final_list + list(map(lambda x: [chars, x] , dic_encoder[next_number]))        
    except IndexError:
        print('Out')
276/76:
number = '' 
final_list = []
if len(str(number)) < 2:
    print('Sorry, to short number')
for num_enum in range(len(str(number))):
    try:
        own_number = str(number)[num_enum]
        next_number = str(number)[num_enum + 1]
        for chars in dic_encoder[own_number]:
            final_list = final_list + list(map(lambda x: [chars, x] , dic_encoder[next_number]))        
    except IndexError:
        print('Out')
276/77:
def telephone_comb(number):
    if len(str(number)) < 2:
        print('Sorry, to short number')
        break
    final_list = []
    for num_enum in range(len(str(number))):
        try:
            own_number = str(number)[num_enum]
            next_number = str(number)[num_enum + 1]
            for chars in dic_encoder[own_number]:
                final_list = final_list + list(map(lambda x: [chars, x] , dic_encoder[next_number]))        
        except IndexError:
            print('Out')
    return final_list
276/78:
def telephone_comb(number, dic_encoder):
    if len(str(number)) < 2:
        print('Sorry, to short number')
        break
    final_list = []
    for num_enum in range(len(str(number))):
        try:
            own_number = str(number)[num_enum]
            next_number = str(number)[num_enum + 1]
            for chars in dic_encoder[own_number]:
                final_list = final_list + list(map(lambda x: [chars, x] , dic_encoder[next_number]))        
        except IndexError:
            print('Out')
    return final_list
276/79:
def telephone_comb(number, dic_encoder):
    if len(str(number)) < 2:
        print('Sorry, to short number')
        break
    final_list = []
    for num_enum in range(len(str(number))):
        print(num_enum)
        try:
            own_number = str(number)[num_enum]
            next_number = str(number)[num_enum + 1]
            for chars in dic_encoder[own_number]:
                final_list = final_list + list(map(lambda x: [chars, x] , dic_encoder[next_number]))        
        except IndexError:
            print('Out')
    return final_list
276/80:
def telephone_comb(number, dic_encoder):
    final_list = []
    if len(str(number)) < 2:
        print('Sorry, to short number')
    else:
        for num_enum in range(len(str(number))):
            print(num_enum)
            try:
                own_number = str(number)[num_enum]
                next_number = str(number)[num_enum + 1]
                for chars in dic_encoder[own_number]:
                    final_list = final_list + list(map(lambda x: [chars, x] , dic_encoder[next_number]))        
            except IndexError:
                print('Out')
    return final_list
276/81:
number = 23 
final_list = []
276/82:
def telephone_comb(number, dic_encoder):
    final_list = []
    if len(str(number)) < 2:
        print('Sorry, to short number')
    else:
        for num_enum in range(len(str(number))):
            print(num_enum)
            try:
                own_number = str(number)[num_enum]
                next_number = str(number)[num_enum + 1]
                for chars in dic_encoder[own_number]:
                    final_list = final_list + list(map(lambda x: [chars, x] , dic_encoder[next_number]))        
            except IndexError:
                print('Out')
    return final_list
276/83: final_list
276/84: final_list = telephone_comb(number, dic_encoder)
276/85: final_list
276/86:
def telephone_comb(number, dic_encoder):
    final_list = []
    if len(str(number)) < 2:
        print('Sorry, to short number')
    else:
        for num_enum in range(len(str(number))):
            try:
                own_number = str(number)[num_enum]
                next_number = str(number)[num_enum + 1]
                for chars in dic_encoder[own_number]:
                    final_list = final_list + list(map(lambda x: [chars, x] , dic_encoder[next_number]))        
            except IndexError:
                print('Out')
    return final_list
276/87: final_list = telephone_comb(number, dic_encoder)
276/88:
def telephone_comb(number, dic_encoder):
    final_list = []
    if len(str(number)) < 2:
        print('Sorry, to short number')
    else:
        for num_enum in range(len(str(number))):
            try:
                own_number = str(number)[num_enum]
                next_number = str(number)[num_enum + 1]
                for chars in dic_encoder[own_number]:
                    final_list = final_list + list(map(lambda x: [chars, x] , dic_encoder[next_number]))        
            except IndexError:
                pass
                
    return final_list
276/89: final_list = telephone_comb(number, dic_encoder)
276/90: final_list
276/91:
number = 23 
final_list = []
276/92:
def telephone_comb(number, dic_encoder):
    final_list = []
    if len(str(number)) < 2:
        print('Sorry, to short number')
    else:
        for num_enum in range(len(str(number))):
            try:
                own_number = str(number)[num_enum]
                next_number = str(number)[num_enum + 1]
                for chars in dic_encoder[own_number]:
                    final_list = final_list + list(map(lambda x: [chars, x] , dic_encoder[next_number]))        
            except IndexError:
                pass
                
    return final_list
276/93: final_list = telephone_comb(number, dic_encoder)
276/94: final_list
276/95:
number = 33 
final_list = []
276/96:
def telephone_comb(number, dic_encoder):
    final_list = []
    if len(str(number)) < 2:
        print('Sorry, to short number')
    else:
        for num_enum in range(len(str(number))):
            try:
                own_number = str(number)[num_enum]
                next_number = str(number)[num_enum + 1]
                for chars in dic_encoder[own_number]:
                    final_list = final_list + list(map(lambda x: [chars, x] , dic_encoder[next_number]))        
            except IndexError:
                pass
                
    return final_list
276/97: final_list = telephone_comb(number, dic_encoder)
276/98: final_list
276/99:
number = '' 
final_list = []
276/100: final_list = telephone_comb(number, dic_encoder)
276/101:
number = 2 
final_list = []
276/102: final_list = telephone_comb(number, dic_encoder)
276/103: final_list
276/104:
number = 3122 
final_list = []
276/105: final_list = telephone_comb(number, dic_encoder)
276/106: final_list
276/107:
number = 31
final_list = []
276/108: final_list = telephone_comb(number, dic_encoder)
276/109: final_list
276/110: zip('as', 'bv')
276/111: list(zip('as', 'bv'))
277/1: import numpy as np
277/2: x = np.array([2.2, 0.9, 4.4, 6.7, 2.8, 3.2, 1.1, 3.5])
277/3: x_var = np.var(x)
277/4: x_var
277/5:
x_var = np.var(x)
x_stdev = np.std(x)
277/6: x_var
277/7: x_stdev
277/8: print('Variance: {:4.2f}'.format(x_var))
277/9: g = lambdag(3,8)
277/10: g = sum(3,8)
277/11: g = np.sum(3,8)
277/12: g = pov(3,8)
277/13: g = pow(3,8)
277/14: g
277/15: g = sum(3,8)
277/16: g = sum((3,8))
277/17: g
278/1:
def solution(A):
    # write your code in Python 3.6
    pass
278/2:
def solution(A):
    # write your code in Python 3.6
    print('My fun')
    pass
278/3: A = [1, 3, 6, 4, 1, 2]
278/4: solution(A)
278/5:
def solution(A):
    # write your code in Python 3.6
    A = a.sort()
    print('My fun')
    pass
278/6: A = [1, 3, 6, 4, 1, 2]
278/7: solution(A)
278/8:
def solution(A):
    # write your code in Python 3.6
    A = A.sort()
    print('My fun')
    pass
278/9: A = [1, 3, 6, 4, 1, 2]
278/10: solution(A)
278/11:
def solution(A):
    # write your code in Python 3.6
    A = A.sort()
    print(A.min())
    print('My fun')
    pass
278/12: A = [1, 3, 6, 4, 1, 2]
278/13: solution(A)
278/14:
def solution(A):
    # write your code in Python 3.6
    A = A.sort()
    print(min(A))
    print('My fun')
    pass
278/15: A = [1, 3, 6, 4, 1, 2]
278/16: solution(A)
278/17:
def solution(A):
    # write your code in Python 3.6
    A = A.sort()
    min(A)
    print('My fun')
    pass
278/18: A = [1, 3, 6, 4, 1, 2]
278/19: solution(A)
278/20:
def solution(A):
    # write your code in Python 3.6
    A = A.sort()
    print(A)
    print('My fun')
    pass
278/21: A = [1, 3, 6, 4, 1, 2]
278/22: solution(A)
278/23:
def solution(A):
    # write your code in Python 3.6
    A.sort()
    print(A)
    print('My fun')
    pass
278/24: A = [1, 3, 6, 4, 1, 2]
278/25: solution(A)
278/26:
def solution(A):
    # write your code in Python 3.6
    set_A = set(A)
    print(set_A)
    print('My fun')
    pass
278/27: A = [1, 3, 6, 4, 1, 2]
278/28: solution(A)
278/29:
def solution(A):
    # write your code in Python 3.6
    set_A = set(A)
    max_A = max(A)
    print(max_A)
    print('My fun')
    pass
278/30: A = [1, 3, 6, 4, 1, 2]
278/31: solution(A)
278/32:
def solution(A):
    # write your code in Python 3.6
    set_A = set(A)
    max_A = max(A)
    for n in arrange(100):
        print(n)
    print(max_A)
    print('My fun')
    pass
278/33: A = [1, 3, 6, 4, 1, 2]
278/34: solution(A)
278/35:
def solution(A):
    # write your code in Python 3.6
    set_A = set(A)
    max_A = max(A)
    for n in arange(100):
        print(n)
    print(max_A)
    print('My fun')
    pass
278/36: A = [1, 3, 6, 4, 1, 2]
278/37: solution(A)
278/38:
def solution(A):
    # write your code in Python 3.6
    set_A = set(A)
    max_A = max(A)
    for n in range(100):
        print(n)
    print(max_A)
    print('My fun')
    pass
278/39: A = [1, 3, 6, 4, 1, 2]
278/40: solution(A)
278/41:
def solution(A):
    # write your code in Python 3.6
    set_A = set(A)
    max_A = max(A)
    for n in range(1, 100):
        print(n)
    print(max_A)
    print('My fun')
    pass
278/42: A = [1, 3, 6, 4, 1, 2]
278/43: solution(A)
278/44:
def solution(A):
    # write your code in Python 3.6
    set_A = set(A)
    max_A = max(A)
    for n in range(1, 101):
        if n > max_A:
            return n
            break
        else:
            print(n)
    print(max_A)
    print('My fun')
    pass
278/45: A = [1, 3, 6, 4, 1, 2]
278/46: solution(A)
278/47:
def solution(A):
    # write your code in Python 3.6
    set_A = set(A)
    max_A = max(A)
    for n in range(1, 101):
        if n > max_A:
            return n
            break
        else:
            if n in set_A:
            continue
            print(n)
        else:
            print('Hi', n)
    print(max_A)
    print('My fun')
    pass
278/48: A = [1, 3, 6, 4, 1, 2]
278/49:
def solution(A):
    # write your code in Python 3.6
    set_A = set(A)
    max_A = max(A)
    for n in range(1, 101):
        if n > max_A:
            return n
            break
        else:
            if n in set_A:
            continue
        else:
            print('Hi', n)
    print(max_A)
    print('My fun')
    pass
278/50: A = [1, 3, 6, 4, 1, 2]
278/51:
def solution(A):
    # write your code in Python 3.6
    set_A = set(A)
    max_A = max(A)
    for n in range(1, 101):
        if n > max_A:
            return n
            break
        else:
            if n in set_A:
            break
        else:
            print('Hi', n)
    print(max_A)
    print('My fun')
    pass
278/52:
def solution(A):
    # write your code in Python 3.6
    set_A = set(A)
    max_A = max(A)
    for n in range(1, 101):
        if n > max_A:
            return n
            break
        else:
            if n in set_A:
                break
        else:
            print('Hi', n)
    print(max_A)
    print('My fun')
    pass
278/53:
def solution(A):
    # write your code in Python 3.6
    set_A = set(A)
    max_A = max(A)
    for n in range(1, 101):
        if n > max_A:
            return n
            break
        elif:
            if n in set_A:
                break
        else:
            print('Hi', n)
    print(max_A)
    print('My fun')
    pass
278/54:
def solution(A):
    # write your code in Python 3.6
    set_A = set(A)
    max_A = max(A)
    for n in range(1, 101):
        if n > max_A:
            return n
            break
        elif:
            if n in set_A:
                break
    print(max_A)
    print('My fun')
    pass
278/55:
def solution(A):
    # write your code in Python 3.6
    set_A = set(A)
    max_A = max(A)
    for n in range(1, 101):
        if n > max_A:
            return n
            break
        else:
            if n in set_A:
                break
    print(max_A)
    print('My fun')
    pass
278/56:
def solution(A):
    # write your code in Python 3.6
    set_A = set(A)
    max_A = max(A)
    for n in range(1, 101):
        if n > max_A:
            return n
            break
        else:
            if n in set_A:
                break
            else:
                print('Hi')
    print(max_A)
    print('My fun')
    pass
278/57:
def solution(A):
    # write your code in Python 3.6
    set_A = set(A)
    max_A = max(A)
    for n in range(1, 101):
        if n > max_A:
            return n
            break
        else:
            if n in set_A:
                break
            else:
                print('Hi', n)
    print(max_A)
    print('My fun')
    pass
278/58: A = [1, 3, 6, 4, 1, 2]
278/59: solution(A)
278/60:
def solution(A):
    # write your code in Python 3.6
    set_A = set(A)
    max_A = max(A)
    for n in range(1, 101):
        if n > max_A:
            return n
            break
        else:
            if n in set_A:
                break
            else:
                return n
                print('Hi', n)
    print(max_A)
    print('My fun')
    pass
278/61: A = [1, 3, 6, 4, 1, 2]
278/62: solution(A)
278/63:
def solution(A):
    # write your code in Python 3.6
    set_A = set(A)
    max_A = max(A)
    for n in range(1, 101):
        if n > max_A:
            return n
            break
        else:
            print(n)
            if n in set_A:
                break
            else:
                return n
                print('Hi', n)
    print(max_A)
    print('My fun')
    pass
278/64: A = [1, 3, 6, 4, 1, 2]
278/65: solution(A)
278/66:
def solution(A):
    # write your code in Python 3.6
    set_A = set(A)
    max_A = max(A)
    while n < 101:
        if n > max_A:
            return n
            break
        else:
            print(n)
            if n in set_A:
                break
            else:
                return n
                print('Hi', n)
    print(max_A)
    print('My fun')
    pass
278/67: A = [1, 3, 6, 4, 1, 2]
278/68: solution(A)
278/69:
def solution(A):
    # write your code in Python 3.6
    set_A = set(A)
    max_A = max(A)
    n = 1
    while n < 101:
        if n > max_A:
            return n
            break
        else:
            print(n)
            if n in set_A:
                break
            else:
                return n
                print('Hi', n)
    print(max_A)
    print('My fun')
    pass
278/70: A = [1, 3, 6, 4, 1, 2]
278/71: solution(A)
278/72:
def solution(A):
    # write your code in Python 3.6
    set_A = set(A)
    max_A = max(A)
    n = 1
    while n < 101:
        print('start')
        if n > max_A:
            return n
            break
        else:
            print(n)
            if n in set_A:
                break
            else:
                return n
                print('Hi', n)
    print(max_A)
    print('My fun')
    pass
278/73: A = [1, 3, 6, 4, 1, 2]
278/74: solution(A)
278/75:
def solution(A):
    # write your code in Python 3.6
    set_A = set(A)
    max_A = max(A)
    n = 1
    while n < 101:
        print('start')
        if n > max_A:
            return n
            break
        else:
            print(n)
            if n in set_A:
                continue
            else:
                return n
                print('Hi', n)
    print(max_A)
    print('My fun')
    pass
278/76: A = [1, 3, 6, 4, 1, 2]
278/77: solution(A)
278/78:
def solution(A):
    # write your code in Python 3.6
    set_A = set(A)
    max_A = max(A)
    for n in range(1, 101):
        if n > max_A:
            return n
            break
        else:
            print(n)
            if n in set_A:
                continue
            else:
                return n
                print('Hi', n)
    print(max_A)
    print('My fun')
    pass
278/79: A = [1, 3, 6, 4, 1, 2]
278/80: solution(A)
278/81:
def solution(A):
    # write your code in Python 3.6
    set_A = set(A)
    max_A = max(A)
    for n in range(1, 101):
        if n > max_A:
            return n
            break
        else:
            print(n)
            if n in set_A:
                continue
            else:
                return n
    pass
278/82: A = [1, 3, 6, 4, 1, 2]
278/83: solution(A)
278/84:
def solution(A):
    # write your code in Python 3.6
    set_A = set(A)
    max_A = max(A)
    for n in range(1, 101):
        if n > max_A:
            return n
            break
        else:
            if n in set_A:
                continue
            else:
                return n
278/85: A = [1, 3, 6, 4, 1, 2]
278/86: A = [1, 2, 3]
278/87: solution(A)
278/88: A = [‚àí1, ‚àí3]
278/89: A = [-3, -1]
278/90: solution(A)
278/91:
def minimumBuckets(street):
    count=0
    if '.' not in street:
        return -1
    for i in range(1,len(street)-1):
        if street[i]=='H' and (street[i-1]=='.' or street[i+1]=='.'):
            count+=1
            street = street[:i] + '-' + street[i+1:]
    print(street)
    return not street.count("H")
278/92: H = "H..H"
278/93: minimumBuckets(H)
278/94:
def minimumBuckets(street):
    count=0
    if '.' not in street:
        return -1
    for i in range(1,len(street)-1):
        print('Hi')
        if street[i]=='H' and (street[i-1]=='.' or street[i+1]=='.'):
            count+=1
            street = street[:i] + '-' + street[i+1:]
    print(street)
    return not street.count("H")
278/95: H = "H..H"
278/96: minimumBuckets(H)
278/97:
def minimumBuckets(street):
    count=0
    if '.' not in street:
        return -1
    for i in range(1,len(street)-1):
        print('Hi')
        if street[i]=='H' and (street[i-1]=='.' or street[i+1]=='.'):
            count+=1
            street = street[:i] + '-' + street[i+1:]
    print(count)
    return not street.count("H")
278/98: H = "H..H"
278/99: minimumBuckets(H)
278/100: H = str.replace("H.", "HS")
278/101: H = H.replace("H.", "HS")
278/102: H
278/103:
def minimumBuckets(street):
    street = street.replace('H.', 'HB')
    street = street.replace('.H', 'B.')
    print(street)
278/104: H = "H..H"
278/105: minimumBuckets(H)
278/106:
def minimumBuckets(street):
    street = street.replace('H.', 'HB')
    street = street.replace('.H', 'BH')
    print(street)
278/107: H = "H..H"
278/108: minimumBuckets(H)
278/109: H = ".H..H"
278/110: minimumBuckets(H)
278/111: H = ".H..H.."
278/112: minimumBuckets(H)
278/113: H = ".H...H.."
278/114: minimumBuckets(H)
278/115: H = ".H...H.."
278/116: H = ".H..H.."
278/117: minimumBuckets(H)
278/118: H = ".H..H.."
278/119:
def minimumBuckets(street):
    street = street.replace('.HH.', 'BHHB')
    street = street.replace('H.H', 'HBH')
    print(street)
278/120: minimumBuckets(H)
278/121:
def minimumBuckets(street):
    street = street.replace('.HH.', 'BHHB')
    street = street.replace('H.H', 'HBH')
    print(street)
278/122:
def minimumBuckets(street):
    street = street.replace('.HH.', 'BHHB')
    street = street.replace('H.H', 'HBH')
    street = street.replace('H..H', 'HBBH')

    print(street)
278/123: H = ".H..H.."
278/124: minimumBuckets(H)
278/125:
def minimumBuckets(street):
    street = street.replace('.HH.', 'BHHB')
    street = street.replace('H.H', 'HBH')
    street = street.replace('H..H', 'HBBH')
    street = street.replace('.H.', '.HB')


    print(street)
278/126: H = ".H..H..H.."
278/127: minimumBuckets(H)
278/128: H = ".H.H."
278/129: minimumBuckets(H)
278/130:
if "*HHH*" in 'aaaaHHHa':
    print('d')
278/131:
if "HHH" in 'aaaaHHHa':
    print('d')
278/132: s = 'HBBHB'
278/133: s.count('B')
278/134:
def minimumBuckets(street):
    if (s[0]=='H' and s[1]!='.') or (s[len(s)-1]=='H' and s[len(s)-2]!='.'):
        return -1
    if "HHH" in street:
        return -1
    street = street.replace('.HH.', 'BHHB')
    street = street.replace('H.H', 'HBH')
    street = street.replace('H..H', 'HBBH')
    street = street.replace('.H.', '.HB')
    return(street.count('B'))
278/135: H = ".H.H."
278/136: minimumBuckets(H)
278/137: H = ".H.H."
278/138: minimumBuckets(H)
278/139:
def minimumBuckets(street):
    if (s[0]=='H' and s[1]!='.') or (s[len(s)-1]=='H' and s[len(s)-2]!='.'):
        return -1

    street = street.replace('.HH.', 'BHHB')
    street = street.replace('H.H', 'HBH')
    street = street.replace('H..H', 'HBBH')
    street = street.replace('.H.', '.HB')
    return(street.count('B'))
278/140: H = ".H.H."
278/141: minimumBuckets(H)
278/142: s = 'HBBHB'
278/143: s[-1]
278/144:
def minimumBuckets(street):
    if (street[0]=='H' and street[1]!='.') or (street[-1]=='H' and street[-2]!='.'):
        return -1
    if "HHH" in street:
        return -1
    street = street.replace('.HH.', 'BHHB')
    street = street.replace('H.H', 'HBH')
    street = street.replace('H..H', 'HBBH')
    street = street.replace('.H.', '.HB')
    return(street.count('B'))
278/145: H = ".H.H."
278/146: minimumBuckets(H)
278/147: H = ".H.HH."
278/148: minimumBuckets(H)
278/149: H = ".H.HH.H"
278/150: minimumBuckets(H)
278/151: H = ".H.HH.HH"
278/152: minimumBuckets(H)
278/153: H = ".H.HH.HH..............H"
278/154: minimumBuckets(H)
278/155: H = "H.H.HH.HH..............H"
278/156: minimumBuckets(H)
278/157:
def minimumBuckets(street):
    if (street[0]=='H' and street[1]!='.') or (street[-1]=='H' and street[-2]!='.'):
        return -1
    if "HHH" in street:
        return -1
    street = street.replace('.HH.', 'BHHB')
    street = street.replace('H.H', 'HBH')
    street = street.replace('H..H', 'HBBH')
    street = street.replace('.H.', '.HB')
    if (street[-1]=='H' and street[-2]=='.'):
        street[-2] = 'B'
    return(street.count('B'))
278/158: H = "H.H.HH.HH..............H"
278/159: minimumBuckets(H)
278/160:
def minimumBuckets(street):
    if (street[0]=='H' and street[1]!='.') or (street[-1]=='H' and street[-2]!='.'):
        return -1
    if "HHH" in street:
        return -1
    street = street.replace('.HH.', 'BHHB')
    street = street.replace('H.H', 'HBH')
    street = street.replace('H..H', 'HBBH')
    street = street.replace('.H.', '.HB')
    count_B = street.count('B')
    if (street[-1]=='H' and street[-2]=='.'):
        count_B+=1
    return(street.count('B'))
278/161: H = "H.H.HH.HH..............H"
278/162: minimumBuckets(H)
278/163:
def minimumBuckets(street):
    if (street[0]=='H' and street[1]!='.') or (street[-1]=='H' and street[-2]!='.'):
        return -1
    if "HHH" in street:
        return -1
    street = street.replace('.HH.', 'BHHB')
    street = street.replace('H.H', 'HBH')
    street = street.replace('H..H', 'HBBH')
    street = street.replace('.H.', '.HB')
    count_B = street.count('B')
    
    if (street[-1]=='H' and street[-2]=='.'):
        count_B+=1
    return count_B
278/164: H = "H.H.HH.HH..............H"
278/165: minimumBuckets(H)
278/166: H = ".HH.H.H.H.."
278/167:
def minimumBuckets(street):
    if street == 'H':
        return -1
    if street == '.':
        return 0
    if (street[0]=='H' and street[1]!='.') or (street[-1]=='H' and street[-2]!='.'):
        return -1
    if "HHH" in street:
        return -1
    street = street.replace('.HH.', 'BHHB')
    street = street.replace('H.H', 'HBH')
    street = street.replace('H..H', 'HBBH')
    street = street.replace('.H.', '.HB')
    count_B = street.count('B')
    
    if (street[-1]=='H' and street[-2]=='.'):
        count_B+=1
    print(street)
    return count_B
278/168: H = ".HH.H.H.H.."
278/169: minimumBuckets(H)
278/170:
def minimumBuckets(street):
    if street == 'H':
        return -1
    if street == '.':
        return 0
    if (street[0]=='H' and street[1]!='.') or (street[-1]=='H' and street[-2]!='.'):
        return -1
    if "HHH" in street:
        return -1
    street = street.replace('.HH.', 'BHHB')
    street = street.replace('H.H', 'HBH')
    print(street)
    street = street.replace('H..H', 'HBBH')
    street = street.replace('.H.', '.HB')
    print(street)
    count_B = street.count('B')
    
    if (street[-1]=='H' and street[-2]=='.'):
        count_B+=1
    print(street)
    return count_B
278/171: H = ".HH.H.H.H.."
278/172: minimumBuckets(H)
278/173:
def minimumBuckets(street):
    if street == 'H':
        return -1
    if street == '.':
        return 0
    if (street[0]=='H' and street[1]!='.') or (street[-1]=='H' and street[-2]!='.'):
        return -1
    if "HHH" in street:
        return -1
    street = street.replace('.HH.', 'BHHB')
    street = street.replace('H.H', 'HBH')
    street = street.replace('BH.H', 'BHBH')
    print(street)
    street = street.replace('H..H', 'HBBH')
    street = street.replace('.H.', '.HB')
    print(street)
    count_B = street.count('B')
    
    if (street[-1]=='H' and street[-2]=='.'):
        count_B+=1
    print(street)
    return count_B
278/174: H = ".HH.H.H.H.."
278/175: minimumBuckets(H)
278/176: H = ".HH.H.H.H.."
278/177: minimumBuckets(H)
278/178:
def minimumBuckets(street):
    if street == 'H':
        return -1
    if street == '.':
        return 0
    if (street[0]=='H' and street[1]!='.') or (street[-1]=='H' and street[-2]!='.'):
        return -1
    if "HHH" in street:
        return -1
    street = street.replace('.HH.', 'BHHB')
    street = street.replace('H.H', 'HBH')
   # street = street.replace('BH.H', 'BHBH')
    print(street)
    street = street.replace('H..H', 'HBBH')
    street = street.replace('.H.', '.HB')
    print(street)
    count_B = street.count('B')
    
    if (street[-1]=='H' and street[-2]=='.'):
        count_B+=1
    print(street)
    return count_B
278/179: H = ".HH.H.H.H.."
278/180: minimumBuckets(H)
278/181:
def minimumBuckets(street):
    if street == 'H':
        return -1
    if street == '.':
        return 0
    if (street[0]=='H' and street[1]!='.') or (street[-1]=='H' and street[-2]!='.'):
        return -1
    if "HHH" in street:
        return -1
    street = street.replace('.HH.', 'BHHB')
    street = street.replace('H.H', 'HBH')
    street = street.replace('H.H', 'HBH')

   # street = street.replace('BH.H', 'BHBH')
    print(street)
    street = street.replace('H..H', 'HBBH')
    street = street.replace('.H.', '.HB')
    print(street)
    count_B = street.count('B')
    
    if (street[-1]=='H' and street[-2]=='.'):
        count_B+=1
    print(street)
    return count_B
278/182: H = ".HH.H.H.H.."
278/183: minimumBuckets(H)
278/184:
def minimumBuckets(street):
    if street == 'H':
        return -1
    if street == '.':
        return 0
    if (street[0]=='H' and street[1]!='.') or (street[-1]=='H' and street[-2]!='.'):
        return -1
    counter = 0
    i = 1
    while i < len(street - 2):
        if street[i] == 'H':
            if street[i+1] == '.':
                counter+=1
                i+=3
                continue
            else:
                if street[i-1] == '.':
                    counter+=1
                    i+=2
                    continue
                else:
                    return -1
278/185:
if "HHH" in 'aaaaHHHa':
    print('d')
278/186: H = ".HH.H.H.H.."
278/187: minimumBuckets(H)
278/188:
def minimumBuckets(street):
    if street == 'H':
        return -1
    if street == '.':
        return 0
    if (street[0]=='H' and street[1]!='.') or (street[-1]=='H' and street[-2]!='.'):
        return -1
    counter = 0
    i = 1
    while i < len(street) - 2:
        if street[i] == 'H':
            if street[i+1] == '.':
                counter+=1
                i+=3
                continue
            else:
                if street[i-1] == '.':
                    counter+=1
                    i+=2
                    continue
                else:
                    return -1
278/189: H = ".HH.H.H.H.."
278/190: minimumBuckets(H)
278/191:
def minimumBuckets(street):
    if street == 'H':
        return -1
    if street == '.':
        return 0
    if (street[0]=='H' and street[1]!='.') or (street[-1]=='H' and street[-2]!='.'):
        return -1
    counter = 0
    i = 1
    while i < len(street) - 2:
        print
        if street[i] == 'H':
            if street[i+1] == '.':
                counter+=1
                i+=3
                continue
            else:
                if street[i-1] == '.':
                    counter+=1
                    i+=2
                    continue
                else:
                    return -1
278/192:
def minimumBuckets(street):
    if street == 'H':
        return -1
    if street == '.':
        return 0
    if (street[0]=='H' and street[1]!='.') or (street[-1]=='H' and street[-2]!='.'):
        return -1
    counter = 0
    i = 1
    while i < len(street) - 2:
        print(i)
        if street[i] == 'H':
            if street[i+1] == '.':
                counter+=1
                i+=3
                continue
            else:
                if street[i-1] == '.':
                    counter+=1
                    i+=2
                    continue
                else:
                    return -1
278/193: H = ".HH.H.H.H.."
278/194: minimumBuckets(H)
278/195: H[1]
278/196:
def minimumBuckets(street):
    if street == 'H':
        return -1
    if street == '.':
        return 0
    if (street[0]=='H' and street[1]!='.') or (street[-1]=='H' and street[-2]!='.'):
        return -1
    counter = 0
    i = 1
    while i < len(street) - 2:
        print(i)
        if street[i] == 'H':
            if street[i+1] == '.':
                print('H', i)
                counter+=1
                i+=3
                continue
            else:
                if street[i-1] == '.':
                    counter+=1
                    i+=2
                    continue
                else:
                    return -1
278/197: H = ".HH.H.H.H.."
278/198: H[1]
278/199: minimumBuckets(H)
278/200:
def minimumBuckets(street):
    if street == 'H':
        return -1
    if street == '.':
        return 0
    if (street[0]=='H' and street[1]!='.') or (street[-1]=='H' and street[-2]!='.'):
        return -1
    counter = 0
    i = 1
    while i < len(street) - 2:
        print(i)
        if street[i] == 'H':
            if street[i+1] == '.':
                print('H', i)
                counter+=1
                i+=3
                continue
            else:
                if street[i-1] == '.':
                    print('.', i)
                    counter+=1
                    i+=2
                    continue
                else:
                    return -1
278/201: minimumBuckets(H)
278/202:
def minimumBuckets(street):
    if street == 'H':
        return -1
    if street == '.':
        return 0
    if (street[0]=='H' and street[1]!='.') or (street[-1]=='H' and street[-2]!='.'):
        return -1
    counter = 0
    i = 1
    while i < len(street) - 2:
        print(i)
        if street[i] == 'H':
            if street[i+1] == '.':
                print('H', i)
                counter+=1
                i+=3
                continue
            else:
                if street[i-1] == '.':
                    print('.', i)
                    counter+=1
                    i+=2
                    print(counter)
                    continue
                else:
                    return -1
278/203: H = ".HH.H.H.H.."
278/204: H[1]
278/205: minimumBuckets(H)
278/206:
def minimumBuckets(street):
    if street == 'H':
        return -1
    if street == '.':
        return 0
    if (street[0]=='H' and street[1]!='.') or (street[-1]=='H' and street[-2]!='.'):
        return -1
    counter = 0
    i = 1
    while i < len(street) - 2:
        print(i)
        peinr(street[i])
        if street[i] == 'H':
            if street[i+1] == '.':
                print('H', i)
                counter+=1
                i+=3
                continue
            else:
                if street[i-1] == '.':
                    print('.', i)
                    counter+=1
                    i+=2
                    print(counter)
                    continue
                else:
                    return -1
278/207: minimumBuckets(H)
278/208:
def minimumBuckets(street):
    if street == 'H':
        return -1
    if street == '.':
        return 0
    if (street[0]=='H' and street[1]!='.') or (street[-1]=='H' and street[-2]!='.'):
        return -1
    counter = 0
    i = 1
    while i < len(street) - 2:
        print(i)
        print(street[i])
        if street[i] == 'H':
            if street[i+1] == '.':
                print('H', i)
                counter+=1
                i+=3
                continue
            else:
                if street[i-1] == '.':
                    print('.', i)
                    counter+=1
                    i+=2
                    print(counter)
                    continue
                else:
                    return -1
278/209: minimumBuckets(H)
278/210:
def minimumBuckets(street):
    if street == 'H':
        return -1
    if street == '.':
        return 0
    if (street[0]=='H' and street[1]!='.') or (street[-1]=='H' and street[-2]!='.'):
        return -1
    counter = 0
    i = 1
    while i < len(street) - 2:
        print(i)
        print(street[i])
        if street[i] == 'H':
            if street[i+1] == '.':
                print('H', i)
                counter+=1
                i+=3
                continue
            else:
                if street[i-1] == '.':
                    print('.', i)
                    counter+=1
                    i+=2
                    print(counter)
                    continue
                else:
                    return -1
        else:
            i+=1
278/211: minimumBuckets(H)
278/212:
def minimumBuckets(street):
    if street == 'H':
        return -1
    if street == '.':
        return 0
    if (street[0]=='H' and street[1]!='.') or (street[-1]=='H' and street[-2]!='.'):
        return -1
    counter = 0
    i = 1
    while i < len(street) - 2:
        print(i)
        print(street[i])
        if street[i] == 'H':
            if street[i+1] == '.':
                print('H', i)
                counter+=1
                i+=3
                continue
            else:
                if street[i-1] == '.':
                    print('.', i)
                    counter+=1
                    i+=2
                    print(counter)
                    continue
                else:
                    return -1
        else:
            i+=1
    print(counter)
278/213: minimumBuckets(H)
278/214:
def minimumBuckets(street):
    if street == 'H':
        return -1
    if street == '.':
        return 0
    if (street[0]=='H' and street[1]!='.') or (street[-1]=='H' and street[-2]!='.'):
        return -1
    counter = 0
    i = 1
    while i < len(street) - 1:
        print(i)
        print(street[i])
        if street[i] == 'H':
            if street[i+1] == '.':
                print('H', i)
                counter+=1
                i+=3
                continue
            else:
                if street[i-1] == '.':
                    print('.', i)
                    counter+=1
                    i+=2
                    print(counter)
                    continue
                else:
                    return -1
        else:
            i+=1
    print(counter)
278/215: minimumBuckets(H)
278/216:
def minimumBuckets(street):
    if street == 'H':
        return -1
    if street == '.':
        return 0
    if (street[0]=='H' and street[1]!='.') or (street[-1]=='H' and street[-2]!='.'):
        return -1
    counter = 0
    i = 1
    while i < len(street) - 1:
        print(i)
        print(street[i])
        if street[i] == 'H':
            if street[i+1] == '.':
                print('H., position T:', i+1)
                counter+=1
                i+=3
                continue
            else:
                if street[i-1] == '.':
                    print('.H, position T:', i-1)
                    counter+=1
                    i+=2
                    print(counter)
                    continue
                else:
                    return -1
        else:
            i+=1
    print(counter)
278/217: minimumBuckets(H)
278/218:
def minimumBuckets(street):
    if street == 'H':
        return -1
    if street == '.':
        return 0
    if (street[0]=='H' and street[1]!='.') or (street[-1]=='H' and street[-2]!='.'):
        return -1
    counter = 0
    i = 1
    while i < len(street) - 1:
        print(i)
        print(street[i])
        if street[i] == 'H':
            if street[i+1] == '.':
                print('H., position T:', i)
                counter+=1
                i+=3
                continue
            else:
                if street[i-1] == '.':
                    print('.H, position T:', i-1)
                    counter+=1
                    i+=2
                    print(counter)
                    continue
                else:
                    return -1
        else:
            i+=1
    print(counter)
278/219: minimumBuckets(H)
278/220:
def minimumBuckets(street):
    if street == 'H':
        return -1
    if street == '.':
        return 0
    if (street[0]=='H' and street[1]!='.') or (street[-1]=='H' and street[-2]!='.'):
        return -1
    counter = 0
    i = 1
    while i < len(street) - 1:
        print(i)
        print(street[i])
        if street[i] == 'H':
            if street[i+1] == '.':
                print('H., position T:', i)
                counter+=1
                i+=3
                continue
            else:
                if street[i-1] == '.':
                    print('.H, position T:', i-1)
                    counter+=1
                    i+=1
                    print(counter)
                    continue
                else:
                    return -1
        else:
            i+=1
    print(counter)
278/221: minimumBuckets(H)
278/222:
def minimumBuckets(street):
    if street == 'H':
        return -1
    if street == '.':
        return 0
    if (street[0]=='H' and street[1]!='.') or (street[-1]=='H' and street[-2]!='.'):
        return -1
    counter = 0
    i = 1
    while i < len(street) - 1:
        print(i)
        print(street[i])
        if street[i] == 'H':
            if street[i+1] == '.':
                print('H., position T:', i+1)
                counter+=1
                i+=3
                continue
            else:
                if street[i-1] == '.':
                    print('.H, position T:', i-1)
                    counter+=1
                    i+=1
                    print(counter)
                    continue
                else:
                    return -1
        else:
            i+=1
    print(counter)
278/223: minimumBuckets(H)
278/224:
def minimumBuckets(street):
    if street == 'H':
        return -1
    if street == '.':
        return 0
    if (street[0]=='H' and street[1]!='.') or (street[-1]=='H' and street[-2]!='.'):
        return -1
    counter = 0
    i = 1
    while i < len(street) - 1:
        if street[i] == 'H':
            if street[i+1] == '.':
                counter+=1
                i+=3
                continue
            else:
                if street[i-1] == '.':
                    counter+=1
                    i+=1
                    continue
                else:
                    return -1
        else:
            i+=1
    return counter
278/225: minimumBuckets(H)
278/226:
def minimumBuckets(street):
    if 'HHH' in street or street[:2]=='HH' or street[-2:]=='HH' or street == 'H':
        return -1
    
    i = 0
    end_string = len(street)
    counter = 1
    
    while i < end_string - 1:
        if street[i] == 'H':
            if street[i+1] == '.':
                counter+=1
                i+=3
                continue
            elif i == 0:
                return -1                
            else:
                if street[i-1] == '.':
                    counter+=1
                    i+=2
                    continue
                else:
                    return -1
        else:
            i+=1
    return counter
278/227: H = ".HH.H.H.H.."
278/228: H[1]
278/229: minimumBuckets(H)
278/230:
def minimumBuckets(street):
    if 'HHH' in street or street[:2]=='HH' or street[-2:]=='HH' or street == 'H':
        return -1
    
    i = 0
    end_string = len(street)
    counter = 1
    
    while i < end_string - 1:
        if street[i] == 'H':
            if street[i+1] == '.':
                counter+=1
                i+=3
                continue
            elif i == 0:
                return -1                
            else:
                if street[i-1] == '.':
                    counter+=1
                    i+=1
                    continue
                else:
                    return -1
        else:
            i+=1
    return counter
278/231: H = ".HH.H.H.H.."
278/232: H[1]
278/233: minimumBuckets(H)
278/234:
def minimumBuckets(street):
    if 'HHH' in street or street[:2]=='HH' or street[-2:]=='HH' or street == 'H':
        return -1
    
    i = 0
    end_string = len(street)
    counter = 0
    
    while i < end_string - 1:
        if street[i] == 'H':
            if street[i+1] == '.':
                counter+=1
                i+=3
                continue
            elif i == 0:
                return -1                
            else:
                if street[i-1] == '.':
                    counter+=1
                    i+=1
                    continue
                else:
                    return -1
        else:
            i+=1
    return counter
278/235:
def minimumBuckets(street):
    if 'HHH' in street or street[:2]=='HH' or street[-2:]=='HH' or street == 'H':
        return -1
    
    i = 0
    end_string = len(street)
    counter = 0
    
    while i < end_string - 1:
        if street[i] == 'H':
            if street[i+1] == '.':
                counter+=1
                i+=3
                continue
            elif i == 0:
                return -1                
            else:
                if street[i-1] == '.':
                    if i>end_string-2:
                        return -1
                    elif street[i+2]=='.':
                        counter+=2
                        i+=4
                        continue
                    else:
                        return -1
                else:
                    return -1
        else:
            i+=1
    return counter
278/236: minimumBuckets(H)
278/237: H = "H..H"
278/238: minimumBuckets(H)
278/239:
def minimumBuckets(street):
    if 'HHH' in street or street[:2]=='HH' or street[-2:]=='HH' or street == 'H':
        return -1
    
    i = 0
    end_string = len(street)
    counter = 0
    
    while i < end_string:
        if street[i] == 'H':
            if street[i+1] == '.':
                counter+=1
                i+=3
                continue
            elif i == 0:
                return -1                
            else:
                if street[i-1] == '.':
                    if i>end_string-2:
                        return -1
                    elif street[i+2]=='.':
                        counter+=2
                        i+=4
                        continue
                    else:
                        return -1
                else:
                    return -1
        else:
            i+=1
    return counter
278/240: minimumBuckets(H)
278/241: len(street)
278/242: len('H..H')
278/243:
def minimumBuckets(street):
    if 'HHH' in street or street[:2]=='HH' or street[-2:]=='HH' or street == 'H':
        return -1
    
    i = 0
    end_string = len(street)
    counter = 0
    
    while i < end_string-1:
        if street[i] == 'H':
            if street[i+1] == '.':
                counter+=1
                i+=3
                continue
            elif i == 0:
                return -1                
            else:
                if street[i-1] == '.':
                    if i>end_string-2:
                        return -1
                    elif street[i+2]=='.':
                        counter+=2
                        i+=4
                        continue
                    else:
                        return -1
                else:
                    return -1
        else:
            i+=1
    return counter
278/244: minimumBuckets(H)
278/245:
def minimumBuckets(street):
    if 'HHH' in street or street[:2]=='HH' or street[-2:]=='HH' or street == 'H':
        return -1
    
    i = 0
    end_string = len(street)
    counter = 0
    
    while i < end_string-1:
        if street[i] == 'H':
            if street[i+1] == '.':
                counter+=1
                i+=3
                continue
            elif i == 0:
                return -1                
            else:
                if street[i-1] == '.':
                    if i>end_string-2:
                        counter+=1
                        i+=3
                    elif street[i+2]=='.':
                        counter+=2
                        i+=4
                        continue
                    else:
                        return -1
                else:
                    return -1
        else:
            i+=1
    return counter
278/246: minimumBuckets(H)
278/247:
def minimumBuckets(street):
    if 'HHH' in street or street[:2]=='HH' or street[-2:]=='HH' or street == 'H':
        return -1
    
    i = 0
    end_string = len(street)
    counter = 0
    
    while i < end_string-1:
        if street[i] == 'H':
            if street[i+1] == '.':
                counter+=1
                i+=3
                continue
            elif i == 0:
                return -1                
            else:
                if street[i-1] == '.':
                    print('end_string', end_string)
                    if i>end_string-2:
                        counter+=1
                        i+=3
                    elif street[i+2]=='.':
                        counter+=2
                        i+=4
                        continue
                    else:
                        return -1
                else:
                    return -1
        else:
            i+=1
    return counter
278/248: minimumBuckets(H)
278/249:
def minimumBuckets(street):
    if 'HHH' in street or street[:2]=='HH' or street[-2:]=='HH' or street == 'H':
        return -1
    
    i = 0
    end_string = len(street)
    counter = 0
    
    while i < end_string-1:
        print(i, street[i])
        if street[i] == 'H':
            if street[i+1] == '.':
                counter+=1
                i+=3
                continue
            elif i == 0:
                return -1                
            else:
                if street[i-1] == '.':
                    print('end_string', end_string)
                    if i>end_string-2:
                        counter+=1
                        i+=3
                    elif street[i+2]=='.':
                        counter+=2
                        i+=4
                        continue
                    else:
                        return -1
                else:
                    return -1
        else:
            i+=1
    return counter
278/250: minimumBuckets(H)
278/251:
def minimumBuckets(street):
    if 'HHH' in street or street[:2]=='HH' or street[-2:]=='HH' or street == 'H':
        return -1
    
    i = 0
    end_string = len(street)
    counter = 0
    
    while i <= end_string-1:
        print(i, street[i])
        if street[i] == 'H':
            if street[i+1] == '.':
                counter+=1
                i+=3
                continue
            elif i == 0:
                return -1                
            else:
                if street[i-1] == '.':
                    print('end_string', end_string)
                    if i>end_string-2:
                        counter+=1
                        i+=3
                    elif street[i+2]=='.':
                        counter+=2
                        i+=4
                        continue
                    else:
                        return -1
                else:
                    return -1
        else:
            i+=1
    return counter
278/252: minimumBuckets(H)
278/253:
def minimumBuckets(street):
    if 'HHH' in street or street[:2]=='HH' or street[-2:]=='HH' or street == 'H':
        return -1
    
    i = 0
    end_string = len(street)
    counter = 0
    
    while i <= end_string-1:
        print(i, street[i])
        if street[i] == 'H' and i<end_string:
            
            if street[i+1] == '.':
                counter+=1
                i+=3
                continue
            elif i == 0:
                return -1                
            else:
                if street[i-1] == '.':
                    print('end_string', end_string)
                    if i>end_string-2:
                        counter+=1
                        i+=3
                    elif street[i+2]=='.':
                        counter+=2
                        i+=4
                        continue
                    else:
                        return -1
                else:
                    return -1
        else:
            i+=1
    return counter
278/254: minimumBuckets(H)
278/255:
def minimumBuckets(street):
    if 'HHH' in street or street[:2]=='HH' or street[-2:]=='HH' or street == 'H':
        return -1
    
    i = 0
    end_string = len(street)
    counter = 0
    
    while i <= end_string-1:
        print(i, street[i])
        if street[i] == 'H' and i<end_string-1:
            
            if street[i+1] == '.':
                counter+=1
                i+=3
                continue
            elif i == 0:
                return -1                
            else:
                if street[i-1] == '.':
                    print('end_string', end_string)
                    if i>end_string-2:
                        counter+=1
                        i+=3
                    elif street[i+2]=='.':
                        counter+=2
                        i+=4
                        continue
                    else:
                        return -1
                else:
                    return -1
        else:
            i+=1
    return counter
278/256: minimumBuckets(H)
278/257:
def minimumBuckets(street):
    if 'HHH' in street or street[:2]=='HH' or street[-2:]=='HH' or street == 'H':
        return -1
    
    i = 0
    end_string = len(street)
    counter = 0
    if (street[-1]=='H' and street[-2]=='.' and i<end_string):
        counter+=1
        end_string-=2
    
    while i < end_string-1:
        print(i, street[i])
        if street[i] == 'H':
            
            if street[i+1] == '.':
                counter+=1
                i+=3
                continue
            elif i == 0:
                return -1                
            else:
                if street[i-1] == '.':
                    print('end_string', end_string)
                    if i>end_string-2:
                        counter+=1
                        i+=3
                    elif street[i+2]=='.':
                        counter+=2
                        i+=4
                        continue
                    else:
                        return -1
                else:
                    return -1
        else:
            i+=1
    return counter
278/258: minimumBuckets(H)
278/259:
def minimumBuckets(street):
    if 'HHH' in street or street[:2]=='HH' or street[-2:]=='HH' or street == 'H':
        return -1
    
    i = 0
    end_string = len(street)
    counter = 0
    if (street[-1]=='H' and street[-2]=='.' and i<end_string):
        counter+=1
        end_string-=2
    
    while i < end_string-1:
        print(i, street[i])
        if street[i] == 'H':
            
            if street[i+1] == '.':
                counter+=1
                i+=3
                continue
            elif i == 0:
                return -1                
            else:
                if street[i-1] == '.':
                    print('end_string', end_string)
                    if i>end_string-2:
                        counter+=1
                        i+=3
                    elif street[i+2]=='.':
                        counter+=2
                        i+=4
                        continue
                    else:
                        return -1
                else:
                    return -1
        else:
            i+=1
    return counter
278/260:
def minimumBuckets(street):
    if 'HHH' in street or street[:2]=='HH' or street[-2:]=='HH' or street == 'H':
        return -1
    
    i = 0
    end_string = len(street)
    counter = 0
    if (street[-1]=='H' and street[-2]=='.' and i<end_string):
        counter+=1
        end_string-=2
    
    while i < end_string-1:
        print(i, street[i])
        if street[i] == 'H':
            
            if street[i+1] == '.':
                counter+=1
                i+=3
                continue
            elif i == 0:
                return -1                
            else:
                if street[i-1] == '.':
                    print('end_string', end_string)
                    if i>end_string-2:
                        counter+=1
                        i+=3
                    elif street[i+2]=='.':
                        counter+=2
                        i+=4
                        continue
                    else:
                        return -1
                else:
                    return -1
        else:
            i+=1
    return counter
278/261:
def minimumBuckets(street):
    if 'HHH' in street or street[:2]=='HH' or street[-2:]=='HH' or street == 'H':
        return -1
    
    i = 0
    end_string = len(street)
    counter = 0
    if (street[-1]=='H' and street[-2]=='.' and i<end_string):
        counter+=1
        end_string-=2
    
    while i < end_string-1:
        if street[i] == 'H':            
            if street[i+1] == '.':
                counter+=1
                i+=3
                continue
            elif i == 0:
                return -1                
            else:
                if street[i-1] == '.':
                    print('end_string', end_string)
                    if i>end_string-2:
                        counter+=1
                        i+=3
                    elif street[i+2]=='.':
                        counter+=2
                        i+=4
                        continue
                    else:
                        return -1
                else:
                    return -1
        else:
            i+=1
    return counter
278/262: minimumBuckets(H)
278/263:
def minimumBuckets(street):
    if 'HHH' in street or street[:2]=='HH' or street[-2:]=='HH' or street == 'H':
        return -1
        
    counter = 0
    i = 1
    end_string = len(street)
    
    if (street[0]=='H' and street[1]=='.'):
        counter+=1
        i+=2
    if (street[-1]=='H' and street[-2]=='.' and i<end_string):
        counter+=1
        end_string-=2
    while i < end_string - 1:
        if street[i] == 'H':
            if street[i+1] == '.':
                counter+=1
                i+=3
                continue
            else:
                if street[i-1] == '.' and street[i+2] == '.':
                    counter+=2
                    i+=4
                    continue
                else:
                    return -1
        else:
            i+=1
    return counter
278/264: H = ".HH.H"
278/265: minimumBuckets(H)
278/266:
def minimumBuckets(street):
    if 'HHH' in street or street[:2]=='HH' or street[-2:]=='HH' or street == 'H':
        return -1
        
    counter = 0
    i = 1
    end_string = len(street)
    
    if (street[0]=='H' and street[1]=='.'):
        counter+=1
        i+=2
    if (street[-1]=='H' and street[-2]=='.' and i<end_string):
        counter+=1
        end_string-=2
    while i < end_string - 1:
        if street[i] == 'H':
            if street[i+1] == '.':
                counter+=1
                i+=3
                continue
            else:
                print(i, end_string)
                if street[i-1] == '.' and street[i+2] == '.':
                    counter+=2
                    i+=4
                    continue
                else:
                    return -1
        else:
            i+=1
    return counter
278/267: minimumBuckets(H)
278/268:
def minimumBuckets(street):
    if 'HHH' in street or street[:2]=='HH' or street[-2:]=='HH' or street == 'H':
        return -1
        
    counter = 0
    i = 1
    end_string = len(street)
    
    if (street[0]=='H' and street[1]=='.'):
        counter+=1
        i+=2
    if (street[-1]=='H' and street[-2]=='.' and i<end_string):
        counter+=1
        end_string-=2
    print end_string
    while i < end_string - 1:
        if street[i] == 'H':
            if street[i+1] == '.':
                counter+=1
                i+=3
                continue
            else:
                print(i, end_string)
                if street[i-1] == '.' and street[i+2] == '.':
                    counter+=2
                    i+=4
                    continue
                else:
                    return -1
        else:
            i+=1
    return counter
278/269:
def minimumBuckets(street):
    if 'HHH' in street or street[:2]=='HH' or street[-2:]=='HH' or street == 'H':
        return -1
        
    counter = 0
    i = 1
    end_string = len(street)
    
    if (street[0]=='H' and street[1]=='.'):
        counter+=1
        i+=2
    if (street[-1]=='H' and street[-2]=='.' and i<end_string):
        counter+=1
        end_string-=2
    print (end_string)
    while i < end_string - 1:
        if street[i] == 'H':
            if street[i+1] == '.':
                counter+=1
                i+=3
                continue
            else:
                print(i, end_string)
                if street[i-1] == '.' and street[i+2] == '.':
                    counter+=2
                    i+=4
                    continue
                else:
                    return -1
        else:
            i+=1
    return counter
278/270: minimumBuckets(H)
278/271:
def minimumBuckets(street):
    if 'HHH' in street or street[:2]=='HH' or street[-2:]=='HH' or street == 'H':
        return -1
        
    counter = 0
    i = 1
    end_string = len(street)
    
    if (street[0]=='H' and street[1]=='.'):
        counter+=1
        i+=2
    if (street[-1]=='H' and street[-2]=='.' and i<end_string):
        counter+=1
        end_string-=2
    while i < end_string - 1:
        if street[i] == 'H':
            if street[i+1] == '.':
                counter+=1
                i+=3
                continue
            else:
                print(i, end_string)
                if street[i-1] == '.' and street[i+2] == '.' and i<end_string-2:
                    counter+=2
                    i+=4
                    continue
                elif street[i-1] == '.':
                    counter+=1
                    i+=1
                else:
                    return -1
        else:
            i+=1
    return counter
278/272: minimumBuckets(H)
278/273: len_a = 3
278/274: len_a * [0]
278/275: len_a * [0] + a
278/276: a = [1,2,3]
278/277: len_a = 3
278/278: len_a * [0] + a
278/279: a = len_a * [0] + a
278/280: a
278/281:
def addTwoNumbers(self, l1, l2):
    len_l1 = len(l1)
    len_l2 = len(l2)
    if len_l1 > len_l2:
        len_l2 = (len_l1 - len_l2) * [0] + len_l2
    elif lenl2 > lenl1:
        len_l1 = (len_l2 - len_l1) * [0] + len_l1
    else:
        pass
    print(l1, '\n', l2)

        """
        :type l1: ListNode
        :type l2: ListNode
        :rtype: ListNode
        """
278/282:
def addTwoNumbers(self, l1, l2):
    len_l1 = len(l1)
    len_l2 = len(l2)
    if len_l1 > len_l2:
        len_l2 = (len_l1 - len_l2) * [0] + len_l2
    elif lenl2 > lenl1:
        len_l1 = (len_l2 - len_l1) * [0] + len_l1
    else:
        pass
    print(l1, '\n', l2)
278/283: l1 = [2,4,3], l2 = [5,6,4,2]
278/284:
l1 = [2,4,3]
l2 = [5,6,4,2]
278/285: addTwoNumbers(l1, l2)
278/286:
def addTwoNumbers(l1, l2):
    len_l1 = len(l1)
    len_l2 = len(l2)
    if len_l1 > len_l2:
        len_l2 = (len_l1 - len_l2) * [0] + len_l2
    elif lenl2 > lenl1:
        len_l1 = (len_l2 - len_l1) * [0] + len_l1
    else:
        pass
    print(l1, '\n', l2)
278/287:
l1 = [2,4,3]
l2 = [5,6,4,2]
278/288: addTwoNumbers(l1, l2)
278/289:
def addTwoNumbers(l1, l2):
    len_l1 = len(l1)
    len_l2 = len(l2)
    if len_l1 > len_l2:
        len_l2 = (len_l1 - len_l2) * [0] + len_l2
    elif len_l2 > len_l1:
        len_l1 = (len_l2 - len_l1) * [0] + len_l1
    else:
        pass
    print(l1, '\n', l2)
278/290:
l1 = [2,4,3]
l2 = [5,6,4,2]
278/291: addTwoNumbers(l1, l2)
278/292:
def addTwoNumbers(l1, l2):
    len_l1 = len(l1)
    len_l2 = len(l2)
    if len_l1 > len_l2:
        len_l2 = (len_l1 - len_l2) * [0] + len_l2
    elif len_l2 > len_l1:
        diff = len_l2 - len_l1
        len_l1 = diff * [0] + len_l1
    else:
        pass
    print(l1, '\n', l2)
278/293:
l1 = [2,4,3]
l2 = [5,6,4,2]
278/294: addTwoNumbers(l1, l2)
278/295: a = [1,2,3]
278/296: len_a = 3
278/297: a = len_a * [0] + a
278/298: a
278/299:
def addTwoNumbers(l1, l2):
    len_l1 = len(l1)
    len_l2 = len(l2)
    if len_l1 > len_l2:
        len_l2 = (len_l1 - len_l2) * [0] + len_l2
    elif len_l2 > len_l1:
        diff = len_l2 - len_l1
        len_l1 = [0,0] + len_l1
    else:
        pass
    print(l1, '\n', l2)
278/300:
l1 = [2,4,3]
l2 = [5,6,4,2]
278/301: addTwoNumbers(l1, l2)
278/302:
def addTwoNumbers(l1, l2):
    len_l1 = len(l1)
    len_l2 = len(l2)
    if len_l1 > len_l2:
        l2 = (len_l1 - len_l2) * [0] + l2
    elif len_l2 > len_l1:
        l1 = (len_l2 - len_l1) * [0] + l1
    else:
        pass
    print(l1, '\n', l2)
278/303:
l1 = [2,4,3]
l2 = [5,6,4,2]
278/304: addTwoNumbers(l1, l2)
278/305:
l = [1, 2, 3]

s = [str(integer) for integer in l]
a_string = "".join(s)
278/306: a_string
278/307: a_string[::-1]
278/308: int(a_string[::-1])
278/309:
l = [1, 2, 3]

s = [str(integer) for integer in l]
a_string = "".join(s::-1)
278/310:
l = [1, 2, 3]

s = [str(integer) for integer in l]
a_string = "".join(s[::-1])
278/311: a_string
278/312:
def addTwoNumbers(l1, l2):
    s_l1 = [str(integer) for integer in l1]
    string_l1 = "".join(s_l1[::-1])
    s_l2 = [str(integer) for integer in l2]
    string_l2 = "".join(s_l2[::-1])
    res = int(string_l1) + int(string_l2)
278/313:
l1 = [2,4,3]
l2 = [5,6,4,2]
278/314: addTwoNumbers(l1, l2)
278/315:
def addTwoNumbers(l1, l2):
    s_l1 = [str(integer) for integer in l1]
    string_l1 = "".join(s_l1[::-1])
    s_l2 = [str(integer) for integer in l2]
    string_l2 = "".join(s_l2[::-1])
    res = int(string_l1) + int(string_l2)
    print(res)
278/316:
l1 = [2,4,3]
l2 = [5,6,4,2]
278/317: addTwoNumbers(l1, l2)
278/318:
l1 = [2,4,3]
l2 = [5,6,4]
278/319: addTwoNumbers(l1, l2)
278/320: a = 102
278/321: str(a)
278/322: str(a)[::-1]
278/323: aa = [x for x in str(a)[::-1]]
278/324: aa
278/325:
def addTwoNumbers(l1, l2):
    s_l1 = [str(integer) for integer in l1]
    string_l1 = "".join(s_l1[::-1])
    s_l2 = [str(integer) for integer in l2]
    string_l2 = "".join(s_l2[::-1])
    res = int(string_l1) + int(string_l2)
    a = [x for x in str(res)[::-1]]
    print(a)
278/326:
l1 = [2,4,3]
l2 = [5,6,4]
278/327: addTwoNumbers(l1, l2)
278/328:
l1 = [9,9,9,9,9,9,9]
l2 = [9,9,9,9]
278/329: addTwoNumbers(l1, l2)
278/330:
def addTwoNumbers(l1, l2):
    s_l1 = [str(integer) for integer in l1]
    string_l1 = "".join(s_l1[::-1])
    s_l2 = [str(integer) for integer in l2]
    string_l2 = "".join(s_l2[::-1])
    res = int(string_l1) + int(string_l2)
    a = [x for x in str(res)[::-1]]
    return a
278/331:
l1 = [9,9,9,9,9,9,9]
l2 = [9,9,9,9]
278/332: addTwoNumbers(l1, l2)
279/1: import pandas as pd
279/2:
df = pd.read_csv(
    "../input/ts-course-data/book_sales.csv",
    index_col='Date',
    parse_dates=['Date'],
).drop('Paperback', axis=1)
279/3:
df = pd.read_csv(
    "../data/book_sales.csv",
    index_col='Date',
    parse_dates=['Date'],
).drop('Paperback', axis=1)
279/4: df
279/5:
df = pd.read_csv(
    "../data/book_sales.csv",
    index_col='Date',
    parse_dates=['Date'],
)
279/6: df
279/7:
df = pd.read_csv(
    "../data/book_sales.csv",
    index_col='Date',
    parse_dates=['Date'],
).drop('Paperback', axis=1)
279/8: df
279/9: df.head(5)
279/10:
import pandas as pd
import numpy as np
279/11: df['Time'] = np.arange(len(df.index))
279/12:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns
279/13:
plt.style.use("seaborn-whitegrid")

fig, ax = plt.subplots()
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Hardcover Sales');
279/14:
plt.style.use("seaborn-whitegrid")
plt.figure(figsize=(15, 8))

fig, ax = plt.subplots()
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Hardcover Sales');
279/15:
plt.style.use("seaborn-whitegrid")
plt.figure(figsize=(5, 8))

fig, ax = plt.subplots()
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Hardcover Sales');
279/16:
plt.style.use("seaborn-whitegrid")
plt.figure(figsize=(5, 8))

fig, ax = plt.subplots()
fig.figsize = (3,3)
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Hardcover Sales');
279/17:
plt.style.use("seaborn-whitegrid")

fig, ax = plt.subplots(figsize = (10, 5))
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Hardcover Sales');
279/18:
plt.style.use("seaborn-whitegrid")

fig, ax = plt.subplots(figsize = (10, 5), titleweight='bold')
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Hardcover Sales');
279/19:
plt.style.use("seaborn-whitegrid")

fig, ax = plt.subplots(figsize = (10, 5))
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Hardcover Sales');
279/20:
plt.style.use("seaborn-whitegrid")

fig, ax = plt.subplots(figsize = (10, 5))
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Hardcover Sales',fontweight='bold');
279/21:
plt.style.use("seaborn-whitegrid")

fig, ax = plt.subplots(figsize = (10, 5))
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'),fontweight='bold')
ax.set_title('Time Plot of Hardcover Sales', fontweight='bold');
279/22:
plt.style.use("seaborn-whitegrid")

fig, ax = plt.subplots(figsize = (10, 5))
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Hardcover Sales');
279/23:
plt.style.use("seaborn-whitegrid")
plt.rc(
    "figure",
    autolayout=True,
    figsize=(11, 4),
    titlesize=18,
    titleweight='bold',
)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=16,
    titlepad=10,
)
fig, ax = plt.subplots(figsize = (10, 5))
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Hardcover Sales');
279/24:
plt.style.use("seaborn-whitegrid")
plt.rc(
    "figure",
    autolayout=True,
    figsize=(11, 4),
    titlesize=18,
    titleweight='bold',
)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=13,
    titlepad=17,
)
fig, ax = plt.subplots(figsize = (10, 5))
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Hardcover Sales');
279/25:
plt.style.use("seaborn-whitegrid")
plt.rc(
    "figure",
    autolayout=True,
    figsize=(11, 4),
    titlesize=18,
    titleweight='bold',
)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=13,
    titlepad=2,
)
fig, ax = plt.subplots(figsize = (10, 5))
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Hardcover Sales');
279/26:
plt.style.use("seaborn-whitegrid")
plt.rc(
    "figure",
    autolayout=True,
    figsize=(11, 4),
    titlesize=18,
    titleweight='bold',
)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=13,
    titlepad=10,
)
fig, ax = plt.subplots(figsize = (10, 5))
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Hardcover Sales');
279/27:
plt.style.use("seaborn-whitegrid")
plt.rc(
    "figure",
    autolayout=True,
    figsize=(11, 4),
    titlesize=14,
    titleweight='bold',
)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=13,
    titlepad=10,
)
fig, ax = plt.subplots(figsize = (10, 5))
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Hardcover Sales');
279/28:
plt.style.use("seaborn-whitegrid")
plt.rc(
    "figure",
    autolayout=True,
    figsize=(11, 4),
    titlesize=4,
    titleweight='bold',
)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=13,
    titlepad=10,
)
fig, ax = plt.subplots(figsize = (10, 5))
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Hardcover Sales');
279/29:
plt.style.use("seaborn-whitegrid")
plt.rc(
    "figure",
    autolayout=True,
    figsize=(11, 4),
    titlesize=54,
    titleweight='bold',
)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=13,
    titlepad=10,
)
fig, ax = plt.subplots(figsize = (10, 5))
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Hardcover Sales');
279/30:
plt.style.use("seaborn-whitegrid")
plt.rc(
    "figure",
    autolayout=True,
    figsize=(11, 4),
    titlesize=54,
    titleweight='bold',
)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=13,
    titlepad=10,
)
fig, ax = plt.subplots()
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Hardcover Sales');
279/31:
plt.style.use("seaborn-whitegrid")
plt.rc(
    "figure",
    autolayout=True,
    figsize=(11, 4),
    titlesize=44,
    titleweight='bold',
)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=13,
    titlepad=10,
)
fig, ax = plt.subplots()
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Hardcover Sales');
279/32:
plt.style.use("seaborn-whitegrid")
plt.rc(
    "figure",
    autolayout=True,
    figsize=(11, 4),
    titlesize=44,
    titleweight='bold',
)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=43,
    titlepad=10,
)
fig, ax = plt.subplots()
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Hardcover Sales');
279/33:
plt.style.use("seaborn-whitegrid")
plt.rc(
    "figure",
    autolayout=True,
    figsize=(11, 4),
    titleweight='bold',
)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=13,
    titlepad=10,
)
fig, ax = plt.subplots()
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Hardcover Sales');
279/34:
plt.style.use("seaborn-whitegrid")
plt.rc(
    "figure",
    autolayout=True,
    figsize=(11, 4),

)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=13,
    titlepad=10,
)
fig, ax = plt.subplots()
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Hardcover Sales');
279/35:
plt.style.use("seaborn-whitegrid")
plt.rc(
    "figure",
    autolayout=True,
    figsize=(11, 4),
    titleweight='bold',
)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=13,
    titlepad=10,
)
fig, ax = plt.subplots()
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Hardcover Sales');
279/36:
plt.style.use("seaborn-whitegrid")
plt.rc(
    "figure",
    autolayout=True,
    figsize=(10, 6),
    titleweight='bold',
)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=13,
    titlepad=10,
)
fig, ax = plt.subplots()
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Hardcover Sales');
279/37:
plt.style.use("seaborn-whitegrid")
plt.rc(
    "figure",
    autolayout=True,
    figsize=(10, 6),
    titleweight='bold',
)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=13,
    titlepad=10,
)
fig, ax = plt.subplots()
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'), scatter_kws={"color": "black"})
ax.set_title('Time Plot of Hardcover Sales');
279/38:
plt.style.use("seaborn-whitegrid")
plt.rc(
    "figure",
    autolayout=True,
    figsize=(10, 6),
    titleweight='bold',
)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=13,
    titlepad=10,
)
fig, ax = plt.subplots()
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Hardcover Sales');
279/39:
plt.style.use("seaborn-whitegrid")
plt.rc(
    "figure",
    autolayout=True,
    figsize=(10, 6),
    titleweight='bold',
)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=13,
    titlepad=10,
)
fig, ax = plt.subplots()
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='r'))
ax.set_title('Time Plot of Hardcover Sales');
279/40:
plt.style.use("seaborn-whitegrid")
plt.rc(
    "figure",
    autolayout=True,
    figsize=(10, 6),
    titleweight='bold',
)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=13,
    titlepad=10,
)
fig, ax = plt.subplots()
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='r'), line_kws={dict(color='b')})
ax.set_title('Time Plot of Hardcover Sales');
279/41:
plt.style.use("seaborn-whitegrid")
plt.rc(
    "figure",
    autolayout=True,
    figsize=(10, 6),
    titleweight='bold',
)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=13,
    titlepad=10,
)
fig, ax = plt.subplots()
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='r'), line_kws=dict(color='b'))
ax.set_title('Time Plot of Hardcover Sales');
279/42:
plt.style.use("seaborn-whitegrid")
plt.rc(
    "figure",
    autolayout=True,
    figsize=(10, 6),
    titleweight='bold',
)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=13,
    titlepad=10,
)
fig, ax = plt.subplots()
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='r'), line_kws=dict(color='y'))
ax.set_title('Time Plot of Hardcover Sales');
279/43:
plt.style.use("seaborn-whitegrid")
plt.rc(
    "figure",
    autolayout=True,
    figsize=(10, 6),
    titleweight='bold',
)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=13,
    titlepad=10,
)
fig, ax = plt.subplots()
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, scatter_kws=dict(color='r'), line_kws=dict(color='y'))
ax.set_title('Time Plot of Hardcover Sales');
279/44:
plt.style.use("seaborn-whitegrid")
plt.rc(
    "figure",
    autolayout=True,
    figsize=(10, 6),
    titleweight='bold',
)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=13,
    titlepad=10,
)
fig, ax = plt.subplots()
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='r'), line_kws=dict(color='y'))
ax.set_title('Time Plot of Hardcover Sales');
279/45:
plt.style.use("seaborn-whitegrid")
plt.rc(
    "figure",
    autolayout=True,
    figsize=(10, 6),
    titleweight='bold',
)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=13,
    titlepad=10,
)
fig, ax = plt.subplots()
ax.plot('Time', 'Hardcover', data=df, color='0.795')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='r'), line_kws=dict(color='b'))
ax.set_title('Time Plot of Hardcover Sales');
279/46:
plt.style.use("seaborn-whitegrid")
plt.rc(
    "figure",
    autolayout=True,
    figsize=(10, 6),
    titleweight='bold',
)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=13,
    titlepad=10,
)
fig, ax = plt.subplots()
ax.plot('Time', 'Hardcover', data=df, color='0.95')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='r'), line_kws=dict(color='b'))
ax.set_title('Time Plot of Hardcover Sales');
279/47:
plt.style.use("seaborn-whitegrid")
plt.rc(
    "figure",
    autolayout=True,
    figsize=(10, 6),
    titleweight='bold',
)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=13,
    titlepad=10,
)
fig, ax = plt.subplots()
ax.plot('Time', 'Hardcover', data=df, color='0.25')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='r'), line_kws=dict(color='b'))
ax.set_title('Time Plot of Hardcover Sales');
279/48:
plt.style.use("seaborn-whitegrid")
plt.rc(
    "figure",
    autolayout=True,
    figsize=(10, 6),
    titleweight='bold',
)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=13,
    titlepad=10,
)
fig, ax = plt.subplots()
ax.plot('Time', 'Hardcover', data=df, color='0.65')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='r'), line_kws=dict(color='b'))
ax.set_title('Time Plot of Hardcover Sales');
279/49:
df['Lag_1'] = df['Hardcover'].shift(1)
df = df.reindex(columns=['Hardcover', 'Lag_1'])

df.head()
279/50:
fig, ax = plt.subplots()
ax = sns.regplot(x='Lag_1', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'))
ax.set_aspect('equal')
ax.set_title('Lag Plot of Hardcover Sales')
280/1:
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
280/2: df = pd.read_csv(../data/'tunnel.csv')
280/3: df = pd.read_csv('../data/tunnel.csv')
280/4: df
280/5: df = pd.read_csv('../data/tunnel.csv', index_col='Day')
280/6: df
280/7: df.to_period()
280/8: df = pd.read_csv('../data/tunnel.csv', parse_dates=["Day"], index_col='Day')
280/9: df.to_period()
280/10: df = pd.read_csv('../data/tunnel.csv', index_col='Day')
280/11: df.dtypes
280/12: df.index.dtypes
280/13: df.index.dtype
280/14: df = pd.read_csv('../data/tunnel.csv', parse_dates=["Day"], index_col='Day')
280/15: df.index.dtype
280/16: df.to_period()
280/17:
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
280/18: df = pd.read_csv('../data/tunnel.csv', parse_dates=["Day"], index_col='Day')
280/19: –≤–∞
280/20: df
280/21: df.index.dtype
280/22: df = df.to_period()
280/23: df.index.dtype
280/24:
#create a time dummy by counting out the length of the series
df_dummy = df.copy()
df_dummy['Time'] = np.arange(len(df.index))
df_dummy.head()
280/25:
from sklearn.linear_model import LinearRegression

# Training data
X = df_dummy.loc[:, ['Time']]  # features
y = df_dummy.loc[:, 'NumVehicles']  # target

# Train the model
model = LinearRegression()
model.fit(X, y)

# Store the fitted values as a time series with the same time index as
# the training data
y_pred = pd.Series(model.predict(X), index=X.index)
280/26: model.coef_
280/27: print(model.coef_, model.intercept_)
280/28: print('Reg formula: Vehicles = {0} * Time + 98176'.format(3))
280/29: print('Reg formula: Vehicles = {0} * Time + {1}'.format(model.coef_, model.intercept_))
280/30: print('Reg formula: Vehicles = {0} * Time + {1}'.format(model.coef_[0], model.intercept_))
280/31: print('Reg formula: Vehicles = {0}.2f * Time + {1}'.format(model.coef_[0], model.intercept_))
280/32: print('Reg formula: Vehicles = {0:.2f} * Time + {1}'.format(model.coef_[0], model.intercept_))
280/33: print('Reg formula: Vehicles = {0:.2f} * Time + {1:.2f}'.format(model.coef_[0], model.intercept_))
280/34:
# Set Matplotlib defaults
plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True, figsize=(11, 4))
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)
plot_params = dict(
    color="0.75",
    style=".-",
    markeredgecolor="0.25",
    markerfacecolor="0.25",
    legend=False,
)
280/35:
ax = y.plot(**plot_params)
ax = y_pred.plot(ax=ax, linewidth=3)
ax.set_title('Time Plot of Tunnel Traffic')
280/36:
# Set Matplotlib defaults
plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True, figsize=(11, 4))
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)
plot_params = dict(
    color="0.75",
    style=".-",
    markeredgecolor="0.25",
    markerfacecolor="0.25",
    legend=False,
)
ax = y_pred.plot(ax=ax, linewidth=3)
ax.set_title('Time Plot of Tunnel Traffic'
280/37:
# Set Matplotlib defaults
plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True, figsize=(11, 4))
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)
plot_params = dict(
    color="0.75",
    style=".-",
    markeredgecolor="0.25",
    markerfacecolor="0.25",
    legend=False,
)
ax = y.plot(**plot_params)
ax = y_pred.plot(ax=ax, linewidth=3)
ax.set_title('Time Plot of Tunnel Traffic')
280/38:
# Set Matplotlib defaults
plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True, figsize=(11, 4))
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)
plot_params = dict(
    color="0.75",
    style=".-",
    markeredgecolor="0.25",
    markerfacecolor="0.25",
    legend=False,
)
280/39:
#ax = y.plot(**plot_params)
ax = y_pred.plot(ax=ax, linewidth=3)
ax.set_title('Time Plot of Tunnel Traffic')
280/40:
ax = y.plot(**plot_params)
ax = y_pred.plot(ax=ax, linewidth=3)
ax.set_title('Time Plot of Tunnel Traffic')
280/41: df_dummy
280/42:
plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True, figsize=(11, 4))
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)
plot_params = dict(
    color="0.75",
    style=".-",
    markeredgecolor="0.25",
    markerfacecolor="0.25",
    legend=False,
)

fig, ax = plt.subplots()
ax = sns.regplot(x='Day', y='NumVehicles', data=df_dummy, ci=None, scatter_kws=dict(color='0.25'))
ax = y_pred.plot(ax=ax, linewidth=3)
ax.set_title('Time Plot of Tunnel Traffic')
280/43:
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
280/44:
plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True, figsize=(11, 4))
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)
plot_params = dict(
    color="0.75",
    style=".-",
    markeredgecolor="0.25",
    markerfacecolor="0.25",
    legend=False,
)

fig, ax = plt.subplots()
ax = sns.regplot(x='Day', y='NumVehicles', data=df_dummy, ci=None, scatter_kws=dict(color='0.25'))
ax = y_pred.plot(ax=ax, linewidth=3)
ax.set_title('Time Plot of Tunnel Traffic')
280/45:
plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True, figsize=(11, 4))
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)
plot_params = dict(
    color="0.75",
    style=".-",
    markeredgecolor="0.25",
    markerfacecolor="0.25",
    legend=False,
)

fig, ax = plt.subplots()
ax = sns.regplot(x=df.index, y='NumVehicles', data=df_dummy, ci=None, scatter_kws=dict(color='0.25'))
ax = y_pred.plot(ax=ax, linewidth=3)
ax.set_title('Time Plot of Tunnel Traffic')
280/46:
plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True, figsize=(11, 4))
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)
plot_params = dict(
    color="0.75",
    style=".-",
    markeredgecolor="0.25",
    markerfacecolor="0.25",
    legend=False,
)

fig, ax = plt.subplots()
ax = sns.regplot(x=df_dummy.index, y='NumVehicles', data=df_dummy, ci=None, scatter_kws=dict(color='0.25'))
ax = y_pred.plot(ax=ax, linewidth=3)
ax.set_title('Time Plot of Tunnel Traffic')
280/47:
plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True, figsize=(11, 4))
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)
plot_params = dict(
    color="0.75",
    style=".-",
    markeredgecolor="0.25",
    markerfacecolor="0.25",
    legend=False,
)

fig, ax = plt.subplots()
ax = sns.regplot(x=df.index, y='NumVehicles', data=df_dummy, ci=None, scatter_kws=dict(color='0.25'))
ax = y_pred.plot(ax=ax, linewidth=3)
ax.set_title('Time Plot of Tunnel Traffic')
280/48:
plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True, figsize=(11, 4))
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)
plot_params = dict(
    color="0.75",
    style=".-",
    markeredgecolor="0.25",
    markerfacecolor="0.25",
    legend=False,
)

fig, ax = plt.subplots()
ax = sns.regplot(x=df.index.to_timestamp(), y='NumVehicles', data=df_dummy, ci=None, scatter_kws=dict(color='0.25'))
ax = y_pred.plot(ax=ax, linewidth=3)
ax.set_title('Time Plot of Tunnel Traffic')
280/49:
plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True, figsize=(11, 4))
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)
plot_params = dict(
    color="0.75",
    style=".-",
    markeredgecolor="0.25",
    markerfacecolor="0.25",
    legend=False,
)

fig, ax = plt.subplots()
#ax = sns.regplot(x=df.index.to_timestamp(), y='NumVehicles', data=df_dummy, ci=None, scatter_kws=dict(color='0.25'))
ax = y_pred.plot(ax=ax, linewidth=3)
ax.set_title('Time Plot of Tunnel Traffic')
280/50:
plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True, figsize=(11, 4))
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)
plot_params = dict(
    color="0.75",
    style=".-",
    markeredgecolor="0.25",
    markerfacecolor="0.25",
    legend=False,
)

fig, ax = plt.subplots()
ax.plot(ax=y_pred, linewidth=3)
ax = sns.regplot(x=df.index.to_timestamp(), y='NumVehicles', data=df_dummy, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Tunnel Traffic')
280/51:
plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True, figsize=(11, 4))
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)
plot_params = dict(
    color="0.75",
    style=".-",
    markeredgecolor="0.25",
    markerfacecolor="0.25",
    legend=False,
)

fig, ax = plt.subplots()
ax.plot(ax=y_pred, linewidth=3)
#ax = sns.regplot(x=df.index.to_timestamp(), y='NumVehicles', data=df_dummy, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Tunnel Traffic')
280/52:
plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True, figsize=(11, 4))
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)
plot_params = dict(
    color="0.75",
    style=".-",
    markeredgecolor="0.25",
    markerfacecolor="0.25",
    legend=False,
)

fig, ax = plt.subplots()
ax.plot(x=y_pred, linewidth=3)
#ax = sns.regplot(x=df.index.to_timestamp(), y='NumVehicles', data=df_dummy, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Tunnel Traffic')
280/53:
plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True, figsize=(11, 4))
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)
plot_params = dict(
    color="0.75",
    style=".-",
    markeredgecolor="0.25",
    markerfacecolor="0.25",
    legend=False,
)

fig, ax = plt.subplots()
ax.plot(ax=y_pred, linewidth=3)
ax = sns.regplot(x=df.index.to_timestamp(), y='NumVehicles', data=df_dummy, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Tunnel Traffic')
280/54:
plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True, figsize=(11, 4))
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)
plot_params = dict(
    color="0.75",
    style=".-",
    markeredgecolor="0.25",
    markerfacecolor="0.25",
    legend=False,
)

fig, ax = plt.subplots()
ax = sns.regplot(x=df.index.to_timestamp(), y='NumVehicles', data=df_dummy, ci=None, scatter_kws=dict(color='0.25'))
ax = y_pred.plot(ax=ax, linewidth=3)
ax.set_title('Time Plot of Tunnel Traffic')
280/55:
plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True, figsize=(11, 4))
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)
plot_params = dict(
    color="0.75",
    style=".-",
    markeredgecolor="0.25",
    markerfacecolor="0.25",
    legend=False,
)

fig, ax = plt.subplots()
ax = y_pred.plot(ax=ax, linewidth=3)
#ax = sns.regplot(x=df.index.to_timestamp(), y='NumVehicles', data=df_dummy, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Tunnel Traffic')
280/56:
plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True, figsize=(11, 4))
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)
plot_params = dict(
    color="0.75",
    style=".-",
    markeredgecolor="0.25",
    markerfacecolor="0.25",
    legend=False,
)

fig, ax = plt.subplots()
ax = y_pred.plot(ax=ax, linewidth=3)
ax = sns.regplot(y='NumVehicles', data=df_dummy, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Tunnel Traffic')
280/57:
plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True, figsize=(11, 4))
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)
plot_params = dict(
    color="0.75",
    style=".-",
    markeredgecolor="0.25",
    markerfacecolor="0.25",
    legend=False,
)

fig, ax = plt.subplots()
ax = y_pred.plot(ax=ax, linewidth=3)
ax = sns.regplot(x=df.index.to_timestamp(), y='NumVehicles', data=df_dummy, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Tunnel Traffic')
280/58:
plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True, figsize=(11, 4))
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)
plot_params = dict(
    color="0.75",
    style=".-",
    markeredgecolor="0.25",
    markerfacecolor="0.25",
    legend=False,
)

fig, ax = plt.subplots()
ax = y_pred.plot(ax=ax, linewidth=3)
ax = sns.regplot(x=df.index.to_timestamp(), 
                 y='NumVehicles', 
                 data=df_dummy, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Tunnel Traffic')
280/59:
plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True, figsize=(11, 4))
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)
plot_params = dict(
    color="0.75",
    style=".-",
    markeredgecolor="0.25",
    markerfacecolor="0.25",
    legend=False,
)

fig, ax = plt.subplots()
ax = y_pred.plot(ax=ax, linewidth=3)
ax = sns.regplot(x=df.index.to_timestamp(), 
                 y='NumVehicles', 
                 data=df_dummy, 
                 ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Tunnel Traffic')
280/60:
plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True, figsize=(11, 4))
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)
plot_params = dict(
    color="0.75",
    style=".-",
    markeredgecolor="0.25",
    markerfacecolor="0.25",
    legend=False,
)

fig, ax = plt.subplots()
ax = y_pred.plot(ax=ax, linewidth=3)
ax = sns.regplot(x=df.index.to_timestamp(), 
                 y='NumVehicles', 
                 data=df_dummy, 
                 ci=None, 
                 scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Tunnel Traffic')
280/61:
plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True, figsize=(11, 4))
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)
plot_params = dict(
    color="0.75",
    style=".-",
    markeredgecolor="0.25",
    markerfacecolor="0.25",
    legend=False,
)

fig, ax = plt.subplots()
ax = y_pred.plot(ax=ax, linewidth=3)
ax = sns.regplot(x=df.index.to_timestamp(), 
                 y='NumVehicles', 
                 data=df_dummy, 
                 ci=None)
ax.set_title('Time Plot of Tunnel Traffic')
280/62:
plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True, figsize=(11, 4))
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)
plot_params = dict(
    color="0.75",
    style=".-",
    markeredgecolor="0.25",
    markerfacecolor="0.25",
    legend=False,
)

fig, ax = plt.subplots()
ax = y_pred.plot(ax=ax, linewidth=3)
ax = sns.regplot(x=df.index, 
                 y='NumVehicles', 
                 data=df_dummy, 
                 ci=None, 
                 scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Tunnel Traffic')
280/63:
plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True, figsize=(11, 4))
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)
plot_params = dict(
    color="0.75",
    style=".-",
    markeredgecolor="0.25",
    markerfacecolor="0.25",
    legend=False,
)
280/64: df_dummy
280/65:
#Lag feature
df_dummy['Lag_1'] = df_dummy['NumVehicles'].shift(1)
df_dummy.head()
280/66: df.loc[:, ['Lag_1']]
280/67: df_dummy.loc[:, ['Lag_1']]
280/68:
from sklearn.linear_model import LinearRegression

X = df.loc[:, ['Lag_1']]
X.dropna(inplace=True)  # drop missing values in the feature set
y = df.loc[:, 'NumVehicles']  # create the target
280/69:
from sklearn.linear_model import LinearRegression

X = df_dummy.loc[:, ['Lag_1']]
X.dropna(inplace=True)  # drop missing values in the feature set
y = df_dummy.loc[:, 'NumVehicles']  # create the target
280/70: X
280/71: y
280/72: y, X = y.align(X, join='inner')  # drop corresponding values in target
280/73: y
280/74: X
280/75:
model = LinearRegression()
model.fit(X, y)

y_pred = pd.Series(model.predict(X), index=X.index)
280/76:
fig, ax = plt.subplots()
ax.plot(X['Lag_1'], y, '.', color='0.25')
ax.plot(X['Lag_1'], y_pred)
ax.set_aspect('equal')
ax.set_ylabel('NumVehicles')
ax.set_xlabel('Lag_1')
ax.set_title('Lag Plot of Tunnel Traffic');
280/77:
fig, ax = plt.subplots()
ax.plot(X['Lag_1'], y, '.', color='0.25')
ax.plot(X['Lag_1'], y_pred)
ax.set_aspect(30)
ax.set_ylabel('NumVehicles')
ax.set_xlabel('Lag_1')
ax.set_title('Lag Plot of Tunnel Traffic');
280/78:
fig, ax = plt.subplots()
ax.plot(X['Lag_1'], y, '.', color='0.25')
ax.plot(X['Lag_1'], y_pred)
ax.set_aspect(30, 30)
ax.set_ylabel('NumVehicles')
ax.set_xlabel('Lag_1')
ax.set_title('Lag Plot of Tunnel Traffic');
280/79:
fig, ax = plt.subplots()
ax.plot(X['Lag_1'], y, '.', color='0.25')
ax.plot(X['Lag_1'], y_pred)
ax.set_aspect((30, 30))
ax.set_ylabel('NumVehicles')
ax.set_xlabel('Lag_1')
ax.set_title('Lag Plot of Tunnel Traffic');
280/80:
fig, ax = plt.subplots()
ax.plot(X['Lag_1'], y, '.', color='0.25')
ax.plot(X['Lag_1'], y_pred)
ax.set_aspect((10,10))
ax.set_ylabel('NumVehicles')
ax.set_xlabel('Lag_1')
ax.set_title('Lag Plot of Tunnel Traffic');
280/81:
fig, ax = plt.subplots()
ax.plot(X['Lag_1'], y, '.', color='0.25')
ax.plot(X['Lag_1'], y_pred)
ax.set_aspect((10))
ax.set_ylabel('NumVehicles')
ax.set_xlabel('Lag_1')
ax.set_title('Lag Plot of Tunnel Traffic');
280/82:
fig, ax = plt.subplots()
ax.plot(X['Lag_1'], y, '.', color='0.25')
ax.plot(X['Lag_1'], y_pred)
ax.set_aspect((10), (10))
ax.set_ylabel('NumVehicles')
ax.set_xlabel('Lag_1')
ax.set_title('Lag Plot of Tunnel Traffic');
280/83:
fig, ax = plt.subplots()
ax.plot(X['Lag_1'], y, '.', color='0.25')
ax.plot(X['Lag_1'], y_pred)
ax.set_aspect(1)
ax.set_ylabel('NumVehicles')
ax.set_xlabel('Lag_1')
ax.set_title('Lag Plot of Tunnel Traffic');
280/84:
fig, ax = plt.subplots()
ax.plot(X['Lag_1'], y, '.', color='0.25')
ax.plot(X['Lag_1'], y_pred)
ax.set_aspect(2)
ax.set_ylabel('NumVehicles')
ax.set_xlabel('Lag_1')
ax.set_title('Lag Plot of Tunnel Traffic');
280/85:
fig, ax = plt.subplots()
ax.plot(X['Lag_1'], y, '.', color='0.25')
ax.plot(X['Lag_1'], y_pred)
ax.set_aspect(.3)
ax.set_ylabel('NumVehicles')
ax.set_xlabel('Lag_1')
ax.set_title('Lag Plot of Tunnel Traffic');
280/86:
fig, ax = plt.subplots()
ax.plot(X['Lag_1'], y, '.', color='0.25')
ax.plot(X['Lag_1'], y_pred)
ax.set_aspect(.5)
ax.set_ylabel('NumVehicles')
ax.set_xlabel('Lag_1')
ax.set_title('Lag Plot of Tunnel Traffic');
280/87:
fig, ax = plt.subplots()
ax.plot(X['Lag_1'], y, '.', color='0.25')
ax.plot(X['Lag_1'], y_pred)
ax.set_aspect(1)
ax.set_ylabel('NumVehicles')
ax.set_xlabel('Lag_1')
ax.set_title('Lag Plot of Tunnel Traffic');
280/88:
ax = y.plot(**plot_params)
ax = y_pred.plot()
281/1:
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
281/2:
plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True, figsize=(11, 5))
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)
plot_params = dict(
    color="0.75",
    style=".-",
    markeredgecolor="0.25",
    markerfacecolor="0.25",
    legend=False,
)
281/3: df = pd.read_csv('../data/tunnel.csv', parse_dates=['Day'] index='Day')
281/4: df = pd.read_csv('../data/tunnel.csv', parse_dates=['Day'], index='Day')
281/5: df = pd.read_csv('../data/tunnel.csv', parse_dates=['Day'], set_index='Day')
281/6: df = pd.read_csv('../data/tunnel.csv', parse_dates=['Day'], index_col='Day')
281/7: df
281/8: df = df.to_period()
281/9: df
281/10: df.plot()
281/11:
df.rolling(window=365,
          center=True,
          min_periods=182
          ).mean()
281/12:
moving_avg = df.rolling(window=365,
                        center=True,
                        min_periods=182
                       ).mean()
281/13: moving_avg
281/14: ax = moving_avg.plot(style='.', color='r')
281/15: ax = moving_avg.plot(style='.-', color='r')
281/16: ax = moving_avg.plot(style=':', color='r')
281/17:
ax = moving_avg.plot(style=':', color='r')
moving_avg.plot()
281/18:
ax = moving_avg.plot(style=':', color='r')
moving_avg.plot(ax = ax)
281/19:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax)
281/20:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, style='-')
281/21:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, style='-.')
281/22:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, style='-')
281/23:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, style='-', linewidth = 1)
281/24:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, style='-', linewidth = 1, color = .5)
281/25:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, style='-', linewidth = 1, legend=False)
281/26:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, style='-', linewidth = 1, legend=False, color='.5')
281/27:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, style='-', linewidth = 1, legend=False, color='.2')
281/28:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, style='-', linewidth = 1, legend=False, color='.8')
281/29:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, style='-', linewidth = 1, legend=False, color='.7')
281/30:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, style='-', linewidth = 1, legend=False, color='.7b')
281/31:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, style='-', linewidth = 1, legend=False, color='.7')
281/32:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, style='-', linewidth = 1, legend=False, alpha=.6)
281/33:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, style='-', linewidth = 1, legend=False, alpha=.6, color=√Ω)
281/34:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, style='-', linewidth = 1, legend=False, alpha=.6, color='√Ω')
281/35:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, style='-', linewidth = 1, legend=False, alpha=.6, color='y')
281/36:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, style='-', linewidth = 1, legend=False, alpha=.6, color='lightblue')
281/37:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, style='-', linewidth = 1, legend=False, alpha=.6, color='hotpink')
281/38:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, linewidth = 1, legend=False, alpha=.6, color='hotpink')
281/39:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax=ax, linewidth=3, title="Tunnel Traffic - 365-Day Moving Average", legend=False,)
281/40:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, linewidth = 1, legend=False, alpha=.6, color='hotpink')
281/41:
ax = moving_avg.plot(style=':', color='r')
df.scutterplot(ax = ax, linewidth = 1, legend=False, alpha=.6, color='hotpink')
281/42:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, linewidth = 1, legend=False, alpha=.6, color='hotpink')
281/43:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, marker='.', linestyle='none', legend=False, alpha=.6, color='hotpink')
281/44:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, marker='s', linestyle='none', legend=False, alpha=.6, color='hotpink')
281/45:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, marker='c', linestyle='none', legend=False, alpha=.6, color='hotpink')
281/46:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, marker='o', linestyle='none', legend=False, alpha=.6, color='hotpink')
281/47:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, marker='o', linestyle='none', legend=False, alpha=.3, color='hotpink')
281/48:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, marker='o', linestyle='none', legend=False, alpha=.3, color='blue')
281/49:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, marker='o', linestyle='none', legend=False, alpha=.1, color='blue')
281/50:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, marker='o', linestyle='none', legend=False, alpha=.1, color='lightblue')
281/51:
ax = moving_avg.plot(style=':', color='r')
df.plot(ax = ax, marker='o', linestyle='none', legend=False, alpha=.31, color='lightblue')
281/52:
ax = moving_avg.plot(style='--', color='r')
df.plot(ax = ax, marker='o', linestyle='none', legend=False, alpha=.31, color='lightblue')
281/53:
ax = moving_avg.plot(style='-', color='r')
df.plot(ax = ax, marker='o', linestyle='none', legend=False, alpha=.31, color='lightblue')
281/54:
ax = moving_avg.plot(style='-', color='r', linewidth = 2)
df.plot(ax = ax, marker='o', linestyle='none', legend=False, alpha=.31, color='lightblue')
281/55:
ax = moving_avg.plot(style='-', color='b', linewidth = 2)
df.plot(ax = ax, marker='o', linestyle='none', legend=False, alpha=.31, color='lightblue')
281/56:
ax = moving_avg.plot(style='-', color='b', linewidth = 2)
df.plot(ax = ax, marker='o', linestyle='none', legend=False, alpha=.31, color='crimson')
281/57:
ax = moving_avg.plot(style='-', color='crimson', linewidth = 2)
df.plot(ax = ax, marker='o', linestyle='none', legend=False, alpha=.31, color='lightblue')
281/58: from statsmodels.tsa.deterministic import DeterministicProcess
281/59: import statsmodels
281/60: from statsmodels.tsa.deterministic import DeterministicProcess
281/61: !pip install statsmodels
281/62: !conda install statsmodels
283/1: import statsmodels
283/2: from statsmodels.tsa.deterministic import DeterministicProcess
284/1: import statsmodels
284/2: from statsmodels.tsa.deterministic import DeterministicProcess
284/3:
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
284/4:
plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True, figsize=(11, 5))
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)
plot_params = dict(
    color="0.75",
    style=".-",
    markeredgecolor="0.25",
    markerfacecolor="0.25",
    legend=False,
)
284/5: df = pd.read_csv('../data/tunnel.csv', parse_dates=['Day'], index_col='Day')
284/6: df = df.to_period()
284/7: df.plot()
284/8:
moving_avg = df.rolling(window=365,
                        center=True,
                        min_periods=182
                       ).mean()
284/9:
ax = moving_avg.plot(style='-', color='crimson', linewidth = 2)
df.plot(ax = ax, marker='o', linestyle='none', legend=False, alpha=.31, color='lightblue')
284/10: import statsmodels
284/11: from statsmodels.tsa.deterministic import DeterministicProcess
284/12:
dp = DeterministicProcess(
    index=tunnel.index,  # dates from the training data
    constant=True,       # dummy feature for the bias (y_intercept)
    order=1,             # the time dummy (trend)
    drop=True,           # drop terms if necessary to avoid collinearity
)
# `in_sample` creates features for the dates given in the `index` argument
X = dp.in_sample()

X.head()
284/13:
dp = DeterministicProcess(
    index=df.index,  # dates from the training data
    constant=True,       # dummy feature for the bias (y_intercept)
    order=1,             # the time dummy (trend)
    drop=True,           # drop terms if necessary to avoid collinearity
)
# `in_sample` creates features for the dates given in the `index` argument
X = dp.in_sample()

X.head()
284/14: X
284/15: X.describe
284/16: X.describe()
284/17:
from sklearn.linear_model import LinearRegression

y = df["NumVehicles"]  # the target

# The intercept is the same as the `const` feature from
# DeterministicProcess. LinearRegression behaves badly with duplicated
# features, so we need to be sure to exclude it here.
model = LinearRegression(fit_intercept=False)
model.fit(X, y)

y_pred = pd.Series(model.predict(X), index=X.index)
284/18:
ax = tunnel.plot(style=".", color="0.5", title="Tunnel Traffic - Linear Trend")
_ = y_pred.plot(ax=ax, linewidth=3, label="Trend")
284/19:
ax = df.plot(style=".", color="0.5", title="Tunnel Traffic - Linear Trend")
_ = y_pred.plot(ax=ax, linewidth=3, label="Trend")
284/20:
ax = df.plot(style=".", color="0.5", title="Tunnel Traffic - Linear Trend")
ax = y_pred.plot(ax=ax, linewidth=3, label="Trend")
284/21:
ax = df.plot(style=".", color="0.5", title="Tunnel Traffic - Linear Trend")
_ = y_pred.plot(ax=ax, linewidth=3, label="Trend")
284/22: X
284/23: X = dp.out_of_sample(steps=30)
284/24: X
284/25: dp
284/26: dp.head()
284/27: dp.show()
284/28: y_force = pd.Series(model.predict(X), index=X.index)
284/29:
y_force = pd.Series(model.predict(X), index=X.index)
print(y_force)
284/30:
ax = tunnel["2005-05":].plot(title="Tunnel Traffic - Linear Trend Forecast", **plot_params)
ax = y_pred["2005-05":].plot(ax=ax, linewidth=3, label="Trend")
ax = y_fore.plot(ax=ax, linewidth=3, label="Trend Forecast", color="C3")
_ = ax.legend()
284/31:
ax = df["2005-05":].plot(title="Tunnel Traffic - Linear Trend Forecast", **plot_params)
ax = y_pred["2005-05":].plot(ax=ax, linewidth=3, label="Trend")
ax = y_fore.plot(ax=ax, linewidth=3, label="Trend Forecast", color="C3")
_ = ax.legend()
284/32:
y_fore = pd.Series(model.predict(X), index=X.index)
print(y_force)
284/33:
ax = df["2005-05":].plot(title="Tunnel Traffic - Linear Trend Forecast", **plot_params)
ax = y_pred["2005-05":].plot(ax=ax, linewidth=3, label="Trend")
ax = y_fore.plot(ax=ax, linewidth=3, label="Trend Forecast", color="C3")
_ = ax.legend()
284/34: df
284/35: df['2003-03', :]
284/36: df['2003-03':]
284/37: df['2004-03':]
284/38: df['2004-03':'2004-04']
284/39: df['2004-03':'2004-03']
284/40: df['2004-03':'2004-03',,2]
284/41: df['2004-03':'2004-03':2]
284/42: df['2004-03':'2004-03':-2]
284/43: df['2004-03':'2004-03']
284/44: import pyearth
284/45: !conda install pyearth
285/1: a = 234
285/2: str(a)[::-1]
285/3: int(str(a)[::-1])
285/4:
for c in '284':
    c_0 = ''
    print(c)
285/5:
c_0 = ''
for c in '284':
    c_0 = c + c_0
285/6: c_0
285/7:
c_0 = ''
for c in '28401':
    c_0 = c + c_0
285/8: c_0
285/9: reversed('123')
285/10: ''.join(reversed('123'))
285/11:
rev_num = 0
while a > 0:
    remainder = a%10
    rev_num = (rev_num * 10) + remainder
    a = a//10
285/12: rev_num
285/13: a = 123456789
285/14:
rev_num = 0
while a > 0:
    remainder = a%10
    rev_num = (rev_num * 10) + remainder
    a = a//10
285/15: rev_num
285/16: list(34)
285/17: list('34')
285/18: list('34').reverse()
285/19: rev_list = list('34').reverse()
285/20: (strrev_list)
285/21: (str)rev_list
285/22: ''.join(rev_list)
285/23: rev_list = list('34').reverse()
285/24: ''.join(rev_list)
285/25: rev_list
285/26: list('34').reverse()
285/27: list('34')
285/28:
rev_list = list('34')
rev_list = rev_list.reverse()
285/29: ''.join(rev_list)
285/30: rev_list = list('34')
285/31: rev_num
285/32: rev_list = list('34')
285/33: rev_list
285/34: rev_list = rev_list.reverse()
285/35: rev_list
285/36: rev_list = list('34')
285/37: rev_list.reverse()
285/38: rev_list
285/39: ''.join(rev_list)
285/40: str(rev_list)
285/41: [4,5,2,3,4,5,2,3]
285/42: list_1 = [4,5,2,3,4,5,2,3]
285/43: sum(list_1)
285/44: sum(list_1)/len(list_1)
285/45: import numpy as np
285/46: np.mean(list_1)
285/47: my_mean = sum(list_1)/len(list_1)
285/48: my_mean - list_1
285/49:
sum_squars = 0
for l in list_1:
    sum_squars += (l - my_mean)**2
285/50: sum_squars
285/51: sum_squars/len(list_1)
285/52: (sum_squars/len(list_1))**(1/2)
285/53: np.std(list_1)
285/54:
def compute_deviation(list_numbers):
    sum_squars = 0
    mean_num = sum(list_numbers)/len(list_numbers)
    for i in list_numbers:
        sum_squars += (i - mean_num)**2
    list_std = (sum_squars/len(list_numbers))**(1/2)
    return list_std
285/55: compute_deviation(list_1)
285/56:
input = [
    {
        'key': 'list1',
        'values': [4,5,2,3,4,5,2,3],
    },
    {
        'key': 'list2',
        'values': [1,1,34,12,40,3,9,7],
    }
]
285/57:
def compute_deviation(list_numbers):
    sum_squars = 0
    mean_num = sum(list_numbers)/len(list_numbers)
    for i in list_numbers:
        sum_squars += (i - mean_num)**2
    list_std = (sum_squars/len(list_numbers))**(1/2)
    return list_std

for el in input:
    print(el['key'], ':', compute_deviation(el['values']))
285/58:
def compute_deviation(list_numbers):
    sum_squars = 0
    mean_num = sum(list_numbers)/len(list_numbers)
    for i in list_numbers:
        sum_squars += (i - mean_num)**2
    list_std = (sum_squars/len(list_numbers))**(1/2)
    return round(list_std, 2)

for el in input:
    print(el['key'], ':', compute_deviation(el['values']))
285/59: list_1 = [4,5,2,3,4,5,2,3]
285/60: my_mean = sum(list_1)/len(list_1)
285/61: [(i-my_mean)**2 in list_1]
285/62: [(i-my_mean)**2 for i in list_1]
285/63: list_1 = [4,5,2,3,4,5,2,3]
285/64: my_mean = sum(list_1)/len(list_1)
285/65: [(i-my_mean)**2 for i in list_1]
285/66: list_1 = [4,5,2,3,4,5,2,3]
285/67: my_mean = sum(list_1)/len(list_1)
285/68: sum_squars
285/69: mean([(i-my_mean)**2 for i in list_1])
285/70: sum([(i-my_mean)**2 for i in list_1])
285/71:
def compute_deviation(input):
    new_dict = {}
    for el in list_numbers:
        new_dict[el['key']] = std_dev(el['values'])
    return new_dict
285/72: compute_deviation(input)
285/73:
def compute_deviation(list_numbers):
    new_dict = {}
    for el in list_numbers:
        new_dict[el['key']] = std_dev(el['values'])
    return new_dict
285/74: compute_deviation(input)
285/75:
def compute_deviation(list_numbers):
    new_dict = {}
    for el in list_numbers:
        new_dict[el['key']] = compute_deviation(el['values'])
    return new_dict
285/76: compute_deviation(input)
285/77:
def compute_deviation(list_numbers):
    new_dict = {}
    for el in list_numbers:
        compute_deviation(el['values']
        new_dict[el['key']] = compute_deviation(el['values'])
    return new_dict
285/78:
def compute_deviation(list_numbers):
    new_dict = {}
    for el in list_numbers:
        compute_deviation(el['values'])
        new_dict[el['key']] = compute_deviation(el['values'])
    return new_dict
285/79: compute_deviation(input)
285/80:
def compute_deviation(list_numbers):
    new_dict = {}
    for el in list_numbers:
        print(el['values'])
        new_dict[el['key']] = compute_deviation(el['values'])
    return new_dict
285/81: compute_deviation(input)
285/82:
def compute_deviation(list_numbers):
    new_dict = {}
    for el in list_numbers:
        list_val = el['values']
        print(list_val)
        new_dict[el['key']] = compute_deviation(el['values'])
    return new_dict
285/83: compute_deviation(input)
285/84:
def compute_deviation(list_numbers):
    new_dict = {}
    for el in list_numbers:
        list_val = el['values']
        print(list_val)
        new_dict[el['key']] = compute_deviation(list_val)
    return new_dict
285/85: compute_deviation(input)
285/86:
def compute_deviation(list_numbers):
    new_dict = {}
    for el in list_numbers:
        list_val = el['values']
        print(list_val)
        new_dict[el['key']] = 'compute_deviation(list_val)'
    return new_dict
285/87: compute_deviation(input)
285/88:
def compute_deviation(list_numbers):
    sum_squars = 0
    mean_num = sum(list_numbers)/len(list_numbers)
    for i in list_numbers:
        sum_squars += (i - mean_num)**2
    list_std = (sum_squars/len(list_numbers))**(1/2)
    return round(list_std, 2)

for el in input:
    print(el['key'], ':', compute_deviation(el['values']))
285/89:
def compute_deviation(list_numbers):
    new_dict = {}
    for el in list_numbers:
        list_val = el['values']
        print(list_val)
        new_dict[el['key']] = compute_deviation(list_val)
    return new_dict
285/90: compute_deviation(input)
285/91:
def compute_deviation(list_numbers):
    new_dict = {}
    for el in list_numbers:
        list_val = list(el['values'])
        print(list_val)
        new_dict[el['key']] = compute_deviation(list_val)
    return new_dict
285/92: compute_deviation(input)
285/93:
def compute_deviation(list_numbers):
    sum_squars = 0
    mean_num = sum(list_numbers)/len(list_numbers)
    for i in list_numbers:
        sum_squars += (i - mean_num)**2
    list_std = (sum_squars/len(list_numbers))**(1/2)
    return round(list_std, 2)
285/94:
def compute_deviation(list_numbers):
    new_dict = {}
    for el in list_numbers:
        list_val = list(el['values'])
        print(list_val)
      # new_dict[el['key']] = compute_deviation(list_val)
    return new_dict
285/95: compute_deviation(input)
285/96:
def compute_deviation(list_numbers):
    new_dict = {}
    for el in list_numbers:
        list_val = list(el['values'])
        print(list_val)
        print(compute_deviation(list_val))
      # new_dict[el['key']] = compute_deviation(list_val)
    return new_dict
285/97: compute_deviation(input)
285/98:
def std_dev(list_numbers):
    sum_squars = 0
    mean_num = sum(list_numbers)/len(list_numbers)
    for i in list_numbers:
        sum_squars += (i - mean_num)**2
    list_std = (sum_squars/len(list_numbers))**(1/2)
    return round(list_std, 2)
285/99:
def compute_deviation(list_numbers):
    new_dict = {}
    for el in list_numbers:
        list_val = list(el['values'])
        print(list_val)
       new_dict[el['key']] = std_dev(list_val)
    return new_dict
285/101: compute_deviation(input)
285/102:
def compute_deviation(list_numbers):
    new_dict = {}
    for el in list_numbers:
        list_val = list(el['values'])
        print(list_val)
        new_dict[el['key']] = std_dev(list_val)
    return new_dict
285/103: compute_deviation(input)
285/104: compute_deviation(input)
285/105:
def compute_deviation(list_numbers):
    new_dict = {}
    for el in list_numbers:
        list_val = list(el['values'])
        new_dict[el['key']] = std_dev(list_val)
    return new_dict
285/106: compute_deviation(input)
285/107: np.random.normal(0,1,100)
285/108: samples = np.random.normal(0,1,100)
285/109: samples.hist()
285/110: import pyplot as plt
285/111: import plotly as plt
285/112: plt.hist(samples)
285/113: import matplotlib.pyplot as plt
285/114: plt.hist(samples)
285/115: plt.hist(samples, bins=10)
285/116: plt.hist(samples, bins=30)
285/117: samples = np.random.normal(0,1,1000)
285/118: plt.hist(samples, bins=30)
285/119: a = [0,1,2,3,5,6,7]
285/120:
for i in range(len(a)-1):
    print(i)
285/121:
for i in range(len(a)-1):
    if a[i+1] != a[i] + 1:
        print(a[i] + 1)
285/122:
for i in range(len(a)-1):
    if a[i+1] != a[i] + 1:
        return(a[i] + 1)
285/123:
for i in range(len(a)-1):
    if a[i+1] != a[i] + 1:
        return (a[i] + 1)
285/124:
for i in range(len(a)-1):
    mis = 0
    if a[i+1] != a[i] + 1:
        return (a[i] + 1)
        break
285/125:
for i in range(len(a)-1):
    mis = 0
    if a[i+1] != a[i] + 1:
        mis = (a[i] + 1)
        break
    return mis
285/126:
def mis_val(a):
    for i in range(len(a)-1):
        if a[i+1] != a[i] + 1:
            return (a[i] + 1)
285/127: mis_val(a)
286/1: strings = ["flowers", "flow", "flight"]
286/2:
for x, y in zip(strs[0], strs[-1]):
    print(x,y)
286/3:
for x, y in zip(strings[0], strings[-1]):
    print(x,y)
286/4: strings[-1]
286/5:
for x, y in zip(strings):
    print(x,y)
286/6:
for x, y in zip(strings[0], strings[1], strings[-1]):
    print(x,y)
286/7: zip(strings[0], strings[1], strings[-1])
286/8: list(zip(strings[0], strings[1], strings[-1]))
286/9: a = list(zip(strings[0], strings[1], strings[-1]))
286/10: a[0]
286/11: len(a[0])
286/12: a[0].stype
286/13: a[0].dtype
286/14: set(a[0])
286/15: len(set(a[0]))
286/16:
q = 0
for i in a:
    if len(set(i)) == 0:
        q+=1
286/17: q
286/18:
q = 0
for i in a:
    if len(set(i)) == 1:
        q+=1
286/19: q
286/20: a = list(zip(strings))
286/21: a
286/22: strings = ["flowers", "flow", "flight"]
286/23: a = list(zip(strings))
286/24: a
286/25: strings = ["flowers", "flow", "flight"]
286/26: a = list(zip(strings[0], strings[1], strings[-1]))
286/27: a
289/1:
import numpy as np
import pandas as pd

df = pd.read_csv('../TextFiles/smsspamcollection.tsv', sep='\t')
df.head()
289/2: len(df)
289/3: df.isnull().sum()
289/4: df.isnull().mean()
289/5: df['label'].value_counts()/len(df)
289/6:
import matplotlib.pyplot as plt
%matplotlib inline

#plt.xscale('log')
bins = 1.15**(np.arange(0,50))
plt.hist(df[df['label']=='ham']['length'],bins=bins,alpha=0.8)
plt.hist(df[df['label']=='spam']['length'],bins=bins,alpha=0.8)
plt.legend(('ham','spam'))
plt.show()
289/7: df.length.describe()
289/8: df.dtype
289/9: df.dtype()
289/10: df.dtypes()
289/11: df.columns
289/12: df.dtypes
289/13:
import matplotlib.pyplot as plt
%matplotlib inline

#plt.xscale('log')
#bins = 1.15**(np.arange(0,50))
plt.hist(df[df['label']=='ham']['length'],bins=bins,alpha=0.8)
plt.hist(df[df['label']=='spam']['length'],bins=bins,alpha=0.8)
plt.legend(('ham','spam'))
plt.show()
289/14:
import matplotlib.pyplot as plt
%matplotlib inline

plt.xscale('log')
bins = 1.15**(np.arange(0,50))
plt.hist(df[df['label']=='ham']['length'],bins=bins,alpha=0.8)
plt.hist(df[df['label']=='spam']['length'],bins=bins,alpha=0.8)
plt.legend(('ham','spam'))
plt.show()
289/15:
plt.xscale('log')
bins = 1.5**(np.arange(0,15))
plt.hist(df[df['label']=='ham']['punct'],bins=bins,alpha=0.8)
plt.hist(df[df['label']=='spam']['punct'],bins=bins,alpha=0.8)
plt.legend(('ham','spam'))
plt.show()
289/16:
plt.xscale('log')
#bins = 1.5**(np.arange(0,15))
plt.hist(df[df['label']=='ham']['punct'],bins=bins,alpha=0.8)
plt.hist(df[df['label']=='spam']['punct'],bins=bins,alpha=0.8)
plt.legend(('ham','spam'))
plt.show()
289/17:
# Create Feature and Label sets
X = df[['length','punct']]  # note the double set of brackets
y = df['label']
289/18:
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

print('Training Data Shape:', X_train.shape)
print('Testing Data Shape: ', X_test.shape)
289/19:
from sklearn.linear_model import LogisticRegression

lr_model = LogisticRegression(solver='lbfgs')

lr_model.fit(X_train, y_train)
289/20:
from sklearn import metrics

# Create a prediction set:
predictions = lr_model.predict(X_test)

# Print a confusion matrix
print(metrics.confusion_matrix(y_test,predictions))
289/21:
# You can make the confusion matrix less confusing by adding labels:
df = pd.DataFrame(metrics.confusion_matrix(y_test,predictions), index=['ham','spam'], columns=['ham','spam'])
df
289/22:
from sklearn.naive_bayes import MultinomialNB

nb_model = MultinomialNB()

nb_model.fit(X_train, y_train)
289/23:
predictions = nb_model.predict(X_test)
print(metrics.confusion_matrix(y_test,predictions))
289/24:
pd.DataFrame(metrics.confusion_matrix(y_test,predictions), \
             index=['ham', 'spam'],\
             columns=['ham', 'spam'])
289/25: print(metrics.classification_report(y_test,predictions))
289/26: print(metrics.accuracy_score(y_test,predictions))
289/27: print(metrics.accuracy_score(y_test,predictions))
289/28:
from sklearn.svm import SVC
svc_model = SVC(gamma='auto')
svc_model.fit(X_train,y_train)
289/29:
predictions = svc_model.predict(X_test)
print(metrics.confusion_matrix(y_test,predictions))
290/1:
%%writefile 1.txt
This is a story about cats
our feline pets
Cats are furry animals
290/2:
%%writefile 2.txt
This story is about surfing
Catching waves is fun
Surfing is a popular water sport
290/3:
vocab = {}
i = 1

with open('1.txt') as f:
    x = f.read().lower().split()

for word in x:
    if word in vocab:
        continue
    else:
        vocab[word]=i
        i+=1

print(vocab)
290/4:
with open('2.txt') as f:
    x = f.read().lower().split()

for word in x:
    if word in vocab:
        continue
    else:
        vocab[word]=i
        i+=1

print(vocab)
290/5:
with open('2.txt') as f:
    x = f.read().lower().split()

for word in x:
    if word in vocab:
        continue
    else:
        vocab[word]=i
        i+=1

print(vocab)
290/6: one_my = ['file_name.txt'] + [0] * len(vocab)
290/7:
with open('1.txt') as f:
    x = f.read.lover().split()
290/8:
with open('1.txt') as f:
    x = f.read.lower().split()
290/9:
with open('1.txt') as f:
    x = f.read().lower().split()
290/10: x
290/11: vocab
290/12: vocab['this']
290/13:
with open('1.txt') as f:
    x = f.read().lower().split()
    
for word in x:
    one_my[vocab[word]] += 1
290/14: one_my
290/15:
# map the frequencies of each word in 1.txt to our vector:
with open('1.txt') as f:
    x = f.read().lower().split()
    
for word in x:
    one[vocab[word]]+=1
    
one
290/16:
# Create an empty vector with space for each word in the vocabulary:
one = ['1.txt']+[0]*len(vocab)
one
290/17: one_my = ['file_name.txt'] + [0] * len(vocab)
290/18:
with open('1.txt') as f:
    x = f.read().lower().split()
    
for word in x:
    one_my[vocab[word]] += 1
290/19: one_my
290/20: x
290/21:
# map the frequencies of each word in 1.txt to our vector:
with open('1.txt') as f:
    x = f.read().lower().split()
    
for word in x:
    one[vocab[word]]+=1
    
one
290/22:
# Do the same for the second document:
two = ['2.txt']+[0]*len(vocab)

with open('2.txt') as f:
    x = f.read().lower().split()
    
for word in x:
    two[vocab[word]]+=1
290/23:
# Compare the two vectors:
print(f'{one}\n{two}')
290/24:
# Perform imports and load the dataset:
import numpy as np
import pandas as pd

df = pd.read_csv('../TextFiles/smsspamcollection.tsv', sep='\t')
df.head()
290/25: df.isnull().sum()
290/26: 'qwe'[::-1]
290/27: ''.join(reserved('qwe'))
290/28: ''.join(reversed('qwe'))
290/29:
from sklearn.model_selection import train_test_split

X = df['message']  # this time we want to look at the text
y = df['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
290/30:
from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()

X_train_counts = count_vect.fit_transform(X_train)
X_train_counts.shape
290/31:
from sklearn.feature_extraction.text import TfidfTransformer
tfidf_transformer = TfidfTransformer()

X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
X_train_tfidf.shape
290/32:
from sklearn.svm import LinearSVC
clf = LinearSVC()
clf.fit(X_train_tfidf,y_train)
290/33:
from sklearn.pipeline import Pipeline
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.svm import LinearSVC

text_clf = Pipeline([('tfidf', TfidfVectorizer()),
                     ('clf', LinearSVC()),
])

# Feed the training data through the pipeline
text_clf.fit(X_train, y_train)
290/34:
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()

X_train_tfidf = vectorizer.fit_transform(X_train) # remember to use the original X_train set
X_train_tfidf.shape
290/35: X_train_tfidf[0]
290/36:
from sklearn.svm import LinearSVC
clf = LinearSVC()
clf.fit(X_train_tfidf,y_train)
290/37:
from sklearn.pipeline import Pipeline
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.svm import LinearSVC

text_clf = Pipeline([('tfidf', TfidfVectorizer()),
                     ('clf', LinearSVC()),
])

# Feed the training data through the pipeline
text_clf.fit(X_train, y_train)
290/38:
# Form a prediction set
predictions = text_clf.predict(X_test)
290/39:
# Report the confusion matrix
from sklearn import metrics
print(metrics.confusion_matrix(y_test,predictions))
291/1:
import pandas as pd
import numpy as np

df = pd.read_csv('../TextFiles/moviereviews2.tsv', sep='\t')
291/2:
# Check for NaN values:
df.isnull().mean()
291/3: df.head(3)
291/4: df.head(3)
291/5:
# Check for whitespace strings (it's OK if there aren't any!):

space_lines = []

for i, lab, rv in df.itertuples():
    if str(rv).isspace():
        space_lines.append(i)
291/6: df.dropna(inplace=True)
291/7: df.label.value_counts()
291/8: df.head(3)
291/9:
from sklearn.model_selection import train_test_split
X = df['review']
y = df['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33, random_state = 42)
291/10:
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

text_clf_svc = Pipeline([('tfidf', TfidfVectorizer()),
                         ('clf', LinearSVC())
                        ])

text_clf_svc.fit(X_train, y_train)
291/11:
# Form a prediction set
y_predict = text_clf_svc.predict(X_test)
291/12:
# Report the confusion matrix
print(confusion_matrix(y_test, y_predict))
291/13:
# Print a classification report
print(classification_report(y_test, y_predict))
293/1:
# Import spaCy and load the language library
import spacy
nlp = spacy.load('en_core_web_md')  # make sure to use a larger model!
296/1:
# Import spaCy and load the language library
import spacy
nlp = spacy.load('en_core_web_md')  # make sure to use a larger model!
297/1:
import nltk
nltk.download('vader_lexicon')
297/2:
from nltk.sentiment.vader import SentimentIntensityAnalyzer

sid = SentimentIntensityAnalyzer()
297/3:
a = 'This was a good movie.'
sid.polarity_scores(a)
300/1: import pandas as pd
300/2: df = pd.read_csv('Tweets.csv')
300/3: df
300/4: df.text
300/5: df.describe()
300/6: df
300/7: df.tweet_id.nunique()
300/8: df.tweet_id.duplicated()
300/9: df[df.tweet_id.duplicated()]
300/10: df[df.tweet_id.duplicated()].sort_values('tweet_id')
300/11: df[df.tweet_id == 570268326250745856]
300/12: dupl_id = df[df.tweet_id.duplicated()].tweet_id
300/13: dupl_id
300/14: df[df.tweet_id in dupl_id].sort_values('tweet_id')
300/15: df[df.tweet_id.isin(dupl_id)].sort_values('tweet_id')
300/16: count_words = df.apply(lambda x: df.count_regex(r'\w+', x))
300/17: count_words = df.apply(lambda x: df.text.count_regex(r'\w+', x))
300/18: import re
300/19:
def count_regex(pattern, tweet):
        return len(re.findall(pattern, tweet))
300/20: count_words = df.apply(lambda x: df.text.count_regex(r'\w+', x))
300/21: count_words = df.text.apply(lambda x: df.count_regex(r'\w+', x))
300/22: count_words = df.text.apply(lambda x: count_regex(r'\w+', x))
300/23: count_words
300/24: df.text
300/25: count_mentions  = df.text.apply(lambda x: count_regex(r'@\w+', x))
300/26: count_mentions
300/27: count_hashtags = df.text.apply(lambda x: count_regex(r'#\w_', x))
300/28:
count_capital_words = df.text.apply(lambda x: count_regex(
                                    r'\b[A-Z]{2,}\b', x))
300/29: count_capital_words
300/30: df.text
300/31: df.text[5]
300/32: df.text[6]
300/33: df.text.str.count(r'\b[A-Z]{2,}\b')
300/34: count_capital_words_2 = df.text.str.count(r'\b[A-Z]{2,}\b')
300/35: count_capital_words_2 == count_capital_words
300/36: count_capital_words_2.compare(count_capital_words_2) == count_capital_words
300/37: count_capital_words_2.equals(count_capital_words_2)
300/38: df.text.str.count(r'!|\?')
300/39: df.text[2]
300/40: df.text[12]
300/41: df.text.str.count(r'!|?')
300/42: df.text.str.count(r'!|\?')
300/43: count_excl_quest_marks = df.text.str.count(r'!|\?')
300/44: df.tetx.str.count(r'http.?://[^\s]+[\s]?')
300/45: df.text.str.count(r'http.?://[^\s]+[\s]?')
300/46: df.text[7]
300/47: df.text.str.count(r'http.?://')
301/1:
import pandas as pd
import emoji
301/2: pd.set_option('display.max_colwidth', -1)
301/3:
import pandas as pd
import emoji
301/4: df = pd.read_csv('Tweets.csv')
301/5: df.describe()
301/6: df.tweet_id.nunique()
301/7: dupl_id = df[df.tweet_id.duplicated()].tweet_id
301/8: emoji.demojize(df.text)
301/9: emoji.demojize(df.text).str
301/10: df.text(lambda x: emoji.demojize(x))
301/11: df.text.str(lambda x: emoji.demojize(x))
301/12: df.text(lambda x: emoji.demojize(str(x)))
301/13: df.text.apply(lambda x: emoji.demojize(str(x)))
301/14: df.text = df.text.apply(lambda x: emoji.demojize(str(x)))
301/15: df
301/16:
import pandas as pd
import emoji

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
301/17: df.tweet_id.nunique()
301/18: df.count()
301/19: df.nunique()
301/20: df.count()/ df.nunique()
301/21: df.nunique()/df.count()
301/22: dupl_id = df[df.tweet_id.duplicated()].tweet_id
301/23: df[df.tweet_id.isin(dupl_id)].sort_values('tweet_id')
301/24:
import pandas as pd
import emoji
import re
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
301/25: df.text = df.text.apply(lambda x: emoji.demojize(str(x)))
301/26: df
301/27: df.text.str.count(r'@\w+')
301/28: df.text.apply(lambda x: re.sub(r'@\w+', '', x))
301/29: df.text[1]
301/30: df.text[0]
301/31: df.text = df.text.apply(lambda x: re.sub(r'@\w+', '', x))
301/32: df.text.str.count(r'@\w+')
301/33: df.text.str.count(r'http.?://[^\s]+[\s]?')
301/34: df.text = df.text.apply(lambda x: re.sub(r'@\w+ | http.?://[^\s]+[\s]?', '', x))
301/35: df.text.str.count(r'http.?://[^\s]+[\s]?')
301/36: df.text.str.replace('[^\w\s]','')
301/37: df.text = df.text.apply(lambda x: re.sub(r'@\w+ | http.?://[^\s]+[\s]? | [^\w\s]', '', x))
301/38: df.text[9]
301/39: df.text.str.replace('[^\w\s]','')
301/40: df.text = df.text.str.replace('[^\w\s]','')
301/41: df.text[9]
301/42: df.text[22]
301/43: df.text[222]
301/44: df.text[-1]
301/45: df.text[:-1]
301/46: df.text[-1:]
301/47: df.text = df.text.apply(lambda x: re.sub(r'@\w+ | http.?://[^\s]+[\s]? | \d+', '', x))
301/48: df.text = df.text.str.replace('[^\w\s]','')
301/49: df.text[-1:]
301/50: from nltk.corpus import stopwords
301/51:
def remove_stopwords(input_text):
    stopwords_list = stopwords.words('english')
    # Some words which might indicate a certain sentiment are kept via a whitelist
    whitelist = ["n't", "not", "no"]
    words = input_text.split() 
    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] 
    return " ".join(clean_words)
301/52: df.text
301/53: df.text.apply(lambda x: remove_stopwords(x))
301/54:
import nltk
from nltk.corpus import stopwords
301/55: df.text.apply(lambda x: remove_stopwords(x))
301/56: nltk.corpus.stopwords
301/57: !nltk.download('stopwords')
301/58: nltk.download('stopwords')
301/59: df.text.apply(lambda x: remove_stopwords(x))
301/60: df.text
301/61: df = pd.read_csv('Tweets.csv')
301/62: df.text[14]
301/63: df.text[1]
301/64:
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
301/65:
def stemming(input_text):
    porter = PorterStemmer()
    words = input_text.split() 
    stemmed_words = [porter.stem(word) for word in words]
    return " ".join(stemmed_words)
301/66: df.text.apply(lambda x: stemming(x))
301/67: df.text[3:5]
301/68: df.text.str.count(r'\b[A-Z]{2,}\b')
301/69: df = pd.read_csv('Tweets.csv')
301/70: df.nunique()/df.count()
301/71: ## ID DUPLICATES
301/72: dupl_id = df[df.tweet_id.duplicated()].tweet_id
301/73: df[df.tweet_id.isin(dupl_id)].sort_values('tweet_id')
301/74: regex_pattern_count_of_words = r'\w+'
301/75: r_pattern_count_of_words = r'\w+'
301/76:
def count_regex(pattern, tweet):
        return len(re.findall(pattern, tweet))
301/77:
# add a column with count of words
df['count_words'] = df.text.apply(lambda x: count_regex(r_pattern_count_of_words, x))
301/78: df
303/1:
import nltk
nltk.download('vader_lexicon')
303/2:
from nltk.sentiment.vader import SentimentIntensityAnalyzer

sid = SentimentIntensityAnalyzer()
303/3:
import nltk
nltk.download('vader_lexicon')
303/4:
from nltk.sentiment.vader import SentimentIntensityAnalyzer

sid = SentimentIntensityAnalyzer()
303/5:
a = 'This was a good movie.'
sid.polarity_scores(a)
303/6:
a = 'This was a good movie.'
a_score = sid.polarity_scores(a)
303/7: a
303/8: a_score
303/9: a_score['pos']
303/10: a_score
303/11:
a = 'This was the best, most awesome movie EVER MADE!!!'
sid.polarity_scores(a)
303/12:
a = 'This was the worst film to ever disgrace the screen.'
sid.polarity_scores(a)
303/13:
import numpy as np
import pandas as pd

df = pd.read_csv('../TextFiles/amazonreviews.tsv', sep='\t')
df.head()
303/14:
import nltk
pd.set_option('display.width', 1000)
nltk.download('vader_lexicon')
303/15:
import numpy as np
import pandas as pd

df = pd.read_csv('../TextFiles/amazonreviews.tsv', sep='\t')
df.head()
303/16: df
303/17: df
303/18: pd.set_option('display.expand_frame_repr', False)
303/19:
import numpy as np
import pandas as pd

df = pd.read_csv('../TextFiles/amazonreviews.tsv', sep='\t')
df.head()
303/20:
import numpy as np
import pandas as pd
pd.set_option('display.expand_frame_repr', False)
303/21:
df = pd.read_csv('../TextFiles/amazonreviews.tsv', sep='\t')
df.head()
303/22:
import numpy as np
import pandas as pd
pd.set_option('display.width', 1000)
303/23:
df = pd.read_csv('../TextFiles/amazonreviews.tsv', sep='\t')
df.head()
303/24:
import numpy as np
import pandas as pd
pd.set_option('display.max_colwidth', None)
303/25:
df = pd.read_csv('../TextFiles/amazonreviews.tsv', sep='\t')
df.head()
303/26:
import numpy as np
import pandas as pd
pd.set_option('display.max_colwidth', None)
303/27:
import numpy as np
import pandas as pd
pd.set_option('display.max_columns', 1000, 'display.width', 1000, 'display.max_rows',1000)
303/28:
df = pd.read_csv('../TextFiles/amazonreviews.tsv', sep='\t')
df.head()
303/29:
import numpy as np
import pandas as pd
pd.set_option('display.max_columns', 1000, 'display.width', 1000, 'display.max_rows',1000)
303/30:
df = pd.read_csv('../TextFiles/amazonreviews.tsv', sep='\t')
df.head()
303/31:
import numpy as np
import pandas as pd
pd.set_option('display.max_colwidth', None)
303/32:
df = pd.read_csv('../TextFiles/amazonreviews.tsv', sep='\t')
df.head()
303/33:
import numpy as np
import pandas as pd
303/34: pd.set_option('display.max_colwidth', None)
303/35: pd.set_option('display.max_colwidth', -1)
303/36:
df = pd.read_csv('../TextFiles/amazonreviews.tsv', sep='\t')
df.head()
303/37: df['label'].value_counts()
303/38: df.label.value_counts()/len(df)
303/39: df
303/40: df.isnull().mean()
303/41: len(df)
303/42: '  '.isspace()
303/43: 'e  '.isspace()
303/44: df.review.isspace()
303/45: df.review.str.isspace()
303/46: df.review.str.isspace().sum()
303/47: df.review.str.isspace()
303/48: df.review.str.isspace()
303/49: df[df.review.str.isspace()]
303/50: new_row = {'label': 'pos', 'review': '   '}
303/51: df.append(new_row)
303/52: df.append(new_row, ignore_index=True)
303/53: df[df.review.str.isspace()]
303/54: len(df)
303/55: df = df.append(new_row, ignore_index=True)
303/56: df[df.review.str.isspace()]
303/57: len(df)
303/58: df.dropna(inplace=True)
303/59: df[df.review.str.isspace()]
303/60: df[df.review.str.isspace()].label
303/61: df[df.review.str.isspace()].labels
303/62: df[df.review.str.isspace()].index
303/63: df = df.append(new_row, ignore_index=True)
303/64: df[df.review.str.isspace()].index
303/65: df[df.review.str.isspace()].index[0]
303/66: df[df.review.str.isspace()].index
303/67: null_index = df[df.review.str.isspace()].index
303/68: df.drop(index=null_index)
303/69: df.drop(index=null_index, inplace=True)
303/70: len(df)
303/71:
# REMOVE NaN VALUES AND EMPTY STRINGS:
df.dropna(inplace=True)

blanks = []  # start with an empty list

for i,lb,rv in df.itertuples():  # iterate over the DataFrame
    if type(rv)==str:            # avoid NaN values
        if rv.isspace():         # test 'review' for whitespace
            blanks.append(i)     # add matching index numbers to the list

df.drop(blanks, inplace=True)
303/72: df['label'].value_counts()
303/73: sid.polarity_scores(df.loc[0]['review'])
303/74: df.loc[0]['label']
303/75:
df['scores'] = df['review'].apply(lambda review: sid.polarity_scores(review))

df.head()
303/76:
df['compound']  = df['scores'].apply(lambda score_dict: score_dict['compound'])

df.head()
303/77:
df['comp_score'] = df['compound'].apply(lambda c: 'pos' if c >=0 else 'neg')

df.head()
303/78: from sklearn.metrics import accuracy_score,classification_report,confusion_matrix
303/79: accuracy_score(df['label'],df['comp_score'])
303/80: print(classification_report(df['label'],df['comp_score']))
303/81: print(classification_report(df['label'],df['comp_score']))
303/82: print(confusion_matrix(df['label'],df['comp_score']))
303/83: sid.polarity_scores(df[0].review)
303/84: df[0].review
303/85: df[0]
303/86: df
303/87: df[1]
303/88: df.iloc[1]
303/89: df.iloc[1].review
303/90: sid.polarity_scores(df.iloc[1].review)
303/91: sid.polarity_scores(df.iloc[1].review)['compound']
303/92:
df['my_compound'] = df['review'].apply(lambda review: sid.polarity_scores(review)['compound'])

df.head()
303/93: df.compound.apply(lambda c: 'pos' if c > 0 else 'neg')
303/94: df['compound'].apply(lambda c: 'pos' if c > 0 else 'neg')
304/1:
import numpy as np
import pandas as pd

df = pd.read_csv('../TextFiles/moviereviews.tsv', sep='\t')
df.head()
304/2:
# REMOVE NaN VALUES AND EMPTY STRINGS:
df.dropna(inplace=True)

blanks = []  # start with an empty list

for i,lb,rv in df.itertuples():  # iterate over the DataFrame
    if type(rv)==str:            # avoid NaN values
        if rv.isspace():         # test 'review' for whitespace
            blanks.append(i)     # add matching index numbers to the list

df.drop(blanks, inplace=True)
304/3: df['label'].value_counts()
304/4:
from nltk.sentiment.vader import SentimentIntensityAnalyzer

sid = SentimentIntensityAnalyzer()
304/5:
df['scores'] = df['review'].apply(lambda review: sid.polarity_scores(review))

df['compound']  = df['scores'].apply(lambda score_dict: score_dict['compound'])

df['comp_score'] = df['compound'].apply(lambda c: 'pos' if c >=0 else 'neg')

df.head()
304/6: from sklearn.metrics import accuracy_score,classification_report,confusion_matrix
304/7: accuracy_score(df['label'],df['comp_score'])
304/8: print(classification_report(df['label'],df['comp_score']))
304/9: print(confusion_matrix(df['label'],df['comp_score']))
304/10: pd.set_option('displey.max_width', -1)
304/11: pd.set_option('displey.max_colwidth', -1)
304/12: pd.set_option('display.max_colwidth', -1)
304/13:
df['scores'] = df['review'].apply(lambda review: sid.polarity_scores(review))

df['compound']  = df['scores'].apply(lambda score_dict: score_dict['compound'])

df['comp_score'] = df['compound'].apply(lambda c: 'pos' if c >=0 else 'neg')

df.head()
304/14: df
304/15: df[df.label != df.comp_score]
305/1:
import numpy as np
import pandas as pd
305/2: train_data=pd.read_csv("../data/nlp-getting-started/train.csv")
305/3: train_data=pd.read_csv("../data/train.csv")
306/1:
import pandas as pd
import numpy as np
306/2: train_data=pd.read_csv("../input/nlp-getting-started/train.csv")
306/3: train_data=pd.read_csv("../data/nlp-getting-started/train.csv")
306/4: train_data=pd.read_csv("../data/train.csv")
306/5: train_data
306/6: train_data.head()
306/7: train_data.isna().sum()
306/8: train_data.isna().mean()
306/9: train_data[train_data.keyword != np.nan]
306/10: train_data.isnull().mean()
306/11: train_data.keyword.value_counts()
306/12: train_data.isnull().values.any()
306/13: train_data.isnull()
306/14: train_data.isnull().mean()
306/15: train_data.isnull()
306/16: train_data[~train_data.isnull()]
306/17: train_data[~train_data.isnull()][0]
306/18: train_data[~train_data.isnull()].iloc[0]
306/19: train_data[~train_data.isnull()].iloc[0].keyword
306/20: train_data[~train_data.isnull()].iloc[0].keyword.dtype
306/21: train_data[~train_data.isnull()].iloc[0].keyword.type
306/22: train_data[~train_data.isnull()].iloc[0].keyword
306/23: train_data.isnull().mean()
306/24: train_data.isnull().mean()
306/25: train_data[~train_data.isnull()].iloc[0].keyword
306/26: df_train[~df_train['location'].isna()]
306/27: train_data[~train_data['location'].isna()]
306/28: train_data.head()
306/29: train_data.isnull().mean()
306/30: train_data[~train_data['location'].isna()]
306/31: train_data[~train_data['keyword'].isna()]
306/32: train_data=pd.read_csv("../data/train.csv")
306/33:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
306/34: df_train.head()
306/35: df_train.isnull().mean()
306/36: df_train[~df_train['keyword'].isna()]
306/37: df_train.shape, df_test.shape
306/38:
#cardinality check
for col in df_train.columns:
    print('{} has {} unique instances'.format(col, len(df_train[col].unique())))
306/39: df_train.info(verbose=True, null_counts=True)
306/40: df_train.info(verbose=True)
306/41: df_train.info(verbose=True, null_counts=True)
306/42: df_train.describe()
306/43: df_train.info()
306/44: df_train.info(null_counts=True)
306/45: output = df_train.isnull().sum() * 100  / len(df_train)
306/46: output
306/47: df_train
306/48: df_train.head()
306/49: df_train[~df_train.keyword.isna()]
306/50: df_train.keyword.value_counts()
306/51: import seaborn as sns
306/52:
import matplotlib.pyplot as plt
import seaborn as sns
306/53:
plt.figure(figsize=[10, 60])
sns.countplot(y=KEYWORD,
              data=df_train,
              palette=['grey'],
              order=df_train[KEYWORD].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
306/54:
plt.figure(figsize=[10, 60])
sns.countplot(y=keyword,
              data=df_train,
#              palette=['grey'],
              order=df_train[keyword].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
306/55:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword',
              data=df_train,
#              palette=['grey'],
              order=df_train[keyword].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
306/56:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword',
              data=df_train,
#              palette=['grey'],
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
306/57:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword',
              data=df_train,
              palette=['lightblue'],
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
306/58:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword', 
              hue='target', 
              data=df_train, 
              palette=['blue', 'red'])
plt.xticks(rotation=90)
plt.legend(loc='upper right')
plt.title("Distribution of Target per Keyword ")
plt.show()
306/59:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword', 
              hue='target', 
              data=df_train, 
              palette=['lifhtblue', 'red'])
plt.xticks(rotation=90)
plt.legend(loc='upper right')
plt.title("Distribution of Target per Keyword ")
plt.show()
306/60:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword', 
              hue='target', 
              data=df_train, 
              palette=['lightblue', 'red'])
plt.xticks(rotation=90)
plt.legend(loc='upper right')
plt.title("Distribution of Target per Keyword ")
plt.show()
306/61:
## Let's show the most common keyword for real disasters
df_train
306/62:
## Let's show the most common keyword for real disasters
df_train[df_train.target == 1]
306/63:
## Let's show the most common keyword for real disasters
df_train[df_train.target == 1].keyword.value_couts()
306/64:
## Let's show the most common keyword for real disasters
df_train[df_train.target == 1].keyword.value_counts()
306/65:
## Let's show the most common keyword for real disasters
df_train[df_train.target == 1].keyword.value_counts()[:4]
306/66:
## Let's show the most common keyword for real disasters
df_train[df_train.target == 1].keyword.value_counts()[:10]
306/67:
## Let's show the most common keyword for not real disasters
df_train[df_train.target == 0].keyword.value_counts()[:10]
306/68: real_disaster_keywords = df_train[df_train['target'] == 1].groupby(['keyword', 'target']).count()['id'].reset_index()
306/69: real_disaster_keywords
306/70: real_disaster_keywords.sort_values('id')
306/71: real_disaster_keywords.sort_values('id', ascending=False)
306/72: df_train[df_train.target == 0]
306/73: df_train[df_train.target == 0].groupby('keyword')
306/74: df_train[df_train.target == 0].groupby('keyword').reset_index()
306/75: df_train[df_train.target == 0].groupby(['keyword']).reset_index()
306/76: df_train[df_train.target == 0].groupby(['keyword']).count().reset_index()
306/77: df_train[df_train.target == 0].groupby(['keyword']).count()['id'].reset_index()
306/78: df_target_0 = df_train[df_train.target == 0].groupby(['keyword']).count()['id'].reset_index()
306/79:
df_target_0 = df_train[df_train.target == 0].groupby(['keyword']).count()['id'].reset_index()
df_target_1 = df_train[df_train.target == 1].groupby(['keyword']).count()['id'].reset_index()
306/80: df_target_1
306/81:
merge_counts_keywords = pd.merge(left=df_target_0,
                                 right=df_target_1,
                                 on='keywird'
                                    )
306/82:
merge_counts_keywords = pd.merge(left=df_target_0,
                                 right=df_target_1,
                                 on='keyword'
                                    )
306/83: merge_counts_keywords
306/84:
merge_counts_keywords = pd.merge(left=df_target_0,
                                 right=df_target_1,
                                 on='keyword',
                                 how='outer'
                                    )
306/85: merge_counts_keywords.head(3)
306/86:
merge_counts_keywords = pd.merge(left=df_target_0,
                                 right=df_target_1,
                                 on='keyword',
                                 how='outer'
                                    ).drop(columns=['target_x', 'target_y']).fillna(0)
306/87:
merge_counts_keywords = pd.merge(left=df_target_0,
                                 right=df_target_1,
                                 on='keyword',
                                 how='outer'
                                    ).drop(columns=['id_x', 'id_y']).fillna(0)
306/88: merge_counts_keywords.head(3)
306/89:
merge_counts_keywords = pd.merge(left=df_target_0,
                                 right=df_target_1,
                                 on='keyword',
                                 how='outer'
                                 ).fillna(0)
306/90: merge_counts_keywords.head(3)
306/91: df_train[df_train.keyword == 'aftershock']
306/92: df_train[df_train.keyword == 'aftershock']
306/93: merge_counts_keywords.head(3)
306/94:
merge_counts_keywords = pd.merge(left=df_target_1,
                                 right=df_target_0,
                                 on='keyword',
                                 how='outer'
                                 ).fillna(0)

merge_counts_keywords.columns = ['keyword', 'target_0', 'target_1']
306/95: merge_counts_keywords.head(3)
306/96: merge_counts_keywords.head(4)
306/97: df_train[df_train.keyword == 'accident']
306/98: df_train[df_train.keyword == 'accident'].target.value_counts()
306/99:
merge_counts_keywords = pd.merge(left=df_target_1,
                                 right=df_target_0,
                                 on='keyword',
                                 how='outer'
                                 ).fillna(0)

merge_counts_keywords.columns = ['keyword', 'target_1', 'target_0']
306/100: merge_counts_keywords.head(4)
306/101:
merge_counts_keywords = pd.merge(left=df_target_0,
                                 right=df_target_1,
                                 on='keyword',
                                 how='outer'
                                 ).fillna(0)

merge_counts_keywords.columns = ['keyword', 'target_0', 'target_0']
306/102: merge_counts_keywords.head(4)
306/103:
merge_counts_keywords = pd.merge(left=df_target_0,
                                 right=df_target_1,
                                 on='keyword',
                                 how='outer'
                                 ).fillna(0)

merge_counts_keywords.columns = ['keyword', 'target_0', 'target_1']
306/104: merge_counts_keywords.head(4)
306/105:
merge_counts_keywords = pd.merge(left=df_target_0,
                                 right=df_target_1,
                                 on='keyword',
                                 how='outer'
                                 ).fillna(0)

merge_counts_keywords.columns = ['keyword', 'target_0', 'target_1']
merge_counts_keywords['prob_real_disasters'] = merge_counts_keywords.target_1 / merge_counts_keywords.target_0
306/106: merge_counts_keywords.head(4)
306/107:
merge_counts_keywords = pd.merge(left=df_target_0,
                                 right=df_target_1,
                                 on='keyword',
                                 how='outer'
                                 ).fillna(0)

merge_counts_keywords.columns = ['keyword', 'target_0', 'target_1']
merge_counts_keywords['prob_real_disasters'] = (merge_counts_keywords.target_1 - merge_counts_keywords.target_0)\
                                                /merge_counts_keywords.target_1
306/108: merge_counts_keywords.head(4)
306/109: merge_counts_keywords[merge_counts_keywords.keyword == 'derailment']
306/110: merge_counts_keywords[merge_counts_keywords.keyword == 'bombing']
306/111: df_train.target.value_counts()/len(df_train)
306/112: df_test
306/113: df_train
306/114: df_test
306/115: df_train
306/116: df_train[df_train.keyword.isna()]
306/117: df_train[df_train.keyword.isna()].target.value_counts()
306/118: url_pattern = r'https?:\/\/(www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()@:%_\+.~#?&//=]*)'
306/119: test_string = 'I am at https://www.nabanita.org www.nabanita.org okay'
306/120:
import pandas as pd
import numpy as np
import re
306/121: url_pattern = r'https?:\/\/(www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()@:%_\+.~#?&//=]*)'
306/122: test_string = 'I am at https://www.nabanita.org www.nabanita.org okay'
306/123:
test_op = re.sub(url_pattern, '', test_string)
test_op
306/124: url_pattern = r'(www.|http[s]?://)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
306/125: test_string = 'I am at https://www.nabanita.org www.nabanita.org okay'
306/126:
test_op = re.sub(url_pattern, '', test_string)
test_op
306/127: url_pattern = r'(http|ftp|https):\/\/([\w_-]+(?:(?:\.[\w_-]+)+))([\w.,@?^=%&:\/~+#-]*[\w@?^=%&\/~+#-])'
306/128: test_string = 'I am at https://www.nabanita.org www.nabanita.org okay'
306/129:
test_op = re.sub(url_pattern, '', test_string)
test_op
306/130: url_pattern = r'(http|ftp|https|www):\/\/([\w_-]+(?:(?:\.[\w_-]+)+))([\w.,@?^=%&:\/~+#-]*[\w@?^=%&\/~+#-])'
306/131: url_pattern = r'(www.|http[s]?://)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
306/132: test_string = 'I am at https://www.nabanita.org www.nabanita.org okay'
306/133: url_pattern = r'(http|ftp|https|www):\/\/([\w_-]+(?:(?:\.[\w_-]+)+))([\w.,@?^=%&:\/~+#-]*[\w@?^=%&\/~+#-])'
306/134: test_string = 'I am at https://www.nabanita.org www.nabanita.org okay'
306/135:
test_op = re.sub(url_pattern, '', test_string)
test_op
306/136: url_pattern = r'(http|ftp|http[s]|www):\/\/([\w_-]+(?:(?:\.[\w_-]+)+))([\w.,@?^=%&:\/~+#-]*[\w@?^=%&\/~+#-])'
306/137: url_pattern = r'(www.|http[s]?://)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
306/138: test_string = 'I am at https://www.nabanita.org www.nabanita.org okay'
306/139:
test_op = re.sub(url_pattern, '', test_string)
test_op
306/140: url_pattern = r'(http|ftp|http[s]|www):\/\/([\w_-]+(?:(?:\.[\w_-]+)+))([\w.,@?^=%&:\/~+#-]*[\w@?^=%&\/~+#-])'
306/141: test_string = 'I am at https://www.nabanita.org www.nabanita.org okay'
306/142:
test_op = re.sub(url_pattern, '', test_string)
test_op
306/143: url_pattern = r'(www.|http[s]?://)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
306/144: test_string = 'I am at https://www.nabanita.org www.nabanita.org okay'
306/145:
test_op = re.sub(url_pattern, '', test_string)
test_op
306/146: url_pattern = r'(http|ftp|http[s]|www.):\/\/([\w_-]+(?:(?:\.[\w_-]+)+))([\w.,@?^=%&:\/~+#-]*[\w@?^=%&\/~+#-])'
306/147: test_string = 'I am at https://www.nabanita.org www.nabanita.org okay'
306/148:
test_op = re.sub(url_pattern, '', test_string)
test_op
306/149: url_pattern = r'(www.|http[s]?://)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
306/150: test_string = 'I am at https://www.nabanita.org www.nabanita.org okay'
306/151:
test_op = re.sub(url_pattern, '', test_string)
test_op
306/152:
test_string = 'I am at <p>www.nabanita.org &nbsp;</p>'
html_entities = r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});'
306/153:
test_op = re.sub(html_entities, '', test_string)
test_op
306/154: df_train
306/155: df_train.text.str.contains('www', regex=False)
306/156: df_train[df_train.text.str.contains('www', regex=False)]
306/157: df_train[df_train.text.str.contains('http', regex=False)]
306/158: df_train[df_train.text.str.contains('<p>', regex=False)]
306/159: df_train[df_train.text.str.contains('http', regex=False)]
306/160: test_string = 'I am at http://t.co/rOI2NSmEJJ okay'
306/161:
test_op = re.sub(url_pattern, '', test_string)
test_op
306/162: test_string = 'I am at http://t.co/rOI2NSmEJJ okay'
306/163:
test_op = re.sub(url_pattern, '', test_string)
test_op
306/164: url_pattern = r'(www.|http[s]?://)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
306/165: test_string = 'I am at http://t.co/rOI2NSmEJJ okay'
306/166:
test_op = re.sub(url_pattern, '', test_string)
test_op
306/167: test_string = 'I am at http://t.co/GKYe6gjTk5 okay'
306/168:
test_op = re.sub(url_pattern, '', test_string)
test_op
306/169:
test_string = 'I am at @nabanita #python testing 123'
html_entities = r'@([a-z0-9]+)|#'
test_op = re.sub(html_entities, '', test_string)
test_op
306/170:
url_pattern = r'(www.|http[s]?://)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
test_string = 'I am at http://t.co/GKYe6gjTk5 okay'
test_op = re.sub(url_pattern, '', test_string)
test_op
306/171:
test_string = 'I am at @nabanita #python testing 123'
html_entities = r'@([a-z0-9]+)|#'
test_op = re.sub(html_entities, '', test_string)
test_op
306/172: from nltk.stem import WordNetLemmatizer
306/173:
import nltk
from nltk.stem import WordNetLemmatizer
306/174:
import nltk
from nltk.stem import WordNetLemmatizer
306/175: wnl = WordNetLemmatizer()
306/176:
def remove_urls(text):
    ''' This method takes in text to remove urls and website links, if any'''
    url_pattern = r'(www.|http[s]?://)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
    text = re.sub(url_pattern, '', text)
    return text
306/177:
def remove_social_media_tags(text):
    ''' This method removes @ and # tags'''
    tag_pattern = r'@([a-z0-9]+)|#'
    text = re.sub(tag_pattern, '', text)
    return text
306/178:
def convert_lower_case(text):
    return text.lower()
306/179:
def count_punctuations(text):
    getpunctuation = re.findall('[.?"\'`\,\-\!:;\(\)\[\]\\/‚Äú‚Äù]+?', text)
    return len(getpunctuation)
306/180:
def preprocess_text(x):
    cleaned_text = re.sub(r'[^a-zA-Z\d\s]+', '', x)
    word_list = []
    for each_word in cleaned_text.split(' '):
        word_list.append(contractions.fix(each_word).lower())
    word_list = [
        wnl.lemmatize(each_word.strip()) for each_word in word_list
        if each_word not in STOPWORDS and each_word.strip() != ''
    ]
    return " ".join(word_list)
306/181:
df_train['text'] = df_train['text'].apply(remove_urls)
df_train['text'] = df_train['text'].apply(remove_social_media_tags)
df_train['text'] = df_train['text'].apply(convert_lower_case)
df_train['punctuation_count'] = df_train['text'].apply(count_punctuations)
df_train['text'] = df_train['text'].apply(preprocess_text)
306/182: !pip install contractions
306/183:
import nltk
from nltk.stem import WordNetLemmatizer
import contractions
306/184:
df_train['text'] = df_train['text'].apply(remove_urls)
df_train['text'] = df_train['text'].apply(remove_social_media_tags)
df_train['text'] = df_train['text'].apply(convert_lower_case)
df_train['punctuation_count'] = df_train['text'].apply(count_punctuations)
df_train['text'] = df_train['text'].apply(preprocess_text)
306/185:
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions
306/186: STOPWORDS = set(stopwords.words('english'))
306/187:
def preprocess_text(x):
    cleaned_text = re.sub(r'[^a-zA-Z\d\s]+', '', x)
    word_list = []
    for each_word in cleaned_text.split(' '):
        word_list.append(contractions.fix(each_word).lower())
    word_list = [
        wnl.lemmatize(each_word.strip()) for each_word in word_list
        if each_word not in STOPWORDS and each_word.strip() != ''
    ]
    return " ".join(word_list)
306/188: STOPWORDS = set(stopwords.words('english'))
306/189:
df_train['text'] = df_train['text'].apply(remove_urls)
df_train['text'] = df_train['text'].apply(remove_social_media_tags)
df_train['text'] = df_train['text'].apply(convert_lower_case)
df_train['punctuation_count'] = df_train['text'].apply(count_punctuations)
df_train['text'] = df_train['text'].apply(preprocess_text)
306/190: !nltk.download('wordnet')
306/191: nltk.download('wordnet')
306/192:
df_train['text'] = df_train['text'].apply(remove_urls)
df_train['text'] = df_train['text'].apply(remove_social_media_tags)
df_train['text'] = df_train['text'].apply(convert_lower_case)
df_train['punctuation_count'] = df_train['text'].apply(count_punctuations)
df_train['text'] = df_train['text'].apply(preprocess_text)
306/193:
def preprocess_text(x):
    cleaned_text = re.sub(r'[^a-zA-Z\d\s]+', '', x)
    word_list = []
    for each_word in cleaned_text.split(' '):
        word_list.append(contractions.fix(each_word).lower())
    word_list = [
        wnl.lemmatize(each_word.strip()) for each_word in word_list
        if each_word not in STOPWORDS and each_word.strip() != ''
    ]
    return " ".join(word_list)
306/194: nltk.download('wordnet')
306/195:
df_train['text'] = df_train['text'].apply(remove_urls)
df_train['text'] = df_train['text'].apply(remove_social_media_tags)
df_train['text'] = df_train['text'].apply(convert_lower_case)
df_train['punctuation_count'] = df_train['text'].apply(count_punctuations)
df_train['text'] = df_train['text'].apply(preprocess_text)
306/196: whl
306/197:
STOPWORDS = set(stopwords.words('english'))
wnl = WordNetLemmatizer()
306/198:
df_train['text'] = df_train['text'].apply(remove_urls)
df_train['text'] = df_train['text'].apply(remove_social_media_tags)
df_train['text'] = df_train['text'].apply(convert_lower_case)
df_train['punctuation_count'] = df_train['text'].apply(count_punctuations)
df_train['text'] = df_train['text'].apply(preprocess_text)
306/199: whl
306/200: wnl
306/201: wnl.lemmatize('garden')
306/202:
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizerd
306/203: wnl.lemmatize('garden')
306/204: from nltk.stem import PorterStemmer, WordNetLemmatizer
306/205: from nltk.stem import PorterStemmer, WordNetLemmatizer
306/206:
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizerd
306/207: from nltk.stem import PorterStemmer, WordNetLemmatizer
306/208: wnl = WordNetLemmatizer()
306/209: wnl.lemmatize('garden')
306/210:
sent = 'The laughs you two heard were triggered by memories of his own high j-flying exits for moving beasts'
sent_tokenized = sent.split(" ")
lemmatizer = WordNetLemmatizer()
words = [lemmatizer.lemmatize(word) for word in sent_tokenized]
306/211:
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag, word_tokenize
wnl = WordNetLemmatizer()
sent = 'This is a foo bar sentence'
pos_tag(word_tokenize(sent))
306/212: nltk.download('punkt')
306/213:
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag, word_tokenize
wnl = WordNetLemmatizer()
sent = 'This is a foo bar sentence'
pos_tag(word_tokenize(sent))
306/214: wnl.lemmatize('garden')
306/215: nltk.download()
306/216: nltk.download('punkt')
306/217: nltk.download('omw-1.4')
306/218: from nltk.stem import PorterStemmer, WordNetLemmatizer
306/219: #nltk.download('punkt')
306/220: wnl = WordNetLemmatizer()
306/221: wnl.lemmatize('garden')
306/222:
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizerd
306/223: nltk.download('all')
306/224: from nltk.stem import WordNetLemmatizerd
306/225:
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizerd
313/1:
# Import spaCy and load the language library
import spacy
nlp = spacy.load('en_core_web_md')  # make sure to use a larger model!
313/2: !python -m spacy download en_core_web_sm
313/3: python -m spacy download en
313/4: !python -m spacy download en
313/5: !python -m spacy download en_core_web_md
313/6:
# Import spaCy and load the language library
import spacy
nlp = spacy.load('en_core_web_md')  # make sure to use a larger model!
313/7: !python -m spacy download en
313/8: #python -m spacy download en
313/9: #python -m spacy download en_core_web_md
313/10: nlp(u'lion').vector
306/226: from nltk.stem import PorterStemmer, WordNetLemmatizer
306/227: #nltk.download('punkt')
306/228:
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizerd
314/1:
import pandas as pd
import numpy as np
import re
314/2:
import matplotlib.pyplot as plt
import seaborn as sns
314/3:
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions
314/4:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
314/5:
nltk.download('omw-1.4')
STOPWORDS = set(stopwords.words('english'))
wnl = WordNetLemmatizer()
314/6: wnl
314/7: wnl.info
314/8: wnl
314/9: wnl.lemmatize('gar der')
314/10: nltk.download('wordnet')
314/11: wnl.lemmatize('gar der')
314/12:
import nltk
nltk.download('wordnet')
315/1:
import pandas as pd
import numpy as np
import re
315/2:
import matplotlib.pyplot as plt
import seaborn as sns
315/3:
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions
315/4:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
315/5:
nltk.download('omw-1.4')
STOPWORDS = set(stopwords.words('english'))
wnl = WordNetLemmatizer()
315/6: wnl.lemmatize('gar der')
315/7:
import nltk
nltk.download('wordnet')
315/8: wnl.lemmatize('gar der')
315/9: wnl.lemmatize('gar der')
316/1:
import pandas as pd
import numpy as np
import re
316/2:
import matplotlib.pyplot as plt
import seaborn as sns
316/3:
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions
316/4:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
316/5:
nltk.download('omw-1.4')
STOPWORDS = set(stopwords.words('english'))
wnl = WordNetLemmatizer()
316/6:
import nltk
nltk.download('wordnet')
316/7: wnl.lemmatize('gar der')
316/8:
import nltk
nltk.download('wordnet', '/usr/share/nltk_data')
316/9:
import nltk
nltk.download('wordnet', 'C:\\Users\\Me\\Anaconda3\\envs\\nlp_course\\share\\nltk_data')
316/10: wnl.lemmatize('gar der')
316/11:
import nltk
nltk.download('wordnet', 'C:\\Users\\Me\\Anaconda3\\envs\\nlp_course\\share\\nltk_data')
316/12: wnl.lemmatize('gar der')
316/13:
import nltk
nltk.download('wordnet', 'C:\\Users\\Me\\AppData\\Roaming\\nltk_data')
316/14: wnl.lemmatize('gar der')
316/15:
nltk.download('omw-1.4')
STOPWORDS = set(stopwords.words('english'))
wnl = WordNetLemmatizer()
316/16:
nltk.download('C:\\Users\\Me\\Anaconda3\\envs\\nlp_course\\share\\nltk_data')
STOPWORDS = set(stopwords.words('english'))
wnl = WordNetLemmatizer()
316/17:
nltk.download('C:\Users\Me\Anaconda3\envs\nlp_course\share\nltk_data')
STOPWORDS = set(stopwords.words('english'))
wnl = WordNetLemmatizer()
316/18: nltk.download('omw-1.4')
316/19:
STOPWORDS = set(stopwords.words('english'))
wnl = WordNetLemmatizer()
316/20:
nltk.download('omw-1.4')
nltk.download('stopwords', 'C:\Users\Me\Anaconda3\envs\nlp_course\share\nltk_data')
316/21:
nltk.download('omw-1.4')
nltk.download('stopwords','C:\Users\Me\Anaconda3\envs\nlp_course\share\nltk_data')
316/22:
nltk.download('omw-1.4')
nltk.download('stopwords','C:\Users\Me\Anaconda3\envs\nlp_course\share\nltk_data')
316/23:
nltk.download('omw-1.4')
nltk.download('stopwords')
316/24:
STOPWORDS = set(stopwords.words('english'))
wnl = WordNetLemmatizer()
316/25: STOPWORDS = set(stopwords.words('english'))
317/1:
import pandas as pd
import numpy as np
import re
317/2:
import matplotlib.pyplot as plt
import seaborn as sns
317/3:
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions
317/4:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
317/5:
nltk.download('omw-1.4')
nltk.download('stopwords')
317/6: STOPWORDS = set(stopwords.words('english'))
317/7: nltk.download('wordnet')
317/8: wnl = WordNetLemmatizer()
317/9: wnl.lemmatize('gar der')
317/10: nltk.download('wordnet', 'C:\Users\Me\Anaconda3\envs\nlp_course\share\nltk_data')
317/11: nltk.download('wordnet')
317/12: wnl = WordNetLemmatizer()
317/13: wnl.lemmatize('gar der')
317/14:
import nltk
nltk.download('wordnet')
317/15: wnl = WordNetLemmatizer()
317/16: wnl.lemmatize('gar der')
317/17: wnl.lemmatize()
317/18: wnl.lemmatize('word', pos='n')
317/19:
import nltk
dler = nltk.downloader.Downloader()
dler._update_index()
dler.download('all')
317/20: wnl = WordNetLemmatizer()
317/21: wnl.lemmatize('word', pos='n')
317/22:
import nltk
nltk.download('wordnet')
317/23: wnl.lemmatize('word', pos='n')
320/1:
import pandas as pd
import numpy as np
import re
320/2:
import matplotlib.pyplot as plt
import seaborn as sns
320/3:
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions
320/4:
nltk.download('all')
nltk.download('stopwords')
320/5: STOPWORDS = set(stopwords.words('english'))
320/6: wnl = WordNetLemmatizer()
320/7: wnl.lemmatize('word', pos='n')
320/8:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
320/9: df_train.head()
320/10: df_train.isnull().mean()
320/11: df_train.shape, df_test.shape
320/12:
#cardinality check
for col in df_train.columns:
    print('{} has {} unique instances'.format(col, len(df_train[col].unique())))
320/13: df_train.info(null_counts=True)
320/14: df_train.keyword.value_counts()
320/15:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword',
              data=df_train,
              palette=['lightblue'],
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
320/16:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword', 
              hue='target', 
              data=df_train, 
              palette=['lightblue', 'red'])
plt.xticks(rotation=90)
plt.legend(loc='upper right')
plt.title("Distribution of Target per Keyword ")
plt.show()
320/17:
## Let's show the most common keyword for real disasters
df_train[df_train.target == 1].keyword.value_counts()[:10]
320/18:
## Let's show the most common keyword for not real disasters
df_train[df_train.target == 0].keyword.value_counts()[:10]
320/19:
df_target_0 = df_train[df_train.target == 0].groupby(['keyword']).count()['id'].reset_index()
df_target_1 = df_train[df_train.target == 1].groupby(['keyword']).count()['id'].reset_index()
320/20: df_target_1
320/21: merge_counts_keywords[merge_counts_keywords.keyword == 'bombing']
320/22: df_train.target.value_counts()/len(df_train)
320/23: df_train[df_train.keyword.isna()].target.value_counts()
320/24:
url_pattern = r'(www.|http[s]?://)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
test_string = 'I am at http://t.co/GKYe6gjTk5 okay'
test_op = re.sub(url_pattern, '', test_string)
test_op
320/25:
test_string = 'I am at @nabanita #python testing 123'
html_entities = r'@([a-z0-9]+)|#'
test_op = re.sub(html_entities, '', test_string)
test_op
320/26:
def remove_urls(text):
    ''' This method takes in text to remove urls and website links, if any'''
    url_pattern = r'(www.|http[s]?://)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
    text = re.sub(url_pattern, '', text)
    return text
320/27:
def remove_social_media_tags(text):
    ''' This method removes @ and # tags'''
    tag_pattern = r'@([a-z0-9]+)|#'
    text = re.sub(tag_pattern, '', text)
    return text
320/28:
def convert_lower_case(text):
    return text.lower()
320/29:
def count_punctuations(text):
    getpunctuation = re.findall('[.?"\'`\,\-\!:;\(\)\[\]\\/‚Äú‚Äù]+?', text)
    return len(getpunctuation)
320/30:
def preprocess_text(x):
    cleaned_text = re.sub(r'[^a-zA-Z\d\s]+', '', x)
    word_list = []
    for each_word in cleaned_text.split(' '):
        word_list.append(contractions.fix(each_word).lower())
    word_list = [
        wnl.lemmatize(each_word.strip()) for each_word in word_list
        if each_word not in STOPWORDS and each_word.strip() != ''
    ]
    return " ".join(word_list)
320/31:
df_train['text'] = df_train['text'].apply(remove_urls)
df_train['text'] = df_train['text'].apply(remove_social_media_tags)
df_train['text'] = df_train['text'].apply(convert_lower_case)
df_train['punctuation_count'] = df_train['text'].apply(count_punctuations)
df_train['text'] = df_train['text'].apply(preprocess_text)
320/32: df_train
320/33:
import plotly.graph_objs as go 
from plotly.offline import init_notebook_mode, iplot 
init_notebook_mode(connected = True) 
import os 
import matplotlib.pyplot as plt#visualization 
%matplotlib inline 
warnings.filterwarnings("ignore") 
import io 
import plotly.offline as py#visualization 
py.init_notebook_mode(connected=True)#visualization 
import plotly.graph_objs as go#visualization 
import plotly.tools as tls#visualization 
import plotly.figure_factory as ff 
#from MF_Helpers import *
320/34: !pip install plotly
322/1:
import pandas as pd
import numpy as np
import re
322/2:
import matplotlib.pyplot as plt
import seaborn as sns
322/3:
import plotly.graph_objs as go 
from plotly.offline import init_notebook_mode, iplot 
init_notebook_mode(connected = True) 
import os 
import matplotlib.pyplot as plt#visualization 
%matplotlib inline 
warnings.filterwarnings("ignore") 
import io 
import plotly.offline as py#visualization 
py.init_notebook_mode(connected=True)#visualization 
import plotly.graph_objs as go#visualization 
import plotly.tools as tls#visualization 
import plotly.figure_factory as ff
322/4:
import plotly.graph_objs as go 
from plotly.offline import init_notebook_mode, iplot 
init_notebook_mode(connected = True) 
import os 
import matplotlib.pyplot as plt#visualization 
%matplotlib inline 
#warnings.filterwarnings("ignore") 
import io 
import plotly.offline as py#visualization 
py.init_notebook_mode(connected=True)#visualization 
import plotly.graph_objs as go#visualization 
import plotly.tools as tls#visualization 
import plotly.figure_factory as ff
322/5:
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions
322/6:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
322/7: STOPWORDS = set(stopwords.words('english'))
322/8:
import nltk
nltk.download('wordnet')
322/9: wnl = WordNetLemmatizer()
322/10: wnl.lemmatize('word', pos='n')
322/11:
#import nltk
#dler = nltk.downloader.Downloader()
#dler._update_index()
#dler.download('all')
322/12:
#import nltk
#nltk.download('wordnet')
322/13: df_train.head()
322/14: df_train.isnull().mean()
322/15: df_train.shape, df_test.shape
322/16:
#cardinality check
for col in df_train.columns:
    print('{} has {} unique instances'.format(col, len(df_train[col].unique())))
322/17: df_train.info(null_counts=True)
322/18: df_train.keyword.value_counts()
322/19:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword',
              data=df_train,
              palette=['lightblue'],
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
322/20:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword', 
              hue='target', 
              data=df_train, 
              palette=['lightblue', 'red'])
plt.xticks(rotation=90)
plt.legend(loc='upper right')
plt.title("Distribution of Target per Keyword ")
plt.show()
322/21:
## Let's show the most common keyword for real disasters
df_train[df_train.target == 1].keyword.value_counts()[:10]
322/22:
## Let's show the most common keyword for not real disasters
df_train[df_train.target == 0].keyword.value_counts()[:10]
322/23:
df_target_0 = df_train[df_train.target == 0].groupby(['keyword']).count()['id'].reset_index()
df_target_1 = df_train[df_train.target == 1].groupby(['keyword']).count()['id'].reset_index()
322/24: df_target_1
322/25:
merge_counts_keywords = pd.merge(left=df_target_0,
                                 right=df_target_1,
                                 on='keyword',
                                 how='outer'
                                 ).fillna(0)

merge_counts_keywords.columns = ['keyword', 'target_0', 'target_1']
merge_counts_keywords['prob_real_disasters'] = (merge_counts_keywords.target_1 - merge_counts_keywords.target_0)\
                                                /merge_counts_keywords.target_1
322/26: merge_counts_keywords[merge_counts_keywords.keyword == 'bombing']
322/27: df_train.target.value_counts()/len(df_train)
322/28: df_train[df_train.keyword.isna()].target.value_counts()
322/29:
url_pattern = r'(www.|http[s]?://)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
test_string = 'I am at http://t.co/GKYe6gjTk5 okay'
test_op = re.sub(url_pattern, '', test_string)
test_op
322/30:
test_string = 'I am at @nabanita #python testing 123'
html_entities = r'@([a-z0-9]+)|#'
test_op = re.sub(html_entities, '', test_string)
test_op
322/31:
def remove_urls(text):
    ''' This method takes in text to remove urls and website links, if any'''
    url_pattern = r'(www.|http[s]?://)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
    text = re.sub(url_pattern, '', text)
    return text
322/32:
def remove_social_media_tags(text):
    ''' This method removes @ and # tags'''
    tag_pattern = r'@([a-z0-9]+)|#'
    text = re.sub(tag_pattern, '', text)
    return text
322/33:
def convert_lower_case(text):
    return text.lower()
322/34:
def count_punctuations(text):
    getpunctuation = re.findall('[.?"\'`\,\-\!:;\(\)\[\]\\/‚Äú‚Äù]+?', text)
    return len(getpunctuation)
322/35:
def preprocess_text(x):
    cleaned_text = re.sub(r'[^a-zA-Z\d\s]+', '', x)
    word_list = []
    for each_word in cleaned_text.split(' '):
        word_list.append(contractions.fix(each_word).lower())
    word_list = [
        wnl.lemmatize(each_word.strip()) for each_word in word_list
        if each_word not in STOPWORDS and each_word.strip() != ''
    ]
    return " ".join(word_list)
322/36:
df_train['text'] = df_train['text'].apply(remove_urls)
df_train['text'] = df_train['text'].apply(remove_social_media_tags)
df_train['text'] = df_train['text'].apply(convert_lower_case)
df_train['punctuation_count'] = df_train['text'].apply(count_punctuations)
df_train['text'] = df_train['text'].apply(preprocess_text)
322/37: df_train
322/38: df_train[df_train.text.str.contains('http', regex=False)]
322/39: df_train[df_train.text.str.contains('http', regex=False)].text
322/40: print(df_train[df_train.text.str.contains('http', regex=False)].text)
322/41: print(df_train[df_train.text.str.contains('http', regex=False)]['text'])
322/42: pd.set_option('max_colwidth', -1)
322/43: df_train[df_train.text.str.contains('http', regex=False)].text
322/44: df_train[df_train.text.str.contains('http', regex=False)]
322/45:
def histogram2data(data, column1, name1, data2, column2, name2, bound) :
    trace1 = go.Histogram(x  = data[column1],
                          histnorm= "percent",
                          name = name1,
                          xbins=dict(start=np.min(data[column1]), size=bound, end=np.max(data[column1])),
                          marker = dict(line = dict(width = .5,
                                                    color = "black"
                                                    )
                                        ),
                         opacity = .5 
                         ) 
    trace2 = go.Histogram(x  = data2[column2],
                          histnorm= "percent",
                          name = name2,
                          xbins=dict(start=np.min(data2[column2]), size=bound, end=np.max(data2[column2])),
                          marker = dict(line = dict(width = .5,
                                                    color = "black"
                                                    )
                                        ),
                         opacity = .5 
                         ) 
    datadata = [trace1, trace2]
    layout = go.Layout(dict(title =column1 + " distribution ",
                            plot_bgcolor  = "rgba(243,243,243, 0.5)",
                            paper_bgcolor = "rgba(243,243,243, 0.5)",
                            barmode='overlay',
                            
                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',
                                             title = column1,
#                                             zerolinewidth=1,
                                             ticklen=5,
                                             gridwidth=2
                                            ),
                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',
                                             title = "percent",
                                             zerolinewidth=1,
                                             ticklen=5,
                                             gridwidth=2
                                            ),
                           )
                      )
    fig  = go.Figure(data=datadata, layout=layout)
    py.iplot(fig)
322/46: df_train
322/47: histogram2data(df_train[df_train.target == 0], 'punctuation_count', '0', df_train[df_train.target == 1], 'punctuation_count', '1', 50)
322/48: histogram2data(df_train[df_train.target == 0], 'punctuation_count', '0', df_train[df_train.target == 1], 'punctuation_count', '1', 2)
322/49: histogram2data(df_train[df_train.target == 0], 'punctuation_count', '0', df_train[df_train.target == 1], 'punctuation_count', '1', 1)
322/50:
histogram2data(df_train[df_train.target == 0], 'punctuation_count', '0', \
               df_train[df_train.target == 1], 'punctuation_count', '1', 1)
322/51: df_train.keyword
322/52: df_train.keyword.value_counts()
322/53:
def clean_keyword(text):
    text = text.replace('%20', ' ')
    return text
322/54: df_train.keyword.apply(clean_keyword)
322/55:
def clean_keyword(text):
    text = text.str.replace('%20', ' ')
    return text
322/56: df_train.keyword.apply(clean_keyword)
322/57:
def clean_keyword(text):
    text = str(text).replace('%20', ' ')
    return text
322/58: df_train.keyword.apply(clean_keyword)
322/59: df_train.keyword.apply(clean_keyword).value_counts()
322/60:
def clean_keyword(text):
    text = str(text).replace('%20', ' ')
    text = str(text).replace('nan', 'no info')
    return text
322/61: df_train.keyword.apply(clean_keyword).value_counts()
322/62:
def clean_keyword(text, inplace=True):
    text = str(text).replace('%20', ' ', inplace=inplace)
    text = str(text).replace('nan', 'no info' inplace=inplace)
    return text
322/63: df_train.keyword.apply(clean_keyword).value_counts()
322/64:
def clean_keyword(text, inplace=True):
    text = str(text).replace('%20', ' ', inplace=inplace)
    text = str(text).replace('nan', 'no info', inplace=inplace)
    return text
322/65: df_train.keyword.apply(clean_keyword(inplace=False)).value_counts()
322/66: df_train.keyword.apply(clean_keyword.value_counts()
322/67: df_train.keyword.apply(clean_keyword).value_counts()
322/68:
def clean_keyword(text):
    text = str(text).replace('%20', ' ')
    text = str(text).replace('nan', 'no info')
    return text
322/69: df_train.keyword.apply(clean_keyword).value_counts()
322/70: df_train.keyword = df_train.keyword.apply(clean_keyword).value_counts()
322/71: df_train
322/72: df_train.keyword.value_counts()
322/73: df_train.keyword
322/74: df_train.keyword.value_counts()
322/75:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
322/76:
df_train['text'] = df_train['text'].apply(remove_urls)
df_train['text'] = df_train['text'].apply(remove_social_media_tags)
df_train['text'] = df_train['text'].apply(convert_lower_case)
df_train['punctuation_count'] = df_train['text'].apply(count_punctuations)
df_train['text'] = df_train['text'].apply(preprocess_text)
322/77: df_train
322/78: df_train.keyword.value_counts()
322/79:
def clean_keyword(text):
    text = str(text).replace('%20', ' ')
    text = str(text).replace('nan', 'no info')
    return text
322/80: df_train.keyword = df_train.keyword.apply(clean_keyword).value_counts()
322/81: df_train.keyword
322/82: df_train.keyword.value_counts()
322/83: df_train
322/84:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
322/85:
df_train['text'] = df_train['text'].apply(remove_urls)
df_train['text'] = df_train['text'].apply(remove_social_media_tags)
df_train['text'] = df_train['text'].apply(convert_lower_case)
df_train['punctuation_count'] = df_train['text'].apply(count_punctuations)
df_train['text'] = df_train['text'].apply(preprocess_text)
322/86: df_train
322/87: df_train.keyword.value_counts()
322/88:
def clean_keyword(text):
    text = str(text).replace('%20', ' ')
    return text
322/89: df_train.keyword = df_train.keyword.apply(clean_keyword)
322/90: df_train
322/91: df_train.keyword.value_counts()
322/92:
def clean_keyword(text):
    text = str(text).replace('%20', ' ')
    text = str(text).replace('nan', 'no info')
    return text
322/93: df_train.keyword = df_train.keyword.apply(clean_keyword)
322/94: df_train
322/95: df_train.keyword.value_counts()
322/96:
def get_numbers_in_tweet(text):
    list_numbers = re.findall(r'\d+', text)
    if list_numbers:
        return 1
    return 0
322/97: df_train[num_in_tweets] = df_train.text.apply(get_numbers_in_tweet)
322/98: df_train['num_in_tweets'] = df_train.text.apply(get_numbers_in_tweet)
322/99: df_train
322/100: re.findall(r'\d+', '3 3 3')
322/101: re.findall(r'\d+', '3 3ddv dd 3')
322/102: re.findall(r'\d+', '3 33ddv dd 3')
322/103:
def get_numbers_in_tweet_flag(text):
    list_numbers = re.findall(r'\d+', text)
    if list_numbers:
        return 1
    return 0
322/104:
def get_numbers_in_tweet_flag(text):
    list_numbers = re.findall(r'\d+', text)
    if list_numbers:
        return 1
    return 0
322/105:
def get_numbers_in_tweet_count(text):
    list_numbers = re.findall(r'\d+', text)
    return len(list_numbers)
322/106:
df_train['num_in_tweets_flag'] = df_train.text.apply(get_numbers_in_tweet_flag)
df_train['num_in_tweets_count'] = df_train.text.apply(get_numbers_in_tweet_count)
322/107: df_train
322/108:
histogram2data(df_train[df_train.target == 0], 'num_in_tweets_count', '0', \
               df_train[df_train.target == 1], 'num_in_tweets_count', '1', 1)
322/109:
histogram2data(df_train[df_train.target == 0], 'num_in_tweets_count', '0', \
               df_train[df_train.target == 1], 'num_in_tweets_count', '1', .5)
322/110:
histogram2data(df_train[df_train.target == 0], 'num_in_tweets_count', '0', \
               df_train[df_train.target == 1], 'num_in_tweets_count', '1', .8)
322/111:
histogram2data(df_train[df_train.target == 0], 'num_in_tweets_count', '0', \
               df_train[df_train.target == 1], 'num_in_tweets_count', '1', 1)
322/112: from nltk.sentiment.vader import SentimentIntensityAnalyzer
322/113: sid = SentimentIntensityAnalyzer()
321/1: sid.polarity_scores('I love u')
322/114: sid.polarity_scores('I love u')
322/115: sid.polarity_scores('I love u')['compound']
322/116: df_train['sentiment_compound']  = df_train.text.apply(lambda review: sid.polarity_scores(review)['compound'])
322/117: df_train
322/118: df['sentiment_compound_score'] = df['sentiment_compound'].apply(lambda c: 'pos' if c >=0 else 'neg')
322/119: df_train['sentiment_compound_score'] = df_train['sentiment_compound'].apply(lambda c: 'pos' if c >=0 else 'neg')
322/120:
histogram2data(df_train[df_train.target == 0], 'sentiment_compound', '0', \
               df_train[df_train.target == 1], 'sentiment_compound', '1', 1)
322/121:
histogram2data(df_train[df_train.target == 0], 'sentiment_compound', '0', \
               df_train[df_train.target == 1], 'sentiment_compound', '1', .1)
322/122:
histogram2data(df_train[df_train.target == 0], 'sentiment_compound', '0', \
               df_train[df_train.target == 1], 'sentiment_compound', '1', .05)
322/123:
histogram2data(df_train[df_train.target == 0], 'sentiment_compound', '0', \
               df_train[df_train.target == 1], 'sentiment_compound', '1', .01)
322/124:
histogram2data(df_train[df_train.target == 0]*100, 'sentiment_compound', '0', \
               df_train[df_train.target == 1]*100, 'sentiment_compound', '1', .1)
322/125:
histogram2data(df_train[df_train.target == 0]*100, 'sentiment_compound', '0', \
               df_train[df_train.target == 1]*100, 'sentiment_compound', '1', 1)
322/126:
histogram2data(df_train[df_train.target == 0]*100, 'sentiment_compound', '0', \
               df_train[df_train.target == 1]*100, 'sentiment_compound', '1', .5)
322/127:
histogram2data(df_train[df_train.target == 0]*100, 'sentiment_compound', '0', \
               df_train[df_train.target == 1]*100, 'sentiment_compound', '1', 1)
322/128:
histogram2data(df_train[df_train.target == 0], 'sentiment_compound', '0', \
               df_train[df_train.target == 1], 'sentiment_compound', '1', .01)
322/129:
histogram2data(df_train[df_train.target == 0], 'sentiment_compound', '0', \
               df_train[df_train.target == 1], 'sentiment_compound', '1', .05)
322/130:
sns.countplot(x=target, hue=sentiment_compound_score, data=df_train)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
322/131:
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
322/132: df_train['text_tokenized'] = df_train.text.apply(word_tokenize)
322/133:
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize
322/134: df_train['text_tokenized'] = df_train.text.apply(word_tokenize)
322/135: len('rkd ekk e')
322/136:
df_train['words_per_tweet'] = df_train.text_tokenized.apply(len)
df_train['char_per_tweet'] = df_train.text.apply(len)
322/137:
histogram2data(df_train[df_train.target == 0], 'words_per_tweet', '0', \
               df_train[df_train.target == 1], 'words_per_tweet', '1', 1)
322/138:
histogram2data(df_train[df_train.target == 0], 'char_per_tweet', '0', \
               df_train[df_train.target == 1], 'char_per_tweet', '1', 1)
322/139: df_train
322/140: df_train.location
322/141: df_train.location.value_counts()
322/142: df_train
322/143: df_train.location.isna().mean()
322/144:
cols_to_train = ['all_text_joined']

tt = TweetTokenizer()
322/145:
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
322/146:
cols_to_train = ['all_text_joined']

tt = TweetTokenizer()
322/147: from sklearn.model_selection import train_test_split
322/148:
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
322/149:
X_train, X_test, y_train, y_test = train_test_split(df_train[cols_to_train],
                                                    df_train['target'].values,
                                                    test_size=0.2,
                                                    random_state=42)
322/150: df_train
322/151: df_train['all_text_joined'] = df_train['text_tokenized'].apply(lambda x: " ".join(x))
322/152: df_train
322/153:
cols_to_train = ['text']
tt = TweetTokenizer()
322/154:
X_train, X_test, y_train, y_test = train_test_split(df_train[cols_to_train],
                                                    df_train['target'].values,
                                                    test_size=0.2,
                                                    random_state=42)
322/155:
ct = ColumnTransformer([('count_vec',
                         CountVectorizer(tokenizer=tt.tokenize,
                                         ngram_range=(1, 2)),
                                         'text')],
                       remainder='passthrough')
322/156:
from sklearn.feature_extraction import FeatureHasher
from sklearn.preprocessing import MinMaxScaler
import pandas as pd   
X = pd.DataFrame({
    "documents": ["First item", "second one here", "Is this the last?"],
    "width": [3, 4, 5],
})  
# "documents" is a string which configures ColumnTransformer to
# pass the documents column as a 1d array to the FeatureHasher
ct = ColumnTransformer(
    [("text_preprocess", FeatureHasher(input_type="string"), "documents"),
     ("num_preprocess", MinMaxScaler(), ["width"])])
X_trans = ct.fit_transform(X)
322/157:
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
322/158:
from sklearn.feature_extraction import FeatureHasher
from sklearn.preprocessing import MinMaxScaler
import pandas as pd   
X = pd.DataFrame({
    "documents": ["First item", "second one here", "Is this the last?"],
    "width": [3, 4, 5],
})  
# "documents" is a string which configures ColumnTransformer to
# pass the documents column as a 1d array to the FeatureHasher
ct = ColumnTransformer(
    [("text_preprocess", FeatureHasher(input_type="string"), "documents"),
     ("num_preprocess", MinMaxScaler(), ["width"])])
X_trans = ct.fit_transform(X)
322/159: X_trans
322/160: print(X_trans)
322/161: X
322/162:
from sklearn.feature_extraction import FeatureHasher
from sklearn.preprocessing import MinMaxScaler
import pandas as pd   
X = pd.DataFrame({
    "documents": ["First item", "second one here", "Is this the last?"],
    "width": [3, 4, 5],
})  
# "documents" is a string which configures ColumnTransformer to
# pass the documents column as a 1d array to the FeatureHasher
ct = ColumnTransformer(
    [("text_preprocess", FeatureHasher(input_type="string"), "documents")])
X_trans = ct.fit_transform(X)
322/163: X
322/164: print(X_trans)
322/165:
import numpy as np
from sklearn.impute import SimpleImputer
imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')
imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])
#SimpleImputer()
X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]
print(imp_mean.transform(X))
322/166:
import numpy as np
from sklearn.impute import SimpleImputer
imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')
imp_mean.fit([[7, 12, 3], [4, np.nan, 6], [10, 5, 9]])
#SimpleImputer()
X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]
print(imp_mean.transform(X))
322/167:
import numpy as np
from sklearn.impute import SimpleImputer
imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')
imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])
#SimpleImputer()
X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]
print(imp_mean.transform(X))
322/168:
import numpy as np
from sklearn.impute import SimpleImputer
imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')
imp_mean.fit([[5, 2, 3], [4, np.nan, 6], [10, 5, 9]])
#SimpleImputer()
X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]
print(imp_mean.transform(X))
322/169:
import numpy as np
from sklearn.impute import SimpleImputer
imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')
imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])
#SimpleImputer()
X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]
print(imp_mean.transform(X))
322/170: mean([7, 2, 3])
322/171: [7, 2, 3]
322/172: [7, 2, 3].mean()
322/173:
import statistics
statistics.mean([7, 2, 3])
322/174:
import statistics
statistics.mean([7, 2, 3, 4, 6, 10, 5, 9])
322/175: X_train
322/176: X_test
322/177: X
322/178:
import numpy as np
from sklearn.impute import SimpleImputer
imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')
imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])
#SimpleImputer()
X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]
print(imp_mean.transform(X))
322/179: X
322/180:
from sklearn.feature_extraction import FeatureHasher
from sklearn.preprocessing import MinMaxScaler
import pandas as pd   
X = pd.DataFrame({
    "documents": ["First item", "second one here", "Is this the last?"],
    "width": [3, 4, 5],
})  
# "documents" is a string which configures ColumnTransformer to
# pass the documents column as a 1d array to the FeatureHasher
ct = ColumnTransformer(
    [("text_preprocess", FeatureHasher(input_type="string"), "documents")])
X_trans = ct.fit_transform(X)
322/181: X
322/182:
...
# determine categorical and numerical features
numerical_ix = X.select_dtypes(include=['int64', 'float64']).columns
categorical_ix = X.select_dtypes(include=['object', 'bool']).columns
322/183: numerical_ix
322/184: X.info()
322/185: categorical_ix
322/186: numerical_ix
322/187:
from sklearn.feature_extraction import FeatureHasher
from sklearn.preprocessing import MinMaxScaler
import pandas as pd   
X = pd.DataFrame({
    "documents": ["First item", "second one here", "Is this the last?"],
    "width": [3, 4, 5],
    "width2": [3, 4, 5],
    "width3": [.3, 4, 5],
})  
# "documents" is a string which configures ColumnTransformer to
# pass the documents column as a 1d array to the FeatureHasher
ct = ColumnTransformer(
    [("text_preprocess", FeatureHasher(input_type="string"), "documents")])
X_trans = ct.fit_transform(X)
322/188:
...
# determine categorical and numerical features
numerical_ix = X.select_dtypes(include=['int64', 'float64']).columns
categorical_ix = X.select_dtypes(include=['object', 'bool']).columns
322/189: numerical_ix
322/190: numerical_ix.dtype
322/191: numerical_ix
322/192:
ct = ColumnTransformer([('count_vec',
                         CountVectorizer(tokenizer=tt.tokenize,
                                         ngram_range=(1, 2)),
                                         'text')],
                       remainder='passthrough')
322/193:
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, CountVectorizer
322/194:
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
322/195:
ct = ColumnTransformer([('count_vec',
                         CountVectorizer(tokenizer=tt.tokenize,
                                         ngram_range=(1, 2)),
                                         'text')],
                       remainder='passthrough')
322/196:
ct.fit(X_train)
X_train_sparse = ct.transform(X_train)
X_test_sparse = ct.transform(X_test)
322/197: X_train_sparse
322/198: print(X_train_sparse)
322/199: X_test_sparse
322/200: print(X_train_sparse.toarray())
322/201:
log_reg = LogisticRegression()
log_reg.fit(X_train_sparse, y_train)
test_prediction = log_reg.predict(X_test_sparse)
training_prediction = log_reg.predict(X_train_sparse)
322/202:
log_reg = LogisticRegression()
log_reg.fit(X_train_sparse, y_train)
y_test = log_reg.predict(X_test_sparse)
y_train = log_reg.predict(X_train_sparse)
322/203:
X_train, X_test, y_train, y_test = train_test_split(df_train[cols_to_train],
                                                    df_train['target'].values,
                                                    test_size=0.2,
                                                    random_state=42)
322/204:
ct = ColumnTransformer([('count_vec',
                         CountVectorizer(tokenizer=tt.tokenize,
                                         ngram_range=(1, 2)),
                                         'text')],
                       remainder='passthrough')
322/205:
ct.fit(X_train)
X_train_sparse = ct.transform(X_train)
X_test_sparse = ct.transform(X_test)
322/206: classification_report(y_train, y_train_pred)
322/207:
log_reg = LogisticRegression()
log_reg.fit(X_train_sparse, y_train)
y_test_pred = log_reg.predict(X_test_sparse)
y_train_pred = log_reg.predict(X_train_sparse)
322/208: classification_report(y_train, y_train_pred)
322/209: print(classification_report(y_train, y_train_pred))
322/210: print(classification_report(y_test, y_test_pred))
322/211:
cols_to_train = ['text']

tt = TweetTokenizer()

X_train, X_test, y_train, y_test = train_test_split(df_train[cols_to_train],
                                                    df_train['target'].values,
                                                    test_size=0.2,
                                                    random_state=42)

ct = ColumnTransformer([('tfidf',
                         TfidfVectorizer(tokenizer=tt.tokenize,
                                         ngram_range=(1, 2),
                                         smooth_idf=True), ALL_TEXT_JOINED)],
                       remainder='passthrough')

ct.fit(X_train)
X_train_sparse = ct.transform(X_train)
X_test_sparse = ct.transform(X_test)
322/212:
cols_to_train = ['text']

tt = TweetTokenizer()

X_train, X_test, y_train, y_test = train_test_split(df_train[cols_to_train],
                                                    df_train['target'].values,
                                                    test_size=0.2,
                                                    random_state=42)

ct = ColumnTransformer([('tfidf',
                         TfidfVectorizer(tokenizer=tt.tokenize,
                                         ngram_range=(1, 2),
                                         smooth_idf=True), 'text')],
                       remainder='passthrough')

ct.fit(X_train)
X_train_sparse = ct.transform(X_train)
X_test_sparse = ct.transform(X_test)
322/213:
log_reg = LogisticRegression()
log_reg.fit(X_train_sparse, y_train)
y_test_pred = log_reg.predict(X_test_sparse)
y_train_pred = log_reg.predict(X_train_sparse)
322/214: print(classification_report(y_train, y_train_pred))
322/215: print(classification_report(y_test, y_test_pred))
322/216: log_reg.coef_
322/217:
cols_to_train = ['text']

tt = TweetTokenizer()

X_train, X_test, y_train, y_test = train_test_split(df_train[cols_to_train],
                                                    df_train['target'].values,
                                                    test_size=0.2,
                                                    random_state=42)

ct = ColumnTransformer([('tfidf',
                         TfidfVectorizer(tokenizer=tt.tokenize,
                                         ngram_range=(1, 2),
                                         smooth_idf=True), 'text')],
                       remainder='passthrough')

ct.fit(X_train)
X_train_sparse = ct.transform(X_train)
X_test_sparse = ct.transform(X_test)
X_train_sparse.shape
322/218:
cols_to_train = ['text']

tt = TweetTokenizer()

X_train, X_test, y_train, y_test = train_test_split(df_train[cols_to_train],
                                                    df_train['target'].values,
                                                    test_size=0.2,
                                                    random_state=42)

ct = ColumnTransformer([('tfidf',
                         TfidfVectorizer(tokenizer=tt.tokenize,
                                         ngram_range=(1, 2),
                                         smooth_idf=True), 'text')],
                       remainder='drop')

ct.fit(X_train)
X_train_sparse = ct.transform(X_train)
X_test_sparse = ct.transform(X_test)
X_train_sparse.shape
322/219: df_train.head(4)
322/220:
cols_to_train = ['text', 'words_per_tweet', 'char_per_tweet', 'sentiment_compound']

tt = TweetTokenizer()

X_train, X_test, y_train, y_test = train_test_split(df_train[cols_to_train],
                                                    df_train['target'].values,
                                                    test_size=0.2,
                                                    random_state=42)

ct = ColumnTransformer([('tfidf',
                         TfidfVectorizer(tokenizer=tt.tokenize,
                                         ngram_range=(1, 2),
                                         smooth_idf=True), 'text')],
                       remainder='drop')

ct.fit(X_train)
X_train_sparse = ct.transform(X_train)
X_test_sparse = ct.transform(X_test)
X_train_sparse.shape
322/221:
cols_to_train = ['text', 'words_per_tweet', 'char_per_tweet', 'sentiment_compound']

tt = TweetTokenizer()

X_train, X_test, y_train, y_test = train_test_split(df_train[cols_to_train],
                                                    df_train['target'].values,
                                                    test_size=0.2,
                                                    random_state=42)

ct = ColumnTransformer([('tfidf',
                         TfidfVectorizer(tokenizer=tt.tokenize,
                                         ngram_range=(1, 2),
                                         smooth_idf=True), 'text')],
                       remainder='passthrough')

ct.fit(X_train)
X_train_sparse = ct.transform(X_train)
X_test_sparse = ct.transform(X_test)
X_train_sparse.shape
322/222:
log_reg = LogisticRegression()
log_reg.fit(X_train_sparse, y_train)
y_test_pred = log_reg.predict(X_test_sparse)
y_train_pred = log_reg.predict(X_train_sparse)
322/223: print(classification_report(y_train, y_train_pred))
322/224: print(classification_report(y_test, y_test_pred))
323/1:
import nltk
pd.set_option('display.width', 1000)
nltk.download('vader_lexicon')
323/2:
import nltk
#pd.set_option('display.width', 1000)
nltk.download('vader_lexicon')
323/3:
from nltk.sentiment.vader import SentimentIntensityAnalyzer

sid = SentimentIntensityAnalyzer()
323/4:
a = 'This was a good movie.'
a_score = sid.polarity_scores(a)
323/5: a_score
323/6:
a = 'This was the best, most awesome movie EVER MADE!!!'
sid.polarity_scores(a)
323/7:
a = 'This was the worst film to ever disgrace the screen.'
sid.polarity_scores(a)
323/8:
import numpy as np
import pandas as pd
323/9: pd.set_option('display.max_colwidth', -1)
323/10: pd.set_option('display.max_colwidth', -1)
323/11:
df = pd.read_csv('../TextFiles/amazonreviews.tsv', sep='\t')
df.head()
323/12: df
323/13: df['label'].value_counts()
323/14: df.label.value_counts()/len(df)
323/15: df.isnull().mean()
323/16: len(df)
323/17: new_row = {'label': 'pos', 'review': '   '}
323/18: df = df.append(new_row, ignore_index=True)
323/19: null_index = df[df.review.str.isspace()].index
323/20: df.dropna(inplace=True)
323/21: df.drop(index=null_index, inplace=True)
323/22:
# REMOVE NaN VALUES AND EMPTY STRINGS:
df.dropna(inplace=True)

blanks = []  # start with an empty list

for i,lb,rv in df.itertuples():  # iterate over the DataFrame
    if type(rv)==str:            # avoid NaN values
        if rv.isspace():         # test 'review' for whitespace
            blanks.append(i)     # add matching index numbers to the list

df.drop(blanks, inplace=True)
323/23: df['label'].value_counts()
323/24: sid.polarity_scores(df.loc[0]['review'])
323/25: df.loc[0]['label']
323/26: df.iloc[1].review
323/27: sid.polarity_scores(df.iloc[1].review)['compound']
323/28:
df['my_compound'] = df['review'].apply(lambda review: sid.polarity_scores(review)['compound'])

df.head()
323/29:
df['compound']  = df['scores'].apply(lambda score_dict: score_dict['compound'])

df.head()
321/2:
import numpy as np
import pandas as pd

df = pd.read_csv('../TextFiles/moviereviews.tsv', sep='\t')
df.head()
321/3: pd.set_option('display.max_colwidth', -1)
321/4:
# REMOVE NaN VALUES AND EMPTY STRINGS:
df.dropna(inplace=True)

blanks = []  # start with an empty list

for i,lb,rv in df.itertuples():  # iterate over the DataFrame
    if type(rv)==str:            # avoid NaN values
        if rv.isspace():         # test 'review' for whitespace
            blanks.append(i)     # add matching index numbers to the list

df.drop(blanks, inplace=True)
321/5: df['label'].value_counts()
321/6:
from nltk.sentiment.vader import SentimentIntensityAnalyzer

sid = SentimentIntensityAnalyzer()
321/7: sid.polarity_scores('I love u')
324/1:
# Import spaCy and load the language library. Remember to use a larger model!
import spayy
324/2:
# Import spaCy and load the language library. Remember to use a larger model!
import spacy
324/3:
# Import spaCy and load the language library. Remember to use a larger model!
import spacy
nlp = spacy.load('en_core_web_md')
324/4:
# Choose the words you wish to compare, and obtain their vectors
nlp(u'lion').vector
324/5:
# Choose the words you wish to compare, and obtain their vectors
nlp(u'cat').vector
324/6: import numpy as np
324/7:
# Choose the words you wish to compare, and obtain their vectors
norm(nlp(u'cat').vector)
324/8:
# Choose the words you wish to compare, and obtain their vectors
np.linalg.norm(nlp(u'cat').vector)
324/9:
# Import spatial and define a cosine_similarity function
def cos_simularity(vec_a, vec_b):
    return np.dot(vec_a, vec_b)/np.linalg.norm(vec_a)*np.linalg.norm(vec_b)
324/10: cos_simularity(nlp(u'cat').vector, nlp(u'dog').vector)
324/11: cos_simularity(nlp(u'cat').vector, nlp(u'table').vector)
324/12: cos_simularity(nlp(u'cat').vector, nlp(u'tiger').vector)
324/13: cos_simularity(nlp(u'cat').vector, nlp(u'kitten').vector)
324/14: nlp(u'cat').similarity(nlp(u'cat'))
324/15: cos_simularity(nlp(u'cat').vector, nlp(u'cat').vector)
324/16:
# Import spatial and define a cosine_similarity function
def cos_simularity(vec_a, vec_b):
    return np.dot(vec_a, vec_b)/(np.linalg.norm(vec_a)*np.linalg.norm(vec_b)
)
324/17: cos_simularity(nlp(u'cat').vector, nlp(u'table').vector)
324/18: cos_simularity(nlp(u'cat').vector, nlp(u'cat').vector)
324/19: cos_simularity(nlp(u'cat').vector, nlp(u'dog').vector)
324/20: nlp(u'cat').similarity(nlp(u'cat'))
324/21: from scipy import special
324/22:
from scipy import special
cos_similarity = lambda vec1, vec2: 1 - special.distance.cosine(vec1, vec2)
324/23: cos_similarity(nlp(u'cat').vector, nlp(u'dog').vector)
324/24: !import scipy
324/25: !install scipy
324/26: !instal scipy
324/27: !conda install scipy
324/28:
from scipy import special
cos_similarity = lambda vec1, vec2: 1 - special.distance.cosine(vec1, vec2)
324/29: nlp(u'cat').similarity(nlp(u'cat'))
324/30:
from scipy import spetial
cos_similarity = lambda vec1, vec2: 1 - spetial.distance.cosine(vec1, vec2)
324/31:
from scipy import spatial
cos_similarity = lambda vec1, vec2: 1 - spatial.distance.cosine(vec1, vec2)
324/32: cos_similarity(nlp(u'cat').vector, nlp(u'dog').vector)
324/33:
# List the top ten closest vectors in the vocabulary to the result of the expression above
com_sim = []

for word in nlp.vocab:
    print(word)
325/1:
# Import spaCy and load the language library. Remember to use a larger model!
import spacy
nlp = spacy.load('en_core_web_md')
325/2: import numpy as np
325/3:
# Choose the words you wish to compare, and obtain their vectors
np.linalg.norm(nlp(u'cat').vector)
325/4:
# Import spatial and define a cosine_similarity function
def cos_simularity(vec_a, vec_b):
    return np.dot(vec_a, vec_b)/(np.linalg.norm(vec_a)*np.linalg.norm(vec_b)
)
325/5: cos_simularity(nlp(u'cat').vector, nlp(u'table').vector)
325/6: cos_simularity(nlp(u'cat').vector, nlp(u'cat').vector)
325/7: cos_simularity(nlp(u'cat').vector, nlp(u'dog').vector)
325/8: cos_similarity(nlp(u'cat').vector, nlp(u'dog').vector)
325/9:
from scipy import spatial
cos_similarity = lambda vec1, vec2: 1 - spatial.distance.cosine(vec1, vec2)
325/10:
# Write an expression for vector arithmetic
# For example: new_vector = word1 - word2 + word3
new_vec = pet - cat + wild
325/11:
# Write an expression for vector arithmetic
# For example: new_vector = word1 - word2 + word3
new_vec = nlp(u'pet') - nlp(u'cat') + nlp(u'wild)
325/12:
# Write an expression for vector arithmetic
# For example: new_vector = word1 - word2 + word3
new_vec = nlp(u'pet') - nlp(u'cat') + nlp(u'wild')
325/13:
pet = nlp.vocab['pet'].vector
pet = nlp.vocab['cat'].vector
pet = nlp.vocab['wild'].vector
325/14:
# Write an expression for vector arithmetic
# For example: new_vector = word1 - word2 + word3
new_vec = pet - cat + wild
325/15:
pet = nlp.vocab['pet'].vector
cat = nlp.vocab['cat'].vector
wild = nlp.vocab['wild'].vector
325/16:
# Write an expression for vector arithmetic
# For example: new_vector = word1 - word2 + word3
new_vec = pet - cat + wild
325/17:
# List the top ten closest vectors in the vocabulary to the result of the expression above
cos_sim_list = []

for word in nlp.vocab:
    if word.had_vector:
        if word.is_lower:
            if word.is_alpha:
                similarity = cos_similarity(new_vec, word.vector)
                cos_sim_list.append((word, similarity))
325/18:
# List the top ten closest vectors in the vocabulary to the result of the expression above
cos_sim_list = []

for word in nlp.vocab:
    if word.has_vector:
        if word.is_lower:
            if word.is_alpha:
                similarity = cos_similarity(new_vec, word.vector)
                cos_sim_list.append((word, similarity))
325/19: cos_sim_list = sorted(cos_sim_list, key=lambda item: -item[1])
325/20: cos_sim_list
325/21: print([t[0].text for t in cos_sim_list[:10]])
325/22:
pet = nlp.vocab['pet'].vector
chicken = nlp.vocab['chicken'].vector
wild = nlp.vocab['wild'].vector
325/23:
# Write an expression for vector arithmetic
# For example: new_vector = word1 - word2 + word3
new_vec = pet - chicken + wild
325/24:
# List the top ten closest vectors in the vocabulary to the result of the expression above
cos_sim_list = []

for word in nlp.vocab:
    if word.has_vector:
        if word.is_lower:
            if word.is_alpha:
                similarity = cos_similarity(new_vec, word.vector)
                cos_sim_list.append((word, similarity))
325/25: cos_sim_list = sorted(cos_sim_list, key=lambda item: -item[1])
325/26: print([t[0].text for t in cos_sim_list[:10]])
327/1: import pandas as pd
327/2: npr = pd.read_csv('npr.csv')
327/3: npr.head()
327/4: from sklearn.feature_extraction.text import CountVectorizer
327/5: cv = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')
327/6: dtm = cv.fit_transform(npr['Article'])
327/7: dtm
327/8: from sklearn.decomposition import LatentDirichletAllocation
327/9: LDA = LatentDirichletAllocation(n_components=7,random_state=42)
327/10:
# This can take awhile, we're dealing with a large amount of documents!
LDA.fit(dtm)
327/11: len(npr)
327/12: npr[3]
327/13: npr[3, 1]
327/14: npr.iloc[3, 1]
327/15: npr.iloc[1,1]
327/16: npr.iloc[3]
327/17: npr.iloc[3]['Article']
327/18:
for i in range(10):
    random_word_id = random.randint(0,54776)
    print(cv.get_feature_names()[random_word_id])
327/19: import random
327/20:
for i in range(10):
    random_word_id = random.randint(0,54776)
    print(cv.get_feature_names()[random_word_id])
329/1: import numpy as np
329/2: from sklearn.datasets import load_iris
329/3: iris = load_iris()
329/4: type(iris)
329/5: print(iris.DESCR)
329/6: X = iris.data
329/7: X
329/8: y = iris.target
329/9: y
329/10: from keras.utils import to_categorical
330/1: import numpy as np
330/2: from sklearn.datasets import load_iris
330/3: iris = load_iris()
330/4: type(iris)
330/5: print(iris.DESCR)
330/6: X = iris.data
330/7: X
330/8: y = iris.target
330/9: y
330/10: from keras.utils import to_categorical
331/1: y
332/1: import numpy as np
332/2: from sklearn.datasets import load_iris
332/3: iris = load_iris()
332/4: type(iris)
332/5: print(iris.DESCR)
332/6: X = iris.data
332/7: X
332/8: y = iris.target
332/9: y
332/10: from keras.utils import to_categorical
334/1: import numpy as np
334/2:
import tensorflow as tf
from tensorflow import keras
336/1: import numpy as np
336/2: from sklearn.datasets import load_iris
336/3: iris = load_iris()
336/4: type(iris)
336/5: print(iris.DESCR)
336/6: X = iris.data
336/7: X
336/8: y = iris.target
336/9: y
336/10: from keras.utils import to_categorical
336/11: import tensorflow
337/1: import numpy as np
337/2: from sklearn.datasets import load_iris
337/3: iris = load_iris()
337/4: type(iris)
337/5: print(iris.DESCR)
337/6: X = iris.data
337/7: X
337/8: y = iris.target
337/9: y
337/10: from keras.utils import to_categorical
337/11: import tensorflow
337/12: import tensorflow
337/13: import tensorflow as tf
337/14: import tensorflow as tf
338/1: from keras.utils import to_categorical
338/2: import tensorflow as tf
339/1: import tensorflow as tf
339/2: from keras.utils import to_categorical
340/1: from keras.utils import to_categorical
340/2: import tensorflow as tf
340/3: import tensorflow as tf
341/1: import tensorflow as tf
342/1: import tensorflow as tf
343/1: import tensorflow as tf
344/1: import tensorflow as tf
344/2: from keras.utils import to_categorical
345/1: import tensorflow as tf
345/2: from keras.utils import to_categorical
345/3: from tensorflow.keras.utils import to_categorical
346/1: import numpy as np
346/2: from sklearn.datasets import load_iris
346/3: type(iris)
347/1:
# Import spaCy and load the language library
import spacy
nlp = spacy.load('en_core_web_md')  # make sure to use a larger model!
349/1:
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
350/1: import numpy as np
350/2: from sklearn.datasets import load_iris
350/3: iris = load_iris()
350/4: import tensorflow as tf
351/1: import numpy as np
351/2: from sklearn.datasets import load_iris
351/3: iris = load_iris()
351/4: type(iris)
351/5: print(iris.DESCR)
351/6: X = iris.data
351/7: X
351/8: y = iris.target
351/9: y
351/10: import tensorflow as tf
351/11: from keras.utils import to_categorical
351/12: import numpy as np
351/13: from sklearn.datasets import load_iris
351/14: iris = load_iris()
351/15: type(iris)
351/16: print(iris.DESCR)
351/17: X = iris.data
351/18: X
351/19: y = iris.target
351/20: y
351/21: import tensorflow as tf
351/22: from keras.utils import to_categorical
351/23: from tensorflow.keras.utils import to_categorical
351/24: y = to_categorical(y)
351/25: y.shape
351/26: y
351/27: from sklearn.model_selection import train_test_split
351/28: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
351/29: X_train
351/30: X_test
351/31: y_train
351/32: y_test
351/33: from sklearn.preprocessing import MinMaxScaler
351/34: scaler_object = MinMaxScaler()
351/35: scaler_object.fit(X_train)
351/36: scaled_X_train = scaler_object.transform(X_train)
351/37: scaled_X_test = scaler_object.transform(X_test)
351/38: X_train.max()
351/39: scaled_X_train.max()
351/40: X_train
351/41: scaled_X_train
351/42:
from keras.models import Sequential
from keras.layers import Dense
351/43:
model = Sequential()
model.add(Dense(8, input_dim=4, activation='relu'))
model.add(Dense(8, input_dim=4, activation='relu'))
model.add(Dense(3, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
351/44: model.summary()
351/45:
# Play around with number of epochs as well!
model.fit(scaled_X_train,y_train,epochs=150, verbose=2)
351/46: scaled_X_test
351/47:
# Spits out probabilities by default.
# model.predict(scaled_X_test)
351/48: model.predict_classes(scaled_X_test)
351/49: model.predict(scaled_X_test)
351/50: model.metrics_names
351/51: model.evaluate(x=scaled_X_test,y=y_test)
351/52: from sklearn.metrics import confusion_matrix,classification_report
351/53: predictions = model.predict_classes(scaled_X_test)
351/54: predictions
351/55: predictions = model.argmax(scaled_X_test)
351/56: np.argmax(model.predict(scaled_X_test), axis=-1)
351/57: model.predict(scaled_X_test)
351/58: np.argmax(model.predict(scaled_X_test), axis=-1)
351/59: np.argmax(model.predict(scaled_X_test), axis=1)
351/60: predictions = np.argmax(model.predict(scaled_X_test), axis=1)
351/61: predictions
351/62: y_test.argmax(axis=1)
351/63: confusion_matrix(y_test.argmax(axis=1),predictions)
351/64: print(classification_report(y_test.argmax(axis=1),predictions))
352/1:
def read_file(filepath):
    
    with open(filepath) as f:
        str_text = f.read()
    
    return str_text
352/2: read_file('moby_dick_four_chapters.txt')
352/3:
import spacy
nlp = spacy.load('en',disable=['parser', 'tagger','ner'])

nlp.max_length = 1198623
352/4:
import spacy
nlp = spacy.load('en_core_web_sm',disable=['parser', 'tagger','ner'])

nlp.max_length = 1198623
352/5:
import spacy
nlp = spacy.load('en_core_web_sm',disable=['parser', 'tagger','ner'])

nlp.max_length = 1198623
352/6:
import spacy
nlp = spacy.load('en_core_web_sm',disable=['parser', 'tagger','ner'])

nlp.max_length = 1198623
352/7:
import spacy
nlp = spacy.load('en',disable=['parser', 'tagger','ner'])

nlp.max_length = 1198623
352/8:
import spacy
nlp = spacy.load('en_core_web_sm',disable=['parser', 'tagger','ner'])

nlp.max_length = 1198623
352/9:
def separate_punc(doc_text):
    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\n\n \n\n\n!"-#$%&()--.*+,-/:;<=>?@[\\]^_`{|}~\t\n ']
352/10:
d = read_file('melville-moby_dick.txt')
tokens = separate_punc(d)
352/11: tokens
352/12: len(tokens)
352/13: 4431/25
353/1:
import pandas as pd
import numpy as np
import re
353/2: !conada install pandas
353/3:
import pandas as pd
import numpy as np
import re
353/4:
import matplotlib.pyplot as plt
import seaborn as sns
353/5:
import pandas as pd
import numpy as np
import re
353/6:
import matplotlib.pyplot as plt
import seaborn as sns
353/7:
import pandas as pd
import numpy as np
import re
353/8:
import matplotlib.pyplot as plt
import seaborn as sns
353/9: pd.set_option('max_colwidth', -1)
353/10:
import plotly.graph_objs as go 
from plotly.offline import init_notebook_mode, iplot 
init_notebook_mode(connected = True) 
import os 
import matplotlib.pyplot as plt#visualization 
%matplotlib inline 
#warnings.filterwarnings("ignore") 
import io 
import plotly.offline as py#visualization 
py.init_notebook_mode(connected=True)#visualization 
import plotly.graph_objs as go#visualization 
import plotly.tools as tls#visualization 
import plotly.figure_factory as ff
353/11:
import pandas as pd
import numpy as np
import re
353/12:
import matplotlib.pyplot as plt
import seaborn as sns
353/13: pd.set_option('max_colwidth', -1)
353/14:
import plotly.graph_objs as go 
from plotly.offline import init_notebook_mode, iplot 
init_notebook_mode(connected = True) 
import os 
import matplotlib.pyplot as plt#visualization 
%matplotlib inline 
#warnings.filterwarnings("ignore") 
import io 
import plotly.offline as py#visualization 
py.init_notebook_mode(connected=True)#visualization 
import plotly.graph_objs as go#visualization 
import plotly.tools as tls#visualization 
import plotly.figure_factory as ff
353/15:
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
353/16: !conda install contractions
353/17:
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
353/18: !conda install contractions
353/19:
import nltk
nltk.download()
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
353/20: !pip install contractions
353/21:
import nltk
nltk.download()
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
353/22:
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
353/23:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
353/24:
nltk.download('all')
nltk.download('stopwords')
353/25: STOPWORDS = set(stopwords.words('english'))
353/26: wnl = WordNetLemmatizer()
353/27: wnl.lemmatize('word', pos='n')
353/28: df_train.head()
353/29: df_train.isnull().mean()
353/30: df_train.shape, df_test.shape
353/31:
#cardinality check
for col in df_train.columns:
    print('{} has {} unique instances'.format(col, len(df_train[col].unique())))
353/32: df_train.info(null_counts=True)
353/33: df_train.keyword.value_counts()
353/34:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword',
              data=df_train,
              palette=['lightblue'],
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
353/35:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword', 
              hue='target', 
              data=df_train, 
              palette=['lightblue', 'red'])
plt.xticks(rotation=90)
plt.legend(loc='upper right')
plt.title("Distribution of Target per Keyword ")
plt.show()
353/36:
## Let's show the most common keyword for real disasters
df_train[df_train.target == 1].keyword.value_counts()[:10]
353/37:
## Let's show the most common keyword for not real disasters
df_train[df_train.target == 0].keyword.value_counts()[:10]
353/38:
df_target_0 = df_train[df_train.target == 0].groupby(['keyword']).count()['id'].reset_index()
df_target_1 = df_train[df_train.target == 1].groupby(['keyword']).count()['id'].reset_index()
353/39: df_target_1
353/40:
merge_counts_keywords = pd.merge(left=df_target_0,
                                 right=df_target_1,
                                 on='keyword',
                                 how='outer'
                                 ).fillna(0)

merge_counts_keywords.columns = ['keyword', 'target_0', 'target_1']
merge_counts_keywords['prob_real_disasters'] = (merge_counts_keywords.target_1 - merge_counts_keywords.target_0)\
                                                /merge_counts_keywords.target_1
353/41: merge_counts_keywords[merge_counts_keywords.keyword == 'bombing']
353/42: df_train.target.value_counts()/len(df_train)
353/43: df_train[df_train.keyword.isna()].target.value_counts()
353/44:
url_pattern = r'(www.|http[s]?://)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
test_string = 'I am at http://t.co/GKYe6gjTk5 okay'
test_op = re.sub(url_pattern, '', test_string)
test_op
353/45:
test_string = 'I am at @nabanita #python testing 123'
html_entities = r'@([a-z0-9]+)|#'
test_op = re.sub(html_entities, '', test_string)
test_op
353/46:
def remove_urls(text):
    ''' This method takes in text to remove urls and website links, if any'''
    url_pattern = r'(www.|http[s]?://)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
    text = re.sub(url_pattern, '', text)
    return text
353/47:
def remove_social_media_tags(text):
    ''' This method removes @ and # tags'''
    tag_pattern = r'@([a-z0-9]+)|#'
    text = re.sub(tag_pattern, '', text)
    return text
353/48:
def convert_lower_case(text):
    return text.lower()
353/49:
def count_punctuations(text):
    getpunctuation = re.findall('[.?"\'`\,\-\!:;\(\)\[\]\\/‚Äú‚Äù]+?', text)
    return len(getpunctuation)
353/50:
def preprocess_text(x):
    cleaned_text = re.sub(r'[^a-zA-Z\d\s]+', '', x)
    word_list = []
    for each_word in cleaned_text.split(' '):
        word_list.append(contractions.fix(each_word).lower())
    word_list = [
        wnl.lemmatize(each_word.strip()) for each_word in word_list
        if each_word not in STOPWORDS and each_word.strip() != ''
    ]
    return " ".join(word_list)
353/51:
df_train['text'] = df_train['text'].apply(remove_urls)
df_train['text'] = df_train['text'].apply(remove_social_media_tags)
df_train['text'] = df_train['text'].apply(convert_lower_case)
df_train['punctuation_count'] = df_train['text'].apply(count_punctuations)
df_train['text'] = df_train['text'].apply(preprocess_text)
353/52: df_train
353/53: df_train[df_train.text.str.contains('http', regex=False)]
353/54:
def histogram2data(data, column1, name1, data2, column2, name2, bound) :
    trace1 = go.Histogram(x  = data[column1],
                          histnorm= "percent",
                          name = name1,
                          xbins=dict(start=np.min(data[column1]), size=bound, end=np.max(data[column1])),
                          marker = dict(line = dict(width = .5,
                                                    color = "black"
                                                    )
                                        ),
                         opacity = .5 
                         ) 
    trace2 = go.Histogram(x  = data2[column2],
                          histnorm= "percent",
                          name = name2,
                          xbins=dict(start=np.min(data2[column2]), size=bound, end=np.max(data2[column2])),
                          marker = dict(line = dict(width = .5,
                                                    color = "black"
                                                    )
                                        ),
                         opacity = .5 
                         ) 
    datadata = [trace1, trace2]
    layout = go.Layout(dict(title =column1 + " distribution ",
                            plot_bgcolor  = "rgba(243,243,243, 0.5)",
                            paper_bgcolor = "rgba(243,243,243, 0.5)",
                            barmode='overlay',
                            
                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',
                                             title = column1,
#                                             zerolinewidth=1,
                                             ticklen=5,
                                             gridwidth=2
                                            ),
                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',
                                             title = "percent",
                                             zerolinewidth=1,
                                             ticklen=5,
                                             gridwidth=2
                                            ),
                           )
                      )
    fig  = go.Figure(data=datadata, layout=layout)
    py.iplot(fig)
353/55:
histogram2data(df_train[df_train.target == 0], 'punctuation_count', '0', \
               df_train[df_train.target == 1], 'punctuation_count', '1', 1)
353/56: df_train.keyword.value_counts()
353/57:
def clean_keyword(text):
    text = str(text).replace('%20', ' ')
    text = str(text).replace('nan', 'no info')
    return text
353/58: df_train.keyword = df_train.keyword.apply(clean_keyword)
353/59:
def get_numbers_in_tweet_flag(text):
    list_numbers = re.findall(r'\d+', text)
    if list_numbers:
        return 1
    return 0
353/60:
def get_numbers_in_tweet_count(text):
    list_numbers = re.findall(r'\d+', text)
    return len(list_numbers)
353/61:
df_train['num_in_tweets_flag'] = df_train.text.apply(get_numbers_in_tweet_flag)
df_train['num_in_tweets_count'] = df_train.text.apply(get_numbers_in_tweet_count)
353/62: df_train
353/63:
histogram2data(df_train[df_train.target == 0], 'num_in_tweets_count', '0', \
               df_train[df_train.target == 1], 'num_in_tweets_count', '1', 1)
353/64: from nltk.sentiment.vader import SentimentIntensityAnalyzer
353/65: sid = SentimentIntensityAnalyzer()
353/66: sid.polarity_scores('I love u')['compound']
353/67: df_train['sentiment_compound']  = df_train.text.apply(lambda review: sid.polarity_scores(review)['compound'])
353/68: df_train['sentiment_compound_score'] = df_train['sentiment_compound'].apply(lambda c: 'pos' if c >=0 else 'neg')
353/69:
histogram2data(df_train[df_train.target == 0], 'sentiment_compound', '0', \
               df_train[df_train.target == 1], 'sentiment_compound', '1', .05)
353/70:
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
353/71: df_train['text_tokenized'] = df_train.text.apply(word_tokenize)
353/72:
df_train['words_per_tweet'] = df_train.text_tokenized.apply(len)
df_train['char_per_tweet'] = df_train.text.apply(len)
353/73:
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
sns.set(rc = {'figure.figsize':(15,8)})
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
353/74:
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
sns.set(rc = {'figure.figsize':(3,8)})
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
353/75:
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
sns.set(rc = {'figure.figsize':(5,3)})
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
353/76:
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
splt.figure(figsize = (15,8))
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
353/77:
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
plt.figure(figsize = (15,8))
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
353/78:
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
353/79:
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.figure(figsize = (15,8))
plt.show()
353/80:
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.figure(figsize = (3,8))
plt.show()
353/81:
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.figure(figsize = (3,1))
plt.show()
353/82:
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
353/83:
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
plt.figure(figsize=(1,1))
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
353/84:
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
plt.figure(figsize=(15,1))
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
353/85:
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
353/86:
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.figure(figsize=(15,1))
plt.show()
353/87:
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.figure(figsize=(15,10))
plt.show()
353/88:
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
353/89: df_train['text_tokenized'] = df_train.text.apply(word_tokenize)
353/90:
df_train['words_per_tweet'] = df_train.text_tokenized.apply(len)
df_train['char_per_tweet'] = df_train.text.apply(len)
353/91:
histogram2data(df_train[df_train.target == 0], 'words_per_tweet', '0', \
               df_train[df_train.target == 1], 'words_per_tweet', '1', 1)
353/92:
histogram2data(df_train[df_train.target == 0], 'char_per_tweet', '0', \
               df_train[df_train.target == 1], 'char_per_tweet', '1', 1)
353/93: df_train.location.isna().mean()
353/94:
plt.figure(figsize=[10, 60])
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
353/95:
plt.figure(figsize=[10, 5])
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
353/96:
plt.figure(figsize=[6, 4])
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
353/97:
plt.figure(figsize=[5, 3])
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
353/98: df_train
353/99: df_train['all_text_joined'] = df_train['text_tokenized'].apply(lambda x: " ".join(x))
353/100:
cols_to_train = ['text']
tt = TweetTokenizer()
353/101:
X_train, X_test, y_train, y_test = train_test_split(df_train[cols_to_train],
                                                    df_train['target'].values,
                                                    test_size=0.2,
                                                    random_state=42)
353/102:
ct = ColumnTransformer([('count_vec',
                         CountVectorizer(tokenizer=tt.tokenize,
                                         ngram_range=(1, 2)),
                                         'text')],
                       remainder='passthrough')
353/103:
ct.fit(X_train)
X_train_sparse = ct.transform(X_train)
X_test_sparse = ct.transform(X_test)
353/104: print(X_train_sparse.toarray())
353/105:
log_reg = LogisticRegression()
log_reg.fit(X_train_sparse, y_train)
y_test_pred = log_reg.predict(X_test_sparse)
y_train_pred = log_reg.predict(X_train_sparse)
353/106: print(classification_report(y_train, y_train_pred))
353/107: print(classification_report(y_test, y_test_pred))
353/108:
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
353/109:
mnb = MultinomialNB()
mnb.fit(X_train_sparse, y_train)
y_test_pred = log_reg.predict(X_test_sparse)
y_train_pred = log_reg.predict(X_train_sparse)
353/110: print(classification_report(y_train, y_train_pred))
353/111: print(classification_report(y_test, y_test_pred))
353/112:
mnb = MultinomialNB()
mnb.fit(X_train_sparse, y_train)
y_test_pred = mnb.predict(X_test_sparse)
y_train_pred = mnb.predict(X_train_sparse)
353/113: print(classification_report(y_train, y_train_pred))
353/114: print(classification_report(y_test, y_test_pred))
353/115: df_train.head(4)
353/116:
cols_to_train = ['text', 'words_per_tweet', 'char_per_tweet', 'sentiment_compound']

tt = TweetTokenizer()

X_train, X_test, y_train, y_test = train_test_split(df_train[cols_to_train],
                                                    df_train['target'].values,
                                                    test_size=0.2,
                                                    random_state=42)

ct = ColumnTransformer([('tfidf',
                         TfidfVectorizer(tokenizer=tt.tokenize,
                                         ngram_range=(1, 2),
                                         smooth_idf=True), 'text')],
                       remainder='passthrough')

ct.fit(X_train)
X_train_sparse = ct.transform(X_train)
X_test_sparse = ct.transform(X_test)
X_train_sparse.shape
353/117: 6090, 52758)
353/118:
log_reg = LogisticRegression()
log_reg.fit(X_train_sparse, y_train)
y_test_pred = log_reg.predict(X_test_sparse)
y_train_pred = log_reg.predict(X_train_sparse)
353/119: print(classification_report(y_train, y_train_pred))
353/120: print(classification_report(y_test, y_test_pred))
353/121:
mnb = MultinomialNB()
mnb.fit(X_train_sparse, y_train)
y_test_pred = mnb.predict(X_test_sparse)
y_train_pred = mnb.predict(X_train_sparse)
353/122: X_train_sparse
353/123: X_train_sparse[1]
353/124: X_train_sparse[1].show
353/125: print(X_train_sparse[1])
353/126:
cols_to_train = ['text', 'words_per_tweet', 'char_per_tweet']

tt = TweetTokenizer()

X_train, X_test, y_train, y_test = train_test_split(df_train[cols_to_train],
                                                    df_train['target'].values,
                                                    test_size=0.2,
                                                    random_state=42)

ct = ColumnTransformer([('tfidf',
                         TfidfVectorizer(tokenizer=tt.tokenize,
                                         ngram_range=(1, 2),
                                         smooth_idf=True), 'text')],
                       remainder='passthrough')

ct.fit(X_train)
X_train_sparse = ct.transform(X_train)
X_test_sparse = ct.transform(X_test)
X_train_sparse.shape
353/127: 6090, 52758)
353/128: print(classification_report(y_train, y_train_pred))
353/129: print(classification_report(y_test, y_test_pred))
353/130:
mnb = MultinomialNB()
mnb.fit(X_train_sparse, y_train)
y_test_pred = mnb.predict(X_test_sparse)
y_train_pred = mnb.predict(X_train_sparse)
353/131: print(X_train_sparse[1])
353/132:
mnb = MultinomialNB()
mnb.fit(X_train_sparse, y_train)
y_test_pred = mnb.predict(X_test_sparse)
y_train_pred = mnb.predict(X_train_sparse)
353/133: print(classification_report(y_train, y_train_pred))
353/134: print(classification_report(y_test, y_test_pred))
353/135:
mnb = MultinomialNB()
mnb.fit(X_train_sparse, y_train)
y_test_pred = mnb.predict(X_test_sparse)
y_train_pred = mnb.predict(X_train_sparse)
353/136:
X_train, X_test, y_train, y_test = train_test_split(df_train[cols_to_train],
                                                    df_train['target'].values,
                                                    test_size=0.2,
                                                    random_state=42)
353/137:
cols_to_train = ['text', 'words_per_tweet', 'char_per_tweet']
tt = TweetTokenizer()
353/138:
X_train, X_test, y_train, y_test = train_test_split(df_train[cols_to_train],
                                                    df_train['target'].values,
                                                    test_size=0.2,
                                                    random_state=42)
353/139:
ct = ColumnTransformer([('count_vec',
                         CountVectorizer(tokenizer=tt.tokenize,
                                         ngram_range=(1, 2)),
                                         'text')],
                       remainder='passthrough')
353/140:
ct.fit(X_train)
X_train_sparse = ct.transform(X_train)
X_test_sparse = ct.transform(X_test)
353/141: print(X_train_sparse.toarray())
353/142:
mnb = MultinomialNB()
mnb.fit(X_train_sparse, y_train)
y_test_pred = mnb.predict(X_test_sparse)
y_train_pred = mnb.predict(X_train_sparse)
353/143: print(classification_report(y_train, y_train_pred))
353/144: print(classification_report(y_test, y_test_pred))
353/145:
cols_to_train = ['text', 'words_per_tweet', 'char_per_tweet']

tt = TweetTokenizer()

X_train, X_test, y_train, y_test = train_test_split(df_train[cols_to_train],
                                                    df_train['target'].values,
                                                    test_size=0.2,
                                                    random_state=42)

ct = ColumnTransformer([('tfidf',
                         TfidfVectorizer(tokenizer=tt.tokenize,
                                         ngram_range=(1, 2),
                                         smooth_idf=True), 'text')],
                       remainder='passthrough')

ct.fit(X_train)
X_train_sparse = ct.transform(X_train)
X_test_sparse = ct.transform(X_test)
X_train_sparse.shape
353/146:
mnb = MultinomialNB()
mnb.fit(X_train_sparse, y_train)
y_test_pred = mnb.predict(X_test_sparse)
y_train_pred = mnb.predict(X_train_sparse)
353/147: print(classification_report(y_train, y_train_pred))
353/148: print(classification_report(y_test, y_test_pred))
354/1:
import pandas as pd
import numpy as np
import re
354/2:
import matplotlib.pyplot as plt
import seaborn as sns
354/3: pd.set_option('max_colwidth', -1)
354/4:
import plotly.graph_objs as go 
from plotly.offline import init_notebook_mode, iplot 
init_notebook_mode(connected = True) 
import os 
import matplotlib.pyplot as plt#visualization 
%matplotlib inline 
#warnings.filterwarnings("ignore") 
import io 
import plotly.offline as py#visualization 
py.init_notebook_mode(connected=True)#visualization 
import plotly.graph_objs as go#visualization 
import plotly.tools as tls#visualization 
import plotly.figure_factory as ff
354/5:
import nltk
nltk.download()
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
355/1:
import pandas as pd
import numpy as np
import re
355/2: pd.set_option('max_colwidth', -1)
355/3:
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objs as go 
from plotly.offline import init_notebook_mode, iplot 
init_notebook_mode(connected = True) 
import os 
import matplotlib.pyplot as plt#visualization 
%matplotlib inline 
#warnings.filterwarnings("ignore") 
import io 
import plotly.offline as py#visualization 
py.init_notebook_mode(connected=True)#visualization 
import plotly.graph_objs as go#visualization 
import plotly.tools as tls#visualization 
import plotly.figure_factory as ff
355/4:
import nltk
nltk.download()
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
355/5:
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
#from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
#from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
355/6:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
355/7:
nltk.download('all')
nltk.download('stopwords')
356/1:
import pandas as pd
import numpy as np
import re
356/2: pd.set_option('max_colwidth', -1)
356/3:
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objs as go 
from plotly.offline import init_notebook_mode, iplot 
init_notebook_mode(connected = True) 
import os 
import matplotlib.pyplot as plt#visualization 
%matplotlib inline 
#warnings.filterwarnings("ignore") 
import io 
import plotly.offline as py#visualization 
py.init_notebook_mode(connected=True)#visualization 
import plotly.graph_objs as go#visualization 
import plotly.tools as tls#visualization 
import plotly.figure_factory as ff
356/4:
import nltk
#nltk.download()
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
356/5:
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
#from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
#from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
356/6:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
356/7:
nltk.download('all')
#nltk.download('stopwords')
356/8: STOPWORDS = set(stopwords.words('english'))
356/9:
#import nltk
#nltk.download('wordnet')
356/10:
#import nltk
#dler = nltk.downloader.Downloader()
#dler._update_index()
#dler.download('all')
356/11: wnl = WordNetLemmatizer()
356/12: wnl.lemmatize('word', pos='n')
356/13: df_train.head()
356/14: df_train.isnull().mean()
356/15: df_train.shape, df_test.shape
356/16:
#cardinality check
for col in df_train.columns:
    print('{} has {} unique instances'.format(col, len(df_train[col].unique())))
356/17: df_train.info(null_counts=True)
356/18: df_train.info()
356/19: df_train.keyword.value_counts()
356/20:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword',
              data=df_train,
              palette=['lightblue'],
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
356/21:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword', 
              hue='target', 
              data=df_train, 
              palette=['lightblue', 'red'])
plt.xticks(rotation=90)
plt.legend(loc='upper right')
plt.title("Distribution of Target per Keyword ")
plt.show()
356/22:
## Let's show the most common keyword for real disasters
df_train[df_train.target == 1].keyword.value_counts()[:10]
356/23:
## Let's show the most common keyword for not real disasters
df_train[df_train.target == 0].keyword.value_counts()[:10]
356/24:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword', 
              hue='target', 
              data=df_train, 
              palette=['lightblue', 'red'])
plt.xticks(rotation=90)
plt.legend(loc='upper right')
plt.title("Distribution of Target per Keyword ")
plt.show()
356/25:
df_target_0 = df_train[df_train.target == 0].groupby(['keyword']).count()['id'].reset_index()
df_target_1 = df_train[df_train.target == 1].groupby(['keyword']).count()['id'].reset_index()
356/26: df_target_1
356/27:
merge_counts_keywords = pd.merge(left=df_target_0,
                                 right=df_target_1,
                                 on='keyword',
                                 how='outer'
                                 ).fillna(0)

merge_counts_keywords.columns = ['keyword', 'target_0', 'target_1']
merge_counts_keywords['prob_real_disasters'] = (merge_counts_keywords.target_1 - merge_counts_keywords.target_0)\
                                                /merge_counts_keywords.target_1
356/28: merge_counts_keywords[merge_counts_keywords.keyword == 'bombing']
356/29: df_train.keyword.value_counts()
356/30: df_train.keyword.inoque()
356/31: df_train.keyword.unoque()
356/32: df_train.keyword.unique()
356/33: df_train.keyword.nunique()
356/34: df_test.head()
356/35: df_train.target.value_counts()/len(df_train)
356/36: df_train[df_train.keyword.isna()].target.value_counts()
356/37:
url_pattern = r'(www.|http[s]?://)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
test_string = 'I am at http://t.co/GKYe6gjTk5 okay'
test_op = re.sub(url_pattern, '', test_string)
test_op
356/38:
test_string = 'I am at @nabanita #python testing 123'
html_entities = r'@([a-z0-9]+)|#'
test_op = re.sub(html_entities, '', test_string)
test_op
356/39:
def remove_urls(text):
    ''' This method takes in text to remove urls and website links, if any'''
    url_pattern = r'(www.|http[s]?://)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
    text = re.sub(url_pattern, '', text)
    return text
356/40:
def remove_social_media_tags(text):
    ''' This method removes @ and # tags'''
    tag_pattern = r'@([a-z0-9]+)|#'
    text = re.sub(tag_pattern, '', text)
    return text
356/41:
def convert_lower_case(text):
    return text.lower()
356/42:
def count_punctuations(text):
    getpunctuation = re.findall('[.?"\'`\,\-\!:;\(\)\[\]\\/‚Äú‚Äù]+?', text)
    return len(getpunctuation)
356/43:
def preprocess_text(x):
    cleaned_text = re.sub(r'[^a-zA-Z\d\s]+', '', x)
    word_list = []
    for each_word in cleaned_text.split(' '):
        word_list.append(contractions.fix(each_word).lower())
    word_list = [
        wnl.lemmatize(each_word.strip()) for each_word in word_list
        if each_word not in STOPWORDS and each_word.strip() != ''
    ]
    return " ".join(word_list)
356/44:
df_train['text'] = df_train['text'].apply(remove_urls)
df_train['text'] = df_train['text'].apply(remove_social_media_tags)
df_train['text'] = df_train['text'].apply(convert_lower_case)
df_train['punctuation_count'] = df_train['text'].apply(count_punctuations)
df_train['text'] = df_train['text'].apply(preprocess_text)
356/45: df_train
356/46: df_train[df_train.text.str.contains('http', regex=False)]
356/47:
def histogram2data(data, column1, name1, data2, column2, name2, bound) :
    trace1 = go.Histogram(x  = data[column1],
                          histnorm= "percent",
                          name = name1,
                          xbins=dict(start=np.min(data[column1]), size=bound, end=np.max(data[column1])),
                          marker = dict(line = dict(width = .5,
                                                    color = "black"
                                                    )
                                        ),
                         opacity = .5 
                         ) 
    trace2 = go.Histogram(x  = data2[column2],
                          histnorm= "percent",
                          name = name2,
                          xbins=dict(start=np.min(data2[column2]), size=bound, end=np.max(data2[column2])),
                          marker = dict(line = dict(width = .5,
                                                    color = "black"
                                                    )
                                        ),
                         opacity = .5 
                         ) 
    datadata = [trace1, trace2]
    layout = go.Layout(dict(title =column1 + " distribution ",
                            plot_bgcolor  = "rgba(243,243,243, 0.5)",
                            paper_bgcolor = "rgba(243,243,243, 0.5)",
                            barmode='overlay',
                            
                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',
                                             title = column1,
#                                             zerolinewidth=1,
                                             ticklen=5,
                                             gridwidth=2
                                            ),
                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',
                                             title = "percent",
                                             zerolinewidth=1,
                                             ticklen=5,
                                             gridwidth=2
                                            ),
                           )
                      )
    fig  = go.Figure(data=datadata, layout=layout)
    py.iplot(fig)
356/48:
histogram2data(df_train[df_train.target == 0], 'punctuation_count', '0', \
               df_train[df_train.target == 1], 'punctuation_count', '1', 1)
356/49: df_train.keyword.value_counts()
356/50:
def clean_keyword(text):
    text = str(text).replace('%20', ' ')
    text = str(text).replace('nan', 'no info')
    return text
356/51: df_train.keyword = df_train.keyword.apply(clean_keyword)
356/52:
def get_numbers_in_tweet_flag(text):
    list_numbers = re.findall(r'\d+', text)
    if list_numbers:
        return 1
    return 0
356/53:
def get_numbers_in_tweet_count(text):
    list_numbers = re.findall(r'\d+', text)
    return len(list_numbers)
356/54:
df_train['num_in_tweets_flag'] = df_train.text.apply(get_numbers_in_tweet_flag)
df_train['num_in_tweets_count'] = df_train.text.apply(get_numbers_in_tweet_count)
356/55: df_train
356/56:
histogram2data(df_train[df_train.target == 0], 'num_in_tweets_count', '0', \
               df_train[df_train.target == 1], 'num_in_tweets_count', '1', 1)
356/57: from nltk.sentiment.vader import SentimentIntensityAnalyzer
356/58: sid = SentimentIntensityAnalyzer()
356/59: sid.polarity_scores('I love u')['compound']
356/60: df_train['sentiment_compound']  = df_train.text.apply(lambda review: sid.polarity_scores(review)['compound'])
356/61: df_train['sentiment_compound_score'] = df_train['sentiment_compound'].apply(lambda c: 'pos' if c >=0 else 'neg')
356/62:
histogram2data(df_train[df_train.target == 0], 'sentiment_compound', '0', \
               df_train[df_train.target == 1], 'sentiment_compound', '1', .05)
356/63:
plt.figure(figsize=[5, 3])
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
356/64: df_train['text_tokenized'] = df_train.text.apply(word_tokenize)
356/65:
df_train['words_per_tweet'] = df_train.text_tokenized.apply(len)
df_train['char_per_tweet'] = df_train.text.apply(len)
356/66:
histogram2data(df_train[df_train.target == 0], 'words_per_tweet', '0', \
               df_train[df_train.target == 1], 'words_per_tweet', '1', 1)
356/67:
plt.figure(figsize=[5, 3])
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
356/68:
plt.figure(figsize=[4, 2])
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
356/69:
plt.figure(figsize=[4, 4])
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
356/70:
plt.figure(figsize=[3, 3])
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
356/71:
plt.figure(figsize=[3, 4])
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
356/72: df_train['text_tokenized'] = df_train.text.apply(word_tokenize)
356/73:
df_train['words_per_tweet'] = df_train.text_tokenized.apply(len)
df_train['char_per_tweet'] = df_train.text.apply(len)
356/74:
histogram2data(df_train[df_train.target == 0], 'words_per_tweet', '0', \
               df_train[df_train.target == 1], 'words_per_tweet', '1', 1)
356/75:
histogram2data(df_train[df_train.target == 0], 'char_per_tweet', '0', \
               df_train[df_train.target == 1], 'char_per_tweet', '1', 1)
356/76: df_train.location.isna().mean()
356/77: df_train['all_text_joined'] = df_train['text_tokenized'].apply(lambda x: " ".join(x))
356/78:
cols_to_train = ['text', 'words_per_tweet', 'char_per_tweet']
tt = TweetTokenizer()
356/79:
X_train, X_test, y_train, y_test = train_test_split(df_train[cols_to_train],
                                                    df_train['target'].values,
                                                    test_size=0.2,
                                                    random_state=42)
356/80:
ct = ColumnTransformer([('count_vec',
                         CountVectorizer(tokenizer=tt.tokenize,
                                         ngram_range=(1, 2)),
                                         'text')],
                       remainder='passthrough')
356/81:
ct.fit(X_train)
X_train_sparse = ct.transform(X_train)
X_test_sparse = ct.transform(X_test)
356/82: print(X_train_sparse.toarray())
356/83:
log_reg = LogisticRegression()
log_reg.fit(X_train_sparse, y_train)
y_test_pred = log_reg.predict(X_test_sparse)
y_train_pred = log_reg.predict(X_train_sparse)
356/84: print(classification_report(y_train, y_train_pred))
356/85:
mnb = MultinomialNB()
mnb.fit(X_train_sparse, y_train)
y_test_pred = mnb.predict(X_test_sparse)
y_train_pred = mnb.predict(X_train_sparse)
356/86: print(classification_report(y_train, y_train_pred))
356/87: print(classification_report(y_test, y_test_pred))
356/88:

# search thresholds for imbalanced classification
from numpy import arange
from numpy import argmax
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
 
# apply threshold to positive probabilities to create labels
def to_labels(pos_probs, threshold):
    return (pos_probs >= threshold).astype('int')
 
# generate dataset
X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,
    n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=4)
# split into train/test sets
trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2, stratify=y)
# fit a model
model = LogisticRegression(solver='lbfgs')
model.fit(trainX, trainy)
# predict probabilities
yhat = model.predict_proba(testX)
# keep probabilities for the positive outcome only
probs = yhat[:, 1]
# define thresholds
thresholds = arange(0, 1, 0.001)
# evaluate each threshold
scores = [f1_score(testy, to_labels(probs, t)) for t in thresholds]
# get best threshold
ix = argmax(scores)
print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
356/89: probs
356/90: probs >= ,5
356/91: probs >= .5
356/92: probs
356/93: probs >= .05
356/94: probs >= .005
356/95: yhat
356/96: yhat[:, 1]
356/97: max(yhat[:, 1])
356/98: min(yhat[:, 1])
356/99: probs.astype('int')
356/100: sum(probs.astype('int'))
356/101: max(yhat[:, 1])
356/102: max(yhat[:, 1]).astype('int')
356/103: (probs >= .005).astype('int')
356/104: (probs >= .001).astype('int')
356/105: sum(probs >= .001).astype('int')
356/106: sum(probs >= .011).astype('int')
356/107: sum(probs >= .111).astype('int')
356/108: sum(probs >= .211).astype('int')
356/109: sum(probs >= .511).astype('int')
356/110: sum(probs >= .811).astype('int')
356/111: sum(probs >= .911).astype('int')
356/112: from sklearn.metrics import roc_curve
356/113:
mnb = MultinomialNB()
mnb.fit(X_train_sparse, y_train)
y_train_pred = mnb.predict(X_train_sparse)
y_test_pred = mnb.predict(X_test_sparse)
y_test_pred_proba = mnb.predict_proba(X_test_sparse)
356/114: print(classification_report(y_train, y_train_pred))
356/115: print(classification_report(y_test, y_test_pred))
356/116: fpr1, tpr1, thresh1 = roc_curve(y_test, y_test_pred_proba[:,1], pos_label=1)
356/117:
plt.plot(fpr1, tpr1, linestyle='--',color='orange', label='mnb')
# title
plt.title('ROC curve')
# x label
plt.xlabel('False Positive Rate')
# y label
plt.ylabel('True Positive rate')

plt.legend(loc='best')
plt.savefig('ROC',dpi=300)
plt.show();
356/118:
sns.lineplot(x=fpr1, y=tpr1)
#plt.plot(fpr1, tpr1, linestyle='--',color='orange', label='mnb')
# title
plt.title('ROC curve')
# x label
plt.xlabel('False Positive Rate')
# y label
plt.ylabel('True Positive rate')
plt.legend(loc='best')
plt.savefig('ROC',dpi=300)
plt.show();
356/119:
sns.lineplot(x=fpr1, y=tpr1, linestyle='--')
#plt.plot(fpr1, tpr1, linestyle='--',color='orange', label='mnb')
# title
plt.title('ROC curve')
# x label
plt.xlabel('False Positive Rate')
# y label
plt.ylabel('True Positive rate')
plt.savefig('ROC',dpi=300)
plt.show();
356/120:
sns.lineplot(x=fpr1, y=tpr1, linestyle='--')
#plt.plot(fpr1, tpr1, linestyle='--',color='orange', label='mnb')
# title
plt.title('ROC curve')
# x label
plt.xlabel('False Positive Rate')
# y label
plt.ylabel('True Positive rate')
plt.savefig('ROC',dpi=300)
plt.grid()
plt.show();
356/121:
sns.lineplot(x=fpr1, y=tpr1, linestyle='--')
#plt.plot(fpr1, tpr1, linestyle='--',color='orange', label='mnb')
# title
plt.title('ROC curve')
# x label
plt.xlabel('False Positive Rate')
# y label
plt.ylabel('True Positive rate')
plt.savefig('ROC',dpi=300)
plt.grid(linestyle='--')
plt.show();
356/122:
fpr1, tpr1, thresh1 = roc_curve(y_test, y_test_pred_proba[:,1], pos_label=1)
gmeans = sqrt(tpr * (1-fpr))
ix = argmax(gmeans)
print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
356/123:
fpr1, tpr1, thresh1 = roc_curve(y_test, y_test_pred_proba[:,1], pos_label=1)
gmeans = np.sqrt(tpr * (1-fpr))
ix = argmax(gmeans)
print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
356/124:
fpr, tpr, thresh = roc_curve(y_test, y_test_pred_proba[:,1], pos_label=1)
gmeans = np.sqrt(tpr * (1-fpr))
ix = argmax(gmeans)
print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
356/125:
sns.lineplot(x=fpr, y=tpr, linestyle='--')
#plt.plot(fpr1, tpr1, linestyle='--',color='orange', label='mnb')
# title
plt.title('ROC curve')
# x label
plt.xlabel('False Positive Rate')
# y label
plt.ylabel('True Positive rate')
plt.savefig('ROC',dpi=300)
plt.grid(linestyle='--')
plt.show();
356/126:
sns.lineplot(x=fpr, y=tpr, linestyle='--')
sns.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')
#plt.plot(fpr1, tpr1, linestyle='--',color='orange', label='mnb')
# title
plt.title('ROC curve')
# x label
plt.xlabel('False Positive Rate')
# y label
plt.ylabel('True Positive rate')
plt.savefig('ROC',dpi=300)
plt.grid(linestyle='--')
plt.show();
356/127:
sns.lineplot(x=fpr, y=tpr, linestyle='--')
plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')
#plt.plot(fpr1, tpr1, linestyle='--',color='orange', label='mnb')
# title
plt.title('ROC curve')
# x label
plt.xlabel('False Positive Rate')
# y label
plt.ylabel('True Positive rate')
plt.savefig('ROC',dpi=300)
plt.grid(linestyle='--')
plt.show();
356/128:
sns.lineplot(x=fpr, y=tpr, linestyle='--')
plt.scatter(fpr[ix], tpr[ix], marker='o', color='r', label='Best')
#plt.plot(fpr1, tpr1, linestyle='--',color='orange', label='mnb')
# title
plt.title('ROC curve')
# x label
plt.xlabel('False Positive Rate')
# y label
plt.ylabel('True Positive rate')
plt.savefig('ROC',dpi=300)
plt.grid(linestyle='--')
plt.show();
356/129: (pos_probs >= 0.195).astype('int')
356/130: (y_test_pred_proba >= 0.195).astype('int')
356/131: (y_test_pred_proba[:, 1] >= 0.195).astype('int')
356/132: new_y = (y_test_pred_proba[:, 1] >= 0.195).astype('int')
356/133: classification_report(y_test, new_y)
356/134: print(classification_report(y_test, new_y))
356/135: new_y = (y_test_pred_proba[:, 1] >= 0.5).astype('int')
356/136: print(classification_report(y_test, new_y))
356/137: print(classification_report(y_test, new_y))
356/138: new_y = (y_test_pred_proba[:, 1] >= 0.195).astype('int')
356/139: print(classification_report(y_test, new_y))
356/140: new_y = (y_test_pred_proba[:, 1] >= 0.1).astype('int')
356/141: print(classification_report(y_test, new_y))
356/142: print(classification_report(y_test, new_y))
356/143:
def remove_emoji(string):
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', string)
356/144: remove_emoji('qwe %)')
356/145: remove_emoji('qwe =)')
356/146:
def remove_emoji(string):
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    return emoji_pattern.findall(r'', string)
356/147: remove_emoji('qwe =)')
356/148: import emoji as emj
356/149: !conda install emoji
356/150: import emoji as emj
356/151:
def extract_emoji(df):
    df["emoji"] = ""
    for index, row in df.iterrows():
        for emoji in EMOJIS:
            if emoji in row["text"]:
                row["text"] = row["text"].replace(emoji, "")
                row["emoji"] += emoji

extract_emoji(df_train)
print(df_train.to_string())
356/152:
def extract_emoji(df):
    df["emoji"] = ""
    for index, row in df.iterrows():
        for emoji in EMOJIS:
            if emoji in row["text"]:
                row["text"] = row["text"].replace(emoji, "")
                row["emoji"] += emoji

extract_emoji(df_train)
print(df_train.to_string())
356/153:
EMOJIS = emj.UNICODE_EMOJI["en"]
def extract_emoji(df):
    df["emoji"] = ""
    for index, row in df.iterrows():
        for emoji in EMOJIS:
            if emoji in row["text"]:
                row["text"] = row["text"].replace(emoji, "")
                row["emoji"] += emoji

extract_emoji(df_train)
print(df_train.to_string())
356/154:
def emj_list(text):
    new_line_list=[]
    for word in text:
        emojis = emoji.distinct_emoji_list(word)
        new_line_list.extend([emoji.demojize(is_emoji) for is_emoji in emojis])
    print(new_line_list)
356/155: df_train['text'].apply(emj_list)
356/156:
def emj_list(text):
    new_line_list=[]
    for word in text:
        emojis = emj.distinct_emoji_list(word)
        new_line_list.extend([emoji.demojize(is_emoji) for is_emoji in emojis])
    print(new_line_list)
356/157: df_train['text'].apply(emj_list)
356/158:
def emj_list(text):
    new_line_list=[]
    for word in text:
        emojis = emj.distinct_emoji_list(word)
        new_line_list.extend([emoji.demojize(is_emoji) for is_emoji in emojis])
    if len(new_line_list) > 0:
        print(new_line_list)
356/159: df_train['text'].apply(emj_list)
356/160: df_train
356/161:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
356/162: df_train['text'].apply(emj_list)
356/163:
def emj_list(text):
    new_line_list=[]
    for word in text:
        emojis = emj.distinct_emoji_list(word)
        new_line_list.extend([emj.demojize(is_emoji) for is_emoji in emojis])
    if len(new_line_list) > 0:
        print(new_line_list)
356/164: df_train['text'].apply(emj_list)
356/165:
def emj_list(text):
    new_line_list=[]
    for word in text:
        emojis = emj.distinct_emoji_list(word)
        new_line_list.extend([emj.demojize(is_emoji) for is_emoji in emojis])
    if len(new_line_list) > 0:
        print(text)
        print(new_line_list)
356/166: df_train['text'].apply(emj_list)
356/167: df_train.text.str.contains('#fun')
356/168: df_train[df_train.text.str.contains('#fun')]
356/169:
df_train[df_train.text.str.contains('#love
                                    
                                    ')]
356/170: df_train[df_train.text.str.contains('#love')]
356/171: df_train[df_train.text.str.contains('#fear')]
356/172: df_train[df_train.text.str.contains('#')]
356/173: df_train[df_train.text.str.contains('#')]['target'].value_counts()
356/174: df_train[df_train.text.str.contains('#')]
356/175: df_train['hashtags'].apply(lambda x: [d['text'] for d in x])
356/176: df_train['hashtags'] = []
356/177: df_train['hashtags'] = []
356/178: df_train.apply(lambda x: [d['text'] for d in x])
356/179: df_train['text'].apply(lambda x: [d['text'] for d in x])
356/180:
def hashtags_extract(text):
    ''' This method removes @ and # tags'''
    tag_pattern = r'#(\w+)'
    text = re.findall(tag_pattern, text)
    print(text)
    return text
356/181: df_train['text'].apply(hashtags_extract())
356/182: df_train['text'].apply(hashtags_extract
356/183: df_train['text'].apply(hashtags_extract)
356/184: df_train['hash'] = df_train['text'].apply(hashtags_extract)
356/185: df_train
356/186: df_train[df_train.target == 0]
356/187: len(df_train.hash)
356/188: df_train[(df_train.target == 0) & (df_train.hash != [])]
356/189: str_ex = ['RockyFire', 'CAfire', 'wildfires']
356/190: ' '.join(for i in str_ex)
356/191: ' '.join([str(elem) for elem in str_ex])
356/192:
def hashtags_extract(text):
    tag_pattern = r'#(\w+)'
    text = re.findall(tag_pattern, text)
    print(text)
    return ' '.join([str(elem) for elem in text])
356/193: df_train['hash'] = df_train['text'].apply(hashtags_extract)
356/194: df_train
356/195: df_train[(df_train.target == 0) & (len(df_train.hash) > 0)]
356/196: df_train[(df_train.target == 0) && (len(df_train.hash) > 0)]
356/197: df_train[(df_train.target == 0) & (len(df_train.hash) > 0)]
356/198: df_train[(len(df_train.hash) > 0)]
356/199: df_train[(len(df_train.hash) == '')]
356/200: df_train[df_train.hash == '']
356/201: df_train[df_train.hash != '']
356/202: df_train[(df_train.target == 0) & (df_train.hash != '')]
356/203: df_train[df_train['target']==0]['text'].str.cat(sep=" ")
356/204: df_train[df_train['target']==0]['hash'].str.cat(sep=" ")
356/205: df_train[df_train['target']==0]['hash'].str.cat(sep=" ").split()
356/206: Counter(df_train[df_train['target']==0]['hash'].str.cat(sep=" ").split())
356/207: !from collections import Counter
356/208: from collections import Counter
356/209: Counter(df_train[df_train['target']==0]['hash'].str.cat(sep=" ").split())
356/210: dic_0 = Counter(df_train[df_train['target']==0]['hash'].str.cat(sep=" ").split())
356/211: sorted(dic_0.values())
356/212: dic_0
356/213: dic_0.most_common(10)
356/214: dic_1 = Counter(df_train[df_train['target']==1]['hash'].str.cat(sep=" ").split())
356/215: dic_1.most_common(10)
356/216:
def hashtags_extract(text):
    tag_pattern = r'#(\w+)'
    text = re.findall(tag_pattern, text)
    print(text)
    return ' '.join([str(elem).lower() for elem in text])
356/217: dic_0 = Counter(df_train[df_train['target']==0]['hash'].str.cat(sep=" ").split())
356/218: dic_0.most_common(10)
356/219: df_train['hash'] = df_train['text'].apply(hashtags_extract)
356/220: dic_0 = Counter(df_train[df_train['target']==0]['hash'].str.cat(sep=" ").split())
356/221: dic_0.most_common(10)
356/222: dic_1 = Counter(df_train[df_train['target']==1]['hash'].str.cat(sep=" ").split())
356/223: dic_1.most_common(10)
356/224: dic_1.most_common(15)
356/225: dic_0.most_common(15)
356/226: df_train
356/227: df_train.iloc[40:60]
356/228:
import spacy    
nlp = spacy.load('en')
356/229:
import spacy    
nlp = spacy.load('en_core_web_sm')
356/230:
def GPE_counter(text):
    nlp_text = nlp(text)
    for words in nlp_text:
        if words.ent.label_ == 'GPE':
            print(words)
356/231: df_train.apply(GPE_counter)
356/232: df_train.text.apply(GPE_counter)
356/233: #df_train.text.apply(GPE_counter)
356/234: nlp('e raw')
356/235: nlp('e raw').ent
356/236: nlp('Nastya').lab_
356/237: nlp('Nastya').ents
356/238: nlp('Nastya').ents.label_
356/239: nlp('Nastya is here').ents.label_
356/240: nlp('Nastya is here').ents
356/241: nlp('Nastya is here').ents
356/242: nlp('Nastya is here')
356/243: s = nlp('Nastya is here')
356/244: s.ents
356/245:
# Write a function to display basic entity info:
def show_ents(doc):
    if doc.ents:
        for ent in doc.ents:
            print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
    else:
        print('No named entities found.')
356/246:
doc = nlp(u'May I go to Washington, DC next May to see the Washington Monument?')

show_ents(doc)
356/247: df_train.text.apply(show_ents)
356/248:
# Write a function to display basic entity info:
def show_ents(doc):
    doc = nlp(doc)
    if doc.ents:
        for ent in doc.ents:
            print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
    else:
        print('No named entities found.')
356/249: df_train.text.apply(show_ents)
356/250:
# Write a function to display basic entity info:
def show_ents(doc):
    doc = nlp(doc)
    if doc.ents:
        for ent in doc.ents:
            if ent.label_ == 'GPE':
                print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
    else:
        print('No named entities found.')
356/251: df_train.text.apply(show_ents)
356/252:
def GPE_counter(text):
    nlp_text = nlp(text)
    for words in nlp_text:
        if words.ent.label_ == 'LOC':
            print(words)
356/253: df_train.text.apply(show_ents)
356/254:
# Write a function to display basic entity info:
def show_ents(doc):
    doc = nlp(doc)
    if doc.ents:
        for ent in doc.ents:
            if ent.label_ == 'LOC':
                print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
    else:
        print('No named entities found.')
356/255: df_train.text.apply(show_ents)
356/256:
# Write a function to display basic entity info:
def show_ents(doc):
    doc = nlp(doc)
    count_GPE = 0
    if doc.ents:
        for ent in doc.ents:
            if ent.label_ == 'GPE':
                #print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
                count_GPE += 1
    return count_GPE
356/257: df_train['count_GPE'] = df_train.text.apply(show_ents)
356/258: df_train
356/259: df_train.count_GPE.value_counts()
356/260:
histogram2data(df_train[df_train.target == 0], 'count_GPE', '0', \
               df_train[df_train.target == 1], 'count_GPE', '1', 1)
356/261:
# Write a function to display basic entity info:
def show_ents(doc):
    doc = nlp(doc)
    count_GPE = 0
    if doc.ents:
        for ent in doc.ents:
            if ent.label_ == 'GPE':
                #print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
                count_GPE = 1
                break
    return count_GPE
356/262: df_train['count_GPE'] = df_train.text.apply(show_ents)
356/263: df_train.count_GPE.value_counts()
356/264:
histogram2data(df_train[df_train.target == 0], 'count_GPE', '0', \
               df_train[df_train.target == 1], 'count_GPE', '1', 1)
356/265:
histogram2data(df_train[df_train.target == 0], 'count_GPE', '0', \
               df_train[df_train.target == 1], 'count_GPE', '1', .8)
356/266:
plt.figure(figsize=[3, 4])
sns.countplot(x='target', hue='count_GPE', data=df_train)
plt.title('GPE Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['0', 'Positive'])
plt.show()
356/267:
plt.figure(figsize=[3, 4])
sns.countplot(x='target', hue='count_GPE', data=df_train)
plt.title('GPE Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['0', '1'])
plt.show()
356/268: df_train[df_train.target == 0].count_GPE.value_counts()
356/269: df_train[df_train.target == 0].count_GPE.value_counts()
356/270: df_train[df_train.target == 1].count_GPE.value_counts()
356/271: df_train
356/272: df_train.location
356/273: df_train['loc_count_GPE'] = df_train.location.apply(show_ents)
356/274:
def clean_keyword(text):
    text = str(text).replace('%20', ' ')
    text = str(text).replace('nan', 'no info')
    return text
356/275: df_train.keyword = df_train.keyword.apply(clean_keyword)
356/276: df_train['loc_count_GPE'] = df_train.location.apply(show_ents)
356/277: df_train
356/278: df_train.location.fillna('no info', inplace=True)
356/279: df_train['loc_count_GPE'] = df_train.location.apply(show_ents)
356/280: df_train
356/281: df_train.loc_count_GPE.value_counts()
356/282: df_train.groupby(['target', 'loc_count_GPE']).count()
356/283: df_train.groupby(['target', 'loc_count_GPE'])['id'].count()
356/284:
plt.figure(figsize=[3, 4])
sns.countplot(x='target', hue='loc_count_GPE', data=df_train)
plt.title('GPE Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['0', '1'])
plt.show()
356/285:
for i in np.unique(df_train.target):
    plt.hist(df_train[df_train.target == i].iloc[:,0],
             bins=15,
             alpha=0.3,
             color=color_list[i],
             histtype='stepfilled',
             #rwidth=.8
             label=mapping[i]
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
356/286:
for i in np.unique(df_train.target):
    plt.hist(df_train[df_train.target == i].iloc[:,0],
             bins=15,
             alpha=0.3,
        #     color=color_list[i],
             histtype='stepfilled',
             #rwidth=.8
             label=mapping[i]
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
356/287:
for i in np.unique(df_train.target):
    plt.hist(df_train[df_train.target == i].iloc[:,0],
             bins=15,
             alpha=0.3,
             color=['b', 'y'],
             histtype='stepfilled',
             #rwidth=.8
             label=i
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
356/288: color_list = ['b', 'y']
356/289:
for i in np.unique(df_train.target):
    plt.hist(df_train[df_train.target == i].iloc[:,0],
             bins=15,
             alpha=0.3,
             color=color_list[i],
             histtype='stepfilled',
             #rwidth=.8
             label=i
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
356/290: df_train[df_train.target == 0].iloc[:,0]
356/291: df_train[df_train.target == 0]
356/292:
for i in np.unique(df_train.target):
    plt.hist(df_train[df_train.target == i]['count_GPE'],
             bins=15,
             alpha=0.3,
             color=color_list[i],
             histtype='stepfilled',
             #rwidth=.8
             label=i
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
356/293: color_list = ['r', 'y']
356/294:
for i in np.unique(df_train.target):
    plt.hist(df_train[df_train.target == i]['count_GPE'],
             bins=15,
             alpha=0.3,
             color=color_list[i],
             histtype='stepfilled',
             #rwidth=.8
             label=i
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
356/295:
for i in np.unique(df_train.target):
    sns.hist(df_train[df_train.target == i]['count_GPE'],
             bins=15,
             alpha=0.3,
             color=color_list[i],
             histtype='stepfilled',
             #rwidth=.8
             label=i
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
356/296:
for i in np.unique(df_train.target):
    sns.histplot(df_train[df_train.target == i]['count_GPE'],
             bins=15,
             alpha=0.3,
             color=color_list[i],
             histtype='stepfilled',
             #rwidth=.8
             label=i
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
356/297:
for i in np.unique(df_train.target):
    sns.histplot(data = df_train[df_train.target == i],
                 x='count_GPE'
             bins=15,
             alpha=0.3,
             color=color_list[i],
             histtype='stepfilled',
             #rwidth=.8
             label=i
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
356/298:
for i in np.unique(df_train.target):
    sns.histplot(data = df_train[df_train.target == i],
                 x='count_GPE',
             bins=15,
             alpha=0.3,
             color=color_list[i],
             histtype='stepfilled',
             #rwidth=.8
             label=i
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
356/299:
for i in np.unique(df_train.target):
    sns.histplot(data = df_train[df_train.target == i],
                 x='count_GPE'
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
356/300:
for i in np.unique(df_train.target):
    sns.histplot(data = df_train[df_train.target == i],
                 x='count_GPE',
             bins=15
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
356/301:
for i in np.unique(df_train.target):
    sns.histplot(data = df_train[df_train.target == i],
                 x='count_GPE',
             bins=15,
             alpha=0.3,
             color=color_list[i]
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
356/302:
for i in np.unique(df_train.target):
    sns.histplot(data = df_train[df_train.target == i],
                 x='count_GPE',
             bins=15,
             alpha=0.3,
             color=color_list[i],
             histtype='stepfilled',
             #rwidth=.8
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
356/303:
for i in np.unique(df_train.target):
    sns.histplot(data = df_train[df_train.target == i],
                 x='count_GPE',
             bins=15,
             alpha=0.3,
             color=color_list[i]
             #rwidth=.8
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
356/304:
for i in np.unique(df_train.target):
    sns.histplot(data = df_train[df_train.target == i],
                 x='count_GPE',
             bins=15,
             alpha=0.3,
             color=color_list[i]
             #rwidth=.8
            )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend(labels = [0, 1])
362/1:
import pandas as pd
import numpy as np
import re
from collections import Counter
362/2: pd.set_option('max_colwidth', -1)
362/3:
import matplotlib.pyplot as plt
import seaborn as sns
#import plotly.graph_objs as go 
#from plotly.offline import init_notebook_mode, iplot 
#init_notebook_mode(connected = True) 
#import os 
#import matplotlib.pyplot as plt#visualization 
#%matplotlib inline 
##warnings.filterwarnings("ignore") 
#import io 
#import plotly.offline as py#visualization 
#py.init_notebook_mode(connected=True)#visualization 
#import plotly.graph_objs as go#visualization 
#import plotly.tools as tls#visualization 
#import plotly.figure_factory as ff
362/4:
import nltk
#nltk.download()
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
362/5:
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
#from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
#from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
362/6:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
362/7:
nltk.download('all')
#nltk.download('stopwords')
362/8: STOPWORDS = set(stopwords.words('english'))
362/9:
nltk.download('')
#nltk.download('stopwords')
362/10:
nltk.download()
#nltk.download('stopwords')
362/11: STOPWORDS = set(stopwords.words('english'))
362/12:
#import nltk
#nltk.download('wordnet')
362/13:
#import nltk
#dler = nltk.downloader.Downloader()
#dler._update_index()
#dler.download('all')
362/14: wnl = WordNetLemmatizer()
362/15: wnl.lemmatize('word', pos='n')
363/1:
import pandas as pd
import numpy as np
import re
from collections import Counter
363/2: pd.set_option('max_colwidth', None)
363/3:
import matplotlib.pyplot as plt
import seaborn as sns
#import plotly.graph_objs as go 
#from plotly.offline import init_notebook_mode, iplot 
#init_notebook_mode(connected = True) 
#import os 
#import matplotlib.pyplot as plt#visualization 
#%matplotlib inline 
##warnings.filterwarnings("ignore") 
#import io 
#import plotly.offline as py#visualization 
#py.init_notebook_mode(connected=True)#visualization 
#import plotly.graph_objs as go#visualization 
#import plotly.tools as tls#visualization 
#import plotly.figure_factory as ff
363/4:
import nltk
#nltk.download()
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer

import emoji as emj
import spacy    
nlp = spacy.load('en_core_web_sm')
363/5:
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, roc_curve
#from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
#from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
363/6:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
363/7:
nltk.download()
#nltk.download('stopwords')
363/8: STOPWORDS = set(stopwords.words('english'))
363/9: wnl = WordNetLemmatizer()
363/10: wnl.lemmatize('word', pos='n')
363/11: wnl.lemmatize('words', pos='n')
363/12: df_train.head()
363/13: df_train.isnull().mean()
363/14: df_train.shape, df_test.shape
363/15:
#cardinality check
for col in df_train.columns:
    print('{} has {} unique instances'.format(col, len(df_train[col].unique())))
363/16: df_train.info()
363/17: df_train.keyword.value_counts()
363/18:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword',
              data=df_train,
              palette=['lightblue'],
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
363/19:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword',
              data=df_train,
              color='lightblue',
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
363/20:
plt.figure(figsize=[8, 60])
sns.countplot(y='keyword',
              data=df_train,
              color='lightblue',
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
363/21:
plt.figure(figsize=[8, 45])
sns.countplot(y='keyword',
              data=df_train,
              color='lightblue',
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
363/22: df_train.keyword.str.contains('%20')
363/23: df_train[df_train.keyword.str.contains('%20')]
363/24: df_train[df_train.keyword.str.contains('%20', na=False)]
363/25: df_train[df_train.keyword.str.contains('%20', na=False)].target.value_counts()
363/26: df_train[df_train.keyword.str.contains('%20', na=False)].target.
363/27: df_train[df_train.keyword.str.contains('%20', na=False)]
363/28: df_train[df_train.keyword.str.contains('%20', na=False)].groupby(['keyword', 'target'])
363/29: df_train[df_train.keyword.str.contains('%20', na=False)].groupby(['keyword', 'target']).count()
363/30: df_train[df_train.keyword.str.contains('%20', na=False)].groupby(['keyword', 'target']).count()['id']
363/31: df_train[df_train.keyword.str.contains('%20', na=False)].groupby(['keyword', 'target']).count()['id'].reset_index)
363/32: df_train[df_train.keyword.str.contains('%20', na=False)].groupby(['keyword', 'target']).count()['id'].reset_index())
363/33: df_train[df_train.keyword.str.contains('%20', na=False)].groupby(['keyword', 'target']).count()['id'].reset_index()
363/34: df_keyword_20 = df_train[df_train.keyword.str.contains('%20', na=False)].groupby(['keyword', 'target']).count()['id'].reset_index()
363/35: df_keyword_20
363/36:
plt.figure(figsize=[8, 45])
sns.countplot(y='keyword',
              x='id',
              data=df_keyword_20,
              hue='target',
    #          color='lightblue',
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
363/37:
plt.figure(figsize=[8, 45])
sns.countplot(y='keyword',
              data=df_keyword_20,
              hue='target',
    #          color='lightblue',
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
363/38:
plt.figure(figsize=[8, 45])
sns.countplot(y='keyword',
              data=df_train.keyword.str.contains('%20', na=False),
              hue='target',
    #          color='lightblue',
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
363/39:
plt.figure(figsize=[8, 45])
sns.countplot(y='keyword',
              data=df_train[df_train.keyword.str.contains('%20', na=False)],
              hue='target',
    #          color='lightblue',
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
363/40: df_keyword_20 = df_train[df_train.keyword.str.contains('%20', na=False)]
363/41:
plt.figure(figsize=[8, 45])
sns.countplot(y='keyword',
              data=df_keyword_20,
              hue='target',
    #          color='lightblue',
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
363/42: df_keyword_20
363/43:
plt.figure(figsize=[8, 45])
sns.countplot(y='keyword',
              data=df_keyword_20,
              hue='target',
    #          color='lightblue',
              order=df_keyword_20['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
363/44:
plt.figure(figsize=[8, 25])
sns.countplot(y='keyword',
              data=df_keyword_20,
              hue='target',
    #          color='lightblue',
              order=df_keyword_20['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
363/45:
plt.figure(figsize=[8, 15])
sns.countplot(y='keyword',
              data=df_keyword_20,
              hue='target',
    #          color='lightblue',
              order=df_keyword_20['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
363/46:
plt.figure(figsize=[6, 10])
sns.countplot(y='keyword',
              data=df_keyword_20,
              hue='target',
    #          color='lightblue',
              order=df_keyword_20['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
363/47:
plt.figure(figsize=[8, 45])
sns.countplot(y='keyword',
              data=df_keyword_20,
              hue='target',
    #          color='lightblue',
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
363/48:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword', 
              hue='target', 
              data=df_train, 
              palette=['lightblue', 'red'])
plt.xticks(rotation=90)
plt.legend(loc='upper right')
plt.title("Distribution of Target per Keyword ")
plt.show()
363/49:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword', 
              hue='target', 
              data=df_train, 
              palette=['lightblue', 'red'])
plt.xticks(rotation=90)
plt.legend(loc='upper right')
plt.title("Distribution of Target per Keyword ")
plt.show()
363/50: df_train.keyword.str.replace('%20', '_')
363/51: df_train.keyword = df_train.keyword.str.replace('%20', '_')
363/52:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword', 
              hue='target', 
              data=df_train, 
              palette=['lightblue', 'red']
             )
plt.xticks(rotation=90)
plt.legend(loc='upper right')
plt.title("Distribution of Target per Keyword ")
plt.show()
363/53:
# Let's show most common keywords for target = 1
df_train[df_train.target == 1].keyword.value_counts()[:10]
363/54:
# Let's show most common keywords for target = 1
df_train[df_train.target == 0].keyword.value_counts()[:10]
363/55:
df_target_0 = df_train[df_train.target == 0].groupby(['keyword']).count()['id'].reset_index()
df_target_1 = df_train[df_train.target == 1].groupby(['keyword']).count()['id'].reset_index()
363/56: df_target_1
363/57:
merge_counts_keywords = pd.merge(left=df_target_0,
                                 right=df_target_1,
                                 on='keyword',
                                 how='outer'
                                 ).fillna(0)

merge_counts_keywords.columns = ['keyword', 'target_0', 'target_1']
merge_counts_keywords['prob_real_disasters'] = (merge_counts_keywords.target_1 - merge_counts_keywords.target_0)\
                                                /merge_counts_keywords.target_1
363/58: merge_counts_keywords[merge_counts_keywords.keyword == 'bombing']
363/59: df_train.groupby(['keyword', 'target']).count()
363/60: df_train.groupby(['keyword', 'target'])['id'].count()
363/61: df_train.groupby(['keyword', 'target'])['id'].count().reset_index()
363/62: df_train.groupby(['keyword', 'target'])['id'].count()
363/63: df_train.groupby(['keyword', 'target'])['id'].size().unstack(fill_value=0)
363/64: df_train.groupby(['keyword', 'target'])['id'].size(
363/65: df_train.groupby(['keyword', 'target'])['id'].size()
363/66: df_train.groupby(['keyword', 'target'])['id'].size().reset_index()
363/67: df_train.groupby(['keyword', 'target'])['id'].size()
363/68: df_train.groupby(['keyword', 'target'])['id'].cout()
363/69: df_train.groupby(['keyword', 'target'])['id'].count()
363/70: df_train.groupby(['keyword', 'target'])['id'].count().reset_index()
363/71:
pd.pivot_table(ddf, values=['id'], index=['keyword'],
                    aggfunc={'D': np.mean})
363/72: ddf = df_train.groupby(['keyword', 'target'])['id'].count().reset_index()
363/73:
pd.pivot_table(ddf, values=['id'], index=['keyword'],
                    aggfunc={'D': np.mean})
363/74: pd.pivot_table(ddf, values=['id'], index=['keyword'])
363/75: ddf.head()
363/76: pd.pivot_table(ddf, values=['target'], index=['keyword'])
363/77: pd.pivot_table(ddf, values=['target', id], index=['keyword'])
363/78: pd.pivot_table(ddf, values=['target', 'id'], index=['keyword'])
363/79: df_train.groupby(['keyword', 'target'])['id'].count()
363/80: df_train.groupby(['keyword', 'target'])['id'].count().unstack(fill_value=0)
363/81: df_train.groupby(['keyword', 'target'])['id'].count().unstack(fill_value=0).reset_index()
363/82:
df_trainf_group = df_train.groupby(['keyword', 'target'])['id'].count().unstack(fill_value=0).reset_index()
df_trainf_group.columns = ['id', 'keyword', 'target_0', 'target_1']
363/83: df_trainf_group
363/84: df_trainf_group.reset_index()
363/85:
df_trainf_group = df_train.groupby(['keyword', 'target'])['id'].count().unstack(fill_value=0).reset_index()
df_trainf_group.columns = ['keyword', 'target_0', 'target_1']
363/86: df_trainf_group
363/87: df_trainf_group.keyword.str.contains('ablaze')
363/88: df_trainf.keyword.str.contains('ablaze')
363/89: df_train.keyword.str.contains('ablaze')
363/90: df_train[df_train.keyword.str.contains('ablaze')]
363/91: df_train[df_train.keyword.str.contains('ablaze', regex=False)]
363/92: df_train[df_train.keyword.str.contains('ablaze', na=False)]
363/93: df_train[df_train.keyword.str.contains('ablaze', na=False)].target.value_counts()
363/94: df_train
363/95: df_trainf_group
363/96:
df_trainf_group = df_train.groupby(['keyword', 'target'])['id'].count().unstack(fill_value=0).reset_index()
df_trainf_group.columns = ['keyword', 'target_0', 'target_1']
df_trainf_group['prob_real_disasters'] = df_trainf_group.target_1/(df_trainf_group.target_0 + df_trainf_group.target_1)
363/97: df_trainf_group['prob_real_disasters']
363/98: df_trainf_group
363/99: df_trainf_group.sort_values(prob_real_disasters)
363/100:
df_train_group = df_train.groupby(['keyword', 'target'])['id'].count().unstack(fill_value=0).reset_index()
df_train_group.columns = ['keyword', 'target_0', 'target_1']
df_train_group['prob_real_disasters'] = df_train_group.target_1/(df_trainf_group.target_0 + df_trainf_group.target_1)
363/101: df_train[df_train.keyword.str.contains('ablaze', na=False)].target.value_counts()
363/102: df_trainf_group.sort_values(prob_real_disasters)
363/103: df_train_group.sort_values('prob_real_disasters')
363/104: df_train_group.sort_values('prob_real_disasters', ascending=False)
363/105: df_train.keyword.nunique()
363/106: df_train.keyword = df_train.keyword.str.replace('%20', '_')
363/107:
# Let's show most common keywords for target = 1
df_train[df_train.target == 1].keyword.value_counts()[:10]
363/108:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
363/109: df_train.keyword.value_counts()
363/110:
plt.figure(figsize=[8, 45])
sns.countplot(y='keyword',
              data=df_train,
              color='lightblue',
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
363/111:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword', 
              hue='target', 
              data=df_train, 
              palette=['lightblue', 'red']
             )
plt.xticks(rotation=90)
plt.legend(loc='upper right')
plt.title("Distribution of Target per Keyword ")
plt.show()
363/112: df_train.keyword = df_train.keyword.str.replace('%20', 'v')
363/113:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
363/114: df_train.keyword = df_train.keyword.str.replace('%20', ' ')
363/115:
# Let's show most common keywords for target = 1
df_train[df_train.target == 1].keyword.value_counts()[:10]
363/116:
# Let's show most common keywords for target = 1
df_train[df_train.target == 0].keyword.value_counts()[:10]
363/117:
df_train_group = df_train.groupby(['keyword', 'target'])['id'].count().unstack(fill_value=0).reset_index()
df_train_group.columns = ['keyword', 'target_0', 'target_1']
df_train_group['prob_real_disasters'] = df_train_group.target_1/(df_trainf_group.target_0 + df_trainf_group.target_1)
363/118: df_train[df_train.keyword.str.contains('ablaze', na=False)].target.value_counts()
363/119: df_train_group.sort_values('prob_real_disasters', ascending=False)
363/120: df_train.keyword.nunique()
363/121: df_train.target.value_counts()/len(df_train)
363/122: df_train[df_train.keyword.isna()].target.value_counts()
363/123:
def count_punctuations(text):
    punctuations='!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
    d=dict()
    for i in punctuations:
        d[str(i)+' count']=text.count(i)
    return d
363/124: count_punctuations(',.mjm.')
363/125: import string
363/126:
import string
string.punctuation
363/127: df_train["text"].apply(lambda x:count_mentions(x))
363/128: df_train["text"].apply(lambda x:count_punctuations(x))
363/129: df_train["text"].apply(lambda x:count_punctuations(x))
363/130: df_train['punct_count'] = df_train["text"].apply(lambda x:count_punctuations(x))
363/131: df_train
363/132: df_punct= pd.DataFrame(list(df_train.punct_count))
363/133: df_punct
363/134: df_punct.merge(df_train.target, left_on=index, right_on=index)
363/135: df_punct.merge(df_train.target)
363/136: df_punct.merge(df_train.target, on='index')
363/137: pd.merge(df_train,df_punct,left_index=True, right_index=True)
363/138: pd.merge(df_train.target,df_punct,left_index=True, right_index=True)
363/139: merge_df = pd.merge(df_train.target,df_punct,left_index=True, right_index=True)
363/140: df_train.head()
363/141: merge_df
363/142: merge_df.T
363/143: merge_df
363/144: merge_df.columns
363/145: merge_df.columns[1:]
363/146: col_list = merge_df.columns[1:]
363/147:
for i in col_list:
    sns.countplot(y=i,
                  hue='target', 
              data=df_train, 
              palette=['lightblue', 'red']
             )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
363/148:
for i in col_list:
    sns.countplot(y=i,
                  hue='target', 
              data=merge_df, 
              palette=['lightblue', 'red']
             )
#plt.xlabel('x')
#plt.ylabel('y')
plt.legend()
363/149:
for i in col_list:
    sns.countplot(y=i,
                  hue='target',
                  data=merge_df,
                  palette=['lightblue', 'red'],
                  orient = 'h'
             )
#plt.xlabel('x')
#plt.ylabel('y')
#plt.legend()
363/150:
for i in col_list:
    print(i)
    sns.countplot(y=i,
                  hue='target',
                  data=merge_df,
                  palette=['lightblue', 'red'],
                  orient = 'h'
             )
#plt.xlabel('x')
#plt.ylabel('y')
#plt.legend()
363/151: col_list = merge_df.columns[1:3]
363/152: col_list
363/153: col_list[0]
363/154:
for i in col_list:
    print(i)
    sns.countplot(y=i,
                  hue='target',
                  data=merge_df,
                  palette=['lightblue', 'red'],
                  orient = 'h'
             )
#plt.xlabel('x')
#plt.ylabel('y')
#plt.legend()
363/155:
for i in col_list:
    print(i)
    sns.countplot(y=i,
                  hue='target',
                  data=merge_df,
                  palette=['lightblue', 'red'],
                 # orient = 'h'
             )
#plt.xlabel('x')
#plt.ylabel('y')
#plt.legend()
363/156: col_list = merge_df.columns[1:4]
363/157: col_list[0]
363/158:
for i in col_list:
    print(i)
    sns.countplot(y=i,
                  hue='target',
                  data=merge_df,
                  palette=['lightblue', 'red'],
                  orient = 'h'
             )
#plt.xlabel('x')
#plt.ylabel('y')
#plt.legend()
363/159: merge_df
363/160: merge_df.groupby('target').sum()
363/161: merge_df.groupby('target').sum()/len(df_train)
363/162: merge_df.groupby('target').sum()
363/163: merge_df.groupby('target').sum().reset_index()
363/164: merge_df_gr = merge_df.groupby('target').sum().reset_index()
363/165: merge_df_gr = merge_df.groupby('target').sum().reset_index()
363/166: merge_df_gr
363/167: merge_df_gr.sum(axis=0)
363/168: merge_df_gr / merge_df_gr.sum(axis=0)
363/169: merge_df_gr / merge_df_gr.sum(axis=0).T
363/170: merge_df_gr / merge_df_gr.sum(axis=0)
363/171: merge_df_gr = merge_df_gr / merge_df_gr.sum(axis=0)
363/172: merge_df_gr.T
363/173: merge_df
363/174: merge_df_gr
363/175: pd.merge(df_train.target,df_punct,left_index=True, right_index=True)
363/176: merge_df.groupby('target').sum().reset_index()
363/177: merge_df.groupby('target').sum().reset_index().T
363/178: len(df_train)
363/179: wnl = WordNetLemmatizer()
363/180: %history -g -f filename
363/181: STOPWORDS = set(stopwords.words('english'))
363/182: df_train.head()
363/183:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
363/184: df_train.head()
363/185: df_train.isnull().mean()
363/186: df_train.shape, df_test.shape
363/187:
#cardinality check
for col in df_train.columns:
    print('{} has {} unique instances'.format(col, len(df_train[col].unique())))
363/188: df_train.info()
363/189:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword',
              data=df_train,
              palette=['lightblue'],
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
363/190:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword',
              data=df_train,
              color='lightblue',
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
363/191:
plt.figure(figsize=[8, 45])
sns.countplot(y='keyword',
              data=df_train,
              color='lightblue',
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
363/192:
sns.countplot(y='keyword', 
              hue='target', 
              data=df_train, 
              palette=['lightblue', 'red']
             )
plt.xticks(rotation=90)
plt.legend(loc='upper right')
plt.title("Distribution of Target per Keyword ")
plt.show()
363/193:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword', 
              hue='target', 
              data=df_train, 
              palette=['lightblue', 'red']
             )
plt.xticks(rotation=90)
plt.legend(loc='upper right')
plt.title("Distribution of Target per Keyword ")
plt.show()
363/194:
plt.figure(figsize=[8, 45])
sns.countplot(y='keyword',
              data=df_keyword_20,
              hue='target',
              color='lightblue',
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
363/195:
plt.figure(figsize=[8, 45])
sns.countplot(y='keyword',
              data=df_train,
              color='lightblue',
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
363/196:
plt.figure(figsize=[8, 40])
sns.countplot(y='keyword',
              data=df_train,
              color='lightblue',
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
363/197: df_train.keyword = df_train.keyword.str.replace('%20', ' ')
363/198:
# Let's show most common keywords for target = 1
df_train[df_train.target == 1].keyword.value_counts()[:10]
363/199:
# Let's show most common keywords for target = 1
df_train[df_train.target == 0].keyword.value_counts()[:10]
363/200:
df_trainf_group = df_train.groupby(['keyword', 'target'])['id'].count().unstack(fill_value=0).reset_index()
df_trainf_group.columns = ['keyword', 'target_0', 'target_1']
df_trainf_group['prob_real_disasters']
363/201:
df_trainf_group = df_train.groupby(['keyword', 'target'])['id'].count().unstack(fill_value=0).reset_index()
df_trainf_group.columns = ['keyword', 'target_0', 'target_1']
363/202:
df_trainf_group = df_train.groupby(['keyword', 'target'])['id'].count().unstack(fill_value=0).reset_index()
df_trainf_group.columns = ['keyword', 'target_0', 'target_1']
df_trainf_group['prob_real_disasters'] = df_trainf_group.target_1/(df_trainf_group.target_0 + df_trainf_group.target_1)
363/203: df_trainf_group.sort_values(prob_real_disasters)
363/204: df_trainf_group.sort_values('prob_real_disasters')
363/205: df_trainf_group.sort_values('prob_real_disasters', ascending=False)
363/206: df_train.keyword.nunique()
363/207:
df_train_group = df_train.groupby(['keyword', 'target'])['id'].count().unstack(fill_value=0).reset_index()
df_train_group.columns = ['keyword', 'target_0', 'target_1']
df_train_group['prob_real_disasters'] = df_train_group.target_1/(df_trainf_group.target_0 + df_trainf_group.target_1)
363/208: df_trainf_group.sort_values('prob_real_disasters', ascending=False)
363/209: df_train.target.value_counts()/len(df_train)
363/210: df_train.target.value_counts()/len(df_train)
363/211: df_train[df_train.keyword.isna()].target.value_counts()
363/212:
def count_punctuations(text):
    punctuations='!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
    d=dict()
    for i in punctuations:
        d[str(i)+' count']=text.count(i)
    return d
363/213: string.punctuation
363/214:
def count_punctuations(text):
    #punctuations='!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
    d=dict()
    for i in string.punctuation:
        d[str(i)+' count']=text.count(i)
    return d
363/215:
#string.punctuation = '!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
def count_punctuations(text):
    d=dict()
    for i in string.punctuation:
        d[str(i)+' count']=text.count(i)
    return d
363/216: df_train["text"].apply(lambda x:count_punctuations(x))
363/217: df_train['punct_count'] = df_train["text"].apply(lambda x:count_punctuations(x))
363/218:
df_train['punct_count'] = df_train["text"].apply(lambda x:count_punctuations(x))
df_punct= pd.DataFrame(list(df_train.punct_count))
merge_df = pd.merge(df_train.target,df_punct,left_index=True, right_index=True)
363/219: merge_df
363/220:
merge_df_gr = merge_df.groupby('target').sum().reset_index()
merge_df_gr = merge_df_gr / merge_df_gr.sum(axis=0)
363/221: merge_df_gr
363/222:
merge_df_gr = merge_df.groupby('target').sum().reset_index()
merge_df_gr = merge_df_gr
363/223: merge_df_gr
363/224: merge_df_gr.T
363/225:
merge_df_gr = merge_df.groupby('target').sum().reset_index()
merge_df_gr = merge_df_gr.T
363/226: merge_df_gr
363/227: merge_df_gr[0] + merge_df_gr[1]
363/228: merge_df_gr['sum_0_1'] = merge_df_gr[0] + merge_df_gr[1]
363/229: merge_df_gr
363/230: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
363/231:
df_train['punct_count'] = df_train["text"].apply(lambda x:count_punctuations(x))
df_punct= pd.DataFrame(list(df_train.punct_count))
merge_df = pd.merge(df_train.target,df_punct,left_index=True, right_index=True)
363/232:
merge_df_gr = merge_df.groupby('target').sum().reset_index()
merge_df_gr = merge_df_gr.T
merge_df_gr.columns = ['target_0', 'target_1']
363/233: merge_df_gr['sum_0_1'] = merge_df_gr['target_0'] + merge_df_gr['target_1']
363/234: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
363/235: len(df_train)
363/236: len(df_train)
363/237: re.findall("a", 'a ddkj a'))
363/238: re.findall("a", 'a ddkj a')
363/239: len(re.findall("a", 'a ddkj a'))
363/240:
def punct_add_!(text):
    return len(re.findall("!", text))
363/241: merge_df
363/242:
def punct_add_exclamation(text):
    return len(re.findall("!", text))
363/243:
def punct_add_question(text):
    return len(re.findall("?", text))
363/244:
def punct_add_quotation(text):
    return len(re.findall("'", text))
363/245: df_train
363/246: df_train['count_exclamation'] = df_train.apply(punct_add_exclamation)
363/247: df_train['count_exclamation'] = df_train.text.apply(punct_add_exclamation)
363/248: df_train
363/249:
df_train['count_exclamation'] = df_train.text.apply(punct_add_exclamation)
df_train['count_question'] = df_train.text.apply(punct_add_question)
df_train['count_quotation'] = df_train.text.apply(punct_add_quotation)
363/250:
df_train['count_exclamation'] = df_train.text.apply(punct_add_exclamation)
#df_train['count_question'] = df_train.text.apply(punct_add_question)
#df_train['count_quotation'] = df_train.text.apply(punct_add_quotation)
363/251:
df_train['count_exclamation'] = df_train.text.apply(punct_add_exclamation)
df_train['count_question'] = df_train.text.apply(punct_add_question)
#df_train['count_quotation'] = df_train.text.apply(punct_add_quotation)
363/252:
def punct_add_question(text):
    return len(re.findall("/?", text))
363/253:
def punct_add_quotation(text):
    return len(re.findall("'", text))
363/254:
df_train['count_exclamation'] = df_train.text.apply(punct_add_exclamation)
df_train['count_question'] = df_train.text.apply(punct_add_question)
#df_train['count_quotation'] = df_train.text.apply(punct_add_quotation)
363/255: df_train
363/256:
def punct_add_question(text):
    return len(re.findall("?", text))
363/257:
def punct_add_quotation(text):
    return len(re.findall("'", text))
363/258:
df_train['count_exclamation'] = df_train.text.apply(punct_add_exclamation)
df_train['count_question'] = df_train.text.apply(punct_add_question)
#df_train['count_quotation'] = df_train.text.apply(punct_add_quotation)
363/259:
def punct_add_question(text):
    return len(re.findall("\?", text))
363/260:
def punct_add_quotation(text):
    return len(re.findall("'", text))
363/261:
df_train['count_exclamation'] = df_train.text.apply(punct_add_exclamation)
df_train['count_question'] = df_train.text.apply(punct_add_question)
#df_train['count_quotation'] = df_train.text.apply(punct_add_quotation)
363/262: df_train
363/263: df_train[df_train.count_question > 0]
363/264:
df_train['count_exclamation'] = df_train.text.apply(punct_add_exclamation)
df_train['count_question'] = df_train.text.apply(punct_add_question)
df_train['count_quotation'] = df_train.text.apply(punct_add_quotation)
363/265: df_train[df_train.count_question > 0]
363/266:
fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)
#fig.suptitle('Initial Pokemon - 1st Generation')

# Bulbasaur
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[0],
                 data = df_train[df_train.target == i],
                 x='count_exclamation',
             bins=15,
             alpha=0.3,
             color=color_list[i]
             #rwidth=.8
            )
axes[0].set_title(bulbasaur.name)

# Charmander
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[1],
                 data = df_train[df_train.target == i],
                 x='count_exclamation',
             bins=15,
             alpha=0.3,
             color=color_list[i]
             #rwidth=.8
            )
axes[1].set_title(charmander.name)

# Squirtle
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[2],
                 data = df_train[df_train.target == i],
                 x='count_exclamation',
             bins=15,
             alpha=0.3,
             color=color_list[i]
             #rwidth=.8
            )
axes[2].set_title(squirtle.name)
363/267:
fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)
#fig.suptitle('Initial Pokemon - 1st Generation')

# Bulbasaur
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[0],
                 data = df_train[df_train.target == i],
                 x='count_exclamation',
             bins=15,
             alpha=0.3,
             color=['r', 'b']
             #rwidth=.8
            )
axes[0].set_title(bulbasaur.name)

# Charmander
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[1],
                 data = df_train[df_train.target == i],
                 x='count_exclamation',
             bins=15,
             alpha=0.3,
             color=color_list[i]
             #rwidth=.8
            )
axes[1].set_title(charmander.name)

# Squirtle
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[2],
                 data = df_train[df_train.target == i],
                 x='count_exclamation',
             bins=15,
             alpha=0.3,
             color=color_list[i]
             #rwidth=.8
            )
axes[2].set_title(squirtle.name)
363/268:
fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)
#fig.suptitle('Initial Pokemon - 1st Generation')

# Bulbasaur
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[0],
                 data = df_train[df_train.target == i],
                 x='count_exclamation',
             bins=15,
             alpha=0.3,
            # color=['r', 'b']
             #rwidth=.8
            )
axes[0].set_title(bulbasaur.name)

# Charmander
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[1],
                 data = df_train[df_train.target == i],
                 x='count_exclamation',
             bins=15,
             alpha=0.3,
           #  color=color_list[i]
             #rwidth=.8
            )
axes[1].set_title(charmander.name)

# Squirtle
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[2],
                 data = df_train[df_train.target == i],
                 x='count_exclamation',
             bins=15,
             alpha=0.3,
           #  color=color_list[i]
             #rwidth=.8
            )
axes[2].set_title(squirtle.name)
363/269:
fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)
#fig.suptitle('Initial Pokemon - 1st Generation')

# Bulbasaur
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[0],
                 data = df_train[df_train.target == i],
                 x='count_exclamation',
             bins=15,
             alpha=0.3,
            # color=['r', 'b']
             #rwidth=.8
            )
axes[0].set_title('count_exclamation')

# Charmander
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[1],
                 data = df_train[df_train.target == i],
                 x='count_question',
             bins=15,
             alpha=0.3,
           #  color=color_list[i]
             #rwidth=.8
            )
axes[1].set_title('count_question')

# Squirtle
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[2],
                 data = df_train[df_train.target == i],
                 x='count_quotation',
             bins=15,
             alpha=0.3,
           #  color=color_list[i]
             #rwidth=.8
            )
axes[2].set_title('count_quotation')
363/270:
fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)
#fig.suptitle('Initial Pokemon - 1st Generation')

# Bulbasaur
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[0],
                 data = df_train[df_train.target == i],
                 x='count_exclamation',
             bins=15,
             alpha=0.3,
             color=['r', 'b']
             #rwidth=.8
            )
axes[0].set_title('count_exclamation')

# Charmander
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[1],
                 data = df_train[df_train.target == i],
                 x='count_question',
             bins=15,
             alpha=0.3,
           #  color=color_list[i]
             #rwidth=.8
            )
axes[1].set_title('count_question')

# Squirtle
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[2],
                 data = df_train[df_train.target == i],
                 x='count_quotation',
             bins=15,
             alpha=0.3,
           #  color=color_list[i]
             #rwidth=.8
            )
axes[2].set_title('count_quotation')
363/271:
fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)
#fig.suptitle('Initial Pokemon - 1st Generation')

# Bulbasaur
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[0],
                 data = df_train[df_train.target == i],
                 x='count_exclamation',
             bins=15,
             alpha=0.3,
             palette==['r', 'b']
             #rwidth=.8
            )
axes[0].set_title('count_exclamation')

# Charmander
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[1],
                 data = df_train[df_train.target == i],
                 x='count_question',
             bins=15,
             alpha=0.3,
           #  color=color_list[i]
             #rwidth=.8
            )
axes[1].set_title('count_question')

# Squirtle
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[2],
                 data = df_train[df_train.target == i],
                 x='count_quotation',
             bins=15,
             alpha=0.3,
           #  color=color_list[i]
             #rwidth=.8
            )
axes[2].set_title('count_quotation')
363/272:
fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)
#fig.suptitle('Initial Pokemon - 1st Generation')

# Bulbasaur
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[0],
                 data = df_train[df_train.target == i],
                 x='count_exclamation',
             bins=15,
             alpha=0.3,
             palette=['r', 'b']
             #rwidth=.8
            )
axes[0].set_title('count_exclamation')

# Charmander
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[1],
                 data = df_train[df_train.target == i],
                 x='count_question',
             bins=15,
             alpha=0.3,
           #  color=color_list[i]
             #rwidth=.8
            )
axes[1].set_title('count_question')

# Squirtle
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[2],
                 data = df_train[df_train.target == i],
                 x='count_quotation',
             bins=15,
             alpha=0.3,
           #  color=color_list[i]
             #rwidth=.8
            )
axes[2].set_title('count_quotation')
363/273:
color_list = ['r', 'y']
fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)
#fig.suptitle('Initial Pokemon - 1st Generation')

# Bulbasaur
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[0],
                 data = df_train[df_train.target == i],
                 x='count_exclamation',
             bins=15,
             alpha=0.3,
             color=color_list[i]
             #rwidth=.8
            )
axes[0].set_title('count_exclamation')

# Charmander
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[1],
                 data = df_train[df_train.target == i],
                 x='count_question',
             bins=15,
             alpha=0.3,
           #  color=color_list[i]
             #rwidth=.8
            )
axes[1].set_title('count_question')

# Squirtle
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[2],
                 data = df_train[df_train.target == i],
                 x='count_quotation',
             bins=15,
             alpha=0.3,
           #  color=color_list[i]
             #rwidth=.8
            )
axes[2].set_title('count_quotation')
363/274:
color_list = ['r', 'y']
fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)
#fig.suptitle('Initial Pokemon - 1st Generation')

# Bulbasaur
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[0],
                 data = df_train[df_train.target == i],
                 x='count_exclamation',
             bins=15,
             alpha=0.3,
             color=color_list[i]
             #rwidth=.8
            )
axes[0].set_title('count_exclamation')

# Charmander
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[1],
                 data = df_train[df_train.target == i],
                 x='count_question',
             bins=15,
             alpha=0.3,
                 label = i
           #  color=color_list[i]
             #rwidth=.8
            )
axes[1].set_title('count_question')

# Squirtle
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[2],
                 data = df_train[df_train.target == i],
                 x='count_quotation',
             bins=15,
             alpha=0.3,
           #  color=color_list[i]
             #rwidth=.8
            )
axes[2].set_title('count_quotation')
363/275:
color_list = ['r', 'y']
fig, axes = plt.subplots(3, 1, figsize=(15, 15), sharey=True)
#fig.suptitle('Initial Pokemon - 1st Generation')

# Bulbasaur
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[0],
                 data = df_train[df_train.target == i],
                 x='count_exclamation',
             bins=15,
             alpha=0.3,
             color=color_list[i]
             #rwidth=.8
            )
axes[0].set_title('count_exclamation')

# Charmander
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[1],
                 data = df_train[df_train.target == i],
                 x='count_question',
             bins=15,
             alpha=0.3,
                 label = i
           #  color=color_list[i]
             #rwidth=.8
            )
axes[1].set_title('count_question')

# Squirtle
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[2],
                 data = df_train[df_train.target == i],
                 x='count_quotation',
             bins=15,
             alpha=0.3,
           #  color=color_list[i]
             #rwidth=.8
            )
axes[2].set_title('count_quotation')
363/276:
color_list = ['r', 'y']
fig, axes = plt.subplots(3, 1, figsize=(15, 15), sharey=True)
#fig.suptitle('Initial Pokemon - 1st Generation')

# Bulbasaur
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[0],
                 data = df_train[df_train.target == i],
                 x='count_exclamation',
             bins=1,
             alpha=0.3,
             color=color_list[i]
             #rwidth=.8
            )
axes[0].set_title('count_exclamation')

# Charmander
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[1],
                 data = df_train[df_train.target == i],
                 x='count_question',
             bins=15,
             alpha=0.3,
                 label = i
           #  color=color_list[i]
             #rwidth=.8
            )
axes[1].set_title('count_question')

# Squirtle
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[2],
                 data = df_train[df_train.target == i],
                 x='count_quotation',
             bins=15,
             alpha=0.3,
           #  color=color_list[i]
             #rwidth=.8
            )
axes[2].set_title('count_quotation')
363/277:
color_list = ['r', 'y']
fig, axes = plt.subplots(3, 1, figsize=(15, 15), sharey=True)
#fig.suptitle('Initial Pokemon - 1st Generation')

# Bulbasaur
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[0],
                 data = df_train[df_train.target == i],
                 x='count_exclamation',
             bins=20,
             alpha=0.3,
             color=color_list[i]
             #rwidth=.8
            )
axes[0].set_title('count_exclamation')

# Charmander
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[1],
                 data = df_train[df_train.target == i],
                 x='count_question',
             bins=15,
             alpha=0.3,
                 label = i
           #  color=color_list[i]
             #rwidth=.8
            )
axes[1].set_title('count_question')

# Squirtle
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[2],
                 data = df_train[df_train.target == i],
                 x='count_quotation',
             bins=15,
             alpha=0.3,
           #  color=color_list[i]
             #rwidth=.8
            )
axes[2].set_title('count_quotation')
363/278:
color_list = ['r', 'y']
fig, axes = plt.subplots(3, 1, figsize=(15, 15), sharey=True)
#fig.suptitle('Initial Pokemon - 1st Generation')

# Bulbasaur
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[0],
                 data = df_train[df_train.target == i],
                 x='count_exclamation',
             bins=20,
             alpha=0.3,
             color=color_list[i]
             rwidth=.8
            )
axes[0].set_title('count_exclamation')

# Charmander
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[1],
                 data = df_train[df_train.target == i],
                 x='count_question',
             bins=15,
             alpha=0.3,
                 label = i
           #  color=color_list[i]
             #rwidth=.8
            )
axes[1].set_title('count_question')

# Squirtle
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[2],
                 data = df_train[df_train.target == i],
                 x='count_quotation',
             bins=15,
             alpha=0.3,
           #  color=color_list[i]
             #rwidth=.8
            )
axes[2].set_title('count_quotation')
363/279:
color_list = ['r', 'y']
fig, axes = plt.subplots(3, 1, figsize=(15, 15), sharey=True)
#fig.suptitle('Initial Pokemon - 1st Generation')

# Bulbasaur
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[0],
                 data = df_train[df_train.target == i],
                 x='count_exclamation',
             bins=20,
             alpha=0.3,
             color=color_list[i],
             rwidth=.8
            )
axes[0].set_title('count_exclamation')

# Charmander
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[1],
                 data = df_train[df_train.target == i],
                 x='count_question',
             bins=15,
             alpha=0.3,
                 label = i
           #  color=color_list[i]
             #rwidth=.8
            )
axes[1].set_title('count_question')

# Squirtle
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[2],
                 data = df_train[df_train.target == i],
                 x='count_quotation',
             bins=15,
             alpha=0.3,
           #  color=color_list[i]
             #rwidth=.8
            )
axes[2].set_title('count_quotation')
363/280:
color_list = ['r', 'y']
fig, axes = plt.subplots(3, 1, figsize=(15, 15), sharey=True)
#fig.suptitle('Initial Pokemon - 1st Generation')

# Bulbasaur
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[0],
                 data = df_train[df_train.target == i],
                 x='count_exclamation',
             bins=20,
             alpha=0.3,
             color=color_list[i]
            )
axes[0].set_title('count_exclamation')

# Charmander
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[1],
                 data = df_train[df_train.target == i],
                 x='count_question',
             bins=15,
             alpha=0.3,
                 label = i
           #  color=color_list[i]
             #rwidth=.8
            )
axes[1].set_title('count_question')

# Squirtle
for i in np.unique(df_train.target):
    sns.histplot(ax=axes[2],
                 data = df_train[df_train.target == i],
                 x='count_quotation',
             bins=15,
             alpha=0.3,
           #  color=color_list[i]
             #rwidth=.8
            )
axes[2].set_title('count_quotation')
363/281: df_train
363/282: df_train.groupby(['target', 'count_exclamation', 'count_question', 'count_quotation']).size()
363/283: df_train.groupby(['target', 'count_exclamation', 'count_question', 'count_quotation']).size().reset_index()
363/284: df_train.groupby(['target', 'count_exclamation', 'count_question', 'count_quotation']).size()
363/285: df_train
363/286: merge_df
363/287:
merge_df_gr = merge_df.groupby('target').size().reset_index()
merge_df_gr = merge_df_gr.T
merge_df_gr.columns = ['target_0', 'target_1']
363/288: merge_df_gr['sum_0_1'] = merge_df_gr['target_0'] + merge_df_gr['target_1']
363/289: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
363/290:
df_train['punct_count'] = df_train["text"].apply(lambda x:count_punctuations(x))
df_punct= pd.DataFrame(list(df_train.punct_count))
merge_df = pd.merge(df_train.target,df_punct,left_index=True, right_index=True)
363/291: merge_df
363/292:
merge_df_gr = merge_df.groupby('target').size().reset_index()
merge_df_gr = merge_df_gr.T
merge_df_gr.columns = ['target_0', 'target_1']
363/293: merge_df_gr['sum_0_1'] = merge_df_gr['target_0'] + merge_df_gr['target_1']
363/294: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
363/295: merge_df_gr
363/296: merge_df_gr = merge_df.groupby('target').size().reset_index()
363/297: merge_df_gr
363/298:
df_train['punct_count'] = df_train["text"].apply(lambda x:count_punctuations(x))
df_punct= pd.DataFrame(list(df_train.punct_count))
merge_df = pd.merge(df_train.target,df_punct,left_index=True, right_index=True)
363/299: merge_df
363/300:
merge_df_gr = merge_df.groupby('target').size().reset_index()
merge_df_gr_T = merge_df_gr.T
merge_df_gr_T.columns = ['target_0', 'target_1']
363/301: merge_df_gr_T
363/302: merge_df
363/303: merge_df.groupby('target').size().reset_index()
363/304:
merge_df_gr = merge_df.groupby('target').count().reset_index()
merge_df_gr_T = merge_df_gr.T
merge_df_gr_T.columns = ['target_0', 'target_1']
363/305: merge_df.groupby('target').size().reset_index()
363/306: merge_df
363/307:
merge_df_gr = merge_df.groupby('target').count().reset_index()
merge_df_gr = merge_df_gr.T
merge_df_gr.columns = ['target_0', 'target_1']
363/308: merge_df_gr
363/309:
#string.punctuation = '!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
def count_punctuations(text):
    d=dict()
    for i in string.punctuation:
        d[str(i)+' count']=text.count(i)
    return d
363/310:
df_train['punct_count'] = df_train["text"].apply(lambda x:count_punctuations(x))
df_punct= pd.DataFrame(list(df_train.punct_count))
merge_df = pd.merge(df_train.target,df_punct,left_index=True, right_index=True)
363/311: merge_df
363/312:
merge_df_gr = merge_df.groupby('target').count().reset_index()
merge_df_gr = merge_df_gr.T
merge_df_gr.columns = ['target_0', 'target_1']
363/313: merge_df_gr
363/314:
merge_df_gr = merge_df.groupby('target').sum().reset_index()
merge_df_gr = merge_df_gr.T
merge_df_gr.columns = ['target_0', 'target_1']
363/315: merge_df_gr
364/1: df_train.head()
364/2:
import pandas as pd
import numpy as np
import re
from collections import Counter
364/3: pd.set_option('max_colwidth', None)
364/4:
import matplotlib.pyplot as plt
import seaborn as sns
#import plotly.graph_objs as go 
#from plotly.offline import init_notebook_mode, iplot 
#init_notebook_mode(connected = True) 
#import os 
#import matplotlib.pyplot as plt#visualization 
#%matplotlib inline 
##warnings.filterwarnings("ignore") 
#import io 
#import plotly.offline as py#visualization 
#py.init_notebook_mode(connected=True)#visualization 
#import plotly.graph_objs as go#visualization 
#import plotly.tools as tls#visualization 
#import plotly.figure_factory as ff
364/5:
import nltk
#nltk.download()
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer

import emoji as emj
import spacy    
nlp = spacy.load('en_core_web_sm')
364/6:
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, roc_curve
#from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
#from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
364/7:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
364/8:
nltk.download()
#nltk.download('stopwords')
364/9: STOPWORDS = set(stopwords.words('english'))
364/10:
#import nltk
#nltk.download('wordnet')
364/11:
#import nltk
#dler = nltk.downloader.Downloader()
#dler._update_index()
#dler.download('all')
364/12: wnl = WordNetLemmatizer()
364/13: df_train.head()
364/14: df_train.isnull().mean()
364/15: df_train.shape, df_test.shape
364/16:
#cardinality check
for col in df_train.columns:
    print('{} has {} unique instances'.format(col, len(df_train[col].unique())))
364/17: df_train.info()
364/18: df_train.keyword.value_counts()
364/19:
plt.figure(figsize=[8, 40])
sns.countplot(y='keyword',
              data=df_train,
              color='lightblue',
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
364/20:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword', 
              hue='target', 
              data=df_train, 
              palette=['lightblue', 'red']
             )
plt.xticks(rotation=90)
plt.legend(loc='upper right')
plt.title("Distribution of Target per Keyword ")
plt.show()
364/21: df_train.keyword = df_train.keyword.str.replace('%20', ' ')
364/22:
# Let's show most common keywords for target = 1
df_train[df_train.target == 1].keyword.value_counts()[:10]
364/23:
# Let's show most common keywords for target = 1
df_train[df_train.target == 0].keyword.value_counts()[:10]
364/24:
df_train_group = df_train.groupby(['keyword', 'target'])['id'].count().unstack(fill_value=0).reset_index()
df_train_group.columns = ['keyword', 'target_0', 'target_1']
df_train_group['prob_real_disasters'] = df_train_group.target_1/(df_trainf_group.target_0 + df_trainf_group.target_1)
364/25: df_train.target.value_counts()/len(df_train)
364/26: df_train[df_train.keyword.isna()].target.value_counts()
364/27:
#string.punctuation = '!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
def count_punctuations(text):
    d=dict()
    for i in string.punctuation:
        d[str(i)+' count']=text.count(i)
    return d
364/28:
df_train['punct_count'] = df_train["text"].apply(lambda x:count_punctuations(x))
df_punct= pd.DataFrame(list(df_train.punct_count))
merge_df = pd.merge(df_train.target,df_punct,left_index=True, right_index=True)
364/29:
import pandas as pd
import numpy as np
import re
from collections import Counter
import string
364/30:
df_train['punct_count'] = df_train["text"].apply(lambda x:count_punctuations(x))
df_punct= pd.DataFrame(list(df_train.punct_count))
merge_df = pd.merge(df_train.target,df_punct,left_index=True, right_index=True)
364/31:
merge_df_gr = merge_df.groupby('target').sum().reset_index()
merge_df_gr = merge_df_gr.T
merge_df_gr.columns = ['target_0', 'target_1']
364/32: merge_df_gr
364/33: merge_df_gr['sum_0_1'] = merge_df_gr['target_0'] + merge_df_gr['target_1']
364/34: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
364/35: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
364/36:
def punct_add_exclamation(text):
    return len(re.findall("!", text))
364/37:
def punct_add_question(text):
    return len(re.findall("\?", text))
364/38:
def punct_add_quotation(text):
    return len(re.findall("'", text))
364/39:
df_train['count_exclamation'] = df_train.text.apply(punct_add_exclamation)
df_train['count_question'] = df_train.text.apply(punct_add_question)
df_train['count_quotation'] = df_train.text.apply(punct_add_quotation)
364/40: new_cols = ['count_exclamation', 'count_question', 'count_quotation']
364/41: df_train
365/1:
import pandas as pd
import numpy as np
import re
from collections import Counter
import string
365/2: pd.set_option('max_colwidth', None)
365/3:
import matplotlib.pyplot as plt
import seaborn as sns
#import plotly.graph_objs as go 
#from plotly.offline import init_notebook_mode, iplot 
#init_notebook_mode(connected = True) 
#import os 
#import matplotlib.pyplot as plt#visualization 
#%matplotlib inline 
##warnings.filterwarnings("ignore") 
#import io 
#import plotly.offline as py#visualization 
#py.init_notebook_mode(connected=True)#visualization 
#import plotly.graph_objs as go#visualization 
#import plotly.tools as tls#visualization 
#import plotly.figure_factory as ff
365/4:
import nltk
#nltk.download()
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer

import emoji as emj
import spacy    
nlp = spacy.load('en_core_web_sm')
365/5:
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, roc_curve
#from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
#from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
365/6:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
365/7:
nltk.download()
#nltk.download('stopwords')
365/8: STOPWORDS = set(stopwords.words('english'))
365/9:
#import nltk
#nltk.download('wordnet')
365/10:
#import nltk
#dler = nltk.downloader.Downloader()
#dler._update_index()
#dler.download('all')
365/11: wnl = WordNetLemmatizer()
365/12:
def preprocess_text(x):
    cleaned_text = re.sub(r'[^a-zA-Z\d\s]+', '', x)
    word_list = []
    for each_word in cleaned_text.split(' '):
        word_list.append(contractions.fix(each_word).lower())
    word_list = [
        wnl.lemmatize(each_word.strip()) for each_word in word_list
        if each_word not in STOPWORDS and each_word.strip() != ''
    ]
    return " ".join(word_list)
365/13: preprocess_text('e EE e')
367/1: !grep -inr 'oov'
367/2: %grep -inr 'oov'
367/3: !%grep -inr 'oov'
365/14: token.is_oov
365/15: df_train
365/16:
e = False
if !e:
    print('d')
365/17:
e = False
if !e:
    print('d')
365/18:
e = False
if ~e:
    print('d')
365/19:
e = True
if ~e:
    print('d')
365/20:
e = True
if e:
    print('d')
365/21:
e = False
if e:
    print('d')
365/22:
e = False
if !(e):
    print('d')
365/23:
e = False
if not e:
    print('d')
365/24:
e = True
if not e:
    print('d')
365/25:
def search_oov(text):
    tokens = nlp(text)
    for token in tokens:
        if not token.is_oov:
            print(token)
365/26: df_train.text.apply(search_oov)
365/27:
i=1
def search_oov(text):
    print(i)
    tokens = nlp(text)
    for token in tokens:
        if not token.is_oov:
            print(token)
    i+=1
365/28: df_train.text.apply(search_oov)
365/29:
def search_oov(text):
    tokens = nlp(text)
    for token in tokens:
        if not token.is_oov:
            print(token)
365/30: df_train.text.apply(search_oov)
365/31:
tokens = nlp(u'IranDeal')

for token in tokens:
    print(token.text, token.has_vector, token.vector_norm, token.is_oov)
365/32:
tokens = nlp(u'MTVHottest')

for token in tokens:
    print(token.text, token.has_vector, token.vector_norm, token.is_oov)
365/33:
tokens = nlp(u'thankU')

for token in tokens:
    print(token.text, token.has_vector, token.vector_norm, token.is_oov)
365/34:
tokens = nlp(u'ibooklove')

for token in tokens:
    print(token.text, token.has_vector, token.vector_norm, token.is_oov)
365/35:
tokens = nlp(u'ibookove')

for token in tokens:
    print(token.text, token.has_vector, token.vector_norm, token.is_oov)
365/36:
tokens = nlp(u'ibokove')

for token in tokens:
    print(token.text, token.has_vector, token.vector_norm, token.is_oov)
365/37:
tokens = nlp(u'frfkgmr')

for token in tokens:
    print(token.text, token.has_vector, token.vector_norm, token.is_oov)
365/38:
import nltk
#nltk.download()
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer

import emoji as emj
import spacy    
nlp = spacy.load('en_core_web_lg')
365/39:
import pandas as pd
import numpy as np
import re
from collections import Counter
import string
365/40: pd.set_option('max_colwidth', None)
365/41:
import matplotlib.pyplot as plt
import seaborn as sns
#import plotly.graph_objs as go 
#from plotly.offline import init_notebook_mode, iplot 
#init_notebook_mode(connected = True) 
#import os 
#import matplotlib.pyplot as plt#visualization 
#%matplotlib inline 
##warnings.filterwarnings("ignore") 
#import io 
#import plotly.offline as py#visualization 
#py.init_notebook_mode(connected=True)#visualization 
#import plotly.graph_objs as go#visualization 
#import plotly.tools as tls#visualization 
#import plotly.figure_factory as ff
365/42:
import nltk
#nltk.download()
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer

import emoji as emj
import spacy    
nlp = spacy.load('en_core_web_lg')
365/43:
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, roc_curve
#from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
#from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
365/44:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
365/45:
nltk.download()
#nltk.download('stopwords')
365/46: STOPWORDS = set(stopwords.words('english'))
365/47:
#import nltk
#nltk.download('wordnet')
365/48:
#import nltk
#dler = nltk.downloader.Downloader()
#dler._update_index()
#dler.download('all')
365/49: wnl = WordNetLemmatizer()
365/50: df_train.head()
365/51: df_train.isnull().mean()
365/52: df_train.shape, df_test.shape
365/53:
#cardinality check
for col in df_train.columns:
    print('{} has {} unique instances'.format(col, len(df_train[col].unique())))
365/54: df_train.info()
365/55: df_train.keyword.value_counts()
365/56:
plt.figure(figsize=[8, 40])
sns.countplot(y='keyword',
              data=df_train,
              color='lightblue',
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
365/57:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword', 
              hue='target', 
              data=df_train, 
              palette=['lightblue', 'red']
             )
plt.xticks(rotation=90)
plt.legend(loc='upper right')
plt.title("Distribution of Target per Keyword ")
plt.show()
365/58: df_train.keyword = df_train.keyword.str.replace('%20', ' ')
365/59:
# Let's show most common keywords for target = 1
df_train[df_train.target == 1].keyword.value_counts()[:10]
365/60:
# Let's show most common keywords for target = 1
df_train[df_train.target == 0].keyword.value_counts()[:10]
365/61:
df_train_group = df_train.groupby(['keyword', 'target'])['id'].count().unstack(fill_value=0).reset_index()
df_train_group.columns = ['keyword', 'target_0', 'target_1']
df_train_group['prob_real_disasters'] = df_train_group.target_1/(df_trainf_group.target_0 + df_trainf_group.target_1)
365/62:
df_train_group = df_train.groupby(['keyword', 'target'])['id'].count().unstack(fill_value=0).reset_index()
df_train_group.columns = ['keyword', 'target_0', 'target_1']
df_train_group['prob_real_disasters'] = df_train_group.target_1/(df_train_group.target_0 + df_train_group.target_1)
365/63: df_trainf_group.sort_values('prob_real_disasters', ascending=False)
365/64: df_train_group.sort_values('prob_real_disasters', ascending=False)
365/65: df_train.keyword.nunique()
365/66: df_test.head()
365/67: df_train.target.value_counts()/len(df_train)
365/68: df_train[df_train.keyword.isna()].target.value_counts()
365/69:
#string.punctuation = '!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
def count_punctuations(text):
    d=dict()
    for i in string.punctuation:
        d[str(i)+' count']=text.count(i)
    return d
365/70:
df_train['punct_count'] = df_train["text"].apply(lambda x:count_punctuations(x))
df_punct= pd.DataFrame(list(df_train.punct_count))
merge_df = pd.merge(df_train.target,df_punct,left_index=True, right_index=True)
365/71:
merge_df_gr = merge_df.groupby('target').sum().reset_index()
merge_df_gr = merge_df_gr.T
merge_df_gr.columns = ['target_0', 'target_1']
365/72: merge_df_gr
365/73: merge_df_gr['sum_0_1'] = merge_df_gr['target_0'] + merge_df_gr['target_1']
365/74: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
365/75: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
365/76:
def punct_add_exclamation(text):
    return len(re.findall("!", text))
365/77:
def punct_add_question(text):
    return len(re.findall("\?", text))
365/78:
def punct_add_quotation(text):
    return len(re.findall("'", text))
365/79:
df_train['count_exclamation'] = df_train.text.apply(punct_add_exclamation)
df_train['count_question'] = df_train.text.apply(punct_add_question)
df_train['count_quotation'] = df_train.text.apply(punct_add_quotation)
365/80: new_cols = ['count_exclamation', 'count_question', 'count_quotation']
365/81: df_train
365/82:
tokens = nlp(u'frfkgmr')

for token in tokens:
    print(token.text, token.has_vector, token.vector_norm, token.is_oov)
365/83:
tokens = nlp(u'lkl')

for token in tokens:
    print(token.text, token.has_vector, token.vector_norm, token.is_oov)
365/84:
def search_oov(text):
    tokens = nlp(text)
    for token in tokens:
        if not token.is_oov:
            print(token)
365/85: df_train.text.apply(search_oov)
365/86:
tokens = nlp(u'I"m afraid that the tornado is coming to ou area')

for token in tokens:
    print(token.text, token.has_vector, token.vector_norm, token.is_oov)
365/87:
def search_oov(text):
    tokens = nlp(text)
    for token in tokens:
        if token.is_oov:
            print(token)
365/88: df_train.text.apply(search_oov)
365/89:
def search_oov(text):
    text = re.sub(r"https?:\/\/t.co\/[A-Za-z0-9]+", "", text)
    tokens = nlp(text)
    for token in tokens:
        if token.is_oov:
            print(token)
365/90: df_train.text.apply(search_oov)
365/91: emj.distinct_emoji_list('¬â√õ√è@BBCWomansHour')
365/92: emj.distinct_emoji_list('√õ√è@BBCWomansHour')
365/93: emj.distinct_emoji_list('√õ√è@BBCWomansHour'.decode('utf-8'))
365/94: emj.distinct_emoji_list('√õ√è@BBCWomansHour'.str.decode('utf-8'))
365/95: emj.distinct_emoji_list('√õ√è@BBCWomansHour'.decode('utf-8'))
365/96:
data = str('√õ√è@BBCWomansHour')
    print(data.decode('utf-8'))
365/97:
data = str('√õ√è@BBCWomansHour')
print(data.decode('utf-8'))
365/98: emj.distinct_emoji_list('√õ√è@BBCWomansHour'.encode().decode('utf-8'))
365/99: '√õ√è@BBCWomansHour'.encode().decode('utf-8')
365/100: df_train[df_train.text.contains('BBCWomansHour')]
365/101: df_train[df_train.text.contain('BBCWomansHour')]
365/102: df_train[df_train.text.str.contain('BBCWomansHour')]
365/103: df_train[df_train.text.str.contains('BBCWomansHour')]
365/104: df_train[df_train.text.str.contains('BBCWomansHour')].text
365/105: df_train[df_train.text.str.contains('BBCWomansHour')].text.encode().decode('utf-8'))
365/106: df_train[df_train.text.str.contains('BBCWomansHour')].text.encode().decode('utf-8')
365/107: df_train[df_train.text.str.contains('BBCWomansHour')].text.decode('utf-8')
365/108: df_train[df_train.text.str.contains('BBCWomansHour')].text
365/109: df_train[df_train.text.str.contains('BBCWomansHour')].text.str.encode().decode('utf-8'))
365/110: df_train[df_train.text.str.contains('BBCWomansHour')].text.str.encode().decode('utf-8')
365/111: df_train[df_train.text.str.contains('BBCWomansHour')].text.str.encode('utf-8').decode('utf-8')
365/112: df_train[df_train.text.str.contains('BBCWomansHour')].text.str
365/113: df_train[df_train.text.str.contains('BBCWomansHour')].text
365/114: unicode(df_train[df_train.text.str.contains('BBCWomansHour')].text, "utf-8")
365/115: str(df_train[df_train.text.str.contains('BBCWomansHour')].text, 'utf-8')
365/116:
def dtr_to_unicode(text):
    return res = ''.join(r'\u{:04X}'.format(ord(chr)) for chr in text)
365/117:
def str_to_unicode(text):
    return ''.join(r'\u{:04X}'.format(ord(chr)) for chr in text)
365/118: df_test.text.apply(str_to_unicode)
365/119: emoji.UNICODE_EMOJI
365/120: emj.UNICODE_EMOJI
365/121: emj.UNICODE_EMOJI['en']
365/122: uni_text = df_test.text.apply(str_to_unicode)
365/123: uni_text
365/124: regex.findall(u'[\U0001F1E6-\U0001F1FF]', uni_text)
365/125: re.findall(u'[\U0001F1E6-\U0001F1FF]', uni_text)
365/126:
def str_to_unicode(text):
    byte_str = ''.join(r'\u{:04X}'.format(ord(chr)) for chr in text)
    flags = re.findall(u'[\U0001F1E6-\U0001F1FF]', uni_text) 
    print(flags)
365/127: re.findall(u'[\U0001F1E6-\U0001F1FF]', uni_text)
365/128: df_test.text.apply(str_to_unicode)
365/129:
def str_to_unicode(text):
    byte_str = ''.join(r'\u{:04X}'.format(ord(chr)) for chr in text)
    flags = re.findall(u'[\U0001F1E6-\U0001F1FF]', byte_str) 
    print(flags)
365/130: #re.findall(u'[\U0001F1E6-\U0001F1FF]', uni_text)
365/131: df_test.text.apply(str_to_unicode)
365/132: df_train.text.apply(str_to_unicode)
365/133:
def str_to_unicode(text):
    byte_str = ''.join(r'\u{:04X}'.format(ord(chr)) for chr in text)
    flags = re.findall(u'[\U0001F1E6-\U0001F1FF]', byte_str) 
    if len(flags)>0:
        print(flags)
365/134: #re.findall(u'[\U0001F1E6-\U0001F1FF]', uni_text)
365/135: df_train.text.apply(str_to_unicode)
365/136: u'[\U0001F1E6-\U0001F1FF]'
365/137:
def extract_emojis(s):
    return ''.join(c for c in s if c in emoji.UNICODE_EMOJI['en'])
365/138: df_train.text.apply(extract_emojis)
365/139:
def extract_emojis(s):
    return ''.join(c for c in s if c in emj.UNICODE_EMOJI['en'])
365/140: u'[\U0001F1E6-\U0001F1FF]'
365/141: #re.findall(u'[\U0001F1E6-\U0001F1FF]', uni_text)
365/142: df_train.text.apply(extract_emojis)
365/143: print('\U00002668')
365/144: emoji_pattern = re.compile('[\U0001F300-\U0001F64F]')
365/145:
def str_to_unicode(text):
    byte_str = ''.join(r'\u{:04X}'.format(ord(chr)) for chr in text)
    emojis_l = emoji_pattern.findall(byte_str)
    print(emojis_l)
365/146: df_train.text.apply(str_to_unicode)
365/147: emoji_pattern = re.compile('[\U0001F680-\U0001F6FF]')
365/148:
def str_to_unicode(text):
    byte_str = ''.join(r'\u{:04X}'.format(ord(chr)) for chr in text)
    emojis_l = emoji_pattern.findall(byte_str)
    print(emojis_l)
365/149: df_train.text.apply(str_to_unicode)
365/150:
def str_to_unicode(text):
    byte_str = ''.join(r'\u{:04X}'.format(ord(chr)) for chr in text)
    emojis_l = emoji_pattern.findall(byte_str)
    print(byte_str)
365/151: emoji_pattern = re.compile('[\U0001F680-\U0001F6FF]')
365/152:
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
365/153: df_train.text.apply(str_to_unicode)
370/1: % grep -iRl 'oov' ./
370/2: !grep -iRl 'oov' ./
371/1: uni_text
371/2:
import pandas as pd
import numpy as np
import re
from collections import Counter
import string
371/3: pd.set_option('max_colwidth', None)
371/4:
import matplotlib.pyplot as plt
import seaborn as sns
#import plotly.graph_objs as go 
#from plotly.offline import init_notebook_mode, iplot 
#init_notebook_mode(connected = True) 
#import os 
#import matplotlib.pyplot as plt#visualization 
#%matplotlib inline 
##warnings.filterwarnings("ignore") 
#import io 
#import plotly.offline as py#visualization 
#py.init_notebook_mode(connected=True)#visualization 
#import plotly.graph_objs as go#visualization 
#import plotly.tools as tls#visualization 
#import plotly.figure_factory as ff
371/5:
import nltk
#nltk.download()
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer

import emoji as emj
import spacy 
## It's important to use _lg for OOV in future
nlp = spacy.load('en_core_web_lg')
371/6:
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, roc_curve
#from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
#from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
371/7:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
371/8:
nltk.download()
#nltk.download('stopwords')
371/9: STOPWORDS = set(stopwords.words('english'))
371/10:
#import nltk
#nltk.download('wordnet')
371/11:
#import nltk
#dler = nltk.downloader.Downloader()
#dler._update_index()
#dler.download('all')
371/12: wnl = WordNetLemmatizer()
371/13: df_train.head()
371/14: df_train.isnull().mean()
371/15: df_train.shape, df_test.shape
371/16:
#cardinality check
for col in df_train.columns:
    print('{} has {} unique instances'.format(col, len(df_train[col].unique())))
371/17: df_train.info()
371/18: df_train.keyword.value_counts()
371/19:
plt.figure(figsize=[8, 40])
sns.countplot(y='keyword',
              data=df_train,
              color='lightblue',
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
371/20:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword', 
              hue='target', 
              data=df_train, 
              palette=['lightblue', 'red']
             )
plt.xticks(rotation=90)
plt.legend(loc='upper right')
plt.title("Distribution of Target per Keyword ")
plt.show()
371/21: df_train.keyword = df_train.keyword.str.replace('%20', ' ')
371/22:
# Let's show most common keywords for target = 1
df_train[df_train.target == 1].keyword.value_counts()[:10]
371/23:
# Let's show most common keywords for target = 1
df_train[df_train.target == 0].keyword.value_counts()[:10]
371/24:
df_train_group = df_train.groupby(['keyword', 'target'])['id'].count().unstack(fill_value=0).reset_index()
df_train_group.columns = ['keyword', 'target_0', 'target_1']
df_train_group['prob_real_disasters'] = df_train_group.target_1/(df_train_group.target_0 + df_train_group.target_1)
371/25: df_train_group.sort_values('prob_real_disasters', ascending=False)
371/26: df_train.keyword.nunique()
371/27: df_test.head()
371/28: df_train.target.value_counts()/len(df_train)
371/29: df_train[df_train.keyword.isna()].target.value_counts()
371/30:
#string.punctuation = '!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
def count_punctuations(text):
    d=dict()
    for i in string.punctuation:
        d[str(i)+' count']=text.count(i)
    return d
371/31:
df_train['punct_count'] = df_train["text"].apply(lambda x:count_punctuations(x))
df_punct= pd.DataFrame(list(df_train.punct_count))
merge_df = pd.merge(df_train.target,df_punct,left_index=True, right_index=True)
371/32:
merge_df_gr = merge_df.groupby('target').sum().reset_index()
merge_df_gr = merge_df_gr.T
merge_df_gr.columns = ['target_0', 'target_1']
371/33: merge_df_gr
371/34: merge_df_gr['sum_0_1'] = merge_df_gr['target_0'] + merge_df_gr['target_1']
371/35: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
371/36: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
371/37:
def punct_add_exclamation(text):
    return len(re.findall("!", text))
371/38:
def punct_add_question(text):
    return len(re.findall("\?", text))
371/39:
def punct_add_quotation(text):
    return len(re.findall("'", text))
371/40:
df_train['count_exclamation'] = df_train.text.apply(punct_add_exclamation)
df_train['count_question'] = df_train.text.apply(punct_add_question)
df_train['count_quotation'] = df_train.text.apply(punct_add_quotation)
371/41: new_cols = ['count_exclamation', 'count_question', 'count_quotation']
371/42: df_train
371/43: df_train.head(2)
371/44: 'Re'.str.isupper()
371/45: 'Re'.isupper()
371/46: 'RE'.isupper()
371/47: 'kRE'.isupper()
371/48: 'kRE tt'.isupper()
371/49:
count=0
for i in 'DME d':
    if i.isupper():
        count+=1
print(count)
371/50:
count=0
for i in 'DME dE':
    if i.isupper():
        count+=1
print(count)
371/51: string.punctuation
371/52: count_punctuations('tre, f!')
371/53: count_punctuations('tre, f!')
371/54:
# count number of characters 
def count_chars(text):
    return len(text)

# count number of words 
def count_words(text):
    return len(text.split())

# count number of capital characters
def count_capital_chars(text):
    count=0
    for i in text:
        if i.isupper():
            count+=1
    return count

# count number of capital words
def count_capital_words(text):
    return sum(map(str.isupper,text.split()))

# count number of punctuations
def count_punctuations(text):
    d=dict()
    for i in string.punctuation:
        d[str(i)+' count']=text.count(i)
    return d

# count number of words in quotes
def count_words_in_quotes(text):
    x = re.findall("\'.\'|\".\"", text)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
    
# count number of sentences
def count_sent(text):
    return len(nltk.sent_tokenize(text))

# calculate average word length
def avg_word_len(char_cnt,word_cnt):
    return char_cnt/word_cnt

# calculate average sentence length
def avg_sent_len(word_cnt,sent_cnt):
    return word_cnt/sent_cnt

# count number of unique words 
def count_unique_words(text):
    return len(set(text.split()))
    
# count of hashtags
def count_htags(text):
    x = re.findall(r'(\#\w[A-Za-z0-9]*)', text)
    return len(x)

# count of mentions
def count_mentions(text):
    x = re.findall(r'(\@\w[A-Za-z0-9]*)', text)
    return len(x)

# count of stopwords
def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)
371/55: re.findall("\'.\'|\".\"", '"text" y')
371/56: re.findall("\'.\'|\".\"", '"eje jj d"')
371/57: re.findall("\'.\'|\".\"", "'dd' d 'dd'")
371/58:
x = re.findall("\'.\'|\".\"", "'dd' d 'dd'")
print(x)
371/59:
x = re.findall("\'(.+?)\'|\".\"", "'dd' d 'dd'")
print(x)
371/60:
x = re.findall("\'(.+?)\'|\"(.+?)\"", "'dd' d 'dd'")
print(x)
371/61:
x = []
x = re.findall("\'(.+?)\'|\"(.+?)\"", "'dd' d 'dd'")
print(x)
371/62:
x = []
x = re.findall("\'(.+?)\'", "'dd' d 'dd'")
print(x)
371/63: df_train
371/64: df_train.text.str.contains('\"')
371/65: df_train[df_train.text.str.contains('\"')]
371/66: df_train[df_train.text.str.contains('\'')]
371/67:
# count number of characters 
def count_chars(text):
    return len(text)

# count number of words 
def count_words(text):
    return len(text.split())

# count number of capital characters
def count_capital_chars(text):
    count=0
    for i in text:
        if i.isupper():
            count+=1
    return count

# count number of capital words
def count_capital_words(text):
    return sum(map(str.isupper,text.split()))

# count number of punctuations
def count_punctuations(text):
    d=dict()
    for i in string.punctuation:
        d[str(i)+' count']=text.count(i)
    return d

# count number of words in quotes
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    for each_word in text.split(' '):
        string_wothout_contractions.join((contractions.fix(each_word).lower()))
    x = re.findall("\'.\'|\".\"", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
    
# count number of sentences
def count_sent(text):
    return len(nltk.sent_tokenize(text))

# calculate average word length
def avg_word_len(char_cnt,word_cnt):
    return char_cnt/word_cnt

# calculate average sentence length
def avg_sent_len(word_cnt,sent_cnt):
    return word_cnt/sent_cnt

# count number of unique words 
def count_unique_words(text):
    return len(set(text.split()))
    
# count of hashtags
def count_htags(text):
    x = re.findall(r'(\#\w[A-Za-z0-9]*)', text)
    return len(x)

# count of mentions
def count_mentions(text):
    x = re.findall(r'(\@\w[A-Za-z0-9]*)', text)
    return len(x)

# count of stopwords
def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)
371/68:
# count number of characters 
def count_chars(text):
    return len(text)

# count number of words 
def count_words(text):
    return len(text.split())

# count number of capital characters
def count_capital_chars(text):
    count=0
    for i in text:
        if i.isupper():
            count+=1
    return count

# count number of capital words
def count_capital_words(text):
    return sum(map(str.isupper,text.split()))

# count number of punctuations
def count_punctuations(text):
    d=dict()
    for i in string.punctuation:
        d[str(i)+' count']=text.count(i)
    return d

# count number of words in quotes
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    for each_word in text.split(' '):
        string_wothout_contractions.join((contractions.fix(each_word).lower()))
    print(string_wothout_contractions)
    x = re.findall("\'.\'|\".\"", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
    
# count number of sentences
def count_sent(text):
    return len(nltk.sent_tokenize(text))

# calculate average word length
def avg_word_len(char_cnt,word_cnt):
    return char_cnt/word_cnt

# calculate average sentence length
def avg_sent_len(word_cnt,sent_cnt):
    return word_cnt/sent_cnt

# count number of unique words 
def count_unique_words(text):
    return len(set(text.split()))
    
# count of hashtags
def count_htags(text):
    x = re.findall(r'(\#\w[A-Za-z0-9]*)', text)
    return len(x)

# count of mentions
def count_mentions(text):
    x = re.findall(r'(\@\w[A-Za-z0-9]*)', text)
    return len(x)

# count of stopwords
def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)
371/69: count_words_in_quotes('All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected')
371/70: count_words_in_quotes("All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected")
371/71:
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    for each_word in text.split(' '):
        string_wothout_contractions.join((contractions.fix(each_word).lower()))
    print(string_wothout_contractions)
    x = re.findall("\'.\'|\".\"", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
371/72:
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    for each_word in text.split(' '):
        ' '.join((contractions.fix(each_word).lower()))
    print(string_wothout_contractions)
    x = re.findall("\'.\'|\".\"", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
371/73: count_words_in_quotes("All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected")
371/74:
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(word))
    print(expanded_words)
    x = re.findall("\'.\'|\".\"", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
371/75: count_words_in_quotes("All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected")
371/76:
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    print(expanded_words)
    x = re.findall("\'.\'|\".\"", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
371/77: count_words_in_quotes("All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected")
371/78:
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    print(string_wothout_contractions)
    x = re.findall("\'.\'|\".\"", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
371/79: count_words_in_quotes("All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected")
371/80:
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    print(string_wothout_contractions)
    x = re.findall("\'(.+?)\'", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
371/81: count_words_in_quotes("All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected")
371/82:
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    string_wothout_contractions.replace('"', '\'')
    print(string_wothout_contractions)
    x = re.findall("\'(.+?)\'", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
371/83: count_words_in_quotes('''All residents asked to 'shelter in place' are "being" notified by officers. No other evacuation or shelter in place orders are expected''')
371/84:
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    string_wothout_contractions.replace('"', '\\'')
    print(string_wothout_contractions)
    x = re.findall("\'(.+?)\'", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
371/85:
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    string_wothout_contractions.replace('"', '\'')
    print(string_wothout_contractions)
    x = re.findall("\'(.+?)\'", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
371/86:
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    string_wothout_contractions.replace('\"', '\'')
    print(string_wothout_contractions)
    x = re.findall("\'(.+?)\'", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
371/87: count_words_in_quotes('''All residents asked to 'shelter in place' are "being" notified by officers. No other evacuation or shelter in place orders are expected''')
371/88: '''dv "eef"'''.replace('\"', '\'')
371/89: '''dv "eef"'''.replace('"', '\'')
371/90:
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    string_wothout_contractions = string_wothout_contractions.replace('"', '\'')
    print(string_wothout_contractions)
    x = re.findall("\'(.+?)\'", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
371/91: '''dv "eef"'''.replace('"', '\'')
371/92: count_words_in_quotes('''All residents asked to 'shelter in place' are "being" notified by officers. No other evacuation or shelter in place orders are expected''')
371/93:
# count number of characters 
def count_chars(text):
    return len(text)

# count number of words 
def count_words(text):
    return len(text.split())

# count number of capital characters
def count_capital_chars(text):
    count=0
    for i in text:
        if i.isupper():
            count+=1
    return count

# count number of capital words
def count_capital_words(text):
    return sum(map(str.isupper,text.split()))

# count number of punctuations
def count_punctuations(text):
    d=dict()
    for i in string.punctuation:
        d[str(i)+' count']=text.count(i)
    return d

# count number of words in quotes
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    string_wothout_contractions = string_wothout_contractions.replace('"', '\'')
    print(string_wothout_contractions)
    x = re.findall("\'(.+?)\'", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
    
# count number of sentences
def count_sent(text):
    return len(nltk.sent_tokenize(text))

# calculate average word length
def avg_word_len(char_cnt,word_cnt):
    return char_cnt/word_cnt

# calculate average sentence length
def avg_sent_len(word_cnt,sent_cnt):
    return word_cnt/sent_cnt

# count number of unique words 
def count_unique_words(text):
    return len(set(text.split()))
    
# count of hashtags
def count_htags(text):
    x = re.findall(r'(\#\w[A-Za-z0-9]*)', text)
    return len(x)

# count of mentions
def count_mentions(text):
    x = re.findall(r'(\@\w[A-Za-z0-9]*)', text)
    return len(x)

# count of stopwords
def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)
371/94:
# count number of characters 
def count_chars(text):
    return len(text)

# count number of words 
def count_words(text):
    return len(text.split())

# count number of capital characters
def count_capital_chars(text):
    count=0
    for i in text:
        if i.isupper():
            count+=1
    return count

# count number of capital words
def count_capital_words(text):
    return sum(map(str.isupper,text.split()))

# count number of punctuations
def count_punctuations(text):
    d=dict()
    for i in string.punctuation:
        d[str(i)+' count']=text.count(i)
    return d

# count number of words in quotes
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    string_wothout_contractions = string_wothout_contractions.replace('"', '\'')
    print(string_wothout_contractions)
    x = re.findall("\'(.+?)\'", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
    
# count number of sentences
def count_sent(text):
    return len(nltk.sent_tokenize(text))

# count number of unique words 
def count_unique_words(text):
    return len(set(text.split()))
    
# count of hashtags
def count_htags(text):
    x = re.findall(r'(\#\w[A-Za-z0-9]*)', text)
    return len(x)

# count of mentions
def count_mentions(text):
    x = re.findall(r'(\@\w[A-Za-z0-9]*)', text)
    return len(x)

# count of stopwords
def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)
371/95:
def feature_eng_fun(df, text):
    df['count_exclamation'] = df[text].apply(punct_add_exclamation)
    df['count_question'] = df[text].apply(punct_add_question)
    df['count_quotation'] = df[text].apply(punct_add_quotation)
    df['count_num_of_chars'] = df[text].apply(count_chars)
    df['count_num_of_words'] = df[text].apply(count_words)
    df['count_num_of_capital_chars'] = df[text].apply(count_capital_chars)
    df['count_num_of_capital_words'] = df[text].apply(count_capital_words)
    df['count_punct'] = df[text].apply(count_punctuations)
    df['count_words_in_quotes'] = df[text].apply(count_words_in_quotes)
    df['count_count_sent'] = df[text].apply(count_sent)
    df['count_unique_words'] = df[text].apply(count_unique_words)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_mentions'] = df[text].apply(count_mentions)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_stopwords'] = df[text].apply(count_stopwords)
    return df
371/96: df_feature_eng = df[['id', 'text', 'target']]
371/97: df_feature_eng = df_train[['id', 'text', 'target']]
371/98: df_feature_eng
371/99: feature_eng_fun(df_feature_eng, text)
371/100: feature_eng_fun(df_feature_eng, 'text')
371/101: df_new = feature_eng_fun(df_feature_eng, 'text')
371/102: df_new
371/103:
# count number of characters 
def count_chars(text):
    return len(text)

# count number of words 
def count_words(text):
    return len(text.split())

# count number of capital characters
def count_capital_chars(text):
    count=0
    for i in text:
        if i.isupper():
            count+=1
    return count

# count number of capital words
def count_capital_words(text):
    return sum(map(str.isupper,text.split()))

# count number of punctuations
def count_punctuations(text):
    return count(s,set(string.punctuation))

# count number of words in quotes
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    string_wothout_contractions = string_wothout_contractions.replace('"', '\'')
    print(string_wothout_contractions)
    x = re.findall("\'(.+?)\'", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
    
# count number of sentences
def count_sent(text):
    return len(nltk.sent_tokenize(text))

# count number of unique words 
def count_unique_words(text):
    return len(set(text.split()))
    
# count of hashtags
def count_htags(text):
    x = re.findall(r'(\#\w[A-Za-z0-9]*)', text)
    return len(x)

# count of mentions
def count_mentions(text):
    x = re.findall(r'(\@\w[A-Za-z0-9]*)', text)
    return len(x)

# count of stopwords
def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)
371/104:
def feature_eng_fun(df, text):
    df['count_exclamation'] = df[text].apply(punct_add_exclamation)
    df['count_question'] = df[text].apply(punct_add_question)
    df['count_quotation'] = df[text].apply(punct_add_quotation)
    df['count_num_of_chars'] = df[text].apply(count_chars)
    df['count_num_of_words'] = df[text].apply(count_words)
    df['count_num_of_capital_chars'] = df[text].apply(count_capital_chars)
    df['count_num_of_capital_words'] = df[text].apply(count_capital_words)
    df['count_punct'] = df[text].apply(count_punctuations)
    df['count_words_in_quotes'] = df[text].apply(count_words_in_quotes)
    df['count_count_sent'] = df[text].apply(count_sent)
    df['count_unique_words'] = df[text].apply(count_unique_words)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_mentions'] = df[text].apply(count_mentions)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_stopwords'] = df[text].apply(count_stopwords)
    return df
371/105: df_feature_eng = df_train[['id', 'text', 'target']]
371/106: df_new = feature_eng_fun(df_feature_eng, 'text')
371/107:
# count number of characters 
def count_chars(text):
    return len(text)

# count number of words 
def count_words(text):
    return len(text.split())

# count number of capital characters
def count_capital_chars(text):
    count=0
    for i in text:
        if i.isupper():
            count+=1
    return count

# count number of capital words
def count_capital_words(text):
    return sum(map(str.isupper,text.split()))

# count number of punctuations
def count_punctuations(text):
    return count(text,set(string.punctuation))

# count number of words in quotes
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    string_wothout_contractions = string_wothout_contractions.replace('"', '\'')
    print(string_wothout_contractions)
    x = re.findall("\'(.+?)\'", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
    
# count number of sentences
def count_sent(text):
    return len(nltk.sent_tokenize(text))

# count number of unique words 
def count_unique_words(text):
    return len(set(text.split()))
    
# count of hashtags
def count_htags(text):
    x = re.findall(r'(\#\w[A-Za-z0-9]*)', text)
    return len(x)

# count of mentions
def count_mentions(text):
    x = re.findall(r'(\@\w[A-Za-z0-9]*)', text)
    return len(x)

# count of stopwords
def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)
371/108:
def feature_eng_fun(df, text):
    df['count_exclamation'] = df[text].apply(punct_add_exclamation)
    df['count_question'] = df[text].apply(punct_add_question)
    df['count_quotation'] = df[text].apply(punct_add_quotation)
    df['count_num_of_chars'] = df[text].apply(count_chars)
    df['count_num_of_words'] = df[text].apply(count_words)
    df['count_num_of_capital_chars'] = df[text].apply(count_capital_chars)
    df['count_num_of_capital_words'] = df[text].apply(count_capital_words)
    df['count_punct'] = df[text].apply(count_punctuations)
    df['count_words_in_quotes'] = df[text].apply(count_words_in_quotes)
    df['count_count_sent'] = df[text].apply(count_sent)
    df['count_unique_words'] = df[text].apply(count_unique_words)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_mentions'] = df[text].apply(count_mentions)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_stopwords'] = df[text].apply(count_stopwords)
    return df
371/109: df_feature_eng = df_train[['id', 'text', 'target']]
371/110: df_new = feature_eng_fun(df_feature_eng, 'text')
371/111:
# count number of characters 
def count_chars(text):
    return len(text)

# count number of words 
def count_words(text):
    return len(text.split())

# count number of capital characters
def count_capital_chars(text):
    count=0
    for i in text:
        if i.isupper():
            count+=1
    return count

# count number of capital words
def count_capital_words(text):
    return sum(map(str.isupper,text.split()))

# count number of punctuations
def count_punctuations(text):
    count = lambda l1,l2: sum([1 for x in l1 if x in l2])
    return count(text,set(string.punctuation))

# count number of words in quotes
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    string_wothout_contractions = string_wothout_contractions.replace('"', '\'')
    print(string_wothout_contractions)
    x = re.findall("\'(.+?)\'", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
    
# count number of sentences
def count_sent(text):
    return len(nltk.sent_tokenize(text))

# count number of unique words 
def count_unique_words(text):
    return len(set(text.split()))
    
# count of hashtags
def count_htags(text):
    x = re.findall(r'(\#\w[A-Za-z0-9]*)', text)
    return len(x)

# count of mentions
def count_mentions(text):
    x = re.findall(r'(\@\w[A-Za-z0-9]*)', text)
    return len(x)

# count of stopwords
def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)
371/112:
def feature_eng_fun(df, text):
    df['count_exclamation'] = df[text].apply(punct_add_exclamation)
    df['count_question'] = df[text].apply(punct_add_question)
    df['count_quotation'] = df[text].apply(punct_add_quotation)
    df['count_num_of_chars'] = df[text].apply(count_chars)
    df['count_num_of_words'] = df[text].apply(count_words)
    df['count_num_of_capital_chars'] = df[text].apply(count_capital_chars)
    df['count_num_of_capital_words'] = df[text].apply(count_capital_words)
    df['count_punct'] = df[text].apply(count_punctuations)
    df['count_words_in_quotes'] = df[text].apply(count_words_in_quotes)
    df['count_count_sent'] = df[text].apply(count_sent)
    df['count_unique_words'] = df[text].apply(count_unique_words)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_mentions'] = df[text].apply(count_mentions)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_stopwords'] = df[text].apply(count_stopwords)
    return df
371/113: df_feature_eng = df_train[['id', 'text', 'target']]
371/114: df_new = feature_eng_fun(df_feature_eng, 'text')
371/115: df_new
371/116:
def count_links(text):
    link_list = re.search("(?P<url>https?://[^\s]+)", myString)
    return(len(link_list))
371/117: count_links('News Update Huge cliff landslide on road in China - Watch the moment a cliff collapses as huge chunks of rock fall... http://t.co/gaBd0cjmAG')
371/118:
def count_links(text):
    link_list = re.search("(?P<url>https?://[^\s]+)", text)
    return(len(link_list))
371/119: count_links('News Update Huge cliff landslide on road in China - Watch the moment a cliff collapses as huge chunks of rock fall... http://t.co/gaBd0cjmAG')
371/120:
def count_links(text):
    link_list = re.findall(r'(https?://[^\s]+)', text)
    return(len(link_list))
371/121: count_links('News Update Huge cliff landslide on road in China - Watch the moment a cliff collapses as huge chunks of rock fall... http://t.co/gaBd0cjmAG')
371/122:
def count_links(text):
    link_list = re.findall(r'(https?://[^\s]+)', text)
    bitly_list = re.findall(r'bit.ly/\S+', text)
    return(len(link_list + link_list))
371/123: count_links('News Update Huge cliff landslide on road in China - Watch the moment a cliff collapses as huge chunks of rock fall... http://t.co/gaBd0cjmAG')
371/124:
def count_links(text):
    link_list = re.findall(r'(https?://[^\s]+)', text)
    bitly_list = re.findall(r'bit.ly/\S+', text)
    return(len(link_list + bitly_list))
371/125: count_links('News Update Huge cliff landslide on road in China - Watch the moment a cliff collapses as huge chunks of rock fall... http://t.co/gaBd0cjmAG')
371/126: df_train.text.str.contains('.ly')
371/127: df_train[df_train.text.str.contains('.ly')]
371/128: df_train[df_train.text.str.contains('bit.ly')]
371/129:
# count number of characters 
def count_chars(text):
    return len(text)

# count number of words 
def count_words(text):
    return len(text.split())

# count number of capital characters
def count_capital_chars(text):
    count=0
    for i in text:
        if i.isupper():
            count+=1
    return count

# count number of capital words
def count_capital_words(text):
    return sum(map(str.isupper,text.split()))

# count number of punctuations
def count_punctuations(text):
    count = lambda l1,l2: sum([1 for x in l1 if x in l2])
    return count(text,set(string.punctuation))

# count number of words in quotes
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    string_wothout_contractions = string_wothout_contractions.replace('"', '\'')
    x = re.findall("\'(.+?)\'", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
    
# count number of sentences
def count_sent(text):
    return len(nltk.sent_tokenize(text))

# count number of unique words 
def count_unique_words(text):
    return len(set(text.split()))
    
# count of hashtags
def count_htags(text):
    x = re.findall(r'(\#\w[A-Za-z0-9]*)', text)
    return len(x)

# count of mentions
def count_mentions(text):
    x = re.findall(r'(\@\w[A-Za-z0-9]*)', text)
    return len(x)

# count of stopwords
def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)

#count of links
def count_links(text):
    link_list = re.findall(r'(https?://[^\s]+)', text)
    bitly_list = re.findall(r'bit.ly/\S+', text)
    return(len(link_list + bitly_list))
371/130:
def feature_eng_fun(df, text):
    df['count_exclamation'] = df[text].apply(punct_add_exclamation)
    df['count_question'] = df[text].apply(punct_add_question)
    df['count_quotation'] = df[text].apply(punct_add_quotation)
    df['count_num_of_chars'] = df[text].apply(count_chars)
    df['count_num_of_words'] = df[text].apply(count_words)
    df['count_num_of_capital_chars'] = df[text].apply(count_capital_chars)
    df['count_num_of_capital_words'] = df[text].apply(count_capital_words)
    df['count_punct'] = df[text].apply(count_punctuations)
    df['count_words_in_quotes'] = df[text].apply(count_words_in_quotes)
    df['count_count_sent'] = df[text].apply(count_sent)
    df['count_unique_words'] = df[text].apply(count_unique_words)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_mentions'] = df[text].apply(count_mentions)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_stopwords'] = df[text].apply(count_stopwords)
    df['count_links'] = df[text].apply(count_links)  
    return df
371/131: df_feature_eng = df_train[['id', 'text', 'target']]
371/132: df_new = feature_eng_fun(df_feature_eng, 'text')
371/133: df['text']
371/134:
def feature_eng_fun(df, text):
    df['count_exclamation'] = df[[text]].apply(punct_add_exclamation)
    df['count_question'] = df[text].apply(punct_add_question)
    df['count_quotation'] = df[text].apply(punct_add_quotation)
    df['count_num_of_chars'] = df[text].apply(count_chars)
    df['count_num_of_words'] = df[text].apply(count_words)
    df['count_num_of_capital_chars'] = df[text].apply(count_capital_chars)
    df['count_num_of_capital_words'] = df[text].apply(count_capital_words)
    df['count_punct'] = df[text].apply(count_punctuations)
    df['count_words_in_quotes'] = df[text].apply(count_words_in_quotes)
    df['count_count_sent'] = df[text].apply(count_sent)
    df['count_unique_words'] = df[text].apply(count_unique_words)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_mentions'] = df[text].apply(count_mentions)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_stopwords'] = df[text].apply(count_stopwords)
    df['count_links'] = df[text].apply(count_links)  
    return df
371/135: df_feature_eng = df_train[['id', 'text', 'target']]
371/136: df_new = feature_eng_fun(df_feature_eng, 'text')
371/137: df['text']
371/138:
def feature_eng_fun(df, text):
    df['count_exclamation'] = df[text].apply(punct_add_exclamation)
    df['count_question'] = df[text].apply(punct_add_question)
    df['count_quotation'] = df[text].apply(punct_add_quotation)
    df['count_num_of_chars'] = df[text].apply(count_chars)
    df['count_num_of_words'] = df[text].apply(count_words)
    df['count_num_of_capital_chars'] = df[text].apply(count_capital_chars)
    df['count_num_of_capital_words'] = df[text].apply(count_capital_words)
    df['count_punct'] = df[text].apply(count_punctuations)
    df['count_words_in_quotes'] = df[text].apply(count_words_in_quotes)
    df['count_count_sent'] = df[text].apply(count_sent)
    df['count_unique_words'] = df[text].apply(count_unique_words)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_mentions'] = df[text].apply(count_mentions)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_stopwords'] = df[text].apply(count_stopwords)
    df['count_links'] = df[text].apply(count_links)  
    return df
371/139: df_feature_eng = df_train[['id', 'text', 'target']]
371/140: df_new = feature_eng_fun(df_feature_eng, 'text')
371/141: df.loc[:, 'text']
371/142: df_train.loc[:, 'text']
371/143:
def feature_eng_fun(df, text):
    df['count_exclamation'] = df_train.loc[:, text].apply(punct_add_exclamation)
    df['count_question'] = df[text].apply(punct_add_question)
    df['count_quotation'] = df[text].apply(punct_add_quotation)
    df['count_num_of_chars'] = df[text].apply(count_chars)
    df['count_num_of_words'] = df[text].apply(count_words)
    df['count_num_of_capital_chars'] = df[text].apply(count_capital_chars)
    df['count_num_of_capital_words'] = df[text].apply(count_capital_words)
    df['count_punct'] = df[text].apply(count_punctuations)
    df['count_words_in_quotes'] = df[text].apply(count_words_in_quotes)
    df['count_count_sent'] = df[text].apply(count_sent)
    df['count_unique_words'] = df[text].apply(count_unique_words)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_mentions'] = df[text].apply(count_mentions)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_stopwords'] = df[text].apply(count_stopwords)
    df['count_links'] = df[text].apply(count_links)  
    return df
371/144: df_feature_eng = df_train[['id', 'text', 'target']]
371/145: df_new = feature_eng_fun(df_feature_eng, 'text')
371/146: len(df_train.loc[:, 'text'])
371/147:
def feature_eng_fun(df, text):
    df['count_exclamation'] = df_train[text].assign(punct_add_exclamation)
    df['count_question'] = df[text].apply(punct_add_question)
    df['count_quotation'] = df[text].apply(punct_add_quotation)
    df['count_num_of_chars'] = df[text].apply(count_chars)
    df['count_num_of_words'] = df[text].apply(count_words)
    df['count_num_of_capital_chars'] = df[text].apply(count_capital_chars)
    df['count_num_of_capital_words'] = df[text].apply(count_capital_words)
    df['count_punct'] = df[text].apply(count_punctuations)
    df['count_words_in_quotes'] = df[text].apply(count_words_in_quotes)
    df['count_count_sent'] = df[text].apply(count_sent)
    df['count_unique_words'] = df[text].apply(count_unique_words)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_mentions'] = df[text].apply(count_mentions)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_stopwords'] = df[text].apply(count_stopwords)
    df['count_links'] = df[text].apply(count_links)  
    return df
371/148: df_feature_eng = df_train[['id', 'text', 'target']]
371/149: df_new = feature_eng_fun(df_feature_eng, 'text')
371/150:
def feature_eng_fun(df, text):
    df['count_exclamation'] = df_train[text].apply(punct_add_exclamation)
    df['count_question'] = df[text].apply(punct_add_question)
    df['count_quotation'] = df[text].apply(punct_add_quotation)
    df['count_num_of_chars'] = df[text].apply(count_chars)
    df['count_num_of_words'] = df[text].apply(count_words)
    df['count_num_of_capital_chars'] = df[text].apply(count_capital_chars)
    df['count_num_of_capital_words'] = df[text].apply(count_capital_words)
    df['count_punct'] = df[text].apply(count_punctuations)
    df['count_words_in_quotes'] = df[text].apply(count_words_in_quotes)
    df['count_count_sent'] = df[text].apply(count_sent)
    df['count_unique_words'] = df[text].apply(count_unique_words)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_mentions'] = df[text].apply(count_mentions)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_stopwords'] = df[text].apply(count_stopwords)
    df['count_links'] = df[text].apply(count_links)  
    return df
371/151: df_feature_eng = df_train[['id', 'text', 'target']]
371/152: df_new = feature_eng_fun(df_feature_eng, 'text')
371/153: df_feature_eng = df_train.loc[:, ('id', 'text', 'target')]
371/154: df_new = feature_eng_fun(df_feature_eng, 'text')
371/155: df_new
371/156: df_new.drop(['text'], inplace=True)
371/157: df_new.drop(['text'], axis=1, inplace=True)
371/158:
plt.figure(figsize=(13,10))
sns.heatmap(df_new.corr(),annot=True)
371/159: df_new.corr()
371/160: df_new.corr()['target']
371/161: sort(df_new.corr()['target'])
371/162: (df_new.corr()['target']).sort()
371/163: (df_new.corr()['target']).sort_values_va()
371/164: (df_new.corr()['target']).sort_values()
371/165: (df_new.corr()['target']).sort_values(ascending=False)
371/166: np.abc(df_new.corr()['target']).sort_values(ascending=False)
371/167: np.abs(df_new.corr()['target']).sort_values(ascending=False)
371/168: np.abs(df_new.corr()['target']).sort_values(ascending=False)[1:9]
371/169: np.abs(df_new.corr()['target']).sort_values(ascending=False)[1:9].index
371/170: cols_choose = np.abs(df_new.corr()['target']).sort_values(ascending=False)[1:9].index
371/171: df_new[cols_choose]
371/172:
plt.figure(figsize=(13,10))
sns.heatmap(df_new[cols_choose],annot=True)
371/173: dfdf = df_new[cols_choose]
371/174:
plt.figure(figsize=(13,10))
sns.heatmap(dfdf,annot=True)
371/175:
plt.figure(figsize=(13,10))
sns.heatmap(dfdf.corr(),annot=True)
371/176: cols_choose
371/177: cols_choose.append('target')
371/178: np.abs(df_new.corr()['target'])
371/179: np.abs(df_new.corr()['target']).sort_values(ascending=False)
371/180: cols_choose = np.abs(df_new.corr()['target']).sort_values(ascending=False)[0:9].index
371/181: np.abs(df_new.corr()['target']).sort_values(ascending=False)[0:9].index
371/182: cols_choose = np.abs(df_new.corr()['target']).sort_values(ascending=False)[0:10].index
371/183: dfdf = df_new[cols_choose]
371/184:
plt.figure(figsize=(13,10))
sns.heatmap(dfdf.corr(),annot=True)
371/185: cols_choose = np.abs(df_new.corr()['target']).sort_values(ascending=False)[0:11].index
371/186: np.abs(df_new.corr()['target']).sort_values(ascending=False)[0:9].index
371/187: dfdf = df_new[cols_choose]
371/188:
plt.figure(figsize=(13,10))
sns.heatmap(dfdf.corr(),annot=True)
371/189: np.abs(df_new.corr()['target']).sort_values(ascending=False)
371/190: cols_choose = np.abs(df_new.corr()['target']).sort_values(ascending=False)[0:10].index
371/191: np.abs(df_new.corr()['target']).sort_values(ascending=False)
371/192: cols_choose
371/193: cols_choose = np.abs(df_new.corr()['target']).sort_values(ascending=False)[0:10].index.to_list()
371/194: cols_choose
371/195: cols_choose = np.abs(df_new.corr()['target']).sort_values(ascending=False)[1:10].index.to_list()
371/196: cols_choose
371/197:
cols_selected = np.abs(df_new.corr()['target']).sort_values(ascending=False)[1:11].index.to_list()
cols_selected.append('id')
371/198: cols_selected
371/199:
# Let's look at some tweets more carefully
df_train.text
371/200:
# Let's look at some tweets more carefully
df_train.text[55:60]
371/201:
# Let's look at some tweets more carefully
df_train[55:60]
371/202:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
371/203:
# Let's look at some tweets more carefully
df_train[55:60]
371/204: df_train
371/205: df_train.location.nunique()
371/206: df_train.location.value_counts()
371/207:
# Write a function to display basic entity info:
def show_ents(doc):
    doc = nlp(doc)
    count_GPE = 0
    if doc.ents:
        for ent in doc.ents:
            if ent.label_ == 'GPE':
                #print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
                count_GPE = 1
                break
    return count_GPE
371/208: df_train['GPE_loc_flag'] = df_train.location.apply(show_ents)
371/209: df_train.location.fillna('no_location')
371/210: df_train.location.fillna('no_location', inplace=True)
371/211:
# Write a function to display basic entity info:
def show_ents(doc):
    doc = nlp(doc)
    count_GPE = 0
    if doc.ents:
        for ent in doc.ents:
            if ent.label_ == 'GPE':
                #print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
                count_GPE = 1
                break
    return count_GPE
371/212: df_train['GPE_loc_flag'] = df_train.location.apply(show_ents)
371/213: df_train
371/214: df_train[df_train.GPE_loc_flag == 1]
371/215: df_train['GPE_text_flag'] = df_train.text.apply(show_ents)
371/216: df_train
371/217: df_train['GPE_flag'] = df_train.GPE_loc_flag or df_train.GPE_text_flag
371/218: df_train['GPE_flag'] = df_train.GPE_loc_flag || df_train.GPE_text_flag
371/219: df_train['GPE_flag'] = df_train.GPE_loc_flag | df_train.GPE_text_flag
371/220: df_train
371/221:
plt.figure(figsize=[3, 4])
sns.countplot(x='GPE_loc_flag', hue='target', data=df_train)
plt.title('GPE Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['0', '1'])
plt.show()
371/222:
plt.figure(figsize=[4, 4])
sns.countplot(x='GPE_loc_flag', hue='target', data=df_train)
plt.title('GPE Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['0', '1'])
plt.show()
371/223:
plt.figure(figsize=[4, 4])
f, axes = plt.subplots(1, 2)
sns.countplot(x='GPE_loc_flag', hue='target', data=df_train, ax=axes[0])
plt.title('GPE Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['0', '1'])
plt.show()
371/224:
plt.figure(figsize=[4, 4])
f, axes = plt.subplots(1, 3)
sns.countplot(x='GPE_loc_flag', hue='target', data=df_train, ax=axes[0])
sns.countplot(x='GPE_text_flag', hue='target', data=df_train, ax=axes[0])
sns.countplot(x='GPE_flag', hue='target', data=df_train, ax=axes[0])
plt.title('GPE Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['0', '1'])
plt.show()
371/225:
plt.figure(figsize=[7, 4])
f, axes = plt.subplots(1, 3)
sns.countplot(x='GPE_loc_flag', hue='target', data=df_train, ax=axes[0])
sns.countplot(x='GPE_text_flag', hue='target', data=df_train, ax=axes[0])
sns.countplot(x='GPE_flag', hue='target', data=df_train, ax=axes[0])
plt.title('GPE Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['0', '1'])
plt.show()
371/226:
plt.figure(figsize=[4, 7])
f, axes = plt.subplots(1, 3)
sns.countplot(x='GPE_loc_flag', hue='target', data=df_train, ax=axes[0])
sns.countplot(x='GPE_text_flag', hue='target', data=df_train, ax=axes[0])
sns.countplot(x='GPE_flag', hue='target', data=df_train, ax=axes[0])
plt.title('GPE Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['0', '1'])
plt.show()
371/227:
plt.figure(figsize=[7, 7])
f, axes = plt.subplots(1, 3)
sns.countplot(x='GPE_loc_flag', hue='target', data=df_train, ax=axes[0])
sns.countplot(x='GPE_text_flag', hue='target', data=df_train, ax=axes[0])
sns.countplot(x='GPE_flag', hue='target', data=df_train, ax=axes[0])
plt.title('GPE Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['0', '1'])
plt.show()
371/228:
f, axes = plt.subplots(1, 3)
plt.figure(figsize=[7, 7])
sns.countplot(x='GPE_loc_flag', hue='target', data=df_train, ax=axes[0])
sns.countplot(x='GPE_text_flag', hue='target', data=df_train, ax=axes[0])
sns.countplot(x='GPE_flag', hue='target', data=df_train, ax=axes[0])
plt.title('GPE Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['0', '1'])
plt.show()
371/229:
f, axes = plt.subplots(1, 3)
axes.figure(figsize=[7, 7])
sns.countplot(x='GPE_loc_flag', hue='target', data=df_train, ax=axes[0])
sns.countplot(x='GPE_text_flag', hue='target', data=df_train, ax=axes[0])
sns.countplot(x='GPE_flag', hue='target', data=df_train, ax=axes[0])
plt.title('GPE Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['0', '1'])
plt.show()
371/230:
f, axes = plt.subplots(1, 3, figsize=(20,8))
sns.countplot(x='GPE_loc_flag', hue='target', data=df_train, ax=axes[0])
sns.countplot(x='GPE_text_flag', hue='target', data=df_train, ax=axes[0])
sns.countplot(x='GPE_flag', hue='target', data=df_train, ax=axes[0])
plt.title('GPE Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['0', '1'])
plt.show()
371/231:
f, axes = plt.subplots(1, 3, figsize=(20,8))
sns.countplot(x='GPE_loc_flag', hue='target', data=df_train, ax=axes[0])
sns.countplot(x='GPE_text_flag', hue='target', data=df_train, ax=axes[1])
sns.countplot(x='GPE_flag', hue='target', data=df_train, ax=axes[2])
plt.title('GPE Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['0', '1'])
plt.show()
371/232:
f, axes = plt.subplots(1, 3, figsize=(20,8))
sns.countplot(x='GPE_loc_flag', hue='target', data=df_train, ax=axes[0])
sns.countplot(x='GPE_text_flag', hue='target', data=df_train, ax=axes[1])
sns.countplot(x='GPE_flag', hue='target', data=df_train, ax=axes[2])
ax[0].set_title('GPE Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['0', '1'])
plt.show()
371/233:
f, axes = plt.subplots(1, 3, figsize=(20,8))
sns.countplot(x='GPE_loc_flag', hue='target', data=df_train, ax=axes[0])
sns.countplot(x='GPE_text_flag', hue='target', data=df_train, ax=axes[1])
sns.countplot(x='GPE_flag', hue='target', data=df_train, ax=axes[2])
axes[0].set_title('GPE Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['0', '1'])
plt.show()
371/234:
f, axes = plt.subplots(1, 3, figsize=(20,8))
sns.countplot(x='GPE_loc_flag', hue='target', data=df_train, ax=axes[0])
sns.countplot(x='GPE_text_flag', hue='target', data=df_train, ax=axes[1])
sns.countplot(x='GPE_flag', hue='target', data=df_train, ax=axes[2])
axes[0].set_title('GPE Analysis')
axes[1].set_title('GPE Analysis')
axes[2].set_title('GPE Analysis')

plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['0', '1'])
plt.show()
371/235:
f, axes = plt.subplots(1, 3, figsize=(20,8))
sns.countplot(x='GPE_loc_flag', hue='target', data=df_train, ax=axes[0])
sns.countplot(x='GPE_text_flag', hue='target', data=df_train, ax=axes[1])
sns.countplot(x='GPE_flag', hue='target', data=df_train, ax=axes[2])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')
axes[2].set_title('GPE')

plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['0', '1'])
plt.show()
371/236:
f, axes = plt.subplots(1, 3, figsize=(20,6))
sns.countplot(x='GPE_loc_flag', hue='target', data=df_train, ax=axes[0])
sns.countplot(x='GPE_text_flag', hue='target', data=df_train, ax=axes[1])
sns.countplot(x='GPE_flag', hue='target', data=df_train, ax=axes[2])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')
axes[2].set_title('GPE')

plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['0', '1'])
plt.show()
371/237:
f, axes = plt.subplots(1, 3, figsize=(20,6))
sns.countplot(x='GPE_loc_flag', hue='target', data=df_train, ax=axes[0])
sns.countplot(x='GPE_text_flag', hue='target', data=df_train, ax=axes[1])
sns.countplot(x='GPE_flag', hue='target', data=df_train, ax=axes[2])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')
axes[2].set_title('GPE')

#plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.legend(labels=['0', '1'])
plt.show()
371/238:
f, axes = plt.subplots(1, 3, figsize=(20,6))
sns.countplot(x='GPE_loc_flag', hue='target', data=df_train, ax=axes[0])
sns.countplot(x='GPE_text_flag', hue='target', data=df_train, ax=axes[1])
sns.countplot(x='GPE_flag', hue='target', data=df_train, ax=axes[2])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')
axes[2].set_title('GPE')

#plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.legend(labels=['tt', '1'])
plt.show()
371/239:
f, axes = plt.subplots(1, 3, figsize=(20,6))
sns.countplot(x='GPE_loc_flag', hue='target', data=df_train, ax=axes[0])
sns.countplot(x='GPE_text_flag', hue='target', data=df_train, ax=axes[1])
sns.countplot(x='GPE_flag', hue='target', data=df_train, ax=axes[2])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')
axes[2].set_title('GPE')

#plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
axes[1].legend(labels=['tt', '1'])
plt.show()
371/240:
f, axes = plt.subplots(1, 3, figsize=(20,6))
sns.countplot(x='GPE_loc_flag', hue='target', data=df_train, ax=axes[0])
sns.countplot(x='GPE_text_flag', hue='target', data=df_train, ax=axes[1])
sns.countplot(x='GPE_flag', hue='target', data=df_train, ax=axes[2])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')
axes[2].set_title('GPE')

ax[0].set_xlabel("Keyword Frequency")
#plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
axes[0].legend(labels=['0', '1'])
axes[1].legend(labels=['0', '1'])
axes[2].legend(labels=['0', '1'])

plt.show()
371/241:
f, axes = plt.subplots(1, 3, figsize=(20,6))
sns.countplot(x='GPE_loc_flag', hue='target', data=df_train, ax=axes[0])
sns.countplot(x='GPE_text_flag', hue='target', data=df_train, ax=axes[1])
sns.countplot(x='GPE_flag', hue='target', data=df_train, ax=axes[2])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')
axes[2].set_title('GPE')

axes[0].set_xlabel("Keyword Frequency")
#plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
axes[0].legend(labels=['0', '1'])
axes[1].legend(labels=['0', '1'])
axes[2].legend(labels=['0', '1'])

plt.show()
371/242: df_train
371/243: df_train.iloc[:, -4:]
371/244: df_train.iloc[:, -4:].corr()
371/245:
# Looks like the location extracted from the tweet will be more informative than the original location feature.
df_train.iloc[:, -4:].corr()
371/246:
def GPE_counter(text):
    nlp_text = nlp(text)
    for words in nlp_text:
        if words.ent.label_ == 'LOC' | 'LOC':
            print(words)
371/247:
# Write a function to display basic entity info:
def show_ents(doc):
    doc = nlp(doc)
    count_GPE = 0
    if doc.ents:
        for ent in doc.ents:
            if ent.label_ == 'GPE':
                #print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
                count_GPE = 1
                break
    return count_GPE
371/248: df_train['GPE_loc_flag'] = df_train.location.apply(show_ents)
371/249:
# Write a function to display basic entity info:
def show_ents(doc):
    doc = nlp(doc)
    count_GPE = 0
    if doc.ents:
        for ent in doc.ents:
            if ent.label_ == 'GPE' | 'LOC':
                #print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
                count_GPE = 1
                break
    return count_GPE
371/250: df_train['GPE_loc_flag'] = df_train.location.apply(show_ents)
371/251:
# Write a function to display basic entity info:
def show_ents(doc):
    doc = nlp(doc)
    count_GPE = 0
    if doc.ents:
        for ent in doc.ents:
            if ent.label_ == 'GPE':
                #print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
                count_GPE = 1
                break
    return count_GPE
371/252: df_train['GPE_loc_flag'] = df_train.location.apply(show_ents)
371/253: url_pattern = r'(www.|http[s]?://)(.+?)+'
371/254:
url_pattern = r'(www.|http[s]?://)(.+?)+'
re.findall(url_pattern, '#Kurds trampling on Turkmen flag later set it ablaze while others vandalized offices of Turkmen Front in #Diyala http://t.co/4IzFdYC3cg')
371/255:
url_pattern = r'https?:\/\/.*[\r\n]*'
re.findall(url_pattern, '#Kurds trampling on Turkmen flag later set it ablaze while others vandalized offices of Turkmen Front in #Diyala http://t.co/4IzFdYC3cg')
371/256:
url_pattern = r'(www.|http[s]?://)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
test_string = 'I am at http://t.co/GKYe6gjTk5 okay'
test_op = re.sub(url_pattern, '', test_string)
test_op
371/257:
url_pattern = r'https?:\/\/.*[\r\n]*'
re.findall(url_pattern, '#Kurds trampling on Turkmen flag later set it ablaze while others vandalized offices of Turkmen Front in #Diyala http://t.co/4IzFdYC3cg fffffffff')
371/258:
url_pattern = r'https?://\S+*'
re.findall(url_pattern, '#Kurds trampling on Turkmen flag later set it ablaze while others vandalized offices of Turkmen Front in #Diyala http://t.co/4IzFdYC3cg fffffffff')
371/259:
url_pattern = r'https?://\S+'
re.findall(url_pattern, '#Kurds trampling on Turkmen flag later set it ablaze while others vandalized offices of Turkmen Front in #Diyala http://t.co/4IzFdYC3cg fffffffff')
371/260:
url_pattern = r'https?://\S+'
re.findall(url_pattern, '#Kurds trahttp://t.co/4IzFdYC3cg mpling on Turkmen flag later set it ablaze while others vandalized offices of Turkmen Front in #Diyala http://t.co/4IzFdYC3cg fffffffff')
371/261:
#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+', '', text)
    return text

#remove # and @word 
def remove_social_media_tags(text):
    text = re.sub(r'@([a-z0-9]+)|#', '', text)
    return text

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()
371/262:
def basic_cleaning(df, text):
    df['text'] = df['text'].apply(remove_urls)
    df['text'] = df['text'].apply(remove_social_media_tags)
    df['text'] = df['text'].apply(clean_text)
    df['text'] = df['text'].apply(convert_lower_case)
371/263: df_text = df_train.loc[:, 'text']
371/264: df_text
371/265: df_text.dtype
371/266: df_text = df_train.loc[:, ['id', 'text', 'target']
371/267: df_text = df_train.loc[:, ['id', 'text', 'target']]
371/268: df_text.dtype
371/269: df_text
371/270: df_text = df_train.loc[:, ['id', 'target', 'text']]
371/271: df_text
371/272: basic_cleaning(df_text, 'text')
371/273: basic_cleaning
371/274:
def basic_cleaning(df, text):
    df['text'] = df['text'].apply(remove_urls)
    df['text'] = df['text'].apply(remove_social_media_tags)
    df['text'] = df['text'].apply(clean_text)
    df['text'] = df['text'].apply(convert_lower_case)
    return df
371/275: df_text = df_train.loc[:, ['id', 'target', 'text']]
371/276: basic_cleaning(df_text, 'text')
371/277: basic_cleaning
371/278: basic_cleaning = basic_cleaning(df_text, 'text')
371/279: basic_cleaning
371/280: 7610
371/281: df_train
371/282: sum(ch.isdigit() for ch in 'jdj 4 kfg 33')
371/283: re.findall("[^0-9]", '8484 fjfjf 4')
371/284: re.findall("[0-9]+", '8484 fjfjf 4')
371/285: re.findall("[0-9]+", '8484 fjfjf 4.4 ffgt')
371/286: re.findall(r"[0-9]+", '8484 fjfjf 4.4 ffgt')
371/287:
# count number of characters 
def count_chars(text):
    return len(text)

# count number of words 
def count_words(text):
    return len(text.split())

# count number of capital characters
def count_capital_chars(text):
    count=0
    for i in text:
        if i.isupper():
            count+=1
    return count

# count number of capital words
def count_capital_words(text):
    return sum(map(str.isupper,text.split()))

# count number of punctuations
def count_punctuations(text):
    count = lambda l1,l2: sum([1 for x in l1 if x in l2])
    return count(text,set(string.punctuation))

# count number of words in quotes
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    string_wothout_contractions = string_wothout_contractions.replace('"', '\'')
    x = re.findall("\'(.+?)\'", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
    
# count number of sentences
def count_sent(text):
    return len(nltk.sent_tokenize(text))

# count number of unique words 
def count_unique_words(text):
    return len(set(text.split()))
    
# count of hashtags
def count_htags(text):
    x = re.findall(r'(\#\w[A-Za-z0-9]*)', text)
    return len(x)

# count of mentions
def count_mentions(text):
    x = re.findall(r'(\@\w[A-Za-z0-9]*)', text)
    return len(x)

# count of stopwords
def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)

#count of links
def count_links(text):
    link_list = re.findall(r'(https?://[^\s]+)', text)
    bitly_list = re.findall(r'bit.ly/\S+', text)
    return (len(link_list + bitly_list))

#count digits
def count_digits(text):
    digit_list = re.findall(r'0-9]+', text)
    return len(digit_list)
371/288: string.punctuation
371/289: re.sub(string.punctuation, ' ', 'jjf.rjj') # strip punctuation
371/290: string.punctuation
371/291: re.sub("'!\"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'", ' ', 'jjf.rjj') # strip punctuation
371/292: string.punctuation.to_list()
371/293: string.punctuation.to_list
371/294: list(string.punctuation)
371/295: punct_l = list(string.punctuation)
371/296: re.sub(punct_l, ' ', 'jjf.rjj') # strip punctuation
371/297: re.sub('[.,!]'', ' ', 'jjf.rjj') # strip punctuation
371/298: re.sub('[.,!], ' ', 'jjf.rjj') # strip punctuation
371/299: re.sub('[.,!]', ' ', 'jjf.rjj') # strip punctuation
371/300: re.sub('[' + string.punctuation +']', ' ', 'jjf.rjj') # strip punctuation
371/301: re.sub('[' + string.punctuation +']', ' ', 'jjf.r!%%...jj') # strip punctuation
371/302:
#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+', '', text)
    return text

#remove @word 
def remove_social_media_tags(text):
    text = re.sub(r'@([a-z0-9]+)', '', text)
    return text

def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

def remove_digits(text):
    text = re.sub(r'[0-9]+', '', text)
    return text   

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()
371/303:
def basic_cleaning(df, text):
    df['text'] = df['text'].apply(remove_urls)
    df['text'] = df['text'].apply(remove_social_media_tags)
    df['text'] = df['text'].apply(remove_punct)
    df['text'] = df['text'].apply(remove_digits)
    df['text'] = df['text'].apply(clean_text)
    df['text'] = df['text'].apply(convert_lower_case)
    return df
371/304: df_text = df_train.loc[:, ['id', 'target', 'text']]
371/305: df_text = basic_cleaning(df_text, 'text')
371/306: basic_cleaning
371/307: df_train
371/308: df_text
371/309:
#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+', '', text)
    return text

#remove @word 
def remove_social_media_tags(text):
    text = re.sub(r'@\S+', '', text)
    return text

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

#remove digits
def remove_digits(text):
    text = re.sub(r'[0-9]+', '', text)
    return text   

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()
371/310: punct_l = list(string.punctuation)
371/311: re.sub('[' + string.punctuation +']', ' ', 'jjf.r!%%...jj') # strip punctuation
371/312: string.punctuation
371/313:
def basic_cleaning(df, text):
    df['text'] = df['text'].apply(remove_urls)
    df['text'] = df['text'].apply(remove_social_media_tags)
    df['text'] = df['text'].apply(remove_punct)
    df['text'] = df['text'].apply(remove_digits)
    df['text'] = df['text'].apply(clean_text)
    df['text'] = df['text'].apply(convert_lower_case)
    return df
371/314: df_text = df_train.loc[:, ['id', 'target', 'text']]
371/315: df_text = basic_cleaning(df_text, 'text')
371/316: df_text
371/317: df_train
371/318:
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
371/319: df_text = df_train.loc[:, ['id', 'target', 'text']]
371/320: df_text.head(3)
371/321: df_text.text.apply(string_contractions)
371/322: df_text = df_text.text.apply(string_contractions)
371/323: df_text.head(3)
371/324: df_text = df_train.loc[:, ['id', 'target', 'text']]
371/325: df_text.head(3)
371/326: df_text['text'] = df_text.text.apply(string_contractions)
371/327: df_text.head(3)
371/328: df_text = df_train.loc[:, ['id', 'target', 'text']]
371/329: df_text.head(5)
371/330: df_text.head(10)
371/331:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
371/332: df_text = df_train.loc[:, ['id', 'target', 'text']]
371/333: df_text.head(10)
371/334: df_text['text'] = df_text.text.apply(string_contractions)
371/335: df_text.head(10)
371/336:
def basic_cleaning(df, text):
    df[text] = df[text].apply(remove_urls)
    df[text] = df[text].apply(remove_social_media_tags)
    df[text] = df[text]apply(string_contractions)    
    df[text] = df[text].apply(remove_punct)
    df[text] = df[text].apply(remove_digits)
    df[text] = df[text].apply(clean_text)
    df[text] = df[text].apply(convert_lower_case)
    return df
371/337:
def basic_cleaning(df, text):
    df[text] = df[text].apply(remove_urls)
    df[text] = df[text].apply(remove_social_media_tags)
    df[text] = df[text].apply(string_contractions)    
    df[text] = df[text].apply(remove_punct)
    df[text] = df[text].apply(remove_digits)
    df[text] = df[text].apply(clean_text)
    df[text] = df[text].apply(convert_lower_case)
    return df
371/338: df_text = df_train.loc[:, ['id', 'target', 'text']]
371/339: df_text = basic_cleaning(df_text, 'text')
371/340: df_text
371/341:
def basic_cleaning(df, column_name):
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_social_media_tags)
    df[column_name] = df[column_name].apply(string_contractions)    
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(remove_digits)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(convert_lower_case)
    return df
371/342: df_text = df_train.loc[:, ['id', 'target', 'text']]
371/343: df_text = basic_cleaning(df_text, 'text')
371/344: df_text
371/345: df_train.text.apply(search_oov)
371/346:
def search_oov(text):
    tokens = nlp(text)
    for token in tokens:
        if token.is_oov:
            print(token)
371/347: df_text.text.apply(search_oov)
371/348: df_text.text.str.contains('don')
371/349: df_text[df_text.text.str.contains('don')]
371/350: df_text[df_text.text.str.contains('don')][50:70]
371/351: df_train[df_train.id == 2420]
371/352: df_train[df_train.id == 2183]
371/353: df_text[df_text.text.str.contains('¬â√õ¬™')][50:70]
371/354: df_text[df_text.text.str.contains('√õ¬™')][50:70]
371/355:
def str_to_unicode(text):
    byte_str = ''.join(r'\u{:04X}'.format(ord(chr)) for chr in text)
    emojis_l = emoji_pattern.findall(byte_str)
    print(byte_str)
371/356: df_train[df_train.id == 2183].text
371/357: df_train[df_train.id == 2183].text.apply(str_to_unicode)
371/358:
def str_to_unicode(text):
    byte_str = ''.join(r'\u{:04X}'.format(ord(chr)) for chr in text)
  #  emojis_l = emoji_pattern.findall(byte_str)
    print(byte_str)
371/359: df_train[df_train.id == 2183].text.apply(str_to_unicode)
371/360: df_text(df_text.text.str.contains('hazard'))
371/361: df_train(df_train.text.str.contains('hazard'))
371/362: df_train
371/363: df_train[df_train.text.str.contains('hazard')]
371/364: '/–π—Ü–±–π—Ü/–±–π—Ü–±—Ü'.decode('utf-8')
371/365: u'/–π—Ü–±–π—Ü/–±–π—Ü–±—Ü'.decode('utf-8')
371/366: b'\x48\x69'.decode('ASCII')
371/367: u'\u0050'.decode('ASCII')
371/368: u'\0050'.decode('ASCII')
371/369: u'\x48\x69'.decode('ASCII')
371/370: b'\x48\x69'.decode('ASCII')
371/371:
def str_to_unicode(text):
    byte_str = ''.join(r'\u{:04X}'.format(ord(chr)) for chr in text)
    byte_str = str.encode(my_str)
  #  emojis_l = emoji_pattern.findall(byte_str)
    print(byte_str)
371/372:
def str_to_unicode(text):
    byte_str = ''.join(r'\u{:04X}'.format(ord(chr)) for chr in text)
    byte_str = str.encode(text)
  #  emojis_l = emoji_pattern.findall(byte_str)
    print(byte_str)
371/373: df_train[df_train.id == 2183].text.apply(str_to_unicode)
371/374:
data = '\xc2\x89\xc3\x9b\xc2\xaa'
data.decode("utf-8")
371/375:
data = "\xc3\x99\xc3\x99\xc3\xa9\xc2\x87-B[x\xc2\x99\xc2\xbe\xc3\xa6\x14Ez\xc2\xab"
udata = data.decode("utf-8")
371/376:
data = "\xc3\x99\xc3\x99\xc3\xa9\xc2\x87-B[x\xc2\x99\xc2\xbe\xc3\xa6\x14Ez\xc2\xab"
udata = data.decode("ascii")
371/377:
my_str = "hello world"
my_str_as_bytes = str.encode(my_str)
print(type(my_str_as_bytes)) # ensure it is byte representation
my_decoded_str = my_str_as_bytes.decode()
print(type(my_decoded_str)) # ensure it is string representation
371/378:
data = b'\xc2\x89\xc3\x9b\xc2\xaa'
data.decode("utf-8")
371/379:
data = b'\xc2\x89\xc3\x9b\xc2\xaa'
data.decode("ascii")
371/380:
data = b'\xc2\x89\xc3\x9b\xc2\xaa'
data.decode("utf-16")
371/381: df_train[df_train.id == 5937].text.apply(str_to_unicode)
371/382:
data = b'\xc2\x89\xc3\x9b\xc3\xb7'
data.decode("utf-16")
371/383:
data = b'\xc2\x89\xc3\x9b\xc2\xaa'
data.decode("utf-16")
371/384:
data = b'\xc2\x89\xc3\x9b\xc3\xb7'
data.decode("utf-16")
371/385:
data = b'\xc2\x89\xc3\x9b\xc2\xaa'
data.decode("utf-16")
371/386:
df_train = pd.read_csv("../data/train.csv", encoding = 'UTF-16'")
df_test = pd.read_csv("../data/test.csv")
371/387:
df_train = pd.read_csv("../data/train.csv", encoding = 'UTF-16')
df_test = pd.read_csv("../data/test.csv")
371/388:
df_train = pd.read_csv("../data/train.csv", encoding = 'utf-16-le')
df_test = pd.read_csv("../data/test.csv")
371/389:
nltk.download()
#nltk.download('stopwords')
371/390: STOPWORDS = set(stopwords.words('english'))
371/391:
#import nltk
#nltk.download('wordnet')
371/392:
#import nltk
#dler = nltk.downloader.Downloader()
#dler._update_index()
#dler.download('all')
371/393: wnl = WordNetLemmatizer()
371/394: df_train.head()
371/395:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
371/396:
nltk.download()
#nltk.download('stopwords')
371/397: STOPWORDS = set(stopwords.words('english'))
371/398:
#import nltk
#nltk.download('wordnet')
371/399:
#import nltk
#dler = nltk.downloader.Downloader()
#dler._update_index()
#dler.download('all')
371/400: wnl = WordNetLemmatizer()
371/401: df_train.head()
371/402:
data = b'\xc2\x89\xc3\x9b\xc2\xaa'
data.decode("ascii")
371/403:
data = b'\xc2\x89\xc3\x9b\xc2\xaa'
data.decode("cp65001")
371/404:
data = b'\xc2\x89\xc3\x9b\xc2\xaa'
data.decode("idna")
371/405:
data = b'\xc2\x89\xc3\x9b\xc2\xaa'
data.decode("utf_32")
371/406:
data = b'\xc2\x89\xc3\x9b\xc2\xaa'
data.decode("utf_32-le")
371/407:
data = b'\xc2\x89\xc3\x9b\xc2\xaa'
data.decode("cp1253")
371/408:
def basic_cleaning(df, column_name):
    df[column_name] = df[column_name].apply(delete_non_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_social_media_tags)
    df[column_name] = df[column_name].apply(string_contractions)    
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(remove_digits)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(convert_lower_case)
    return df
371/409: df_text = df_train.loc[:, ['id', 'target', 'text']]
371/410: df_text = basic_cleaning(df_text, 'text')
371/411:
#remove urls
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', r'', text)
    return text 

def remove_urls(text):
    text = re.sub(r'https?://\S+', '', text)
    return text

#remove @word 
def remove_social_media_tags(text):
    text = re.sub(r'@\S+', '', text)
    return text

def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

#remove digits
def remove_digits(text):
    text = re.sub(r'[0-9]+', '', text)
    return text   

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()
371/412:
def basic_cleaning(df, column_name):
    df[column_name] = df[column_name].apply(delete_non_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_social_media_tags)
    df[column_name] = df[column_name].apply(string_contractions)    
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(remove_digits)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(convert_lower_case)
    return df
371/413: df_text = df_train.loc[:, ['id', 'target', 'text']]
371/414: df_text = basic_cleaning(df_text, 'text')
371/415:
#remove urls
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

def remove_urls(text):
    text = re.sub(r'https?://\S+', '', text)
    return text

#remove @word 
def remove_social_media_tags(text):
    text = re.sub(r'@\S+', '', text)
    return text

def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

#remove digits
def remove_digits(text):
    text = re.sub(r'[0-9]+', '', text)
    return text   

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()
371/416:
def basic_cleaning(df, column_name):
    df[column_name] = df[column_name].apply(delete_non_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_social_media_tags)
    df[column_name] = df[column_name].apply(string_contractions)    
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(remove_digits)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(convert_lower_case)
    return df
371/417: df_text = df_train.loc[:, ['id', 'target', 'text']]
371/418: df_text = basic_cleaning(df_text, 'text')
371/419:
def basic_cleaning(df, column_name):
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_social_media_tags)
    df[column_name] = df[column_name].apply(string_contractions)    
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(remove_digits)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(convert_lower_case)
    return df
371/420: df_text = df_train.loc[:, ['id', 'target', 'text']]
371/421: df_text = basic_cleaning(df_text, 'text')
371/422: df_text
371/423: df_train
371/424: show_ents('la ronge sask')
371/425:
# Write a function to display basic entity info:
def show_ents(doc):
    doc = nlp(doc)
    count_GPE = 0
    if doc.ents:
        for ent in doc.ents:
            if ent.label_ == 'LOC':
                #print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
                count_GPE = 1
                break
    return count_GPE
371/426: show_ents('la ronge sask')
371/427:
# Write a function to display basic entity info:
def show_ents(doc):
    doc = nlp(doc)
    count_GPE = 0
    if doc.ents:
        for ent in doc.ents:
            if ent.label_ == 'GPE':
                #print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
                count_GPE = 1
                break
    return count_GPE
371/428:
def search_oov(text):
    tokens = nlp(text)
    for token in tokens:
        if token.is_oov:
            print(token)
371/429: df_text.text.apply(search_oov)
371/430: df_train.text.str.contains('kisii')
371/431: df_train[df_train.text.str.contains('kisii')]
371/432: df_train[df_train.text.str.contains('mediterran')]
371/433: df_train[df_train.text.str.contains('Mediterran')]
371/434: df_train[df_train.text.str.contains('Kisii')]
371/435: 'Mediterran'.apply(show_ents)
371/436: show_ents('Mediterran')
371/437: show_ents('Kisii')
371/438:
# Write a function to display basic entity info:
def show_ents(doc):
    doc = nlp(doc)
    count_GPE = 0
    if doc.ents:
        for ent in doc.ents:
            if ent.label_ == 'LOC':
                #print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
                count_GPE = 1
                break
    return count_GPE
371/439: show_ents('Kisii')
371/440: show_ents('Mediterran')
371/441: nlp('Mediterran')
371/442: nlp('Mediterran').label_
371/443: nlp('Mediterran')ents.label_
371/444: nlp('Mediterran').ents.label_
371/445: nlp('Mediterran').ents
371/446: nlp('Mediterran s').ents
371/447: nlp('I was in Mediterran s').ents
371/448: nlp('I was in Mediterran s')
371/449: nlp('I was in Mediterran s').ents
371/450: nlp('I was in Mediterran s').ent
371/451: nlp('I was in Mediterran s').ents
371/452: nlp('I was in Mediterran s').ents.label_
371/453:
def show_ents(doc):
    if doc.ents:
        for ent in doc.ents:
            print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
    else:
        print('No named entities found.')
371/454:
doc = nlp(u'I was in Mediterran s')

show_ents(doc)
373/1:
# Perform standard imports
import spacy
nlp = spacy.load('en_core_web_sm')
373/2:
# Write a function to display basic entity info:
def show_ents(doc):
    if doc.ents:
        for ent in doc.ents:
            print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
    else:
        print('No named entities found.')
373/3:
doc = nlp(u'May I go to Washington, DC next May to see the Washington Monument?')

show_ents(doc)
373/4:
doc = nlp(u'Can I please borrow 500 dollars from you to buy some Microsoft stock?')

for ent in doc.ents:
    print(ent.text, ent.start, ent.end, ent.start_char, ent.end_char, ent.label_)
373/5:
doc = nlp(u'Can I please borrow 500 dollars from you to buy some Microsoft stock? kiiki')

for ent in doc.ents:
    print(ent.text, ent.start, ent.end, ent.start_char, ent.end_char, ent.label_)
373/6:
doc = nlp(u'Can I please borrow 500 dollars from you to buy some Microsoft stock? pitmix')

for ent in doc.ents:
    print(ent.text, ent.start, ent.end, ent.start_char, ent.end_char, ent.label_)
373/7:
doc = nlp(u'May I go to Washington, DC next May to see the Washington Monument? mediterran')

show_ents(doc)
373/8:
doc = nlp(u'May I go to Washington, DC next May to see the Washington Monument? Mediterran')

show_ents(doc)
371/455: df_train[df_train.text.str.contains('gameofkittens')]
371/456:
def search_oov(text):
    tokens = nlp(text)
    d=dict()
    for token in tokens:
        if token.is_oov:
            d[token]+=1
            print(token)
    return d
371/457: d = df_text.text.apply(search_oov)
371/458:
def search_oov(text):
    tokens = nlp(text)
    d=dict()
    for token in tokens:
        if token.is_oov:
            d[str(token)]+=1
            print(token)
    return d
371/459: d = df_text.text.apply(search_oov)
371/460:
def search_oov(text):
    tokens = nlp(text)
    d=dict{}
    for token in tokens:
        if token.is_oov:
            d[str(token)]+=1
            print(token)
    return d
371/461:
def search_oov(text):
    tokens = nlp(text)
    d=dict()
    for token in tokens:
        if token.is_oov:
            d[str(token)]=d[str(token)] + 1
            print(token)
    return d
371/462:
def str_to_unicode(text):
    byte_str = ''.join(r'\u{:04X}'.format(ord(chr)) for chr in text)
    byte_str = str.encode(text)
  #  emojis_l = emoji_pattern.findall(byte_str)
    print(byte_str)
371/463: d = df_text.text.apply(search_oov)
371/464:
def search_oov(text):
    tokens = nlp(text)
    d=dict()
    for token in tokens:
        if token.is_oov:
            d[str(token)]=1
            print(token)
    return d
371/465:
def str_to_unicode(text):
    byte_str = ''.join(r'\u{:04X}'.format(ord(chr)) for chr in text)
    byte_str = str.encode(text)
  #  emojis_l = emoji_pattern.findall(byte_str)
    print(byte_str)
371/466: d = df_text.text.apply(search_oov)
371/467:
def search_oov(text):
    tokens = nlp(text)
    d=dict()
    for token in tokens:
        if token.is_oov:
            d[str(token)]=d[str(token)].value + 1
            print(token)
    return d
371/468:
def str_to_unicode(text):
    byte_str = ''.join(r'\u{:04X}'.format(ord(chr)) for chr in text)
    byte_str = str.encode(text)
  #  emojis_l = emoji_pattern.findall(byte_str)
    print(byte_str)
371/469: d = df_text.text.apply(search_oov)
371/470:
def search_oov(text):
    tokens = nlp(text)
    d=dict()
    for token in tokens:
        if token.is_oov:
            d[str(token)]= d.get(str(token), 0) + 1
            print(token)
    return d
371/471:
def str_to_unicode(text):
    byte_str = ''.join(r'\u{:04X}'.format(ord(chr)) for chr in text)
    byte_str = str.encode(text)
  #  emojis_l = emoji_pattern.findall(byte_str)
    print(byte_str)
371/472: d = df_text.text.apply(search_oov)
371/473: d
371/474: d.sort_values
371/475: d.sort_values()
371/476: d.keys
371/477:
def search_oov(text):
    tokens = nlp(text)
    d=dict()
    for token in tokens:
        if token.is_oov:
            d[str(token)]= d.get(str(token), 0) + 1
            print(token)
    return d
371/478: d_oov = df_text.text.apply(search_oov)
371/479: d_oov.keys
371/480:
def search_oov(text):
    tokens = nlp(text)
    d=dict()
    for token in tokens:
        if token.is_oov:
            d[str(token)]=d.get(str(token) + 1
            print(token)
    return d
371/481:
def search_oov(text):
    tokens = nlp(text)
    d=dict()
    for token in tokens:
        if token.is_oov:
            d[str(token)]=d.get(str(token) + 1
            print(token)
    return d
371/482:
def search_oov(text):
    tokens = nlp(text)
    d=dict()
    for token in tokens:
        if token.is_oov:
            d[str(token)]=d.get(str(token) + 1
    return d
371/483:
def search_oov(text):
    tokens = nlp(text)
    d=dict()
    for token in tokens:
        if token.is_oov:
            d[str(token)]=d.get(str(token)+1
    return d
371/484:
def search_oov(text):
    tokens = nlp(text)
    d=dict()
    for token in tokens:
        if token.is_oov:
            d[str(token)] = d.get(str(token)+1
    return d
371/485:
def search_oov(text):
    tokens = nlp(text)
    d=dict()
    for token in tokens:
        if token.is_oov:
            d[str(token)]=d.get(str(token) + 1
            print(token)
    return d
371/486:
def search_oov(text):
    tokens = nlp(text)
    d=dict()
    for token in tokens:
        if token.is_oov:
            d[str(token)]= d.get(str(token), 0) + 1
            print(token)
    return d
371/487:
def search_oov(text):
    tokens = nlp(text)
    d=dict()
    for token in tokens:
        if token.is_oov:
            d[str(token)]=d.get(str(token), 0) + 1
            print(token)
    return d
371/488:
def search_oov(text):
    tokens = nlp(text)
    d=dict()
    for token in tokens:
        if token.is_oov:
            d[token]=d.get(token, 0) + 1
            print(token)
    return d
371/489:
def str_to_unicode(text):
    byte_str = ''.join(r'\u{:04X}'.format(ord(chr)) for chr in text)
    byte_str = str.encode(text)
  #  emojis_l = emoji_pattern.findall(byte_str)
    print(byte_str)
371/490: d_oov = df_text.text.apply(search_oov)
371/491: d_oov.keys
371/492: d_oov
371/493: d_oov
371/494:
d=dict()
def search_oov(text):
    tokens = nlp(text)
    for token in tokens:
        if token.is_oov:
            d[token]=d.get(token, 0) + 1
            print(token)
371/495:
d=dict()
def search_oov(text):
    tokens = nlp(text)
    for token in tokens:
        if token.is_oov:
            d[token]=d.get(token, 0) + 1
            print(token)
371/496: df_text.text.apply(search_oov)
371/497: d
371/498: d.sort()
371/499: dict(sorted(d.items(), key=lambda item: item[1]))
371/500:
def search_oov(text):
    oov_list = []
    tokens = nlp(text)
    for token in tokens:
        if token.is_oov:
            oov_list.append()
    return oov_list
371/501: df_text['oov'] = df_text.text.apply(search_oov)
371/502:
def search_oov(text):
    oov_list = []
    tokens = nlp(text)
    for token in tokens:
        if token.is_oov:
            oov_list.append(token)
    return oov_list
371/503:
def str_to_unicode(text):
    byte_str = ''.join(r'\u{:04X}'.format(ord(chr)) for chr in text)
    byte_str = str.encode(text)
  #  emojis_l = emoji_pattern.findall(byte_str)
    print(byte_str)
371/504: df_text['oov'] = df_text.text.apply(search_oov)
371/505: df_text
371/506: df_text.oov
371/507: len(df_text.oov)
371/508: df_text[len(df_text.oov) > 0]
371/509: df_text
371/510: df_text[40:90]
371/511: df_text.oov.to_list()
371/512: df_text.oov.to_list().ravel()
371/513: df_text.oov.to_list().flatten()
371/514: df_text.oov.to_list().flatten()
371/515: df_text.oov.to_list().ravel()
371/516: df_text.oov.to_list().ravel()
371/517: df_text.oov.to_list().ravel().intersect1d()
371/518: df_text.oov.to_list().ravel()
371/519: df_text.oov.to_list().intersect1d()
371/520: df_text.oov.to_list().ravel()
371/521: df_text.oov.to_list()
371/522: df_text.oov.to_list().dtype
371/523: df_text.oov.to_list()
371/524: q = df_text.oov.to_list()
371/525: q
371/526: q.ravel()
371/527: np.array(q).ravel()
371/528: df_text.oov
371/529: df_text = df_train.loc[:, ['id', 'target', 'text']]
371/530: df_text = basic_cleaning(df_text, 'text')
371/531:
oov_list = []
def search_oov(text):
    tokens = nlp(text)
    for token in tokens:
        if token.is_oov:
            oov_list.append(token)
371/532: df_text.text.apply(search_oov)
371/533: oov_list
371/534: counter = collections.Counter(oov)
371/535: counter = collections.Counter(oov)
371/536:
import collections
counter = collections.Counter(oov)
371/537:
import collections
counter = collections.Counter(oov_list)
371/538: counter
371/539: counter.most_common(10)
371/540:
#remove urls
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

def remove_urls(text):
    text = re.sub(r'https?://\S+', '', text)
    return text

#remove @word 
def remove_social_media_tags(text):
    text = re.sub(r'@\S+', '', text)
    return text

def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

#remove digits
def remove_digits(text):
    text = re.sub(r'[0-9]+', '', text)
    return text   

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()
371/541:
def basic_cleaning(df, column_name):
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_social_media_tags)
    df[column_name] = df[column_name].apply(string_contractions)    
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(remove_digits)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(convert_lower_case)
    return df
371/542: df_text = df_train.loc[:, ['id', 'target', 'text']]
371/543: df_text = basic_cleaning(df_text, 'text')
371/544: df_text
371/545:
oov_list = []
def search_oov(text):
    tokens = nlp(text)
    for token in tokens:
        if token.is_oov:
            oov_list.append(token)
371/546: counter = Counter(oov_list)
371/547: counter.most_common(10)
371/548: oov_list
371/549:
oov_list = []
def search_oov(text):
    tokens = nlp(text)
    for token in tokens:
        if token.is_oov:
            oov_list.append(token)
371/550: oov_list
371/551: df_text.text.apply(search_oov)
371/552: oov_list
371/553: oov_list
371/554:
counter = Counter(oov_list)
counter.most_common(10)
371/555:
import nltk
#nltk.download()
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer

import emoji as emj
import spacy 
## It's important to use _lg for OOV in future
nlp = spacy.load('en_core_web_lg')
371/556: sid = SentimentIntensityAnalyzer()
371/557: df_train['sentiment_compound']  = df_train.text.apply(lambda tweet: sid.polarity_scores(tweet)['compound'])
371/558: df_train['sentiment_compound_score'] = df_train['sentiment_compound'].apply(lambda c: 'pos' if c >=0 else 'neg')
371/559:
plt.figure(figsize=[3, 4])
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
371/560:
plt.figure(figsize=[4, 4])
sns.countplot(x='target', hue='sentiment_compound_score', data=df_train)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
371/561: ### TF-IDF
371/562: df_train['text_tokenized'] = df_train.text.apply(word_tokenize)
371/563: df_train
375/1:
import pandas as pd
import numpy as np
import re
from collections import Counter
import string
375/2: pd.set_option('max_colwidth', None)
375/3:
import matplotlib.pyplot as plt
import seaborn as sns
#import plotly.graph_objs as go 
#from plotly.offline import init_notebook_mode, iplot 
#init_notebook_mode(connected = True) 
#import os 
#import matplotlib.pyplot as plt#visualization 
#%matplotlib inline 
##warnings.filterwarnings("ignore") 
#import io 
#import plotly.offline as py#visualization 
#py.init_notebook_mode(connected=True)#visualization 
#import plotly.graph_objs as go#visualization 
#import plotly.tools as tls#visualization 
#import plotly.figure_factory as ff
375/4:
import nltk
#nltk.download()
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer

import emoji as emj
import spacy 
## It's important to use _lg for OOV in future
nlp = spacy.load('en_core_web_lg')
375/5:
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, roc_curve
#from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
#from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
375/6:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
375/7:
nltk.download()
#nltk.download('stopwords')
375/8: STOPWORDS = set(stopwords.words('english'))
375/9:
#import nltk
#nltk.download('wordnet')
375/10:
#import nltk
#dler = nltk.downloader.Downloader()
#dler._update_index()
#dler.download('all')
375/11: wnl = WordNetLemmatizer()
375/12: df_train.head()
375/13: df_train.isnull().mean()
375/14: df_train.shape, df_test.shape
375/15:
#cardinality check
for col in df_train.columns:
    print('{} has {} unique instances'.format(col, len(df_train[col].unique())))
375/16: df_train.info()
375/17: df_train.keyword.value_counts()
375/18:
plt.figure(figsize=[8, 40])
sns.countplot(y='keyword',
              data=df_train,
              color='lightblue',
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
375/19:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword', 
              hue='target', 
              data=df_train, 
              palette=['lightblue', 'red']
             )
plt.xticks(rotation=90)
plt.legend(loc='upper right')
plt.title("Distribution of Target per Keyword ")
plt.show()
375/20: df_train.keyword = df_train.keyword.str.replace('%20', ' ')
375/21:
# Let's show most common keywords for target = 1
df_train[df_train.target == 1].keyword.value_counts()[:10]
375/22:
# Let's show most common keywords for target = 1
df_train[df_train.target == 0].keyword.value_counts()[:10]
375/23:
df_train_group = df_train.groupby(['keyword', 'target'])['id'].count().unstack(fill_value=0).reset_index()
df_train_group.columns = ['keyword', 'target_0', 'target_1']
df_train_group['prob_real_disasters'] = df_train_group.target_1/(df_train_group.target_0 + df_train_group.target_1)
375/24: df_train_group.sort_values('prob_real_disasters', ascending=False)
375/25: df_train.keyword.nunique()
375/26: df_test.head()
375/27: df_train.target.value_counts()/len(df_train)
375/28: df_train[df_train.keyword.isna()].target.value_counts()
375/29:
#string.punctuation = '!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
def count_punctuations(text):
    d=dict()
    for i in string.punctuation:
        d[str(i)+' count']=text.count(i)
    return d
375/30:
df_train['punct_count'] = df_train["text"].apply(lambda x:count_punctuations(x))
df_punct= pd.DataFrame(list(df_train.punct_count))
merge_df = pd.merge(df_train.target,df_punct,left_index=True, right_index=True)
375/31:
merge_df_gr = merge_df.groupby('target').sum().reset_index()
merge_df_gr = merge_df_gr.T
merge_df_gr.columns = ['target_0', 'target_1']
375/32: merge_df_gr
375/33: merge_df_gr['sum_0_1'] = merge_df_gr['target_0'] + merge_df_gr['target_1']
375/34: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
375/35: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
375/36:
def punct_add_exclamation(text):
    return len(re.findall("!", text))
375/37:
def punct_add_question(text):
    return len(re.findall("\?", text))
375/38:
def punct_add_quotation(text):
    return len(re.findall("'", text))
375/39:
df_train['count_exclamation'] = df_train.text.apply(punct_add_exclamation)
df_train['count_question'] = df_train.text.apply(punct_add_question)
df_train['count_quotation'] = df_train.text.apply(punct_add_quotation)
375/40: new_cols = ['count_exclamation', 'count_question', 'count_quotation']
375/41: df_train.head(2)
375/42:
# count number of characters 
def count_chars(text):
    return len(text)

# count number of words 
def count_words(text):
    return len(text.split())

# count number of capital characters
def count_capital_chars(text):
    count=0
    for i in text:
        if i.isupper():
            count+=1
    return count

# count number of capital words
def count_capital_words(text):
    return sum(map(str.isupper,text.split()))

# count number of punctuations
def count_punctuations(text):
    count = lambda l1,l2: sum([1 for x in l1 if x in l2])
    return count(text,set(string.punctuation))

# count number of words in quotes
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    string_wothout_contractions = string_wothout_contractions.replace('"', '\'')
    x = re.findall("\'(.+?)\'", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
    
# count number of sentences
def count_sent(text):
    return len(nltk.sent_tokenize(text))

# count number of unique words 
def count_unique_words(text):
    return len(set(text.split()))
    
# count of hashtags
def count_htags(text):
    x = re.findall(r'(\#\w[A-Za-z0-9]*)', text)
    return len(x)

# count of mentions
def count_mentions(text):
    x = re.findall(r'(\@\w[A-Za-z0-9]*)', text)
    return len(x)

# count of stopwords
def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)

#count of links
def count_links(text):
    link_list = re.findall(r'(https?://[^\s]+)', text)
    bitly_list = re.findall(r'bit.ly/\S+', text)
    return (len(link_list + bitly_list))

#count digits
def count_digits(text):
    digit_list = re.findall(r'[0-9]+', text)
    return len(digit_list)
375/43: re.findall(r"[0-9]+", '8484 fjfjf 4.4 ffgt')
375/44: sum(ch.isdigit() for ch in 'jdj 4 kfg 33')
375/45: len(df_train.loc[:, 'text'])
375/46:
def feature_eng_fun(df, text):
    df['count_exclamation'] = df_train[text].apply(punct_add_exclamation)
    df['count_question'] = df[text].apply(punct_add_question)
    df['count_quotation'] = df[text].apply(punct_add_quotation)
    df['count_num_of_chars'] = df[text].apply(count_chars)
    df['count_num_of_words'] = df[text].apply(count_words)
    df['count_num_of_capital_chars'] = df[text].apply(count_capital_chars)
    df['count_num_of_capital_words'] = df[text].apply(count_capital_words)
    df['count_punct'] = df[text].apply(count_punctuations)
    df['count_words_in_quotes'] = df[text].apply(count_words_in_quotes)
    df['count_sent'] = df[text].apply(count_sent)
    df['count_unique_words'] = df[text].apply(count_unique_words)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_mentions'] = df[text].apply(count_mentions)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_stopwords'] = df[text].apply(count_stopwords)
    df['count_links'] = df[text].apply(count_links)  
    return df
375/47:
## .loc will create a new copy, not view
df_feature_eng = df_train.loc[:, ('id', 'text', 'target')]
375/48: df_new = feature_eng_fun(df_feature_eng, 'text')
375/49: df_new.drop(['text'], axis=1, inplace=True)
375/50:
plt.figure(figsize=(13,10))
sns.heatmap(df_new.corr(),annot=True)
375/51:
cols_selected = np.abs(df_new.corr()['target']).sort_values(ascending=False)[1:11].index.to_list()
cols_selected.append('id')
375/52: cols_selected
375/53:
## .loc will create a new copy, not view
df_feature_eng = df_train.loc[:, ('id', 'text', 'target')]
375/54: df_feature_eng = feature_eng_fun(df_feature_eng, 'text')
375/55: df_feature_eng.drop(['text'], axis=1, inplace=True)
375/56:
plt.figure(figsize=(13,10))
sns.heatmap(df_feature_eng.corr(),annot=True)
375/57:
cols_selected = np.abs(df_feature_eng.corr()['target']).sort_values(ascending=False)[1:11].index.to_list()
cols_selected.append('id')
375/58: df_feature_eng[cols_selected]
375/59: df_feature_eng[cols_selected].head(3)
375/60: df_train.location.nunique()
375/61: df_train.location.fillna('no_location', inplace=True)
375/62:
def GPE_counter(text):
    nlp_text = nlp(text)
    for words in nlp_text:
        if words.ent.label_ == 'LOC' | 'LOC':
            print(words)
375/63:
# Write a function to display basic entity info:
def show_ents(doc):
    doc = nlp(doc)
    count_GPE = 0
    if doc.ents:
        for ent in doc.ents:
            if ent.label_ == 'GPE':
                #print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
                count_GPE = 1
                break
    return count_GPE
375/64: df_train['GPE_loc_flag'] = df_train.location.apply(show_ents)
375/65: df_train['GPE_text_flag'] = df_train.text.apply(show_ents)
375/66: df_train['GPE_flag'] = df_train.GPE_loc_flag | df_train.GPE_text_flag
375/67: df_train
375/68:
ax[0].set_title("Top 10 Keywords - Disaster Tweets")
ax[0].set_xlabel("Keyword Frequency")
ax[1].set_title("Top 10 Keywords - Non-Disaster Tweets")
ax[1].set_xlabel("Keyword Frequency")
plt.tight_layout()
plt.show()
375/69:
f, axes = plt.subplots(1, 3, figsize=(20,6))
sns.countplot(x='GPE_loc_flag', hue='target', data=df_train, ax=axes[0])
sns.countplot(x='GPE_text_flag', hue='target', data=df_train, ax=axes[1])
sns.countplot(x='GPE_flag', hue='target', data=df_train, ax=axes[2])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')
axes[2].set_title('GPE')

#plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
axes[0].legend(labels=['0', '1'])
axes[1].legend(labels=['0', '1'])
axes[2].legend(labels=['0', '1'])

plt.show()
375/70:
# Looks like the location extracted from the tweet will be more informative than the original location feature.
df_train.iloc[:, -4:].corr()
375/71:
# Let's look at some tweets more carefully
df_train[55:60]
375/72: df_location = df_train.loc[:, ['id', 'target', 'text']]
375/73: df_location.location.fillna('no_location', inplace=True)
375/74: df_location = df_train.loc[:, ['id', 'target', 'location', 'text']]
375/75: df_location.location.fillna('no_location', inplace=True)
375/76: df_location['GPE_loc_flag'] = df_location.location.apply(show_ents)
375/77: df_location['GPE_text_flag'] = df_location.text.apply(show_ents)
375/78:
#remove urls
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

def remove_urls(text):
    text = re.sub(r'https?://\S+', '', text)
    return text

#remove @word 
def remove_social_media_tags(text):
    text = re.sub(r'@\S+', '', text)
    return text

def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

#remove digits
def remove_digits(text):
    text = re.sub(r'[0-9]+', '', text)
    return text   

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()
375/79:
def basic_cleaning(df, column_name):
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_social_media_tags)
    df[column_name] = df[column_name].apply(string_contractions)    
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(remove_digits)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(convert_lower_case)
    return df
375/80: df_text = df_train.loc[:, ['id', 'target', 'text']]
375/81: df_text = basic_cleaning(df_text, 'text')
375/82:
oov_list = []
def search_oov(text):
    tokens = nlp(text)
    for token in tokens:
        if token.is_oov:
            oov_list.append(token)
375/83: df_text.text.apply(search_oov)
375/84:
counter = Counter(oov_list)
counter.most_common(10)
375/85: sid = SentimentIntensityAnalyzer()
375/86: df_sentiment = df_train.loc[:, ['id', 'target', 'text']]
375/87: df_sentiment['sentiment_compound']  = df_sentiment.text.apply(lambda tweet: sid.polarity_scores(tweet)['compound'])
375/88: df_sentiment['sentiment_compound_score'] = df_sentiment['sentiment_compound'].apply(lambda c: 'pos' if c >=0 else 'neg')
375/89:
plt.figure(figsize=[4, 4])
sns.countplot(x='target', hue='sentiment_compound_score', data=df_sentiment)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
375/90: df_train
375/91:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
375/92: df_train
375/93: df_text
375/94:
from sklearn.model_selection import train_test_split

X = df_text['text']
y = df_text['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
375/95:
from sklearn.pipeline import Pipeline

text_clf = Pipeline([('tfidf', TfidfVectorizer()),
                     ('mnb', MultinomialNB()),
])

# Feed the training data through the pipeline
text_clf.fit(X_train, y_train)
375/96:
from sklearn.pipeline import Pipeline

text_mnb = Pipeline([('tfidf', TfidfVectorizer()),
                     ('mnb', MultinomialNB()),
])

# Feed the training data through the pipeline
text_mnb.fit(X_train, y_train)
375/97: predictions = text_mnb.predict(X_test)
375/98: predictions
375/99: print(classification_report(y_train, predictions))
375/100: print(classification_report(y_test, predictions))
375/101: df_location
375/102: df_feature_eng
375/103: df_feature_eng[cols_selected]
375/104: df_train
375/105: df_feature_eng[cols_selected].set_index('id')
375/106: from sklearn.preprocessing import MinMaxScaler
375/107: df_feature_eng = df_feature_eng[cols_selected].set_index('id')
375/108:
scaler = MinMaxScaler()
scaler.fit(df_feature_eng)
375/109: scaler.transform(df_feature_eng)
375/110: scaler.transform(df_feature_eng).to_df
375/111: pd.DataFrame(scaler.transform(df_feature_eng))
375/112:
## .loc will create a new copy, not view
df_feature_eng = df_train.loc[:, ('id', 'text', 'target')]
375/113: df_feature_eng = feature_eng_fun(df_feature_eng, 'text')
375/114: df_feature_eng.drop(['text'], axis=1, inplace=True)
375/115:
plt.figure(figsize=(13,10))
sns.heatmap(df_feature_eng.corr(),annot=True)
375/116:
cols_selected = np.abs(df_feature_eng.corr()['target']).sort_values(ascending=False)[1:11].index.to_list()
cols_selected.append('id')
375/117: df_feature_eng[cols_selected].head(3)
375/118: df_feature_eng = df_feature_eng[cols_selected].set_index('id')
375/119: scaler = MinMaxScaler()
375/120: df_feature_eng[:] = scaler.fit_transform(df_feature_eng)
375/121: df_feature_eng
375/122: df_location
375/123: pd.merge(df_feature_eng, df_location[['id', 'GPE_text_flag']], left_index=True, right_on='id')
375/124: df_feature_eng = pd.merge(df_feature_eng, df_location[['id', 'GPE_text_flag']], left_index=True, right_on='id')
375/125:
# Using TF-IDF
vectorizer = TfidfVectorizer()
train_tf_idf = vectorizer.fit_transform(df_text['text']).toarray()
train_tf_idf= pd.DataFrame(train_tf_idf)
375/126: train_tf_idf
375/127: train_tf_idf.describe()
375/128: train_tf_idf.shape
375/129: df_final = pd.merge(df_feature_eng, train_tf_idf, left_index=True, right_index=True)
375/130: df_final
375/131: X_train, X_test, y_train, y_test = train_test_split(df_final, y, test_size=0.33, random_state=42)
375/132:
mnb = MultinomialNB()
mnb.fit(X_train, y_train)
y_test_pred = mnb.predict(X_test)
375/133: print(classification_report(y_test, y_test_pred))
375/134:
## .loc will create a new copy, not view
df_feature_eng = df_train.loc[:, ('id', 'text', 'target')]
375/135: df_feature_eng = feature_eng_fun(df_feature_eng, 'text')
375/136: df_feature_eng.drop(['text'], axis=1, inplace=True)
375/137:
plt.figure(figsize=(13,10))
sns.heatmap(df_feature_eng.corr(),annot=True)
375/138:
cols_selected = np.abs(df_feature_eng.corr()['target']).sort_values(ascending=False)[1:11].index.to_list()
cols_selected.append('id')
375/139: df_feature_eng[cols_selected].head(3)
375/140: df_feature_eng[:] = scaler.fit_transform(df_feature_eng)
375/141: df_feature_eng = pd.merge(df_feature_eng, df_location[['id', 'GPE_text_flag']], left_index=True, right_on='id')
375/142: df_feature_eng
377/1:
import pandas as pd
import numpy as np
import re
from collections import Counter
import string
from sklearn.preprocessing import MinMaxScaler
377/2: pd.set_option('max_colwidth', None)
377/3:
import matplotlib.pyplot as plt
import seaborn as sns
377/4:
import nltk
#nltk.download()
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer

import emoji as emj
import spacy 
## It's important to use _lg for OOV in future
nlp = spacy.load('en_core_web_lg')
377/5:
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, roc_curve
#from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
#from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
377/6:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
377/7:
nltk.download()
#nltk.download('stopwords')
377/8: STOPWORDS = set(stopwords.words('english'))
377/9: wnl = WordNetLemmatizer()
377/10: df_train.head()
377/11: df_train.isnull().mean()
377/12: df_train.shape, df_test.shape
377/13:
#cardinality check
for col in df_train.columns:
    print('{} has {} unique instances'.format(col, len(df_train[col].unique())))
377/14: df_train.info()
377/15: df_train.keyword.value_counts()
377/16:
plt.figure(figsize=[8, 40])
sns.countplot(y='keyword',
              data=df_train,
              color='lightblue',
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
377/17:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword', 
              hue='target', 
              data=df_train, 
              palette=['lightblue', 'red']
             )
plt.xticks(rotation=90)
plt.legend(loc='upper right')
plt.title("Distribution of Target per Keyword ")
plt.show()
377/18: df_train.keyword = df_train.keyword.str.replace('%20', ' ')
377/19:
# Let's show most common keywords for target = 1
df_train[df_train.target == 1].keyword.value_counts()[:10]
377/20:
# Let's show most common keywords for target = 1
df_train[df_train.target == 0].keyword.value_counts()[:10]
377/21:
df_train_group = df_train.groupby(['keyword', 'target'])['id'].count().unstack(fill_value=0).reset_index()
df_train_group.columns = ['keyword', 'target_0', 'target_1']
df_train_group['prob_real_disasters'] = df_train_group.target_1/(df_train_group.target_0 + df_train_group.target_1)
377/22: df_train_group.sort_values('prob_real_disasters', ascending=False)
377/23: df_train.keyword.nunique()
377/24: df.head()
377/25: df_train.target.value_counts()/len(df_train)
377/26:
#string.punctuation = '!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
def count_punctuations(text):
    d=dict()
    for i in string.punctuation:
        d[str(i)+' count']=text.count(i)
    return d
377/27:
df_train['punct_count'] = df_train["text"].apply(lambda x:count_punctuations(x))
df_punct= pd.DataFrame(list(df_train.punct_count))
merge_df = pd.merge(df_train.target,df_punct,left_index=True, right_index=True)
377/28:
merge_df_gr = merge_df.groupby('target').sum().reset_index()
merge_df_gr = merge_df_gr.T
merge_df_gr.columns = ['target_0', 'target_1']
377/29: merge_df_gr
377/30: merge_df_gr['sum_0_1'] = merge_df_gr['target_0'] + merge_df_gr['target_1']
377/31: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
377/32: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
377/33:
def punct_add_exclamation(text):
    return len(re.findall("!", text))
377/34:
def punct_add_question(text):
    return len(re.findall("\?", text))
377/35:
def punct_add_quotation(text):
    return len(re.findall("'", text))
377/36:
df_train['count_exclamation'] = df_train.text.apply(punct_add_exclamation)
df_train['count_question'] = df_train.text.apply(punct_add_question)
df_train['count_quotation'] = df_train.text.apply(punct_add_quotation)
377/37: new_cols = ['count_exclamation', 'count_question', 'count_quotation']
377/38: df_train.head(2)
377/39:
# count number of characters 
def count_chars(text):
    return len(text)

# count number of words 
def count_words(text):
    return len(text.split())

# count number of capital characters
def count_capital_chars(text):
    count=0
    for i in text:
        if i.isupper():
            count+=1
    return count

# count number of capital words
def count_capital_words(text):
    return sum(map(str.isupper,text.split()))

# count number of punctuations
def count_punctuations(text):
    count = lambda l1,l2: sum([1 for x in l1 if x in l2])
    return count(text,set(string.punctuation))

# count number of words in quotes
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    string_wothout_contractions = string_wothout_contractions.replace('"', '\'')
    x = re.findall("\'(.+?)\'", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
    
# count number of sentences
def count_sent(text):
    return len(nltk.sent_tokenize(text))

# count number of unique words 
def count_unique_words(text):
    return len(set(text.split()))
    
# count of hashtags
def count_htags(text):
    x = re.findall(r'(\#\w[A-Za-z0-9]*)', text)
    return len(x)

# count of mentions
def count_mentions(text):
    x = re.findall(r'(\@\w[A-Za-z0-9]*)', text)
    return len(x)

# count of stopwords
def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)

#count of links
def count_links(text):
    link_list = re.findall(r'(https?://[^\s]+)', text)
    bitly_list = re.findall(r'bit.ly/\S+', text)
    return (len(link_list + bitly_list))

#count digits
def count_digits(text):
    digit_list = re.findall(r'[0-9]+', text)
    return len(digit_list)
377/40:
def feature_eng_fun(df, text):
    df['count_exclamation'] = df_train[text].apply(punct_add_exclamation)
    df['count_question'] = df[text].apply(punct_add_question)
    df['count_quotation'] = df[text].apply(punct_add_quotation)
    df['count_num_of_chars'] = df[text].apply(count_chars)
    df['count_num_of_words'] = df[text].apply(count_words)
    df['count_num_of_capital_chars'] = df[text].apply(count_capital_chars)
    df['count_num_of_capital_words'] = df[text].apply(count_capital_words)
    df['count_punct'] = df[text].apply(count_punctuations)
    df['count_words_in_quotes'] = df[text].apply(count_words_in_quotes)
    df['count_sent'] = df[text].apply(count_sent)
    df['count_unique_words'] = df[text].apply(count_unique_words)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_mentions'] = df[text].apply(count_mentions)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_stopwords'] = df[text].apply(count_stopwords)
    df['count_links'] = df[text].apply(count_links)  
    return df
377/41:
## .loc will create a new copy, not view
df_feature_eng = df_train.loc[:, ('id', 'text', 'target')]
377/42: df_feature_eng = feature_eng_fun(df_feature_eng, 'text')
377/43: df_feature_eng.drop(['text'], axis=1, inplace=True)
377/44:
plt.figure(figsize=(13,10))
sns.heatmap(df_feature_eng.corr(),annot=True)
377/45:
cols_selected = np.abs(df_feature_eng.corr()['target']).sort_values(ascending=False)[1:11].index.to_list()
cols_selected.append('id')
377/46: df_feature_eng[cols_selected].head(3)
377/47: cols_selected = np.abs(df_feature_eng.corr()['target']).sort_values(ascending=False)[1:11].index.to_list()
377/48: df_feature_eng[cols_selected].head(3)
377/49: df_train.location.nunique()
377/50: df_location = df_train.loc[:, ['id', 'target', 'location', 'text']]
377/51: df_location.location.fillna('no_location', inplace=True)
377/52:
# Write a function to display basic entity info:
def show_ents(doc):
    doc = nlp(doc)
    count_GPE = 0
    if doc.ents:
        for ent in doc.ents:
            if ent.label_ == 'GPE':
                #print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
                count_GPE = 1
                break
    return count_GPE
377/53: df_location['GPE_loc_flag'] = df_location.location.apply(show_ents)
377/54: df_location['GPE_text_flag'] = df_location.text.apply(show_ents)
377/55: df_location['GPE_flag'] = df_location.GPE_loc_flag | df_location.GPE_text_flag
377/56:
f, axes = plt.subplots(1, 3, figsize=(20,6))
sns.countplot(x='GPE_loc_flag', hue='target', data=df_location, ax=axes[0])
sns.countplot(x='GPE_text_flag', hue='target', data=df_location, ax=axes[1])
sns.countplot(x='GPE_flag', hue='target', data=df_location, ax=axes[2])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')
axes[2].set_title('GPE')

#plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
axes[0].legend(labels=['0', '1'])
axes[1].legend(labels=['0', '1'])
axes[2].legend(labels=['0', '1'])

plt.show()
377/57:
# Looks like the location extracted from the tweet will be more informative than the original location feature.
df_location.iloc[:, -4:].corr()
377/58:
# Let's look at some tweets more carefully
df_train[55:60]
377/59:
#remove urls
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

def remove_urls(text):
    text = re.sub(r'https?://\S+', '', text)
    return text

#remove @word 
def remove_social_media_tags(text):
    text = re.sub(r'@\S+', '', text)
    return text

def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

#remove digits
def remove_digits(text):
    text = re.sub(r'[0-9]+', '', text)
    return text   

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()
377/60:
def basic_cleaning(df, column_name):
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_social_media_tags)
    df[column_name] = df[column_name].apply(string_contractions)    
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(remove_digits)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(convert_lower_case)
    return df
377/61: df_text = df_train.loc[:, ['id', 'target', 'text']]
377/62: df_text = basic_cleaning(df_text, 'text')
377/63:
oov_list = []
def search_oov(text):
    tokens = nlp(text)
    for token in tokens:
        if token.is_oov:
            oov_list.append(token)
377/64: df_text.text.apply(search_oov)
377/65:
counter = Counter(oov_list)
counter.most_common(10)
377/66: sid = SentimentIntensityAnalyzer()
377/67: df_sentiment = df_train.loc[:, ['id', 'target', 'text']]
377/68: df_sentiment['sentiment_compound']  = df_sentiment.text.apply(lambda tweet: sid.polarity_scores(tweet)['compound'])
377/69: df_sentiment['sentiment_compound_score'] = df_sentiment['sentiment_compound'].apply(lambda c: 'pos' if c >=0 else 'neg')
377/70:
plt.figure(figsize=[4, 4])
sns.countplot(x='target', hue='sentiment_compound_score', data=df_sentiment)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
377/71: df_text
377/72: df_text = df_train.loc[:, ['id', 'target', 'keyword', 'text']]
377/73: df_text = basic_cleaning(df_text, 'text')
377/74: df_text
377/75: df_text["con_text"] = df_text["keyword"].astype(str) + df_text["text"]
377/76: df_text
377/77: df_text[50:80]
377/78: df_text
377/79: df_text["con_text"] = df_text["keyword"] + df_text["text"]
377/80: df_text
377/81: df_text[50:80]
377/82: df_text["con_text"] = df.keyword.map(str) + " " + df.text
377/83: df_text[50:80]
377/84: df_text["con_text"] = df_text.keyword.map(str) + " " + df_text.text
377/85: df_text[50:80]
377/86: df_text
377/87: df_text
377/88:
X = df_text['con_text']
y = df_text['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
377/89:
text_mnb = Pipeline([('tfidf', TfidfVectorizer()),
                     ('mnb', MultinomialNB()),
])

# Feed the training data through the pipeline
text_mnb.fit(X_train, y_train)
377/90:
import pandas as pd
import numpy as np
import re
from collections import Counter
import string
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline
377/91:
text_mnb = Pipeline([('tfidf', TfidfVectorizer()),
                     ('mnb', MultinomialNB()),
])

# Feed the training data through the pipeline
text_mnb.fit(X_train, y_train)
377/92: predictions = text_mnb.predict(X_test)
377/93:
predictions = text_mnb.predict(X_test)
print(classification_report(y_test, predictions))
377/94:
yhat = text_mnb.predict_proba(testX)
# keep probabilities for the positive outcome only
probs = yhat[:, 1]
# define thresholds
thresholds = arange(0, 1, 0.001)
# evaluate each threshold
scores = [f1_score(testy, to_labels(probs, t)) for t in thresholds]
# get best threshold
ix = argmax(scores)
print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
377/95:
yhat = text_mnb.predict_proba(X_test)
# keep probabilities for the positive outcome only
probs = yhat[:, 1]
# define thresholds
thresholds = arange(0, 1, 0.001)
# evaluate each threshold
scores = [f1_score(testy, to_labels(probs, t)) for t in thresholds]
# get best threshold
ix = argmax(scores)
print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
377/96:
yhat = text_mnb.predict_proba(X_test)
# keep probabilities for the positive outcome only
probs = yhat[:, 1]
# define thresholds
thresholds = np.arange(0, 1, 0.001)
# evaluate each threshold
scores = [f1_score(testy, to_labels(probs, t)) for t in thresholds]
# get best threshold
ix = argmax(scores)
print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
377/97:
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, roc_curve
#from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
#from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
from sklearn.metrics import f1_score
377/98:
yhat = text_mnb.predict_proba(X_test)
# keep probabilities for the positive outcome only
probs = yhat[:, 1]
# define thresholds
thresholds = np.arange(0, 1, 0.001)
# evaluate each threshold
scores = [f1_score(testy, to_labels(probs, t)) for t in thresholds]
# get best threshold
ix = argmax(scores)
print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
377/99:
yhat = text_mnb.predict_proba(X_test)
# keep probabilities for the positive outcome only
probs = yhat[:, 1]
# define thresholds
thresholds = np.arange(0, 1, 0.001)
# evaluate each threshold
scores = [f1_score(y_test, to_labels(probs, t)) for t in thresholds]
# get best threshold
ix = argmax(scores)
print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
377/100:
def to_labels(pos_probs, threshold):
    return (pos_probs >= threshold).astype('int')
377/101:
yhat = text_mnb.predict_proba(X_test)
# keep probabilities for the positive outcome only
probs = yhat[:, 1]
# define thresholds
thresholds = np.arange(0, 1, 0.001)
# evaluate each threshold
scores = [f1_score(y_test, to_labels(probs, t)) for t in thresholds]
# get best threshold
ix = argmax(scores)
print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
377/102:
yhat = text_mnb.predict_proba(X_test)
# keep probabilities for the positive outcome only
probs = yhat[:, 1]
# define thresholds
thresholds = np.arange(0, 1, 0.001)
# evaluate each threshold
scores = [f1_score(y_test, to_labels(probs, t)) for t in thresholds]
# get best threshold
ix = np.argmax(scores)
print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
377/103: f1_score(y_test, predictions)
377/104: print(classification_report(y_test, predictions))
377/105:
vectorizer = TfidfVectorizer()
train_tf_idf = vectorizer.fit_transform(X).toarray()
train_tf_idf= pd.DataFrame(train_tf_idf)
377/106: train_tf_idf
377/107: df_feature_eng
377/108: df_feature_eng[cols_selected]
377/109: df_f_eng = df_feature_eng[cols_selected]
377/110: df_f_eng
377/111: df_f_eng = df_feature_eng[cols_selected]
377/112:
scaler = MinMaxScaler()
df_f_eng[:] = scaler.fit_transform(df_f_eng)
377/113: df_f_eng
377/114: df_f_eng_c = pd.merge(df_f_eng, df_location[['GPE_text_flag']], left_index=True, left_index=True)
377/115: df_f_eng_c = pd.merge(df_f_eng, df_location[['GPE_text_flag']], left_index=True, right_index=True)
377/116: df_f_eng_c
377/117: df_f_eng = df_feature_eng[cols_selected]
377/118: df_f_eng = df_feature_eng[cols_selected]
377/119:
scaler = MinMaxScaler()
df_f_eng[:] = scaler.fit_transform(df_f_eng)
377/120: df_f_eng
377/121: df_f_eng_c = pd.merge(df_f_eng, df_location[['GPE_text_flag']], left_index=True, right_index=True)
377/122: df_f_eng_c
377/123: df_f_eng_c_tf = pd.merge(train_tf_idf,df_f_eng_c, left_index=True, right_index=True)
377/124: df_f_eng_c_tf
377/125: X_train, X_test, y_train, y_test = train_test_split(df_f_eng_c_tf, y, test_size=0.2, random_state=42)
377/126:
mnb = MultinomialNB()
mnb.fit(X_train, y_train)
y_test_pred = mnb.predict(X_test)
377/127: print(classification_report(y_test, y_test_pred))
377/128:
yhat = mnb.predict_proba(X_test)
# keep probabilities for the positive outcome only
probs = yhat[:, 1]
# define thresholds
thresholds = np.arange(0, 1, 0.001)
# evaluate each threshold
scores = [f1_score(y_test, to_labels(probs, t)) for t in thresholds]
# get best threshold
ix = np.argmax(scores)
print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
377/129:
### TEST
df_test.head(3)
377/130: df_test_fe = feature_eng_fun(df_test, 'text')
377/131: df_test_fe
377/132: df_test_fe[cols_selected]
377/133: df_test_fe_min_max = scaler.transform(df_test_fe[cols_selected])
377/134: df_test_fe_min_max
377/135: df_test_fe_min_max[:] = scaler.transform(df_test_fe[cols_selected])
377/136: df_test_fe_min_max
377/137: df_test_fe_min_max[:] = scaler.transform(df_test_fe[cols_selected])
377/138: df_test_fe_min_max
377/139: pd.DataFrame(df_test_fe_min_max)
377/140: df_train['GPE_text_flag'] = df_train.text.apply(show_ents)
377/141: df_train
377/142: df_test['GPE_text_flag'] = df_test.text.apply(show_ents)
377/143: df_test
377/144: df_text_test = basic_cleaning(df_test, 'text')
377/145: df_text_test
377/146: df_text
377/147: ## CREATE A MODEL BASED ON TEXT
377/148: df_text["con_text"] = df_text["keyword"] + df_text["text"]
377/149: df_text
377/150: df_text["con_text"] = df_text.keyword.map(str) + " " + df_text.text
377/151: df_text['text_token'] = df_text[con_text].apply(word_tokenize)
377/152: df_text['text_token'] = df_text['con_text'].apply(word_tokenize)
377/153: df_text
377/154:
df_text['text_token'] = df_text['con_text'].apply(word_tokenize)
df_text['all_text'] = df_text['text_token'].apply(lambda x: " ".join(x))
377/155: df_text
377/156: df_text["con_text"] = df_text["keyword"] + df_text["text"]
377/157: df_text["con_text"] = df_text.keyword.map(str) + " " + df_text.text
377/158: df_text
377/159: df_text = df_train.loc[:, ['id', 'target', 'keyword', 'text']]
377/160: df_text = basic_cleaning(df_text, 'text')
377/161: df_text
377/162: df_text["con_text"] = df_text["keyword"] + df_text["text"]
377/163: df_text
377/164: df_text["con_text"] = df_text.keyword.map(str) + " " + df_text.text
377/165: df_text
377/166:
def lemmatize_text(text):
    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]
377/167: df_text['text_lemmatized'] = df_text.text.apply(lemmatize_text)
377/168:
def lemmatize_text(text):
    return [lemmatizer.lemmatize(w) for w in wnl.tokenize(text)]
377/169: df_text['text_lemmatized'] = df_text.text.apply(lemmatize_text)
377/170:
w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
lemmatizer = nltk.stem.WordNetLemmatizer()
377/171: df_text['text_lemmatized'] = df_text.text.apply(lemmatize_text)
377/172:
def lemmatize_text(text):
    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]
377/173: df_text["con_text"] = df_text.keyword.map(str) + " " + df_text.text
377/174: df_text['text_lemmatized'] = df_text.text.apply(lemmatize_text)
377/175: df_text['text_lemmatized'] = df_text.con_text.apply(lemmatize_text)
377/176: df_text
377/177: df_text['all_text'] = df_text['text_lemmatized'].apply(lambda x: " ".join(x))
377/178: df_text
377/179:
X = df_text['all_text']
y = df_text['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
377/180:
text_mnb = Pipeline([('tfidf', TfidfVectorizer()),
                     ('mnb', MultinomialNB()),
])

# Feed the training data through the pipeline
text_mnb.fit(X_train, y_train)
377/181:
predictions = text_mnb.predict(X_test)
print(classification_report(y_test, predictions))
377/182:
def to_labels(pos_probs, threshold):
    return (pos_probs >= threshold).astype('int')
377/183:
yhat = text_mnb.predict_proba(X_test)
# keep probabilities for the positive outcome only
probs = yhat[:, 1]
# define thresholds
thresholds = np.arange(0, 1, 0.001)
# evaluate each threshold
scores = [f1_score(y_test, to_labels(probs, t)) for t in thresholds]
# get best threshold
ix = np.argmax(scores)
print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
377/184: df_text["con_text"] = df_text.keyword.map(str) + " " + df_text.text
377/185: df_text
377/186:
def basic_cleaning(df, column_name):
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_social_media_tags)
    df[column_name] = df[column_name].apply(string_contractions)    
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(remove_digits)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(convert_lower_case)
    return df
377/187: df_text = df_train.loc[:, ['id', 'target', 'keyword', 'text']]
377/188: df_text = basic_cleaning(df_text, 'text')
377/189: df_text["con_text"] = df_text.keyword.map(str) + " " + df_text.text
377/190: df_text
377/191:
X = df_text['all_text']
y = df_text['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
377/192:
X = df_text['con_text']
y = df_text['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
377/193: df_text_test
377/194:
df_text_test_idf = vectorizer.transform(df_text_test[['text']]).toarray()
df_text_test_idf= pd.DataFrame(df_text_test_idf)
377/195: df_text_test_idf
377/196: df_text_test
377/197:
X = df_text_test['text']
y = df_text_test['target']
377/198:
X = df_text['con_text']
y = df_text['target']
377/199: df_text
377/200: df_text_test
377/201: X = df_text_test['text']
377/202:
df_text_test_idf = vectorizer.transform(X).toarray()
df_text_test_idf= pd.DataFrame(df_text_test_idf)
377/203: df_text_test_idf
377/204: df_test_fe_min_max
377/205: df_test_fe_min_max = pd.DataFrame(df_test_fe_min_max)
377/206: df_test_fe_min_max
377/207: df_test_fe_min_max_c = pd.merge(df_test_fe_min_max, df_test[['GPE_text_flag']], left_index=True, right_index=True)
377/208: df_test_final = pd.merge(df_text_test_idf,df_test_fe_min_max_c, left_index=True, right_index=True)
377/209: df_test_final
377/210: X_train.shape
377/211: df_f_eng_c_tf.shape
377/212: y_test_pred = mnb.predict(df_test_final)
377/213: y_test_pred
377/214: y_test_pred = mnb.predict_proba(df_test_final)
377/215: X_train
377/216: df_f_eng_c_tf
377/217:
### TEST
df_test.head(3)
377/218:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
377/219:
### TEST
df_test.head(3)
377/220: df_test_fe = feature_eng_fun(df_test, 'text')
377/221: df_test_fe_min_max[:] = scaler.transform(df_test_fe[cols_selected])
377/222: df_test_fe_min_max = pd.DataFrame(df_test_fe_min_max)
377/223: df_test['GPE_text_flag'] = df_test.text.apply(show_ents)
377/224: df_test_fe_min_max_c = pd.merge(df_test_fe_min_max, df_test[['GPE_text_flag']], left_index=True, right_index=True)
377/225: df_test_fe_min_max
377/226: df_test_fe_min_max[:] = scaler.transform(df_test_fe[cols_selected])
377/227: df_test_fe = feature_eng_fun(df_test, 'text')
377/228: df_test_fe_min_max[:] = scaler.transform(df_test_fe[cols_selected])
377/229: df_test_fe_min_max
377/230: df_test_fe_min_max.columns = cols_selected
377/231: df_test_fe_min_max
377/232: df_test_fe_min_max_c = pd.merge(df_test_fe_min_max, df_test[['GPE_text_flag']], left_index=True, right_index=True)
377/233: df_test_fe_min_max_c
377/234: df_text_test = basic_cleaning(df_test, 'text')
377/235: X = df_text_test['text']
377/236:
df_text_test_idf = vectorizer.transform(X).toarray()
df_text_test_idf= pd.DataFrame(df_text_test_idf)
377/237: df_test_final = pd.merge(df_text_test_idf,df_test_fe_min_max_c, left_index=True, right_index=True)
377/238: df_test_final
377/239: y_test_pred = mnb.predict_proba(df_test_final)
377/240: df_f_eng_c_tf.columns
377/241: y_test_pred
377/242: y_test_pred[1]
377/243: y_test_pred[:1]
377/244: y_test_pred[:, 1]
377/245: y_test_pred[:, 1] >= 0.410
377/246: (y_test_pred[:, 1] >= 0.410).bool
377/247: (y_test_pred[:, 1] >= 0.410)
377/248: (y_test_pred[:, 1] >= 0.410).astype(int)
377/249: (y_test_pred[:, 1] >= 0.410).astype(int).to_list
377/250: df_test
377/251: df_test['id']
377/252: df_test[['id']]
377/253: df_t = df_test[['id']]
377/254: df_t['target'] = (y_test_pred[:, 1] >= 0.410).astype(int)
377/255: df_t
377/256: df_t.describe()
377/257: df_t.to_csv("submission.csv", index=False)
378/1:
import pandas as pd
import numpy as np
import re
from collections import Counter
import string
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline
378/2: pd.set_option('max_colwidth', None)
378/3:
import matplotlib.pyplot as plt
import seaborn as sns
378/4:
import nltk
#nltk.download()
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer

import emoji as emj
import spacy 
## It's important to use _lg for OOV in future
nlp = spacy.load('en_core_web_lg')
378/5:
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, roc_curve
#from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
#from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
from sklearn.metrics import f1_score
378/6:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
378/7:
nltk.download()
#nltk.download('stopwords')
379/1:
import pandas as pd
import numpy as np
import re
from collections import Counter
import string
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline
379/2: pd.set_option('max_colwidth', None)
379/3:
import matplotlib.pyplot as plt
import seaborn as sns
379/4:
import nltk
#nltk.download()
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer

import emoji as emj
import spacy 
## It's important to use _lg for OOV in future
nlp = spacy.load('en_core_web_lg')
379/5:
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, roc_curve
#from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
#from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
from sklearn.metrics import f1_score
379/6:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
379/7:
nltk.download()
#nltk.download('stopwords')
379/8: STOPWORDS = set(stopwords.words('english'))
379/9:
#import nltk
#nltk.download('wordnet')
379/10:
#import nltk
#dler = nltk.downloader.Downloader()
#dler._update_index()
#dler.download('all')
379/11: wnl = WordNetLemmatizer()
379/12: df_train.head()
379/13: df_train.isnull().mean()
379/14: df_train.shape, df_test.shape
379/15:
#cardinality check
for col in df_train.columns:
    print('{} has {} unique instances'.format(col, len(df_train[col].unique())))
379/16: df_train.info()
379/17: df_train.keyword.value_counts()
379/18:
plt.figure(figsize=[8, 40])
sns.countplot(y='keyword',
              data=df_train,
              color='lightblue',
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
379/19:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword', 
              hue='target', 
              data=df_train, 
              palette=['lightblue', 'red']
             )
plt.xticks(rotation=90)
plt.legend(loc='upper right')
plt.title("Distribution of Target per Keyword ")
plt.show()
379/20: df_train.keyword = df_train.keyword.str.replace('%20', ' ')
379/21:
# Let's show most common keywords for target = 1
df_train[df_train.target == 1].keyword.value_counts()[:10]
379/22:
# Let's show most common keywords for target = 1
df_train[df_train.target == 0].keyword.value_counts()[:10]
379/23:
df_train_group = df_train.groupby(['keyword', 'target'])['id'].count().unstack(fill_value=0).reset_index()
df_train_group.columns = ['keyword', 'target_0', 'target_1']
df_train_group['prob_real_disasters'] = df_train_group.target_1/(df_train_group.target_0 + df_train_group.target_1)
379/24: df_train_group.sort_values('prob_real_disasters', ascending=False)
379/25: df_train.keyword.nunique()
379/26: df_train.target.value_counts()/len(df_train)
379/27: df_train.location.nunique()
379/28: df_location = df_train.loc[:, ['id', 'target', 'location', 'text']]
379/29: df_location.location.fillna('no_location', inplace=True)
379/30:
# Write a function to display basic entity info:
def show_ents(doc):
    doc = nlp(doc)
    count_GPE = 0
    if doc.ents:
        for ent in doc.ents:
            if ent.label_ == 'GPE':
                #print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
                count_GPE = 1
                break
    return count_GPE
379/31: df_location['GPE_loc_flag'] = df_location.location.apply(show_ents)
379/32: df_location['GPE_text_flag'] = df_location.text.apply(show_ents)
379/33: df_location['GPE_flag'] = df_location.GPE_loc_flag | df_location.GPE_text_flag
379/34:
f, axes = plt.subplots(1, 3, figsize=(20,6))
sns.countplot(x='GPE_loc_flag', hue='target', data=df_location, ax=axes[0])
sns.countplot(x='GPE_text_flag', hue='target', data=df_location, ax=axes[1])
sns.countplot(x='GPE_flag', hue='target', data=df_location, ax=axes[2])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')
axes[2].set_title('GPE')

#plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
axes[0].legend(labels=['0', '1'])
axes[1].legend(labels=['0', '1'])
axes[2].legend(labels=['0', '1'])

plt.show()
379/35:
# Looks like the location extracted from the tweet will be more informative than the original location feature.
df_location.iloc[:, -4:].corr()
379/36:
#string.punctuation = '!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
def count_punctuations(text):
    d=dict()
    for i in string.punctuation:
        d[str(i)+' count']=text.count(i)
    return d
379/37:
df_train['punct_count'] = df_train["text"].apply(lambda x:count_punctuations(x))
df_punct= pd.DataFrame(list(df_train.punct_count))
merge_df = pd.merge(df_train.target,df_punct,left_index=True, right_index=True)
379/38:
merge_df_gr = merge_df.groupby('target').sum().reset_index()
merge_df_gr = merge_df_gr.T
merge_df_gr.columns = ['target_0', 'target_1']
379/39: merge_df_gr
379/40: merge_df_gr['sum_0_1'] = merge_df_gr['target_0'] + merge_df_gr['target_1']
379/41: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
379/42: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
379/43:
def punct_add_exclamation(text):
    return len(re.findall("!", text))
379/44:
def punct_add_question(text):
    return len(re.findall("\?", text))
379/45:
def punct_add_quotation(text):
    return len(re.findall("'", text))
379/46:
df_train['count_exclamation'] = df_train.text.apply(punct_add_exclamation)
df_train['count_question'] = df_train.text.apply(punct_add_question)
df_train['count_quotation'] = df_train.text.apply(punct_add_quotation)
379/47: new_cols = ['count_exclamation', 'count_question', 'count_quotation']
379/48: df_train.head(2)
379/49:
# count number of characters 
def count_chars(text):
    return len(text)

# count number of words 
def count_words(text):
    return len(text.split())

# count number of capital characters
def count_capital_chars(text):
    count=0
    for i in text:
        if i.isupper():
            count+=1
    return count

# count number of capital words
def count_capital_words(text):
    return sum(map(str.isupper,text.split()))

# count number of punctuations
def count_punctuations(text):
    count = lambda l1,l2: sum([1 for x in l1 if x in l2])
    return count(text,set(string.punctuation))

# count number of words in quotes
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    string_wothout_contractions = string_wothout_contractions.replace('"', '\'')
    x = re.findall("\'(.+?)\'", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
    
# count number of sentences
def count_sent(text):
    return len(nltk.sent_tokenize(text))

# count number of unique words 
def count_unique_words(text):
    return len(set(text.split()))
    
# count of hashtags
def count_htags(text):
    x = re.findall(r'(\#\w[A-Za-z0-9]*)', text)
    return len(x)

# count of mentions
def count_mentions(text):
    x = re.findall(r'(\@\w[A-Za-z0-9]*)', text)
    return len(x)

# count of stopwords
def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)

#count of links
def count_links(text):
    link_list = re.findall(r'(https?://[^\s]+)', text)
    bitly_list = re.findall(r'bit.ly/\S+', text)
    return (len(link_list + bitly_list))

#count digits
def count_digits(text):
    digit_list = re.findall(r'[0-9]+', text)
    return len(digit_list)
379/50:
def feature_eng_fun(df, text):
    df['count_exclamation'] = df_train[text].apply(punct_add_exclamation)
    df['count_question'] = df[text].apply(punct_add_question)
    df['count_quotation'] = df[text].apply(punct_add_quotation)
    df['count_num_of_chars'] = df[text].apply(count_chars)
    df['count_num_of_words'] = df[text].apply(count_words)
    df['count_num_of_capital_chars'] = df[text].apply(count_capital_chars)
    df['count_num_of_capital_words'] = df[text].apply(count_capital_words)
    df['count_punct'] = df[text].apply(count_punctuations)
    df['count_words_in_quotes'] = df[text].apply(count_words_in_quotes)
    df['count_sent'] = df[text].apply(count_sent)
    df['count_unique_words'] = df[text].apply(count_unique_words)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_mentions'] = df[text].apply(count_mentions)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_stopwords'] = df[text].apply(count_stopwords)
    df['count_links'] = df[text].apply(count_links) 
    ## This function from part above
    df['GPE_text_flag'] = df[text].apply(show_ents)    
    return df
379/51:
## .loc will create a new copy, not view
df_feature_eng = df_train.loc[:, ('id', 'text', 'target')]
379/52: df_feature_eng = feature_eng_fun(df_feature_eng, 'text')
379/53: df_feature_eng.drop(['text'], axis=1, inplace=True)
379/54:
plt.figure(figsize=(13,10))
sns.heatmap(df_feature_eng.corr(),annot=True)
379/55: cols_selected = np.abs(df_feature_eng.corr()['target']).sort_values(ascending=False)[1:11].index.to_list()
379/56: df_feature_eng[cols_selected].head(3)
379/57:
# Let's look at some tweets more carefully
df_train[55:60]
379/58: df_punct = df_train[['id', 'target', 'text']]
379/59: df_punct_fe = df_train.loc[:, ['id', 'target', 'text']]
379/60:
import pandas as pd
import numpy as np
import re
from collections import Counter
import string
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline
379/61: pd.set_option('max_colwidth', None)
379/62:
import matplotlib.pyplot as plt
import seaborn as sns
380/1:
import pandas as pd
import numpy as np
import re
from collections import Counter
import string
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline
380/2: pd.set_option('max_colwidth', None)
380/3:
import matplotlib.pyplot as plt
import seaborn as sns
380/4:
import nltk
#nltk.download()
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer

import emoji as emj
import spacy 
## It's important to use _lg for OOV in future
nlp = spacy.load('en_core_web_lg')
380/5:
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, roc_curve
#from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
#from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
from sklearn.metrics import f1_score
380/6:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
380/7:
nltk.download()
#nltk.download('stopwords')
380/8: STOPWORDS = set(stopwords.words('english'))
380/9:
#import nltk
#nltk.download('wordnet')
380/10:
#import nltk
#dler = nltk.downloader.Downloader()
#dler._update_index()
#dler.download('all')
380/11: wnl = WordNetLemmatizer()
380/12: df_train.head()
380/13: df_train.isnull().mean()
380/14: df_train.shape, df_test.shape
380/15:
#cardinality check
for col in df_train.columns:
    print('{} has {} unique instances'.format(col, len(df_train[col].unique())))
380/16: df_train.info()
380/17: df_train.keyword.value_counts()
380/18:
plt.figure(figsize=[8, 40])
sns.countplot(y='keyword',
              data=df_train,
              color='lightblue',
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
380/19:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword', 
              hue='target', 
              data=df_train, 
              palette=['lightblue', 'red']
             )
plt.xticks(rotation=90)
plt.legend(loc='upper right')
plt.title("Distribution of Target per Keyword ")
plt.show()
380/20: df_train.keyword = df_train.keyword.str.replace('%20', ' ')
380/21:
# Let's show most common keywords for target = 1
df_train[df_train.target == 1].keyword.value_counts()[:10]
380/22:
# Let's show most common keywords for target = 1
df_train[df_train.target == 0].keyword.value_counts()[:10]
380/23:
df_train_group = df_train.groupby(['keyword', 'target'])['id'].count().unstack(fill_value=0).reset_index()
df_train_group.columns = ['keyword', 'target_0', 'target_1']
df_train_group['prob_real_disasters'] = df_train_group.target_1/(df_train_group.target_0 + df_train_group.target_1)
380/24: df_train_group.sort_values('prob_real_disasters', ascending=False)
380/25: df_train.keyword.nunique()
380/26: df_train.target.value_counts()/len(df_train)
380/27: df_train.location.nunique()
380/28: df_location = df_train.loc[:, ['id', 'target', 'location', 'text']]
380/29: df_location.location.fillna('no_location', inplace=True)
380/30:
# Write a function to display basic entity info:
def show_ents(doc):
    doc = nlp(doc)
    count_GPE = 0
    if doc.ents:
        for ent in doc.ents:
            if ent.label_ == 'GPE':
                #print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
                count_GPE = 1
                break
    return count_GPE
380/31: df_location['GPE_loc_flag'] = df_location.location.apply(show_ents)
380/32: df_location['GPE_text_flag'] = df_location.text.apply(show_ents)
380/33: df_location['GPE_flag'] = df_location.GPE_loc_flag | df_location.GPE_text_flag
380/34:
f, axes = plt.subplots(1, 3, figsize=(20,6))
sns.countplot(x='GPE_loc_flag', hue='target', data=df_location, ax=axes[0])
sns.countplot(x='GPE_text_flag', hue='target', data=df_location, ax=axes[1])
sns.countplot(x='GPE_flag', hue='target', data=df_location, ax=axes[2])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')
axes[2].set_title('GPE')

#plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
axes[0].legend(labels=['0', '1'])
axes[1].legend(labels=['0', '1'])
axes[2].legend(labels=['0', '1'])

plt.show()
380/35:
# Looks like the location extracted from the tweet will be more informative than the original location feature.
df_location.iloc[:, -4:].corr()
380/36: df_punct_fe = df_train.loc[:, ['id', 'target', 'text']]
380/37:
#string.punctuation = '!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
def count_punctuations(text):
    d=dict()
    for i in string.punctuation:
        d[str(i)+' count']=text.count(i)
    return d
380/38:
df_punct_fe['punct_count'] = df_punct_fe["text"].apply(lambda x:count_punctuations(x))
df_punct= pd.DataFrame(list(df_punct_fe.punct_count))
merge_df = pd.merge(df_punct_fe.target,df_punct,left_index=True, right_index=True)
380/39:
merge_df_gr = merge_df.groupby('target').sum().reset_index()
merge_df_gr = merge_df_gr.T
merge_df_gr.columns = ['target_0', 'target_1']
380/40: merge_df_gr
380/41: merge_df_gr['sum_0_1'] = merge_df_gr['target_0'] + merge_df_gr['target_1']
380/42: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
380/43: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
380/44:
def punct_add_exclamation(text):
    return len(re.findall("!", text))
380/45:
def punct_add_question(text):
    return len(re.findall("\?", text))
380/46:
def punct_add_quotation(text):
    return len(re.findall("'", text))
380/47:
df_punct_fe['count_exclamation'] = df_punct_fe.text.apply(punct_add_exclamation)
df_punct_fe['count_question'] = df_punct_fe.text.apply(punct_add_question)
df_punct_fe['count_quotation'] = df_punct_fe.text.apply(punct_add_quotation)
380/48: new_cols = ['count_exclamation', 'count_question', 'count_quotation']
380/49: df_punct_fe.head(2)
380/50:
# count number of characters 
def count_chars(text):
    return len(text)

# count number of words 
def count_words(text):
    return len(text.split())

# count number of capital characters
def count_capital_chars(text):
    count=0
    for i in text:
        if i.isupper():
            count+=1
    return count

# count number of capital words
def count_capital_words(text):
    return sum(map(str.isupper,text.split()))

# count number of punctuations
def count_punctuations(text):
    count = lambda l1,l2: sum([1 for x in l1 if x in l2])
    return count(text,set(string.punctuation))

# count number of words in quotes
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    string_wothout_contractions = string_wothout_contractions.replace('"', '\'')
    x = re.findall("\'(.+?)\'", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
    
# count number of sentences
def count_sent(text):
    return len(nltk.sent_tokenize(text))

# count number of unique words 
def count_unique_words(text):
    return len(set(text.split()))
    
# count of hashtags
def count_htags(text):
    x = re.findall(r'(\#\w[A-Za-z0-9]*)', text)
    return len(x)

# count of mentions
def count_mentions(text):
    x = re.findall(r'(\@\w[A-Za-z0-9]*)', text)
    return len(x)

# count of stopwords
def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)

#count of links
def count_links(text):
    link_list = re.findall(r'(https?://[^\s]+)', text)
    bitly_list = re.findall(r'bit.ly/\S+', text)
    return (len(link_list + bitly_list))

#count digits
def count_digits(text):
    digit_list = re.findall(r'[0-9]+', text)
    return len(digit_list)
380/51:
def feature_eng_fun(df, text):
    df['count_exclamation'] = df_train[text].apply(punct_add_exclamation)
    df['count_question'] = df[text].apply(punct_add_question)
    df['count_quotation'] = df[text].apply(punct_add_quotation)
    df['count_num_of_chars'] = df[text].apply(count_chars)
    df['count_num_of_words'] = df[text].apply(count_words)
    df['count_num_of_capital_chars'] = df[text].apply(count_capital_chars)
    df['count_num_of_capital_words'] = df[text].apply(count_capital_words)
    df['count_punct'] = df[text].apply(count_punctuations)
    df['count_words_in_quotes'] = df[text].apply(count_words_in_quotes)
    df['count_sent'] = df[text].apply(count_sent)
    df['count_unique_words'] = df[text].apply(count_unique_words)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_mentions'] = df[text].apply(count_mentions)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_stopwords'] = df[text].apply(count_stopwords)
    df['count_links'] = df[text].apply(count_links) 
    ## This function from part above
    df['GPE_text_flag'] = df[text].apply(show_ents)    
    return df
380/52:
## .loc will create a new copy, not view
df_feature_eng = df_train.loc[:, ('id', 'text', 'target')]
380/53: df_feature_eng = feature_eng_fun(df_feature_eng, 'text')
380/54: df_feature_eng.drop(['text'], axis=1, inplace=True)
380/55:
plt.figure(figsize=(13,10))
sns.heatmap(df_feature_eng.corr(),annot=True)
380/56:
plt.figure(figsize=(14,10))
sns.heatmap(df_feature_eng.corr(),annot=True)
380/57:
plt.figure(figsize=(14,10))
sns.heatmap(df_feature_eng.corr(),annot=True)
380/58: cols_selected = np.abs(df_feature_eng.corr()['target']).sort_values(ascending=False)[1:11].index.to_list()
380/59: df_feature_eng[cols_selected].head(3)
380/60: df_feature_eng[cols_selected].head(3)
380/61:
# Let's look at some tweets more carefully
df_train[55:60]
380/62: stopwords
380/63:
#remove urls
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

def remove_urls(text):
    text = re.sub(r'https?://\S+', '', text)
    return text

#remove @word 
def remove_social_media_tags(text):
    text = re.sub(r'@\S+', '', text)
    return text

#contractions and stopwords removing
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (stopwords):
            expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

#remove digits
def remove_digits(text):
    text = re.sub(r'[0-9]+', '', text)
    return text   

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()
380/64:
def basic_cleaning(df, column_name):
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_social_media_tags)
    df[column_name] = df[column_name].apply(string_contractions)    
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(remove_digits)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(convert_lower_case)
    return df
380/65: df_text = df_train.loc[:, ['id', 'target', 'keyword', 'text']]
380/66: df_text
380/67: df_text = basic_cleaning(df_text, 'text')
380/68: stop = stopwords.words('english')
380/69: stop
380/70:
#remove urls
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

def remove_urls(text):
    text = re.sub(r'https?://\S+', '', text)
    return text

#remove @word 
def remove_social_media_tags(text):
    text = re.sub(r'@\S+', '', text)
    return text

#contractions and stopwords removing
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (stopwords.words('english')):
            expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

#remove digits
def remove_digits(text):
    text = re.sub(r'[0-9]+', '', text)
    return text   

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()
380/71:
def basic_cleaning(df, column_name):
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_social_media_tags)
    df[column_name] = df[column_name].apply(string_contractions)    
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(remove_digits)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(convert_lower_case)
    return df
380/72: df_text = df_train.loc[:, ['id', 'target', 'keyword', 'text']]
380/73: df_text = basic_cleaning(df_text, 'text')
380/74: df_text
380/75: df_train
380/76: df_text
380/77: df_text
380/78: all_text = ' '.join(df_text['text'].tolist())
380/79: all_text
380/80:
oov_list = []
def search_oov(text):
    tokens = nlp(text)
    for token in tokens:
        if token.is_oov:
            oov_list.append(token)
380/81: search_oov(all_text)
380/82: oov_list
380/83:
counter = Counter(oov_list)
counter.most_common(10)
380/84: search_oov.index('beyhive')
380/85: oov_list.index('beyhive')
380/86: oov_list
380/87: oov_list.dtype
380/88: oov_list[00.dtype
380/89: oov_list[0].dtype
380/90:
oov_list = []
def search_oov(text):
    tokens = nlp(text)
    for token in tokens:
        if token.is_oov:
            oov_list.append(token.astype(str))
380/91: search_oov(all_text)
380/92: oov_list[0].dtype
380/93: oov_list
380/94:
oov_list = []
def search_oov(text):
    tokens = nlp(text)
    for token in tokens:
        if token.is_oov:
            oov_list.append(token)
380/95: search_oov(all_text)
380/96:
import nltk
#nltk.download()
from nltk.stem import WordNetLemmatizer
from nltk.tokenize.treebank import TreebankWordDetokenizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer

import emoji as emj
import spacy 
## It's important to use _lg for OOV in future
nlp = spacy.load('en_core_web_lg')
380/97: TreebankWordDetokenizer().detokenize(['the', 'quick', 'brown'])
380/98: TreebankWordDetokenizer().detokenize(oov_list)
380/99: oov_list
380/100: oov_list
380/101: oov_list[0]
380/102: oov_list[0].dtype
380/103: oov_list_strings  = [i.text for i in oov_list]
380/104: oov_list_strings
380/105:
counter = Counter(oov_list_strings)
counter.most_common(10)
380/106: sid = SentimentIntensityAnalyzer()
380/107: df_sentiment = df_train.loc[:, ['id', 'target', 'text']]
380/108: df_sentiment['sentiment_compound']  = df_sentiment.text.apply(lambda tweet: sid.polarity_scores(tweet)['compound'])
380/109: df_sentiment['sentiment_compound_score'] = df_sentiment['sentiment_compound'].apply(lambda c: 'pos' if c >=0 else 'neg')
380/110:
plt.figure(figsize=[4, 4])
sns.countplot(x='target', hue='sentiment_compound_score', data=df_sentiment)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
380/111:
plt.figure(figsize=[4, 4])
sns.countplot(x='target', hue='sentiment_compound_score', data=df_sentiment)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
380/112: df_train
380/113: import copy
380/114: df = copy.deepcopy(df_train)
380/115: df["con_text"] = df.keyword.map(str) + " " + df.text
380/116: df
380/117: basic_cleaning = basic_cleaning(df, 'text')
380/118: basic_cleaning
380/119: df = copy.deepcopy(df_train)
380/120: df["con_text"] = df.keyword.map(str) + " " + df.text
380/121: basic_cleaning = basic_cleaning(df, 'con_text')
380/122: basic_cleaning = basic_cleaning(df, 'con_text')
380/123: basic_cleaning
380/124: df_clean = basic_cleaning(df, 'con_text')
380/125: import copy
380/126: df = copy.deepcopy(df_train)
380/127: df["con_text"] = df.keyword.map(str) + " " + df.text
380/128: df
380/129: df_clean = basic_cleaning(df, 'con_text')
380/130: basic_cleaning(df, 'con_text')
380/131: df
380/132: df = copy.deepcopy(df_train)
380/133: df
380/134: basic_cleaning(df, 'text')
380/135: basic_cleaning = basic_cleaning(df, 'text')
381/1:
import pandas as pd
import numpy as np
import re
from collections import Counter
import string
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline
381/2: pd.set_option('max_colwidth', None)
381/3:
import matplotlib.pyplot as plt
import seaborn as sns
381/4:
import nltk
#nltk.download()
from nltk.stem import WordNetLemmatizer
from nltk.tokenize.treebank import TreebankWordDetokenizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer

import emoji as emj
import spacy 
## It's important to use _lg for OOV in future
nlp = spacy.load('en_core_web_lg')
381/5:
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, roc_curve
#from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
#from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
from sklearn.metrics import f1_score
381/6:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
381/7:
nltk.download()
#nltk.download('stopwords')
381/8: STOPWORDS = set(stopwords.words('english'))
381/9:
#import nltk
#nltk.download('wordnet')
381/10:
#import nltk
#dler = nltk.downloader.Downloader()
#dler._update_index()
#dler.download('all')
381/11: wnl = WordNetLemmatizer()
381/12: df_train.head()
381/13: df_train.isnull().mean()
381/14: df_train.shape, df_test.shape
381/15:
#cardinality check
for col in df_train.columns:
    print('{} has {} unique instances'.format(col, len(df_train[col].unique())))
381/16: df_train.info()
381/17: df_train.keyword.value_counts()
381/18:
plt.figure(figsize=[8, 40])
sns.countplot(y='keyword',
              data=df_train,
              color='lightblue',
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
381/19:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword', 
              hue='target', 
              data=df_train, 
              palette=['lightblue', 'red']
             )
plt.xticks(rotation=90)
plt.legend(loc='upper right')
plt.title("Distribution of Target per Keyword ")
plt.show()
381/20: df_train.keyword = df_train.keyword.str.replace('%20', ' ')
381/21:
# Let's show most common keywords for target = 1
df_train[df_train.target == 1].keyword.value_counts()[:10]
381/22:
# Let's show most common keywords for target = 1
df_train[df_train.target == 0].keyword.value_counts()[:10]
381/23:
df_train_group = df_train.groupby(['keyword', 'target'])['id'].count().unstack(fill_value=0).reset_index()
df_train_group.columns = ['keyword', 'target_0', 'target_1']
df_train_group['prob_real_disasters'] = df_train_group.target_1/(df_train_group.target_0 + df_train_group.target_1)
381/24: df_train_group.sort_values('prob_real_disasters', ascending=False)
381/25: df_train.keyword.nunique()
381/26: df_train.target.value_counts()/len(df_train)
381/27: df_train.location.nunique()
381/28: df_location = df_train.loc[:, ['id', 'target', 'location', 'text']]
381/29: df_location.location.fillna('no_location', inplace=True)
381/30:
# Write a function to display basic entity info:
def show_ents(doc):
    doc = nlp(doc)
    count_GPE = 0
    if doc.ents:
        for ent in doc.ents:
            if ent.label_ == 'GPE':
                #print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
                count_GPE = 1
                break
    return count_GPE
381/31: df_location['GPE_loc_flag'] = df_location.location.apply(show_ents)
381/32: df_location['GPE_text_flag'] = df_location.text.apply(show_ents)
381/33: df_location['GPE_flag'] = df_location.GPE_loc_flag | df_location.GPE_text_flag
381/34:
f, axes = plt.subplots(1, 3, figsize=(20,6))
sns.countplot(x='GPE_loc_flag', hue='target', data=df_location, ax=axes[0])
sns.countplot(x='GPE_text_flag', hue='target', data=df_location, ax=axes[1])
sns.countplot(x='GPE_flag', hue='target', data=df_location, ax=axes[2])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')
axes[2].set_title('GPE')

#plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
axes[0].legend(labels=['0', '1'])
axes[1].legend(labels=['0', '1'])
axes[2].legend(labels=['0', '1'])

plt.show()
381/35:
# Looks like the location extracted from the tweet will be more informative than the original location feature.
df_location.iloc[:, -4:].corr()
381/36: df_punct_fe = df_train.loc[:, ['id', 'target', 'text']]
381/37:
#string.punctuation = '!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
def count_punctuations(text):
    d=dict()
    for i in string.punctuation:
        d[str(i)+' count']=text.count(i)
    return d
381/38:
df_punct_fe['punct_count'] = df_punct_fe["text"].apply(lambda x:count_punctuations(x))
df_punct= pd.DataFrame(list(df_punct_fe.punct_count))
merge_df = pd.merge(df_punct_fe.target,df_punct,left_index=True, right_index=True)
381/39:
merge_df_gr = merge_df.groupby('target').sum().reset_index()
merge_df_gr = merge_df_gr.T
merge_df_gr.columns = ['target_0', 'target_1']
381/40: merge_df_gr
381/41: merge_df_gr['sum_0_1'] = merge_df_gr['target_0'] + merge_df_gr['target_1']
381/42: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
381/43: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
381/44:
def punct_add_exclamation(text):
    return len(re.findall("!", text))
381/45:
def punct_add_question(text):
    return len(re.findall("\?", text))
381/46:
def punct_add_quotation(text):
    return len(re.findall("'", text))
381/47:
df_punct_fe['count_exclamation'] = df_punct_fe.text.apply(punct_add_exclamation)
df_punct_fe['count_question'] = df_punct_fe.text.apply(punct_add_question)
df_punct_fe['count_quotation'] = df_punct_fe.text.apply(punct_add_quotation)
381/48: new_cols = ['count_exclamation', 'count_question', 'count_quotation']
381/49: df_punct_fe.head(2)
381/50:
# count number of characters 
def count_chars(text):
    return len(text)

# count number of words 
def count_words(text):
    return len(text.split())

# count number of capital characters
def count_capital_chars(text):
    count=0
    for i in text:
        if i.isupper():
            count+=1
    return count

# count number of capital words
def count_capital_words(text):
    return sum(map(str.isupper,text.split()))

# count number of punctuations
def count_punctuations(text):
    count = lambda l1,l2: sum([1 for x in l1 if x in l2])
    return count(text,set(string.punctuation))

# count number of words in quotes
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    string_wothout_contractions = string_wothout_contractions.replace('"', '\'')
    x = re.findall("\'(.+?)\'", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
    
# count number of sentences
def count_sent(text):
    return len(nltk.sent_tokenize(text))

# count number of unique words 
def count_unique_words(text):
    return len(set(text.split()))
    
# count of hashtags
def count_htags(text):
    x = re.findall(r'(\#\w[A-Za-z0-9]*)', text)
    return len(x)

# count of mentions
def count_mentions(text):
    x = re.findall(r'(\@\w[A-Za-z0-9]*)', text)
    return len(x)

# count of stopwords
def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)

#count of links
def count_links(text):
    link_list = re.findall(r'(https?://[^\s]+)', text)
    bitly_list = re.findall(r'bit.ly/\S+', text)
    return (len(link_list + bitly_list))

#count digits
def count_digits(text):
    digit_list = re.findall(r'[0-9]+', text)
    return len(digit_list)
381/51:
def feature_eng_fun(df, text):
    df['count_exclamation'] = df_train[text].apply(punct_add_exclamation)
    df['count_question'] = df[text].apply(punct_add_question)
    df['count_quotation'] = df[text].apply(punct_add_quotation)
    df['count_num_of_chars'] = df[text].apply(count_chars)
    df['count_num_of_words'] = df[text].apply(count_words)
    df['count_num_of_capital_chars'] = df[text].apply(count_capital_chars)
    df['count_num_of_capital_words'] = df[text].apply(count_capital_words)
    df['count_punct'] = df[text].apply(count_punctuations)
    df['count_words_in_quotes'] = df[text].apply(count_words_in_quotes)
    df['count_sent'] = df[text].apply(count_sent)
    df['count_unique_words'] = df[text].apply(count_unique_words)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_mentions'] = df[text].apply(count_mentions)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_stopwords'] = df[text].apply(count_stopwords)
    df['count_links'] = df[text].apply(count_links) 
    ## This function from part above
    df['GPE_text_flag'] = df[text].apply(show_ents)    
    return df
381/52:
## .loc will create a new copy, not view
df_feature_eng = df_train.loc[:, ('id', 'text', 'target')]
381/53: df_feature_eng = feature_eng_fun(df_feature_eng, 'text')
381/54: df_feature_eng.drop(['text'], axis=1, inplace=True)
381/55:
plt.figure(figsize=(14,10))
sns.heatmap(df_feature_eng.corr(),annot=True)
381/56: cols_selected = np.abs(df_feature_eng.corr()['target']).sort_values(ascending=False)[1:11].index.to_list()
381/57: df_feature_eng[cols_selected].head(3)
381/58:
# Let's look at some tweets more carefully
df_train[55:60]
381/59: stop = stopwords.words('english')
381/60:
#remove urls
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

def remove_urls(text):
    text = re.sub(r'https?://\S+', '', text)
    return text

#remove @word 
def remove_social_media_tags(text):
    text = re.sub(r'@\S+', '', text)
    return text

#contractions and stopwords removing
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (stopwords.words('english')):
            expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

#remove digits
def remove_digits(text):
    text = re.sub(r'[0-9]+', '', text)
    return text   

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()
381/61:
def basic_cleaning(df, column_name):
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_social_media_tags)
    df[column_name] = df[column_name].apply(string_contractions)    
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(remove_digits)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(convert_lower_case)
    return df
381/62: df_text = df_train.loc[:, ['id', 'target', 'keyword', 'text']]
381/63: df_text = basic_cleaning(df_text, 'text')
381/64: df_text
381/65: all_text = ' '.join(df_text['text'].tolist())
381/66:
oov_list = []
def search_oov(text):
    tokens = nlp(text)
    for token in tokens:
        if token.is_oov:
            oov_list.append(token)
381/67: search_oov(all_text)
381/68: oov_list_strings  = [i.text for i in oov_list]
381/69:
counter = Counter(oov_list_strings)
counter.most_common(10)
381/70: sid = SentimentIntensityAnalyzer()
381/71: df_sentiment = df_train.loc[:, ['id', 'target', 'text']]
381/72: df_sentiment['sentiment_compound']  = df_sentiment.text.apply(lambda tweet: sid.polarity_scores(tweet)['compound'])
381/73: df_sentiment['sentiment_compound_score'] = df_sentiment['sentiment_compound'].apply(lambda c: 'pos' if c >=0 else 'neg')
381/74:
plt.figure(figsize=[4, 4])
sns.countplot(x='target', hue='sentiment_compound_score', data=df_sentiment)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
381/75: #not very optimistic result, I won't add a sentiment feature :)
381/76: ## CREATE A MODEL BASED ON TEXT
381/77: df = copy.deepcopy(df_train)
381/78:
import pandas as pd
import numpy as np
import re
import copy
from collections import Counter
import string
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline
381/79: df = copy.deepcopy(df_train)
381/80: basic_cleaning = basic_cleaning(df, 'text')
381/81: df["con_text"] = df.keyword.map(str) + " " + df.text
381/82: df
381/83:
def best_tresholds(y_pred_proba):
    # define thresholds
    thresholds = np.arange(0, 1, 0.001)
    # evaluate each threshold
    scores = [f1_score(y_test, to_labels(probs, t)) for t in thresholds]
    # get best threshold
    ix = np.argmax(scores)
    print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
381/84: df = copy.deepcopy(df_train)
381/85:
def best_tresholds(y_pred_proba):
    # define thresholds
    thresholds = np.arange(0, 1, 0.001)
    # evaluate each threshold
    scores = [f1_score(y_test, to_labels(probs, t)) for t in thresholds]
    # get best threshold
    ix = np.argmax(scores)
    print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
381/86:
## split to test and train
X = df_text['con_text']
y = df_text['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
381/87: df = copy.deepcopy(df_train)
381/88:
def to_labels(pos_probs, threshold):
    return (pos_probs >= threshold).astype('int')

def best_tresholds(y_pred_proba):
    # define thresholds
    thresholds = np.arange(0, 1, 0.001)
    # evaluate each threshold
    scores = [f1_score(y_test, to_labels(probs, t)) for t in thresholds]
    # get best threshold
    ix = np.argmax(scores)
    print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
381/89:
basic_cleaning = basic_cleaning(df, 'text')
df["con_text"] = df.keyword.map(str) + " " + df.text
381/90:
## split to test and train
X = df_text['con_text']
y = df_text['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
381/91:
def basic_cleaning(df, column_name):
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_social_media_tags)
    df[column_name] = df[column_name].apply(string_contractions)    
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(remove_digits)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(convert_lower_case)
    return df
381/92: df = copy.deepcopy(df_train)
381/93:
df = basic_cleaning(df, 'text')
df["con_text"] = df.keyword.map(str) + " " + df.text
381/94: df
381/95:
## split to test and train
X = df_text['con_text']
y = df_text['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
381/96:
#remove urls
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

def remove_urls(text):
    text = re.sub(r'https?://\S+', '', text)
    return text

#remove @word 
def remove_social_media_tags(text):
    text = re.sub(r'@\S+', '', text)
    return text

#contractions and stopwords removing
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

#remove digits
def remove_digits(text):
    text = re.sub(r'[0-9]+', '', text)
    return text   

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()
381/97:
def basic_cleaning(df, column_name):
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_social_media_tags)
    df[column_name] = df[column_name].apply(string_contractions)    
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(remove_digits)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(convert_lower_case)
    return df
381/98: df = copy.deepcopy(df_train)
381/99:
df_text_clean = basic_cleaning(df, 'text')
df["con_text"] = df.keyword.map(str) + " " + df.text
381/100: df_text_clean
381/101: STOPWORDS
381/102: 'the a dc'.apply(string_contractions)
381/103: string_contractions('the a dkd')
381/104: string_contractions('the a dkd The')
381/105:
#remove urls
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

def remove_urls(text):
    text = re.sub(r'https?://\S+', '', text)
    return text

#remove @word 
def remove_social_media_tags(text):
    text = re.sub(r'@\S+', '', text)
    return text

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

#remove digits
def remove_digits(text):
    text = re.sub(r'[0-9]+', '', text)
    return text   

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()

#contractions and stopwords removing
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
381/106: string_contractions('the a dkd The')
381/107:
def basic_cleaning(df, column_name):
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_social_media_tags)    
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(remove_digits)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(string_contractions)
    return df
381/108:
#remove urls
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+', '', text)
    return text

#remove @word 
def remove_social_media_tags(text):
    text = re.sub(r'@\S+', '', text)
    return text

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

#remove digits
def remove_digits(text):
    text = re.sub(r'[0-9]+', '', text)
    return text   

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()

#contractions and stopwords removing
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
381/109:
def basic_cleaning(df, column_name):
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_social_media_tags)    
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(remove_digits)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(string_contractions)
    return df
381/110: df = copy.deepcopy(df_train)
381/111:
def to_labels(pos_probs, threshold):
    return (pos_probs >= threshold).astype('int')

def best_tresholds(y_pred_proba):
    # define thresholds
    thresholds = np.arange(0, 1, 0.001)
    # evaluate each threshold
    scores = [f1_score(y_test, to_labels(probs, t)) for t in thresholds]
    # get best threshold
    ix = np.argmax(scores)
    print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
381/112:
def to_labels(pos_probs, threshold):
    return (pos_probs >= threshold).astype('int')

def best_tresholds(y_pred_proba):
    # define thresholds
    thresholds = np.arange(0, 1, 0.001)
    # evaluate each threshold
    scores = [f1_score(y_test, to_labels(probs, t)) for t in thresholds]
    # get best threshold
    ix = np.argmax(scores)
    print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
381/113:
df_text_clean = basic_cleaning(df, 'text')
df_text_clean["con_text"] = df_text_clean.keyword.map(str) + " " + df_text_clean.text
381/114: df_text_clean
381/115:
## split to test and train
X = df_text_clean['con_text']
y = df_text_clean['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
381/116:
vectorizer = TfidfVectorizer()
train_tf_idf = vectorizer.fit_transform(X_train['con_text']).toarray()
train_tf_idf= pd.DataFrame(train_tf_idf)
381/117: X_train
381/118:
vectorizer = TfidfVectorizer()
train_tf_idf = vectorizer.fit_transform(X_train).toarray()
train_tf_idf= pd.DataFrame(train_tf_idf)
381/119: train_tf_idf
381/120: vectorizer.transform(X_test)
381/121:
vectorizer = TfidfVectorizer()
train_tf_idf = vectorizer.fit_transform(X_train).toarray()
train_tf_idf = pd.DataFrame(train_tf_idf)
print('Train shape:', train_tf_idf.shape)
test_tf_idf = vectorizer.transform(X_test)
train_tf_idf = pd.DataFrame(test_tf_idf)
print('Test shape:', test_tf_idf.shape)
381/122:
mnb = MultinomialNB()
mnb.fit(X_train, y_train)
y_test_pred_proba = mnb.predict_proba(X_test)
381/123:
mnb = MultinomialNB()
mnb.fit(X_train, y_train)
y_test_pred_proba = mnb.predict_proba(train_tf_idf)
381/124:
vectorizer = TfidfVectorizer()
train_tf_idf = vectorizer.fit_transform(X_train).toarray()
train_tf_idf = pd.DataFrame(train_tf_idf)
print('Train shape:', train_tf_idf.shape)
test_tf_idf = vectorizer.transform(X_test)
test_tf_idf = pd.DataFrame(test_tf_idf)
print('Test shape:', test_tf_idf.shape)
381/125:
## split to test and train
X = df_text_clean['con_text']
y = df_text_clean['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
381/126:
vectorizer = TfidfVectorizer()
train_tf_idf = vectorizer.fit_transform(X_train).toarray()
train_tf_idf = pd.DataFrame(train_tf_idf)
print('Train shape:', train_tf_idf.shape)
test_tf_idf = vectorizer.transform(X_test)
test_tf_idf = pd.DataFrame(test_tf_idf)
print('Test shape:', test_tf_idf.shape)
381/127:
vectorizer = TfidfVectorizer()
train_tf_idf = vectorizer.fit_transform(X_train).toarray()
train_tf_idf = pd.DataFrame(train_tf_idf)
print('Train shape:', train_tf_idf.shape)
test_tf_idf = vectorizer.transform(X_test)
#test_tf_idf = pd.DataFrame(test_tf_idf)
print('Test shape:', test_tf_idf.shape)
381/128: test_tf_idf
381/129: pd.DataFrame(test_tf_idf)
381/130:
## split to test and train
X = df_text_clean['con_text']
y = df_text_clean['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
381/131: pd.DataFrame(test_tf_idf)
381/132:
## split to test and train
X = df_text_clean['con_text']
y = df_text_clean['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print('X_train shape', X_train.shape)
print('X_test shape', X_test.shape)
381/133: X_test
381/134:
vectorizer = TfidfVectorizer()
train_tf_idf = vectorizer.fit_transform(X_train).toarray()
train_tf_idf = pd.DataFrame(train_tf_idf)
print('Train shape:', train_tf_idf.shape)
test_tf_idf = vectorizer.transform(X_test)
#test_tf_idf = pd.DataFrame(test_tf_idf)
print('Test shape:', test_tf_idf.shape)
381/135: pd.DataFrame(test_tf_idf)
381/136: train_tf_idf
381/137:
vectorizer = TfidfVectorizer()
train_tf_idf = vectorizer.fit_transform(X_train).toarray()
train_tf_idf = pd.DataFrame(train_tf_idf)
print('Train shape:', train_tf_idf.shape)
test_tf_idf = vectorizer.transform(X_test).toarray()
test_tf_idf = pd.DataFrame(test_tf_idf)
print('Test shape:', test_tf_idf.shape)
381/138: test_tf_idf
381/139:
mnb = MultinomialNB()
mnb.fit(X_train, y_train)
y_test_pred_proba = mnb.predict_proba(train_tf_idf)
381/140:
mnb = MultinomialNB()
mnb.fit(train_tf_idf, y_train)
y_test_pred_proba = mnb.predict_proba(train_tf_idf)
381/141: best_tresholds(y_test_pred_proba[:,1])
381/142:
def to_labels(pos_probs, threshold):
    return (pos_probs >= threshold).astype('int')

def best_tresholds(y_pred_proba):
    # define thresholds
    thresholds = np.arange(0, 1, 0.001)
    # evaluate each threshold
    scores = [f1_score(y_test, to_labels(y_pred_proba, t)) for t in thresholds]
    # get best threshold
    ix = np.argmax(scores)
    print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
381/143: best_tresholds(y_test_pred_proba[:,1])
381/144:
def to_labels(pos_probs, threshold):
    return (pos_probs >= threshold).astype('int')

def best_tresholds(y_pred_proba, y_test):
    # define thresholds
    thresholds = np.arange(0, 1, 0.001)
    # evaluate each threshold
    scores = [f1_score(y_test, to_labels(y_pred_proba, t)) for t in thresholds]
    # get best threshold
    ix = np.argmax(scores)
    print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
381/145: best_tresholds(y_test_pred_proba[:,1], y_test)
381/146:
yhat = mnb.predict_proba(X_test)
# keep probabilities for the positive outcome only
probs = yhat[:, 1]
# define thresholds
thresholds = np.arange(0, 1, 0.001)
# evaluate each threshold
scores = [f1_score(y_test, to_labels(probs, t)) for t in thresholds]
# get best threshold
ix = np.argmax(scores)
print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
381/147:
yhat = mnb.predict_proba(test_tf_idf)
# keep probabilities for the positive outcome only
probs = yhat[:, 1]
# define thresholds
thresholds = np.arange(0, 1, 0.001)
# evaluate each threshold
scores = [f1_score(y_test, to_labels(probs, t)) for t in thresholds]
# get best threshold
ix = np.argmax(scores)
print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
381/148:
mnb = MultinomialNB()
mnb.fit(train_tf_idf, y_train)
y_test_pred_proba = mnb.predict_proba(train_tf_idf)[:,1]
381/149: best_tresholds(y_test_pred_proba, y_test)
381/150: y_test_pred_proba
381/151:
def to_labels(pos_probs, threshold):
    return (pos_probs >= threshold).astype('int')

def best_tresholds(y_pred_proba, y_test):
    # define thresholds
    thresholds = np.arange(0, 1, 0.001)
    # evaluate each threshold
    scores = [f1_score(y_test, to_labels(y_pred_proba, t)) for t in thresholds]
    # get best threshold
    ix = np.argmax(scores)
    print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
381/152: best_tresholds(y_test_pred_proba, y_test)
381/153:
mnb = MultinomialNB()
mnb.fit(train_tf_idf, y_train)
y_test_pred_proba = mnb.predict_proba(test_tf_idf)[:,1]
381/154: best_tresholds(y_test_pred_proba, y_test)
381/155:
vectorizer = TfidfVectorizer(max_features=5000)
train_tf_idf = vectorizer.fit_transform(X_train).toarray()
train_tf_idf = pd.DataFrame(train_tf_idf)
print('Train shape:', train_tf_idf.shape)
test_tf_idf = vectorizer.transform(X_test).toarray()
test_tf_idf = pd.DataFrame(test_tf_idf)
print('Test shape:', test_tf_idf.shape)
381/156:
mnb = MultinomialNB()
mnb.fit(train_tf_idf, y_train)
y_test_pred_proba = mnb.predict_proba(test_tf_idf)[:,1]
381/157: best_tresholds(y_test_pred_proba, y_test)
381/158:
vectorizer = TfidfVectorizer(max_features=30000)
train_tf_idf = vectorizer.fit_transform(X_train).toarray()
train_tf_idf = pd.DataFrame(train_tf_idf)
print('Train shape:', train_tf_idf.shape)
test_tf_idf = vectorizer.transform(X_test).toarray()
test_tf_idf = pd.DataFrame(test_tf_idf)
print('Test shape:', test_tf_idf.shape)
381/159:
mnb = MultinomialNB()
mnb.fit(train_tf_idf, y_train)
y_test_pred_proba = mnb.predict_proba(test_tf_idf)[:,1]
381/160: best_tresholds(y_test_pred_proba, y_test)
381/161:
vectorizer = TfidfVectorizer(max_features=10000)
train_tf_idf = vectorizer.fit_transform(X_train).toarray()
train_tf_idf = pd.DataFrame(train_tf_idf)
print('Train shape:', train_tf_idf.shape)
test_tf_idf = vectorizer.transform(X_test).toarray()
test_tf_idf = pd.DataFrame(test_tf_idf)
print('Test shape:', test_tf_idf.shape)
381/162:
mnb = MultinomialNB()
mnb.fit(train_tf_idf, y_train)
y_test_pred_proba = mnb.predict_proba(test_tf_idf)[:,1]
381/163: best_tresholds(y_test_pred_proba, y_test)
381/164: best_tresholds(y_test_pred_proba, y_test)
381/165:
vectorizer = TfidfVectorizer()
train_tf_idf = vectorizer.fit_transform(X_train).toarray()
train_tf_idf = pd.DataFrame(train_tf_idf)
print('Train shape:', train_tf_idf.shape)
test_tf_idf = vectorizer.transform(X_test).toarray()
test_tf_idf = pd.DataFrame(test_tf_idf)
print('Test shape:', test_tf_idf.shape)
381/166:
mnb = MultinomialNB()
mnb.fit(train_tf_idf, y_train)
y_test_pred_proba = mnb.predict_proba(test_tf_idf)[:,1]
381/167: best_tresholds(y_test_pred_proba, y_test)
381/168:
df = copy.deepcopy(df_train)
df_text_clean = basic_cleaning(df, 'text')
df_text_clean["con_text"] = df_text_clean.keyword.map(str) + " " + df_text_clean.text

X = df_text_clean['con_text']
y = df_text_clean['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print('X_train shape', X_train.shape)
print('X_test shape', X_test.shape)
381/169: feature_eng_fun(df, 'text')
381/170: df_feature_eng = feature_eng_fun(df, 'text')
381/171: df_feature_eng
381/172: df_feature_eng[cols_selected
381/173: df_feature_eng[cols_selected]
381/174:
df_feature_eng = feature_eng_fun(df, 'text')
df_feature_eng = df_feature_eng[cols_selected]
381/175: df_feature_eng
381/176: cols_selected
381/177:
df = copy.deepcopy(df_train)
cols_selected = ['GPE_text_flag', 'count_links', 'count_num_of_chars', 'count_punct', 'count_sent',
                 'count_mentions', 'count_stopwords', 'count_question', 'count_quotation',
                 'count_num_of_capital_chars']
## new features
df_feature_eng = feature_eng_fun(df, 'text')
df_feature_eng = df_feature_eng[cols_selected]

df_text_clean = basic_cleaning(df, 'text')
df_text_clean["con_text"] = df_text_clean.keyword.map(str) + " " + df_text_clean.text
381/178: df_feature_eng
381/179: df_text_clean
381/180: df_train
381/181:
df = copy.deepcopy(df_train)
cols_selected = ['GPE_text_flag', 'count_links', 'count_num_of_chars', 'count_punct', 'count_sent',
                 'count_mentions', 'count_stopwords', 'count_question', 'count_quotation',
                 'count_num_of_capital_chars']
## new features
df_feature_eng = feature_eng_fun(df, 'text')
df_feature_eng = df_feature_eng[cols_selected]

df_text_clean = basic_cleaning(df, 'text')
df_text_clean["con_text"] = df_text_clean.keyword.map(str) + " " + df_text_clean.text
381/182: df_feature_eng
381/183: df_text_clean
381/184: basic_cleaning(df, 'text')
381/185:
df = copy.deepcopy(df_train)
cols_selected = ['GPE_text_flag', 'count_links', 'count_num_of_chars', 'count_punct', 'count_sent',
                 'count_mentions', 'count_stopwords', 'count_question', 'count_quotation',
                 'count_num_of_capital_chars']
## new features
df_feature_eng = feature_eng_fun(df, 'text')
df_feature_eng = df_feature_eng[cols_selected]

df_text_clean_fe = basic_cleaning(df, 'text')
df_text_clean_fe["con_text"] = df_text_clean_fe.keyword.map(str) + " " + df_text_clean_fe.text
381/186: df_text_clean_fe
381/187: df
381/188:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
381/189:
df = copy.deepcopy(df_train)
cols_selected = ['GPE_text_flag', 'count_links', 'count_num_of_chars', 'count_punct', 'count_sent',
                 'count_mentions', 'count_stopwords', 'count_question', 'count_quotation',
                 'count_num_of_capital_chars']
## new features
df_feature_eng = feature_eng_fun(df, 'text')
df_feature_eng = df_feature_eng[cols_selected]

df_text_clean_fe = basic_cleaning(df, 'text')
df_text_clean_fe["con_text"] = df_text_clean_fe.keyword.map(str) + " " + df_text_clean_fe.text
381/190: df_text_clean_fe
381/191: df_feature_eng
381/192: df
381/193:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
381/194: df
381/195: df_train
381/196: df = copy.deepcopy(df_train)
381/197: df
381/198: del df_feature_eng
381/199: del df_text_clean_fe
381/200: df_feature_eng
381/201: df
381/202:
cols_selected = ['GPE_text_flag', 'count_links', 'count_num_of_chars', 'count_punct', 'count_sent',
                 'count_mentions', 'count_stopwords', 'count_question', 'count_quotation',
                 'count_num_of_capital_chars']
## new features
df_feature_eng = feature_eng_fun(df, 'text')
df_feature_eng = df_feature_eng[cols_selected]

df_text_clean_fe = basic_cleaning(df, 'text')
df_text_clean_fe["con_text"] = df_text_clean_fe.keyword.map(str) + " " + df_text_clean_fe.text
381/203: df_text_clean_fe
381/204: df_feature_eng
381/205: df
381/206: df = copy.deepcopy(df_train)
381/207: df
381/208: basic_cleaning(df, 'text')
381/209: df_feature_eng
381/210: df
381/211: del df
381/212: df = copy.deepcopy(df_train)
381/213: df
381/214:
cols_selected = ['GPE_text_flag', 'count_links', 'count_num_of_chars', 'count_punct', 'count_sent',
                 'count_mentions', 'count_stopwords', 'count_question', 'count_quotation',
                 'count_num_of_capital_chars']
## new features
df_feature_eng = feature_eng_fun(df, 'text')
df_feature_eng = df_feature_eng[cols_selected]

df_text_clean_fe = basic_cleaning(df, 'text')
df_text_clean_fe["con_text"] = df_text_clean_fe.keyword.map(str) + " " + df_text_clean_fe.text
381/215: df_text_clean_fe
381/216: df
381/217: df = copy.deepcopy(df_train)
381/218: df
381/219: basic_cleaning(df, 'text')
381/220: df_text_clean_fe = basic_cleaning(df, 'text')
381/221: df_text_clean_fe
381/222: df_text_clean_fe["con_text"] = df_text_clean_fe.keyword.map(str) + " " + df_text_clean_fe.text
381/223: df_text_clean_fe
381/224: df = copy.deepcopy(df_train)
381/225:
cols_selected = ['GPE_text_flag', 'count_links', 'count_num_of_chars', 'count_punct', 'count_sent',
                 'count_mentions', 'count_stopwords', 'count_question', 'count_quotation',
                 'count_num_of_capital_chars']

df_text_clean_fe = basic_cleaning(df, 'text')
df_text_clean_fe["con_text"] = df_text_clean_fe.keyword.map(str) + " " + df_text_clean_fe.text

## new features
df_feature_eng = feature_eng_fun(df, 'text')
df_feature_eng = df_feature_eng[cols_selected]
381/226: df_text_clean_fe
381/227:
cols_selected = ['GPE_text_flag', 'count_links', 'count_num_of_chars', 'count_punct', 'count_sent',
                 'count_mentions', 'count_stopwords', 'count_question', 'count_quotation',
                 'count_num_of_capital_chars']
df = copy.deepcopy(df_train)
df_text_clean_fe = basic_cleaning(df, 'text')
df_text_clean_fe["con_text"] = df_text_clean_fe.keyword.map(str) + " " + df_text_clean_fe.text

## new features
df = copy.deepcopy(df_train)
df_feature_eng = feature_eng_fun(df, 'text')
df_feature_eng = df_feature_eng[cols_selected]
381/228:
cols_selected = ['GPE_text_flag', 'count_links', 'count_num_of_chars', 'count_punct', 'count_sent',
                 'count_mentions', 'count_stopwords', 'count_question', 'count_quotation',
                 'count_num_of_capital_chars']
df = copy.deepcopy(df_train)
df_text_clean_fe = basic_cleaning(df, 'text')
df_text_clean_fe["con_text"] = df_text_clean_fe.keyword.map(str) + " " + df_text_clean_fe.text

## new features
df = copy.deepcopy(df_train)
df_feature_eng = feature_eng_fun(df, 'text')
df_feature_eng = df_feature_eng[cols_selected]
381/229: df_text_clean_fe
381/230: df_feature_eng
381/231:
df_full_fe = pd.merge(df_text_clean_fe[['target', 'con_text']], df_feature_eng, 
                      left_index=True, right_index=True)
381/232: df_full_fe
381/233:
## split to test and train
y = df_full_fe['target']
X = df_full_fe.drop('target', axis=1, inplace=True)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print('X_train shape', X_train.shape)
print('X_test shape', X_test.shape)
381/234: X
381/235: y
381/236: X
381/237: df_full_fe
381/238:
df_full_fe = pd.merge(df_text_clean_fe[['target', 'con_text']], df_feature_eng, 
                      left_index=True, right_index=True)
381/239:
## split to test and train
y = df_full_fe['target']
X = df_full_fe.drop('target', axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print('X_train shape', X_train.shape)
print('X_test shape', X_test.shape)
381/240: X_train
381/241: X_train
381/242:
vectorizer = TfidfVectorizer()
train_tf_idf = vectorizer.fit_transform(X_train[['con_text']]).toarray()
train_tf_idf = pd.DataFrame(train_tf_idf)
print('Train shape:', train_tf_idf.shape)
test_tf_idf = vectorizer.transform(X_test[['con_text']]).toarray()
test_tf_idf = pd.DataFrame(test_tf_idf)
print('Test shape:', test_tf_idf.shape)
381/243: X_train
381/244: X_train[['con_text']]
381/245:
vectorizer = TfidfVectorizer()
train_tf_idf = vectorizer.fit_transform(X_train[['con_text']]).toarray()
train_tf_idf = pd.DataFrame(train_tf_idf)
print('Train shape:', train_tf_idf.shape)
test_tf_idf = vectorizer.transform(X_test[['con_text']]).toarray()
test_tf_idf = pd.DataFrame(test_tf_idf)
print('Test shape:', test_tf_idf.shape)
381/246:  vectorizer.fit_transform(X_train[['con_text']]).toarray()
381/247:  vectorizer.fit_transform(X_train[['con_text']])
381/248:
vectorizer = TfidfVectorizer()
train_tf_idf = vectorizer.fit_transform(X_train['con_text']).toarray()
train_tf_idf = pd.DataFrame(train_tf_idf)
print('Train shape:', train_tf_idf.shape)
test_tf_idf = vectorizer.transform(X_test['con_text']).toarray()
test_tf_idf = pd.DataFrame(test_tf_idf)
print('Test shape:', test_tf_idf.shape)
381/249: train_tf_idf
381/250: X_train
381/251:
scaler = MinMaxScaler()
df_train_fe_min_max[:] = scaler.fit_transform(X_train[cols_selected])
381/252: scaler.fit_transform(X_train[cols_selected])
381/253:
scaler = MinMaxScaler()
df_train_fe_min_max = scaler.fit_transform(X_train[cols_selected])
381/254: df_train_fe_min_max
381/255: pd.DataFrame(df_train_fe_min_max)
381/256: pd.DataFrame(df_train_fe_min_max).columns = cols_selected
381/257: df_train_fe_min_max
381/258:
scaler = MinMaxScaler()
df_train_fe_min_max = scaler.fit_transform(X_train[cols_selected])
pd.DataFrame(df_train_fe_min_max).columns = cols_selected
381/259: df_train_fe_min_max
381/260:
scaler = MinMaxScaler()
df_train_fe_min_max = scaler.fit_transform(X_train[cols_selected])
df_train_fe_min_max = pd.DataFrame(df_train_fe_min_max)
df_train_fe_min_max.columns = cols_selected
381/261: df_train_fe_min_max
381/262:
scaler = MinMaxScaler()
df_train_fe_min_max = scaler.fit_transform(X_train[cols_selected])
df_train_fe_min_max = pd.DataFrame(df_train_fe_min_max)
df_train_fe_min_max.columns = cols_selected

df_test_fe_min_max = scaler.transform(X_test[cols_selected])
381/263: df_test_fe_min_max
381/264:
scaler = MinMaxScaler()
df_train_fe_min_max = scaler.fit_transform(X_train[cols_selected])
df_train_fe_min_max = pd.DataFrame(df_train_fe_min_max)
df_train_fe_min_max.columns = cols_selected

df_test_fe_min_max = scaler.transform(X_test[cols_selected])
df_test_fe_min_max = pd.DataFrame(df_test_fe_min_max)
df_test_fe_min_max.columns = cols_selected
381/265: df_test_fe_min_max
381/266: test_tf_idf
381/267:
df_full_train_with_f = pd.merge(test_tf_idf, df_test_fe_min_max, 
                      left_index=True, right_index=True)
381/268: df_full_train_with_f
381/269:
df_full_train_with_f = pd.merge(train_tf_idf, df_train_fe_min_max, 
                      left_index=True, right_index=True)
381/270: df_full_train_with_f
381/271:
mnb = MultinomialNB()
mnb.fit(df_full_train_with_f, y_train)
y_test_pred_proba = mnb.predict_proba(df_full_test_with_f)[:,1]
381/272:
df_full_train_with_f = pd.merge(train_tf_idf, df_train_fe_min_max, 
                      left_index=True, right_index=True)
381/273:
df_full_test_with_f = pd.merge(test_tf_idf, df_test_fe_min_max, 
                      left_index=True, right_index=True)
381/274:
mnb = MultinomialNB()
mnb.fit(df_full_train_with_f, y_train)
y_test_pred_proba = mnb.predict_proba(df_full_test_with_f)[:,1]
381/275: best_tresholds(y_test_pred_proba, y_test)
381/276: df_t = df_test[['id']]
381/277: df_t['target'] = (y_test_pred[:, 1] >= 0.381).astype(int)
381/278: df_t['target'] = (y_test_pred_proba[:, 1] >= 0.381).astype(int)
381/279: y_test_pred_proba
381/280: df_t
381/281: df_t = X_test[['id']]
381/282: X_test
381/283: df_train
381/284: ### for training
381/285: df_test
381/286:
df_text_clean_fe_test = basic_cleaning(df_test, 'text')
df_text_clean_fe_test["con_text"] = df_text_clean_fe_test.keyword.map(str) + " " + df_text_clean_fe_test.text

## new features
df_feature_eng_test = feature_eng_fun(df_test, 'text')
df_feature_eng_test = df_feature_eng_test[cols_selected]
381/287: df_text_clean_fe_test
381/288: df_feature_eng_test
381/289: df_test
381/290:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
381/291: df_test
381/292:
df_text_clean_fe_test = basic_cleaning(df_test, 'text')
df_text_clean_fe_test["con_text"] = df_text_clean_fe_test.keyword.map(str) + " " + df_text_clean_fe_test.text
381/293: df_text_clean_fe_test
381/294:
## new features
df_feature_eng_test = feature_eng_fun(df_test, 'text')
df_feature_eng_test = df_feature_eng_test[cols_selected]
381/295: df_feature_eng_test
381/296:
df_full_test_fe = pd.merge(df_text_clean_fe_test[['con_text']], df_feature_eng_test, 
                           left_index=True, right_index=True)
381/297: df_full_test_fe
381/298: df_test
381/299:
df_test_tf_idf = vectorizer.transform(df_full_test_fe['con_text']).toarray()
df_test_tf_idf = pd.DataFrame(df_test_tf_idf)
print('Test shape:', df_test_tf_idf.shape)
381/300:
df_test_fe_min_max = scaler.transform(df_full_test_fe[cols_selected])
df_test_fe_min_max = pd.DataFrame(df_test_fe_min_max)
df_test_fe_min_max.columns = cols_selected
381/301: df_test_fe_min_max
381/302:
df_test_full = pd.merge(df_test_tf_idf, df_test_fe_min_max, 
                        left_index=True, right_index=True)
381/303: df_test_full
381/304: df_test_full.to_csv('test_after_preproc.csv')
381/305: y_test_all = mnb.predict_proba(df_test_full)[:,1]
381/306: df_t = X_test[['id']]
381/307: df_t = X_test[['id']]
381/308: X_test
381/309: df_t = df_test[['id']]
381/310: df_t
381/311: df_t['target'] = (y_test_pred_proba[:, 1] >= 0.381).astype(int)
381/312: y_test_all = mnb.predict_proba(df_test_full)
381/313: df_t['target'] = (y_test_all[:, 1] >= 0.381).astype(int)
381/314: df_t
381/315: df_t.info()
381/316: df_t.describe()
381/317: df_t
381/318: df_t.to_csv("submission_2.csv", index=False)
381/319: ### for testing
381/320:
df_text_clean_fe_test = basic_cleaning(df_test, 'text')
df_text_clean_fe_test["con_text"] = df_text_clean_fe_test.keyword.map(str) + " " + df_text_clean_fe_test.text
381/321:
df_test_tf_idf = vectorizer.transform(df_full_test_fe['con_text']).toarray()
df_test_tf_idf = pd.DataFrame(df_test_tf_idf)
print('Test shape:', df_test_tf_idf.shape)
381/322:
def to_labels(pos_probs, threshold):
    return (pos_probs >= threshold).astype('int')

def best_tresholds(y_pred_proba, y_test):
    # define thresholds
    thresholds = np.arange(0, 1, 0.001)
    # evaluate each threshold
    scores = [f1_score(y_test, to_labels(y_pred_proba, t)) for t in thresholds]
    # get best threshold
    ix = np.argmax(scores)
    print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
381/323:
df_text_clean = basic_cleaning(df, 'text')
df_text_clean["con_text"] = df_text_clean.keyword.map(str) + " " + df_text_clean.text
381/324:
## split to test and train
X = df_text_clean['con_text']
y = df_text_clean['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print('X_train shape', X_train.shape)
print('X_test shape', X_test.shape)
381/325:
vectorizer = TfidfVectorizer()
train_tf_idf = vectorizer.fit_transform(X_train).toarray()
train_tf_idf = pd.DataFrame(train_tf_idf)
print('Train shape:', train_tf_idf.shape)
test_tf_idf = vectorizer.transform(X_test).toarray()
test_tf_idf = pd.DataFrame(test_tf_idf)
print('Test shape:', test_tf_idf.shape)
381/326:
mnb = MultinomialNB()
mnb.fit(train_tf_idf, y_train)
y_test_pred_proba = mnb.predict_proba(test_tf_idf)[:,1]
381/327: best_tresholds(y_test_pred_proba, y_test)
381/328:
mnb_tdidf = MultinomialNB()
mnb_tdidf.fit(train_tf_idf, y_train)
y_test_pred_proba = mnb.predict_proba(test_tf_idf)[:,1]
381/329: best_tresholds(y_test_pred_proba, y_test)
381/330: y_test_all = mnb_tdidf.predict_proba(df_test_tf_idf)
381/331: df_t = df_test[['id']]
381/332: best_tresholds(y_test_pred_proba, y_test)
381/333:
def to_labels(pos_probs, threshold):
    return (pos_probs >= threshold).astype('int')

def best_tresholds(y_pred_proba, y_test):
    # define thresholds
    thresholds = np.arange(0, 1, 0.001)
    # evaluate each threshold
    scores = [f1_score(y_test, to_labels(y_pred_proba, t)) for t in thresholds]
    # get best threshold
    ix = np.argmax(scores)
    print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
381/334:
df_text_clean = basic_cleaning(df, 'text')
df_text_clean["con_text"] = df_text_clean.keyword.map(str) + " " + df_text_clean.text
381/335:
## split to test and train
X = df_text_clean['con_text']
y = df_text_clean['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print('X_train shape', X_train.shape)
print('X_test shape', X_test.shape)
381/336:
vectorizer = TfidfVectorizer()
train_tf_idf = vectorizer.fit_transform(X_train).toarray()
train_tf_idf = pd.DataFrame(train_tf_idf)
print('Train shape:', train_tf_idf.shape)
test_tf_idf = vectorizer.transform(X_test).toarray()
test_tf_idf = pd.DataFrame(test_tf_idf)
print('Test shape:', test_tf_idf.shape)
381/337:
mnb_tdidf = MultinomialNB()
mnb_tdidf.fit(train_tf_idf, y_train)
y_test_pred_proba = mnb.predict_proba(test_tf_idf)[:,1]
381/338: df = copy.deepcopy(df_train)
381/339:
def to_labels(pos_probs, threshold):
    return (pos_probs >= threshold).astype('int')

def best_tresholds(y_pred_proba, y_test):
    # define thresholds
    thresholds = np.arange(0, 1, 0.001)
    # evaluate each threshold
    scores = [f1_score(y_test, to_labels(y_pred_proba, t)) for t in thresholds]
    # get best threshold
    ix = np.argmax(scores)
    print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
381/340:
df_text_clean = basic_cleaning(df, 'text')
df_text_clean["con_text"] = df_text_clean.keyword.map(str) + " " + df_text_clean.text
381/341:
## split to test and train
X = df_text_clean['con_text']
y = df_text_clean['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print('X_train shape', X_train.shape)
print('X_test shape', X_test.shape)
381/342:
vectorizer = TfidfVectorizer()
train_tf_idf = vectorizer.fit_transform(X_train).toarray()
train_tf_idf = pd.DataFrame(train_tf_idf)
print('Train shape:', train_tf_idf.shape)
test_tf_idf = vectorizer.transform(X_test).toarray()
test_tf_idf = pd.DataFrame(test_tf_idf)
print('Test shape:', test_tf_idf.shape)
381/343:
mnb_tdidf = MultinomialNB()
mnb_tdidf.fit(train_tf_idf, y_train)
y_test_pred_proba = mnb.predict_proba(test_tf_idf)[:,1]
381/344: best_tresholds(y_test_pred_proba, y_test)
381/345: best_tresholds(y_test_pred_proba, y_test)
381/346:
df_text_clean_fe_test = basic_cleaning(df_test, 'text')
df_text_clean_fe_test["con_text"] = df_text_clean_fe_test.keyword.map(str) + " " + df_text_clean_fe_test.text
381/347:
## new features
df_feature_eng_test = feature_eng_fun(df_test, 'text')
df_feature_eng_test = df_feature_eng_test[cols_selected]
381/348:
df_full_test_fe = pd.merge(df_text_clean_fe_test[['con_text']], df_feature_eng_test, 
                           left_index=True, right_index=True)
381/349:
df_test_tf_idf = vectorizer.transform(df_full_test_fe['con_text']).toarray()
df_test_tf_idf = pd.DataFrame(df_test_tf_idf)
print('Test shape:', df_test_tf_idf.shape)
381/350: y_test_all = mnb_tdidf.predict_proba(df_test_tf_idf)
381/351: df_t['target'] = (y_test_all[:, 1] >= 0.374).astype(int)
381/352: df_t.to_csv("submission_3.csv", index=False)
381/353: df_t
381/354: df_t.describe()
381/355: ### with eng features
381/356: cols_selected
381/357: df = copy.deepcopy(df_train)
381/358:
cols_selected = ['GPE_text_flag', 'count_links', 'count_num_of_chars', 'count_punct', 'count_sent',
                 'count_mentions', 'count_stopwords', 'count_question', 'count_quotation',
                 'count_num_of_capital_chars']
df = copy.deepcopy(df_train)
df_text_clean_fe = basic_cleaning(df, 'text')
df_text_clean_fe["con_text"] = df_text_clean_fe.keyword.map(str) + " " + df_text_clean_fe.text

## new features
df = copy.deepcopy(df_train)
df_feature_eng = feature_eng_fun(df, 'text')
df_feature_eng = df_feature_eng[cols_selected]
381/359:
df_full_fe = pd.merge(df_text_clean_fe[['target', 'con_text']], df_feature_eng, 
                      left_index=True, right_index=True)
381/360:
## split to test and train
y = df_full_fe['target']
X = df_full_fe.drop('target', axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print('X_train shape', X_train.shape)
print('X_test shape', X_test.shape)
381/361:
vectorizer = TfidfVectorizer()
train_tf_idf = vectorizer.fit_transform(X_train['con_text']).toarray()
train_tf_idf = pd.DataFrame(train_tf_idf)
print('Train shape:', train_tf_idf.shape)
test_tf_idf = vectorizer.transform(X_test['con_text']).toarray()
test_tf_idf = pd.DataFrame(test_tf_idf)
print('Test shape:', test_tf_idf.shape)
381/362:
scaler = MinMaxScaler()
df_train_fe_min_max = scaler.fit_transform(X_train[cols_selected])
df_train_fe_min_max = pd.DataFrame(df_train_fe_min_max)
df_train_fe_min_max.columns = cols_selected

df_test_fe_min_max = scaler.transform(X_test[cols_selected])
df_test_fe_min_max = pd.DataFrame(df_test_fe_min_max)
df_test_fe_min_max.columns = cols_selected
381/363:
df_full_train_with_f = pd.merge(train_tf_idf, df_train_fe_min_max, 
                      left_index=True, right_index=True)
381/364:
df_full_test_with_f = pd.merge(test_tf_idf, df_test_fe_min_max, 
                      left_index=True, right_index=True)
381/365:
mnb = MultinomialNB()
mnb.fit(df_full_train_with_f, y_train)
y_test_pred_proba = mnb.predict_proba(df_full_test_with_f)[:,1]
381/366: best_tresholds(y_test_pred_proba, y_test)
381/367:
classifier = RandomForestClassifier()

classifier.fit(df_full_train_with_f, y_train)
381/368:
from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier()

classifier.fit(df_full_train_with_f, y_train)
381/369: y_test_pred_proba_rf = classifier.predict_proba(df_full_test_with_f)[:,1]
381/370: best_tresholds(y_test_pred_proba_rf, y_test)
381/371: logit = LogisticRegression(C=5e1, solver='lbfgs', multi_class='multinomial', random_state=17, n_jobs=4)
381/372:
from sklearn.linear_model import LogisticRegression
logit = LogisticRegression(C=5e1, solver='lbfgs', multi_class='multinomial', random_state=17, n_jobs=4)
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=17)
381/373:
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import confusion_matrix, classification_report, roc_curve
#from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
#from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
from sklearn.metrics import f1_score
381/374:
from sklearn.linear_model import LogisticRegression
logit = LogisticRegression(C=5e1, solver='lbfgs', multi_class='multinomial', random_state=17, n_jobs=4)
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=17)
381/375: cv_results = cross_val_score(logit, df_full_train_with_f, y_train, cv=skf, scoring='f1_micro')
381/376:
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import confusion_matrix, classification_report, roc_curve
#from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
#from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
from sklearn.metrics import f1_score
381/377: cv_results = cross_val_score(logit, df_full_train_with_f, y_train, cv=skf, scoring='f1_micro')
381/378: logit.fit(df_full_train_with_f, y_train)
381/379: y_test_pred_proba_logit = logit.predict_proba(df_full_test_with_f)[:,1]
381/380: best_tresholds(y_test_pred_proba_logit, y_test)
381/381:
from sklearn.linear_model import LogisticRegression
logit = LogisticRegression()
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=17)
381/382: logit.fit(df_full_train_with_f, y_train)
381/383: y_test_pred_proba_logit = logit.predict_proba(df_full_test_with_f)[:,1]
381/384: best_tresholds(y_test_pred_proba_logit, y_test)
381/385: y_test_all = logit.predict_proba(df_test_tf_idf)
381/386:
df_text_clean_fe_test = basic_cleaning(df_test, 'text')
df_text_clean_fe_test["con_text"] = df_text_clean_fe_test.keyword.map(str) + " " + df_text_clean_fe_test.text
381/387:
## new features
df_feature_eng_test = feature_eng_fun(df_test, 'text')
df_feature_eng_test = df_feature_eng_test[cols_selected]
381/388:
df_full_test_fe = pd.merge(df_text_clean_fe_test[['con_text']], df_feature_eng_test, 
                           left_index=True, right_index=True)
381/389:
df_test_tf_idf = vectorizer.transform(df_full_test_fe['con_text']).toarray()
df_test_tf_idf = pd.DataFrame(df_test_tf_idf)
print('Test shape:', df_test_tf_idf.shape)
381/390:
df_test_fe_min_max = scaler.transform(df_full_test_fe[cols_selected])
df_test_fe_min_max = pd.DataFrame(df_test_fe_min_max)
df_test_fe_min_max.columns = cols_selected
381/391:
df_test_full = pd.merge(df_test_tf_idf, df_test_fe_min_max, 
                        left_index=True, right_index=True)
381/392: y_test_all = logit.predict_proba(df_test_full)
381/393: df_t = df_test[['id']]
381/394: df_t['target'] = (y_test_all[:, 1] >= 0.450).astype(int)
381/395: df_t.to_csv("submission_logit.csv", index=False)
381/396: ### with eng features
381/397: cols_selected
381/398: df = copy.deepcopy(df_train)
381/399:
cols_selected = ['GPE_text_flag', 'count_links', 'count_num_of_chars', 'count_punct', 'count_sent',
                 'count_mentions', 'count_stopwords', 'count_question', 'count_quotation',
                 'count_num_of_capital_chars']
df = copy.deepcopy(df_train)
df_text_clean_fe = basic_cleaning(df, 'text')
df_text_clean_fe["con_text"] = df_text_clean_fe.keyword.map(str) + " " + df_text_clean_fe.text

## new features
df = copy.deepcopy(df_train)
df_feature_eng = feature_eng_fun(df, 'text')
df_feature_eng = df_feature_eng[cols_selected]
381/400:
df_full_fe = pd.merge(df_text_clean_fe[['target', 'con_text']], df_feature_eng, 
                      left_index=True, right_index=True)
381/401:
## split to test and train
y = df_full_fe['target']
X = df_full_fe.drop('target', axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print('X_train shape', X_train.shape)
print('X_test shape', X_test.shape)
381/402:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
381/403:
## split to test and train
y = df_full_fe['target']
X = df_full_fe.drop('target', axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print('X_train shape', X_train.shape)
print('X_test shape', X_test.shape)
381/404:
vectorizer = CountVectorizer()()
train_tf_idf = vectorizer.fit_transform(X_train['con_text']).toarray()
train_tf_idf = pd.DataFrame(train_tf_idf)
print('Train shape:', train_tf_idf.shape)
test_tf_idf = vectorizer.transform(X_test['con_text']).toarray()
test_tf_idf = pd.DataFrame(test_tf_idf)
print('Test shape:', test_tf_idf.shape)
381/405:
vectorizer = CountVectorizer()
train_tf_idf = vectorizer.fit_transform(X_train['con_text']).toarray()
train_tf_idf = pd.DataFrame(train_tf_idf)
print('Train shape:', train_tf_idf.shape)
test_tf_idf = vectorizer.transform(X_test['con_text']).toarray()
test_tf_idf = pd.DataFrame(test_tf_idf)
print('Test shape:', test_tf_idf.shape)
381/406:
scaler = MinMaxScaler()
df_train_fe_min_max = scaler.fit_transform(X_train[cols_selected])
df_train_fe_min_max = pd.DataFrame(df_train_fe_min_max)
df_train_fe_min_max.columns = cols_selected

df_test_fe_min_max = scaler.transform(X_test[cols_selected])
df_test_fe_min_max = pd.DataFrame(df_test_fe_min_max)
df_test_fe_min_max.columns = cols_selected
381/407:
df_full_train_with_f = pd.merge(train_tf_idf, df_train_fe_min_max, 
                      left_index=True, right_index=True)
381/408:
df_full_test_with_f = pd.merge(test_tf_idf, df_test_fe_min_max, 
                      left_index=True, right_index=True)
381/409:
logit = LogisticRegression()
logit.fit(df_full_train_with_f, y_train)
381/410: y_test_pred_proba_logit = logit.predict_proba(df_full_test_with_f)[:,1]
381/411: best_tresholds(y_test_pred_proba_logit, y_test)
382/1:
import pandas as pd
import numpy as np
import re
import copy
from collections import Counter
import string
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline
382/2: pd.set_option('max_colwidth', None)
382/3:
import matplotlib.pyplot as plt
import seaborn as sns
382/4:
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.tokenize.treebank import TreebankWordDetokenizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer

import emoji as emj
import spacy 
## It's important to use _lg for OOV in future
nlp = spacy.load('en_core_web_lg')
382/5:
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import confusion_matrix, classification_report, roc_curve
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
from sklearn.metrics import f1_score
382/6:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
382/7:
nltk.download()
#nltk.download('stopwords')
382/8: STOPWORDS = set(stopwords.words('english'))
382/9:
#import nltk
#nltk.download('wordnet')
382/10:
#import nltk
#dler = nltk.downloader.Downloader()
#dler._update_index()
#dler.download('all')
382/11: wnl = WordNetLemmatizer()
382/12: df_train.head()
382/13: df_train.isnull().mean()
382/14: df_train.shape, df_test.shape
382/15:
#cardinality check
for col in df_train.columns:
    print('{} has {} unique instances'.format(col, len(df_train[col].unique())))
382/16: df_train.info()
382/17: df_train.keyword.value_counts()
382/18:
plt.figure(figsize=[8, 40])
sns.countplot(y='keyword',
              data=df_train,
              color='lightblue',
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
382/19:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword', 
              hue='target', 
              data=df_train, 
              palette=['lightblue', 'red']
             )
plt.xticks(rotation=90)
plt.legend(loc='upper right')
plt.title("Distribution of Target per Keyword ")
plt.show()
382/20: df_train.keyword = df_train.keyword.str.replace('%20', ' ')
382/21:
# Let's show most common keywords for target = 1
df_train[df_train.target == 1].keyword.value_counts()[:10]
382/22:
# Let's show most common keywords for target = 1
df_train[df_train.target == 0].keyword.value_counts()[:10]
382/23:
df_train_group = df_train.groupby(['keyword', 'target'])['id'].count().unstack(fill_value=0).reset_index()
df_train_group.columns = ['keyword', 'target_0', 'target_1']
df_train_group['prob_real_disasters'] = df_train_group.target_1/(df_train_group.target_0 + df_train_group.target_1)
382/24: df_train_group.sort_values('prob_real_disasters', ascending=False)
382/25: df_train.keyword.nunique()
382/26: df_train.target.value_counts()/len(df_train)
382/27: df_train.location.nunique()
382/28: df_location = df_train.loc[:, ['id', 'target', 'location', 'text']]
382/29: df_location.location.fillna('no_location', inplace=True)
382/30:
# Write a function to display basic entity info:
def show_ents(doc):
    doc = nlp(doc)
    count_GPE = 0
    if doc.ents:
        for ent in doc.ents:
            if ent.label_ == 'GPE':
                #print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
                count_GPE = 1
                break
    return count_GPE
382/31: df_location['GPE_loc_flag'] = df_location.location.apply(show_ents)
382/32: df_location['GPE_text_flag'] = df_location.text.apply(show_ents)
382/33: df_location['GPE_flag'] = df_location.GPE_loc_flag | df_location.GPE_text_flag
382/34:
f, axes = plt.subplots(1, 3, figsize=(20,6))
sns.countplot(x='GPE_loc_flag', hue='target', data=df_location, ax=axes[0])
sns.countplot(x='GPE_text_flag', hue='target', data=df_location, ax=axes[1])
sns.countplot(x='GPE_flag', hue='target', data=df_location, ax=axes[2])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')
axes[2].set_title('GPE')

#plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
axes[0].legend(labels=['0', '1'])
axes[1].legend(labels=['0', '1'])
axes[2].legend(labels=['0', '1'])

plt.show()
382/35:
# Looks like the location extracted from the tweet will be more informative than the original location feature.
df_location.iloc[:, -4:].corr()
382/36: df_punct_fe = df_train.loc[:, ['id', 'target', 'text']]
382/37:
#string.punctuation = '!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
def count_punctuations(text):
    d=dict()
    for i in string.punctuation:
        d[str(i)+' count']=text.count(i)
    return d
382/38:
df_punct_fe['punct_count'] = df_punct_fe["text"].apply(lambda x:count_punctuations(x))
df_punct= pd.DataFrame(list(df_punct_fe.punct_count))
merge_df = pd.merge(df_punct_fe.target,df_punct,left_index=True, right_index=True)
382/39:
merge_df_gr = merge_df.groupby('target').sum().reset_index()
merge_df_gr = merge_df_gr.T
merge_df_gr.columns = ['target_0', 'target_1']
382/40: merge_df_gr
382/41: merge_df_gr['sum_0_1'] = merge_df_gr['target_0'] + merge_df_gr['target_1']
382/42: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
382/43: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
382/44:
def punct_add_exclamation(text):
    return len(re.findall("!", text))
382/45:
def punct_add_question(text):
    return len(re.findall("\?", text))
382/46:
def punct_add_quotation(text):
    return len(re.findall("'", text))
382/47:
df_punct_fe['count_exclamation'] = df_punct_fe.text.apply(punct_add_exclamation)
df_punct_fe['count_question'] = df_punct_fe.text.apply(punct_add_question)
df_punct_fe['count_quotation'] = df_punct_fe.text.apply(punct_add_quotation)
382/48: new_cols = ['count_exclamation', 'count_question', 'count_quotation']
382/49: df_punct_fe.head(2)
382/50:
# count number of characters 
def count_chars(text):
    return len(text)

# count number of words 
def count_words(text):
    return len(text.split())

# count number of capital characters
def count_capital_chars(text):
    count=0
    for i in text:
        if i.isupper():
            count+=1
    return count

# count number of capital words
def count_capital_words(text):
    return sum(map(str.isupper,text.split()))

# count number of punctuations
def count_punctuations(text):
    count = lambda l1,l2: sum([1 for x in l1 if x in l2])
    return count(text,set(string.punctuation))

# count number of words in quotes
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    string_wothout_contractions = string_wothout_contractions.replace('"', '\'')
    x = re.findall("\'(.+?)\'", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
    
# count number of sentences
def count_sent(text):
    return len(nltk.sent_tokenize(text))

# count number of unique words 
def count_unique_words(text):
    return len(set(text.split()))
    
# count of hashtags
def count_htags(text):
    x = re.findall(r'(\#\w[A-Za-z0-9]*)', text)
    return len(x)

# count of mentions
def count_mentions(text):
    x = re.findall(r'(\@\w[A-Za-z0-9]*)', text)
    return len(x)

# count of stopwords
def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)

#count of links
def count_links(text):
    link_list = re.findall(r'(https?://[^\s]+)', text)
    bitly_list = re.findall(r'bit.ly/\S+', text)
    return (len(link_list + bitly_list))

#count digits
def count_digits(text):
    digit_list = re.findall(r'[0-9]+', text)
    return len(digit_list)
382/51:
def feature_eng_fun(df_in, text):
    df = copy.deepcopy(df_in)
    df['count_exclamation'] = df_train[text].apply(punct_add_exclamation)
    df['count_question'] = df[text].apply(punct_add_question)
    df['count_quotation'] = df[text].apply(punct_add_quotation)
    df['count_num_of_chars'] = df[text].apply(count_chars)
    df['count_num_of_words'] = df[text].apply(count_words)
    df['count_num_of_capital_chars'] = df[text].apply(count_capital_chars)
    df['count_num_of_capital_words'] = df[text].apply(count_capital_words)
    df['count_punct'] = df[text].apply(count_punctuations)
    df['count_words_in_quotes'] = df[text].apply(count_words_in_quotes)
    df['count_sent'] = df[text].apply(count_sent)
    df['count_unique_words'] = df[text].apply(count_unique_words)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_mentions'] = df[text].apply(count_mentions)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_stopwords'] = df[text].apply(count_stopwords)
    df['count_links'] = df[text].apply(count_links) 
    ## This function from part above
    df['GPE_text_flag'] = df[text].apply(show_ents)    
    return df
382/52:
## .loc will create a new copy, not view
df_feature_eng = df_train.loc[:, ('id', 'text', 'target')]
382/53: df_feature_eng = feature_eng_fun(df_feature_eng, 'text')
382/54: df_feature_eng.drop(['text'], axis=1, inplace=True)
382/55:
plt.figure(figsize=(14,10))
sns.heatmap(df_feature_eng.corr(),annot=True)
382/56: cols_selected = np.abs(df_feature_eng.corr()['target']).sort_values(ascending=False)[1:11].index.to_list()
382/57: df_feature_eng[cols_selected].head(3)
382/58: cols_selected = np.abs(df_feature_eng.corr()['target']).sort_values(ascending=False)[1:11].index.to_list()
382/59: df_feature_eng[cols_selected].head(3)
382/60:
# Let's look at some tweets more carefully
df_train[55:60]
382/61: stop = stopwords.words('english')
382/62:
#remove urls
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+', '', text)
    return text

#remove @word 
def remove_social_media_tags(text):
    text = re.sub(r'@\S+', '', text)
    return text

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

#remove digits
def remove_digits(text):
    text = re.sub(r'[0-9]+', '', text)
    return text   

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()

#contractions and stopwords removing
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 3:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
382/63:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_social_media_tags)    
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(remove_digits)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(string_contractions)
    return df
382/64: df_text = df_train.loc[:, ['id', 'target', 'keyword', 'text']]
382/65: df_text = basic_cleaning(df_text, 'text')
382/66: df_text
382/67:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_social_media_tags)    
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(remove_digits)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(string_contractions)
    df['column_token'] = df[column_name].apply(word_tokenize)
    return df
382/68: df_text = df_train.loc[:, ['id', 'target', 'keyword', 'text']]
382/69: df_text = basic_cleaning(df_text, 'text')
382/70: df_text
382/71:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_social_media_tags)    
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(remove_digits)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(convert_lower_case)
   # df[column_name] = df[column_name].apply(string_contractions)
   # df['column_token'] = df[column_name].apply(word_tokenize)
    return df
382/72: df_text = df_train.loc[:, ['id', 'target', 'keyword', 'text']]
382/73: df_text = basic_cleaning(df_text, 'text')
382/74: df_text
382/75: all_text = ' '.join(df_text['text'].tolist())
382/76:
oov_list = []
def search_oov(text):
    tokens = nlp(text)
    for token in tokens:
        if token.is_oov:
            oov_list.append(token)
382/77: search_oov(all_text)
382/78: oov_list_strings  = [i.text for i in oov_list]
382/79:
counter = Counter(oov_list_strings)
counter.most_common(10)
382/80: sid = SentimentIntensityAnalyzer()
382/81: df_sentiment = df_train.loc[:, ['id', 'target', 'text']]
382/82: df_sentiment['sentiment_compound']  = df_sentiment.text.apply(lambda tweet: sid.polarity_scores(tweet)['compound'])
382/83: df_sentiment['sentiment_compound_score'] = df_sentiment['sentiment_compound'].apply(lambda c: 'pos' if c >=0 else 'neg')
382/84:
plt.figure(figsize=[4, 4])
sns.countplot(x='target', hue='sentiment_compound_score', data=df_sentiment)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
382/85: #not very optimistic result, I won't add a sentiment feature :)
382/86: ## CREATE A MODEL BASED ON TEXT
382/87:
# count number of characters 
def count_chars(text):
    return len(text)

# count number of words 
def count_words(text):
    return len(text.split())

# count number of capital characters
def count_capital_chars(text):
    count=0
    for i in text:
        if i.isupper():
            count+=1
    return count

# count number of capital words
def count_capital_words(text):
    return sum(map(str.isupper,text.split()))

# count number of punctuations
def count_punctuations(text):
    count = lambda l1,l2: sum([1 for x in l1 if x in l2])
    return count(text,set(string.punctuation))

# count number of words in quotes
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    string_wothout_contractions = string_wothout_contractions.replace('"', '\'')
    x = re.findall("\'(.+?)\'", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
    
# count number of sentences
def count_sent(text):
    return len(nltk.sent_tokenize(text))

# count number of unique words 
def count_unique_words(text):
    return len(set(text.split()))
    
# count of hashtags
def count_htags(text):
    x = re.findall(r'(\#\w[A-Za-z0-9]*)', text)
    return len(x)

# count of mentions
def count_mentions(text):
    x = re.findall(r'(\@\w[A-Za-z0-9]*)', text)
    return len(x)

# count of stopwords
def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)

#count of links
def count_links(text):
    link_list = re.findall(r'(https?://[^\s]+)', text)
    bitly_list = re.findall(r'bit.ly/\S+', text)
    return (len(link_list + bitly_list))

#count digits
def count_digits(text):
    digit_list = re.findall(r'[0-9]+', text)
    return len(digit_list)

# Factorize repeated punctuation, add REPEAT
def count_repeat_punct(text):
    rep_list = re.findall(r'([!?.]){2,}', text)
    return len(rep_list)
382/88:
def feature_eng_fun(df_in, text):
    df = copy.deepcopy(df_in)
    df['count_exclamation'] = df_train[text].apply(punct_add_exclamation)
    df['count_question'] = df[text].apply(punct_add_question)
    df['count_quotation'] = df[text].apply(punct_add_quotation)
    df['count_num_of_chars'] = df[text].apply(count_chars)
    df['count_num_of_words'] = df[text].apply(count_words)
    df['count_num_of_capital_chars'] = df[text].apply(count_capital_chars)
    df['count_num_of_capital_words'] = df[text].apply(count_capital_words)
    df['count_punct'] = df[text].apply(count_punctuations)
    df['count_words_in_quotes'] = df[text].apply(count_words_in_quotes)
    df['count_sent'] = df[text].apply(count_sent)
    df['count_unique_words'] = df[text].apply(count_unique_words)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_mentions'] = df[text].apply(count_mentions)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_stopwords'] = df[text].apply(count_stopwords)
    df['count_links'] = df[text].apply(count_links) 
    df['count_repeat_punct'] = df[text].apply(count_repeat_punct)    
    ## This function from part above
    df['GPE_text_flag'] = df[text].apply(show_ents)    
    return df
382/89:
## .loc will create a new copy, not view
df_feature_eng = df_train.loc[:, ('id', 'text', 'target')]
382/90: df_feature_eng = feature_eng_fun(df_feature_eng, 'text')
382/91: df_feature_eng.drop(['text'], axis=1, inplace=True)
382/92:
plt.figure(figsize=(14,10))
sns.heatmap(df_feature_eng.corr(),annot=True)
382/93: cols_selected = np.abs(df_feature_eng.corr()['target']).sort_values(ascending=False)[1:11].index.to_list()
382/94: df_feature_eng[cols_selected].head(3)
382/95: ### with eng features
382/96: cols_selected
382/97: df_copy = copy.deepcopy(df_train)
382/98:
cols_selected = ['GPE_text_flag', 'count_links', 'count_num_of_chars', 'count_punct', 'count_sent',
                 'count_mentions', 'count_stopwords', 'count_question', 'count_quotation',
                 'count_num_of_capital_chars']

df_copy = copy.deepcopy(df_train)
df_train_text_clean = basic_cleaning(df_copy, 'text')
df_train_text_clean["con_text"] = df_train_text_clean.keyword.map(str) + " " + df_train_text_clean.text

## new features
df_train_feature_eng = feature_eng_fun(df_copy, 'text')
df_train_feature_eng = df_train_feature_eng[cols_selected]
382/99: df_train_text_clean
382/100: df_train_feature_eng
382/101:
cols_selected = ['GPE_text_flag', 'count_links', 'count_num_of_chars', 'count_punct', 'count_sent',
                 'count_mentions', 'count_stopwords', 'count_question', 'count_quotation',
                 'count_num_of_capital_chars']

df_copy = copy.deepcopy(df_train)
df_train_text_clean = basic_cleaning(df_copy, 'text')
df_train_text_clean["clean_text"] = df_train_text_clean.keyword.map(str) + " " + df_train_text_clean.text

## new features
df_train_feature_eng = feature_eng_fun(df_copy, 'text')
df_train_feature_eng = df_train_feature_eng[cols_selected]
382/102: df_train_text_clean
382/103: df_train_feature_eng
382/104: df_train_text_clean.shape
382/105: df_train_feature_eng.shape
382/106:
df_train_full = pd.merge(df_train_text_clean[['target', 'clean_text']], df_train_feature_eng, 
                         left_index=True, right_index=True)
382/107: df_train_full
382/108:
## split to test and train
y = df_full_fe['target']
X = df_full_fe.drop('target', axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
print('X_train shape', X_train.shape)
print('X_test shape', X_test.shape)
382/109:
## split to test and train
y = df_train_full['target']
X = df_train_full.drop('target', axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
print('X_train shape', X_train.shape)
print('X_test shape', X_test.shape)
382/110: X_train
382/111:
max_features=5000

# CountVectorizer
count_vectorizer = CountVectorizer(max_features=max_features)

def count_vector(data):
    count_vectorizer = CountVectorizer()
    vect = count_vectorizer.fit_transform(data)
    return vect, count_vectorizer

def tfidf_vector(data):
    tfidf_vectorizer = TfidfVectorizer()
    vect = tfidf_vectorizer.fit_transform(data)
    return vect, tfidf_vectorizer

X_train_count, count_vectorizer = count_vector(X_train['clean_text'])
X_train_tfidf, tfidf_vectorizer = tfidf_vector(X_train['clean_text'])

X_test_count = count_vectorizer.transform(X_test ['clean_text'])                                                     
X_test_tfidf = tfidf_vectorizer.transform(X_test ['clean_text'])
382/112: X_test_tfidf
382/113: X_test_tfidf.to_list()
382/114: X_test_tfidf.shape
382/115: X_test_tfidf.to_array()
382/116: X_test_tfidf
382/117: X_test_tfidf.toarray()
382/118: pd.DataFrame(X_test_tfidf.toarray())
382/119: pd.DataFrame(X_train_count.toarray())
382/120:
max_features=5000

# CountVectorizer
count_vectorizer = CountVectorizer(max_features=max_features)

def count_vector(data):
    count_vectorizer = count_vectorizer
    vect = count_vectorizer.fit_transform(data)
    return vect, count_vectorizer

def tfidf_vector(data):
    tfidf_vectorizer = TfidfVectorizer()
    vect = tfidf_vectorizer.fit_transform(data)
    return vect, tfidf_vectorizer

X_train_count, count_vectorizer = count_vector(X_train['clean_text'])
X_train_tfidf, tfidf_vectorizer = tfidf_vector(X_train['clean_text'])

X_test_count = count_vectorizer.transform(X_test ['clean_text'])                                                     
X_test_tfidf = tfidf_vectorizer.transform(X_test ['clean_text'])
382/121:
max_features=5000

# CountVectorizer
count_vectorizer = CountVectorizer(max_features=max_features)

def count_vector(data):
 #   count_vectorizer = count_vectorizer
    vect = count_vectorizer.fit_transform(data)
    return vect, count_vectorizer

def tfidf_vector(data):
    tfidf_vectorizer = TfidfVectorizer()
    vect = tfidf_vectorizer.fit_transform(data)
    return vect, tfidf_vectorizer

X_train_count, count_vectorizer = count_vector(X_train['clean_text'])
X_train_tfidf, tfidf_vectorizer = tfidf_vector(X_train['clean_text'])

X_test_count = count_vectorizer.transform(X_test ['clean_text'])                                                     
X_test_tfidf = tfidf_vectorizer.transform(X_test ['clean_text'])
382/122: pd.DataFrame(X_train_count.toarray())
382/123: pd.DataFrame(X_train_count.toarray()).describe()
382/124:
#max_features=5000

# CountVectorizer
count_vectorizer = CountVectorizer(max_features=max_features)

def count_vector(data):
    count_vectorizer = CountVectorizer()
    vect = count_vectorizer.fit_transform(data)
    return vect, count_vectorizer

def tfidf_vector(data):
    tfidf_vectorizer = TfidfVectorizer()
    vect = tfidf_vectorizer.fit_transform(data)
    return vect, tfidf_vectorizer

X_train_count, count_vectorizer = count_vector(X_train['clean_text'])
X_train_tfidf, tfidf_vectorizer = tfidf_vector(X_train['clean_text'])

X_test_count = count_vectorizer.transform(X_test ['clean_text'])                                                     
X_test_tfidf = tfidf_vectorizer.transform(X_test ['clean_text'])
382/125: import pickle
382/126:
with open('count_vectorizer.pickle', 'wb') as handle:
    pickle.dump(count_vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
with open('tfidf_vectorizer.pickle', 'wb') as handle:
    pickle.dump(tfidf_vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
382/127: metrics = pd.DataFrame(columns=['model' ,'vectoriser', 'f1 score', 'train accuracy','test accuracy'])
382/128:
#max_features=5000

# CountVectorizer
count_vectorizer = CountVectorizer(max_features=max_features)

def count_vector(data):
    count_vectorizer = CountVectorizer()
    vect = count_vectorizer.fit_transform(data)
    return vect, count_vectorizer

def tfidf_vector(data):
    tfidf_vectorizer = TfidfVectorizer()
    vect = tfidf_vectorizer.fit_transform(data)
    return vect, tfidf_vectorizer

def min_max(data):
    min_max_scaler = MinMaxScaler()
    vect = min_max_scaler.fit_transform(data)
    return vect, min_max_scaler

X_train_count, count_vectorizer = count_vector(X_train['clean_text'])
X_train_tfidf, tfidf_vectorizer = tfidf_vector(X_train['clean_text'])

X_test_count = count_vectorizer.transform(X_test ['clean_text'])                                                     
X_test_tfidf = tfidf_vectorizer.transform(X_test ['clean_text'])
382/129: X_train_eng, minmax_vectorizer = min_max(X_train[clean_text])
382/130: X_train[clean_text]
382/131: X_train[cols_selected]
382/132: X_train_eng, minmax_vectorizer = min_max(X_train[cols_selected])
382/133: X_train_eng
382/134: X_train_eng.dtype
382/135: X_train_eng.shape
382/136: X_train_count.shape
382/137: np.concatenate(X_train_count, X_train_eng)
382/138: np.concatenate(X_train_count, X_train_eng, axis=1)
382/139: np.concatenate((X_train_count, X_train_eng), axis=1)
382/140: X_train_eng.shape
382/141: X_train_count.shape
382/142: np.hstack((X_train_count, X_train_eng))
382/143: np.concatenate((X_train_eng,X_train_count),axis=1)
382/144: X_train_count.shape
382/145: np.hstack((X_train_eng,X_train_count))
382/146: np.hstack((X_train_eng,X_train_count.T))
382/147: X_train_eng
382/148: X_train_eng[0]
382/149: X_train_eng[0]
382/150:
b = np.array([[5, 6]])
b.shape
382/151: np.concatenate((X_train_count, X_train_eng.T), axis=1)
382/152: np.concatenate((X_train_count, X_train_eng), axis=1)
382/153: scipy.sparse.hstack([X_train_count, X_train_eng])
382/154: from scipy.sparse import hstack
382/155: hstack([X_train_count, X_train_eng])
382/156: hstack([X_train_count, X_train_eng]).shape
382/157: X_train_count.shape
382/158:
import pandas as pd
import numpy as np
import re
import copy
from collections import Counter
import string
from scipy.sparse import hstack
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline
382/159: X_train
382/160: X_train[cols_selected].to_numpy()
382/161: X_train_eng_minmax
382/162: X_train_eng
382/163: X_train[cols_selected].to_numpy().shape
382/164: X_train_eng.shape
382/165: hstack([X_train_count, X_train_eng])
382/166:
#max_features=5000

# CountVectorizer
count_vectorizer = CountVectorizer(max_features=max_features)

def count_vector(data):
    count_vectorizer = CountVectorizer()
    vect = count_vectorizer.fit_transform(data)
    return vect, count_vectorizer

def tfidf_vector(data):
    tfidf_vectorizer = TfidfVectorizer()
    vect = tfidf_vectorizer.fit_transform(data)
    return vect, tfidf_vectorizer

def min_max(data):
    min_max_scaler = MinMaxScaler()
    vect = min_max_scaler.fit_transform(data)
    return vect, min_max_scaler

X_train_count, count_vectorizer = count_vector(X_train['clean_text'])
X_train_tfidf, tfidf_vectorizer = tfidf_vector(X_train['clean_text'])

X_train_eng_minmax, minmax_vectorizer = min_max(X_train[cols_selected])
X_train_eng = X_train[cols_selected].to_numpy()

#### TEST
X_test_count = count_vectorizer.transform(X_test ['clean_text'])                                                     
X_test_tfidf = tfidf_vectorizer.transform(X_test ['clean_text'])

X_test_eng_minmax, minmax_vectorizer = minmax_vectorizer.transform(X_test[cols_selected])
X_test_eng = X_test[cols_selected].to_numpy()
382/167: minmax_vectorizer
382/168: X_test[cols_selected]
382/169: minmax_vectorizer
382/170: X_test[cols_selected]
382/171:
#max_features=5000

# CountVectorizer
count_vectorizer = CountVectorizer(max_features=max_features)

def count_vector(data):
    count_vectorizer = CountVectorizer()
    vect = count_vectorizer.fit_transform(data)
    return vect, count_vectorizer

def tfidf_vector(data):
    tfidf_vectorizer = TfidfVectorizer()
    vect = tfidf_vectorizer.fit_transform(data)
    return vect, tfidf_vectorizer

def min_max(data):
    min_max_scaler = MinMaxScaler()
    vect = min_max_scaler.fit_transform(data)
    return vect, min_max_scaler

X_train_count, count_vectorizer = count_vector(X_train['clean_text'])
X_train_tfidf, tfidf_vectorizer = tfidf_vector(X_train['clean_text'])

X_train_eng_minmax, minmax_vectorizer = min_max(X_train[cols_selected])
X_train_eng = X_train[cols_selected].to_numpy()

#### TEST
X_test_count = count_vectorizer.transform(X_test ['clean_text'])                                                     
X_test_tfidf = tfidf_vectorizer.transform(X_test ['clean_text'])

X_test_eng_minmax, minmax_vectorizer = minmax_vectorizer.transform(X_test[cols_selected])
X_test_eng = X_test[cols_selected].to_numpy()
382/172: minmax_vectorizer
382/173:
#max_features=5000

# CountVectorizer
count_vectorizer = CountVectorizer(max_features=max_features)

def count_vector(data):
    count_vectorizer = CountVectorizer()
    vect = count_vectorizer.fit_transform(data)
    return vect, count_vectorizer

def tfidf_vector(data):
    tfidf_vectorizer = TfidfVectorizer()
    vect = tfidf_vectorizer.fit_transform(data)
    return vect, tfidf_vectorizer

def min_max(data):
    min_max_scaler = MinMaxScaler()
    vect = min_max_scaler.fit_transform(data)
    return vect, min_max_scaler

X_train_count, count_vectorizer = count_vector(X_train['clean_text'])
X_train_tfidf, tfidf_vectorizer = tfidf_vector(X_train['clean_text'])

X_train_eng_minmax, minmax_vectorizer = min_max(X_train[cols_selected])
X_train_eng = X_train[cols_selected].to_numpy()

#### TEST
X_test_count = count_vectorizer.transform(X_test ['clean_text'])                                                     
X_test_tfidf = tfidf_vectorizer.transform(X_test ['clean_text'])

X_test_eng_minmax = minmax_vectorizer.transform(X_test[cols_selected])
X_test_eng = X_test[cols_selected].to_numpy()
382/174: X_train_count.shpe
382/175: X_train_count.shape
382/176:
#max_features=5000

# CountVectorizer
count_vectorizer = CountVectorizer(max_features=max_features)

def count_vector(data):
    count_vectorizer = CountVectorizer()
    vect = count_vectorizer.fit_transform(data)
    return vect, count_vectorizer

def tfidf_vector(data):
    tfidf_vectorizer = TfidfVectorizer()
    vect = tfidf_vectorizer.fit_transform(data)
    return vect, tfidf_vectorizer

def min_max(data):
    min_max_scaler = MinMaxScaler()
    vect = min_max_scaler.fit_transform(data)
    return vect, min_max_scaler

X_train_count, count_vectorizer = count_vector(X_train['clean_text'])
X_train_tfidf, tfidf_vectorizer = tfidf_vector(X_train['clean_text'])

X_train_eng_minmax, minmax_vectorizer = min_max(X_train[cols_selected])
X_train_eng = X_train[cols_selected].to_numpy()

#### TEST
X_test_count = count_vectorizer.transform(X_test ['clean_text'])                                                     
X_test_tfidf = tfidf_vectorizer.transform(X_test ['clean_text'])

X_test_eng_minmax = minmax_vectorizer.transform(X_test[cols_selected])
X_test_eng = X_test[cols_selected].to_numpy()

### CONCAT tf-idf + minmax, count_vec + feature_eng

X_train_tfidf_minmax = hstack([X_train_tfidf, X_train_eng_minmax])
X_train_count_eng = hstack([X_train_count, X_train_eng])

X_test_tfidf_minmax = hstack([X_test_tfidf, X_test_eng_minmax])
X_test_count_eng = hstack([X_test_count, X_test_eng])
382/177: X_train_count.shape
382/178: X_train_eng.shape
382/179: X_train_count_eng.shape
382/180:
#max_features=5000

# CountVectorizer
count_vectorizer = CountVectorizer(max_features=max_features)

def count_vector(data):
    count_vectorizer = CountVectorizer()
    vect = count_vectorizer.fit_transform(data)
    return vect, count_vectorizer

def tfidf_vector(data):
    tfidf_vectorizer = TfidfVectorizer()
    vect = tfidf_vectorizer.fit_transform(data)
    return vect, tfidf_vectorizer

def min_max(data):
    min_max_scaler = MinMaxScaler()
    vect = min_max_scaler.fit_transform(data)
    return vect, min_max_scaler

X_train_count, count_vectorizer = count_vector(X_train['clean_text'])
X_train_tfidf, tfidf_vectorizer = tfidf_vector(X_train['clean_text'])

X_train_eng_minmax, minmax_scaler = min_max(X_train[cols_selected])
X_train_eng = X_train[cols_selected].to_numpy()

#### TEST
X_test_count = count_vectorizer.transform(X_test ['clean_text'])                                                     
X_test_tfidf = tfidf_vectorizer.transform(X_test ['clean_text'])

X_test_eng_minmax = minmax_scaler.transform(X_test[cols_selected])
X_test_eng = X_test[cols_selected].to_numpy()

### CONCAT tf-idf + minmax, count_vec + feature_eng

X_train_tfidf_minmax = hstack([X_train_tfidf, X_train_eng_minmax])
X_train_count_eng = hstack([X_train_count, X_train_eng])

X_test_tfidf_minmax = hstack([X_test_tfidf, X_test_eng_minmax])
X_test_count_eng = hstack([X_test_count, X_test_eng])
382/181:
with open('count_vectorizer.pickle', 'wb') as handle:
    pickle.dump(count_vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
with open('tfidf_vectorizer.pickle', 'wb') as handle:
    pickle.dump(tfidf_vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
with open('minmax_scaler.pickle', 'wb') as handle:
    pickle.dump(minmax_scaler, handle, protocol=pickle.HIGHEST_PROTOCOL)
382/182:
import pickle
from joblib import dump
382/183:  print('Threshold=%.3f, F-Score=%.5f' % (33.3333, 22))
382/184:  print('Threshold={}%.3f, F-Score=%.5f'.format(33.3333, 22))
382/185:  print('Threshold={%.3f}, F-Score=%.5f'.format(33.3333, 22))
382/186:  print('Threshold={.3f}, F-Score=%.5f'.format(33.3333, 22))
382/187:  print('Threshold={}, F-Score=%.5f'.format(33.3333, 22))
382/188:  print('Threshold={}.3f, F-Score=%.5f'.format(33.3333, 22))
382/189:  print('Threshold={:.2}, F-Score=%.5f'.format(33.3333, 22))
382/190:  print('Threshold={:.2f}, F-Score=%.5f'.format(33.3333, 22))
382/191:  print('Threshold={:.5f}, F-Score=%.5f'.format(33.3333, 22))
382/192:  print('Threshold={:.5f}, F-Score={.5f}'.format(33.3333, 22))
382/193:  print('Threshold={:.5f}, F-Score={:.5f}'.format(33.3333, 22))
382/194:
def fit_and_predict(model, x_train ,x_test, y_train, y_test, vectoriser):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    filename = classifier_name +" using "+ str(vectoriser)+".joblib"
    filename = filename.lower().replace(" ","_")
    dump(model, filename=filename)
    y_pred = classifier.predict(x_test)
    cmatrix = confusion_matrix(y_test,y_pred)
    
    y_hat = classifier.predict_proba(x_test)
    threshold, best_score = best_tresholds(y_hat, y_test)


    f1score = f1_score(y_test,y_pred,average='weighted')
    train_accuracy = round(classifier.score(x_train,y_train)*100)
    test_accuracy =  round(accuracy_score(y_test,y_pred)*100)

    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'f1 score': f1score, 
                              'train accuracy': train_accuracy, 
                              'test accuracy': test_accuracy, 
                              'vectoriser': str(vectoriser),
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using "+ str(vectoriser))
    print(classification_report(y_test,y_pred))    
    print('Threshold={:.5f}, F-Score={:.5f}'.format(threshold, best_score))
    print('Accuracy of classifier on training set:{}%'.format(train_accuracy))
    print('Accuracy of classifier on test set:{}%' .format(test_accuracy))
382/195: rf = RandomForest()
382/196:
from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier()
382/197:
from sklearn.ensemble import RandomForestClassifier
efc = RandomForestClassifier()
382/198:
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier()
382/199: fit_and_predict(rfc, X_train_tfidf, X_test_tfidf, y_train, y_test, tfidf_vector)
382/200:
def fit_and_predict(model, x_train ,x_test, y_train, y_test, vectoriser):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    filename = classifier_name +" using "+ str(vectoriser)+".joblib"
    filename = filename.lower().replace(" ","_")
    print(filename)
    dump(model, filename=filename)
    y_pred = classifier.predict(x_test)
    cmatrix = confusion_matrix(y_test,y_pred)
    
    y_hat = classifier.predict_proba(x_test)
    threshold, best_score = best_tresholds(y_hat, y_test)


    f1score = f1_score(y_test,y_pred,average='weighted')
    train_accuracy = round(classifier.score(x_train,y_train)*100)
    test_accuracy =  round(accuracy_score(y_test,y_pred)*100)

    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'f1 score': f1score, 
                              'train accuracy': train_accuracy, 
                              'test accuracy': test_accuracy, 
                              'vectoriser': str(vectoriser),
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using "+ str(vectoriser))
    print(classification_report(y_test,y_pred))    
    print('Threshold={:.5f}, F-Score={:.5f}'.format(threshold, best_score))
    print('Accuracy of classifier on training set:{}%'.format(train_accuracy))
    print('Accuracy of classifier on test set:{}%' .format(test_accuracy))
382/201:  print('Threshold={:.5f}, F-Score={:.5f}'.format(33.3333, 22))
382/202:
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier()
382/203: fit_and_predict(rfc, X_train_tfidf, X_test_tfidf, y_train, y_test, tfidf_vector)
382/204: fit_and_predict(rfc, X_train_tfidf, X_test_tfidf, y_train, y_test, TfidfVectorizer)
382/205: fit_and_predict(rfc, X_train_tfidf, X_test_tfidf, y_train, y_test, 'TfidfVectorizer')
382/206:
def to_labels(pos_probs, threshold):
    return (pos_probs >= threshold).astype('int')

def best_tresholds(y_pred_proba, y_test):
    # define thresholds
    thresholds = np.arange(0, 1, 0.001)
    # evaluate each threshold
    scores = [f1_score(y_test, to_labels(y_pred_proba, t)) for t in thresholds]
    # get best threshold
    ix = np.argmax(scores)
    #print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
    return thresholds[ix], scores[ix]
382/207: fit_and_predict(rfc, X_train_tfidf, X_test_tfidf, y_train, y_test, 'TfidfVectorizer')
382/208: X_train_tfidf.shape
382/209: X_test_tfidf.shape
382/210: rfc.fit(X_train_tfidf, X_test_tfidf)
382/211: rfc.fit(X_train_tfidf, y_train)
382/212: yyyy = rfc.predict_proba(X_test_tfidf)
382/213: yyyy = rfc.predict(X_test_tfidf)
382/214: f1_score(y_test, yyyy)
382/215:
def fit_and_predict(model, x_train ,x_test, y_train, y_test, vectoriser):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    filename = classifier_name +" using "+ str(vectoriser)+".joblib"
    filename = filename.lower().replace(" ","_")
    print(filename)
    dump(model, filename=filename)
    y_pred = classifier.predict(x_test)
    cmatrix = confusion_matrix(y_test,y_pred)
    
    y_hat = classifier.predict_proba(x_test)[:,1]
    threshold, best_score = best_tresholds(y_hat, y_test)


    f1score = f1_score(y_test,y_pred,average='weighted')
    train_accuracy = round(classifier.score(x_train,y_train)*100)
    test_accuracy =  round(accuracy_score(y_test,y_pred)*100)

    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'f1 score': f1score, 
                              'train accuracy': train_accuracy, 
                              'test accuracy': test_accuracy, 
                              'vectoriser': str(vectoriser),
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using "+ str(vectoriser))
    print(classification_report(y_test,y_pred))    
    print('Threshold={:.5f}, F-Score={:.5f}'.format(threshold, best_score))
    print('Accuracy of classifier on training set:{}%'.format(train_accuracy))
    print('Accuracy of classifier on test set:{}%' .format(test_accuracy))
382/216:  print('Threshold={:.5f}, F-Score={:.5f}'.format(33.3333, 22))
382/217:
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier()
382/218: X_train_tfidf.shape
382/219: X_test_tfidf.shape
382/220: rfc.fit(X_train_tfidf, y_train)
382/221: fit_and_predict(rfc, X_train_tfidf, X_test_tfidf, y_train, y_test, 'TfidfVectorizer')
382/222:
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import confusion_matrix, classification_report, roc_curve
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
from sklearn.metrics import f1_score, accuracy_score
382/223: fit_and_predict(rfc, X_train_tfidf, X_test_tfidf, y_train, y_test, 'TfidfVectorizer')
382/224:
def fit_and_predict(model, x_train ,x_test, y_train, y_test, vectoriser):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    filename = classifier_name +" using "+ str(vectoriser)+".joblib"
    filename = filename.lower().replace(" ","_")
    print(filename)
    dump(model, filename=filename)
    y_pred = classifier.predict(x_test)
    cmatrix = confusion_matrix(y_test,y_pred)
    
    y_hat = classifier.predict_proba(x_test)[:,1]
    threshold, best_score = best_tresholds(y_hat, y_test)


    f1score = f1_score(y_test,y_pred)
    train_accuracy = round(classifier.score(x_train,y_train)*100)
    test_accuracy =  round(accuracy_score(y_test,y_pred)*100)

    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'f1 score': f1score, 
                              'train accuracy': train_accuracy, 
                              'test accuracy': test_accuracy, 
                              'vectoriser': str(vectoriser),
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using "+ str(vectoriser))
    print('f1 score: {}'.format(f1score))
    print(classification_report(y_test,y_pred))    
    print('Threshold={:.5f}, F-Score={:.5f}'.format(threshold, best_score))
    print('Accuracy of classifier on training set:{}%'.format(train_accuracy))
    print('Accuracy of classifier on test set:{}%' .format(test_accuracy))
382/225:  print('Threshold={:.5f}, F-Score={:.5f}'.format(33.3333, 22))
382/226:
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier()
382/227: X_train_tfidf.shape
382/228: X_test_tfidf.shape
382/229: rfc.fit(X_train_tfidf, y_train)
382/230: fit_and_predict(rfc, X_train_tfidf, X_test_tfidf, y_train, y_test, 'TfidfVectorizer')
382/231:
def fit_and_predict(model, x_train ,x_test, y_train, y_test, vectoriser):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    filename = classifier_name +" using "+ str(vectoriser)+".joblib"
    filename = filename.lower().replace(" ","_")
    print(filename)
    dump(model, filename=filename)
    y_pred = classifier.predict(x_test)
    cmatrix = confusion_matrix(y_test,y_pred)
    
    y_hat = classifier.predict_proba(x_test)[:,1]
    threshold, best_score = best_tresholds(y_hat, y_test)


    f1score = f1_score(y_test,y_pred,average='weighted')
    train_accuracy = round(classifier.score(x_train,y_train)*100)
    test_accuracy =  round(accuracy_score(y_test,y_pred)*100)

    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'f1 score': f1score, 
                              'train accuracy': train_accuracy, 
                              'test accuracy': test_accuracy, 
                              'vectoriser': str(vectoriser),
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using "+ str(vectoriser))
    print('f1 score: {}'.format(f1score))
    print(classification_report(y_test,y_pred))    
    print('Threshold={:.5f}, F-Score={:.5f}'.format(threshold, best_score))
    print('Accuracy of classifier on training set:{}%'.format(train_accuracy))
    print('Accuracy of classifier on test set:{}%' .format(test_accuracy))
382/232:  print('Threshold={:.5f}, F-Score={:.5f}'.format(33.3333, 22))
382/233:
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier()
382/234: X_train_tfidf.shape
382/235: X_test_tfidf.shape
382/236: rfc.fit(X_train_tfidf, y_train)
382/237: fit_and_predict(rfc, X_train_tfidf, X_test_tfidf, y_train, y_test, 'TfidfVectorizer')
382/238:
abbreviations = {
    "$" : " dollar ",
    "‚Ç¨" : " euro ",
    "4ao" : "for adults only",
    "a.m" : "before midday",
    "a3" : "anytime anywhere anyplace",
    "aamof" : "as a matter of fact",
    "acct" : "account",
    "adih" : "another day in hell",
    "afaic" : "as far as i am concerned",
    "afaict" : "as far as i can tell",
    "afaik" : "as far as i know",
    "afair" : "as far as i remember",
    "afk" : "away from keyboard",
    "app" : "application",
    "approx" : "approximately",
    "apps" : "applications",
    "asap" : "as soon as possible",
    "asl" : "age, sex, location",
    "atk" : "at the keyboard",
    "ave." : "avenue",
    "aymm" : "are you my mother",
    "ayor" : "at your own risk", 
    "b&b" : "bed and breakfast",
    "b+b" : "bed and breakfast",
    "b.c" : "before christ",
    "b2b" : "business to business",
    "b2c" : "business to customer",
    "b4" : "before",
    "b4n" : "bye for now",
    "b@u" : "back at you",
    "bae" : "before anyone else",
    "bak" : "back at keyboard",
    "bbbg" : "bye bye be good",
    "bbc" : "british broadcasting corporation",
    "bbias" : "be back in a second",
    "bbl" : "be back later",
    "bbs" : "be back soon",
    "be4" : "before",
    "bfn" : "bye for now",
    "blvd" : "boulevard",
    "bout" : "about",
    "brb" : "be right back",
    "bros" : "brothers",
    "brt" : "be right there",
    "bsaaw" : "big smile and a wink",
    "btw" : "by the way",
    "bwl" : "bursting with laughter",
    "c/o" : "care of",
    "cet" : "central european time",
    "cf" : "compare",
    "cia" : "central intelligence agency",
    "csl" : "can not stop laughing",
    "cu" : "see you",
    "cul8r" : "see you later",
    "cv" : "curriculum vitae",
    "cwot" : "complete waste of time",
    "cya" : "see you",
    "cyt" : "see you tomorrow",
    "dae" : "does anyone else",
    "dbmib" : "do not bother me i am busy",
    "diy" : "do it yourself",
    "dm" : "direct message",
    "dwh" : "during work hours",
    "e123" : "easy as one two three",
    "eet" : "eastern european time",
    "eg" : "example",
    "embm" : "early morning business meeting",
    "encl" : "enclosed",
    "encl." : "enclosed",
    "etc" : "and so on",
    "faq" : "frequently asked questions",
    "fawc" : "for anyone who cares",
    "fb" : "facebook",
    "fc" : "fingers crossed",
    "fig" : "figure",
    "fimh" : "forever in my heart", 
    "ft." : "feet",
    "ft" : "featuring",
    "ftl" : "for the loss",
    "ftw" : "for the win",
    "fwiw" : "for what it is worth",
    "fyi" : "for your information",
    "g9" : "genius",
    "gahoy" : "get a hold of yourself",
    "gal" : "get a life",
    "gcse" : "general certificate of secondary education",
    "gfn" : "gone for now",
    "gg" : "good game",
    "gl" : "good luck",
    "glhf" : "good luck have fun",
    "gmt" : "greenwich mean time",
    "gmta" : "great minds think alike",
    "gn" : "good night",
    "g.o.a.t" : "greatest of all time",
    "goat" : "greatest of all time",
    "goi" : "get over it",
    "gps" : "global positioning system",
    "gr8" : "great",
    "gratz" : "congratulations",
    "gyal" : "girl",
    "h&c" : "hot and cold",
    "hp" : "horsepower",
    "hr" : "hour",
    "hrh" : "his royal highness",
    "ht" : "height",
    "ibrb" : "i will be right back",
    "ic" : "i see",
    "icq" : "i seek you",
    "icymi" : "in case you missed it",
    "idc" : "i do not care",
    "idgadf" : "i do not give a damn fuck",
    "idgaf" : "i do not give a fuck",
    "idk" : "i do not know",
    "ie" : "that is",
    "i.e" : "that is",
    "ifyp" : "i feel your pain",
    "IG" : "instagram",
    "iirc" : "if i remember correctly",
    "ilu" : "i love you",
    "ily" : "i love you",
    "imho" : "in my humble opinion",
    "imo" : "in my opinion",
    "imu" : "i miss you",
    "iow" : "in other words",
    "irl" : "in real life",
    "j4f" : "just for fun",
    "jic" : "just in case",
    "jk" : "just kidding",
    "jsyk" : "just so you know",
    "l8r" : "later",
    "lb" : "pound",
    "lbs" : "pounds",
    "ldr" : "long distance relationship",
    "lmao" : "laugh my ass off",
    "lmfao" : "laugh my fucking ass off",
    "lol" : "laughing out loud",
    "ltd" : "limited",
    "ltns" : "long time no see",
    "m8" : "mate",
    "mf" : "motherfucker",
    "mfs" : "motherfuckers",
    "mfw" : "my face when",
    "mofo" : "motherfucker",
    "mph" : "miles per hour",
    "mr" : "mister",
    "mrw" : "my reaction when",
    "ms" : "miss",
    "mte" : "my thoughts exactly",
    "nagi" : "not a good idea",
    "nbc" : "national broadcasting company",
    "nbd" : "not big deal",
    "nfs" : "not for sale",
    "ngl" : "not going to lie",
    "nhs" : "national health service",
    "nrn" : "no reply necessary",
    "nsfl" : "not safe for life",
    "nsfw" : "not safe for work",
    "nth" : "nice to have",
    "nvr" : "never",
    "nyc" : "new york city",
    "oc" : "original content",
    "og" : "original",
    "ohp" : "overhead projector",
    "oic" : "oh i see",
    "omdb" : "over my dead body",
    "omg" : "oh my god",
    "omw" : "on my way",
    "p.a" : "per annum",
    "p.m" : "after midday",
    "pm" : "prime minister",
    "poc" : "people of color",
    "pov" : "point of view",
    "pp" : "pages",
    "ppl" : "people",
    "prw" : "parents are watching",
    "ps" : "postscript",
    "pt" : "point",
    "ptb" : "please text back",
    "pto" : "please turn over",
    "qpsa" : "what happens", #"que pasa",
    "ratchet" : "rude",
    "rbtl" : "read between the lines",
    "rlrt" : "real life retweet", 
    "rofl" : "rolling on the floor laughing",
    "roflol" : "rolling on the floor laughing out loud",
    "rotflmao" : "rolling on the floor laughing my ass off",
    "rt" : "retweet",
    "ruok" : "are you ok",
    "sfw" : "safe for work",
    "sk8" : "skate",
    "smh" : "shake my head",
    "sq" : "square",
    "srsly" : "seriously", 
    "ssdd" : "same stuff different day",
    "tbh" : "to be honest",
    "tbs" : "tablespooful",
    "tbsp" : "tablespooful",
    "tfw" : "that feeling when",
    "thks" : "thank you",
    "tho" : "though",
    "thx" : "thank you",
    "tia" : "thanks in advance",
    "til" : "today i learned",
    "tl;dr" : "too long i did not read",
    "tldr" : "too long i did not read",
    "tmb" : "tweet me back",
    "tntl" : "trying not to laugh",
    "ttyl" : "talk to you later",
    "u" : "you",
    "u2" : "you too",
    "u4e" : "yours for ever",
    "utc" : "coordinated universal time",
    "w/" : "with",
    "w/o" : "without",
    "w8" : "wait",
    "wassup" : "what is up",
    "wb" : "welcome back",
    "wtf" : "what the fuck",
    "wtg" : "way to go",
    "wtpa" : "where the party at",
    "wuf" : "where are you from",
    "wuzup" : "what is up",
    "wywh" : "wish you were here",
    "yd" : "yard",
    "ygtr" : "you got that right",
    "ynk" : "you never know",
    "zzz" : "sleeping bored and tired"
}
382/239: df_train
382/240: The Latest: More Homes Razed by Northern California Wildfire - ABC News http://t.co/YmY4rSkQ3d
382/241: string_1 = 'The Latest: More Homes Razed by Northern California Wildfire - ABC News http://t.co/YmY4rSkQ3d'
382/242: delete_not_ascii(string_1)
382/243: remove_urls(string_1)
382/244:
#remove urls
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    return text

# Change an abbreviation by its true meaning
def word_abbrev(word):
    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word

# Replace all abbreviations
def replace_abbrev(text):
    string = ""
    for word in text.split():
        string += word_abbrev(word) + " "        
    return string

#remove @word 
def remove_social_media_tags(text):
    text = re.sub(r'@\S+', '', text)
    return text

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

#remove digits
def remove_digits(text):
    text = re.sub(r'[0-9]+', '', text)
    return text   

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()

#contractions and remowing stopwords and very short words
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 3:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
382/245:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(remove_social_media_tags)    
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(remove_digits)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(convert_lower_case)
   # df[column_name] = df[column_name].apply(string_contractions)
   # df['column_token'] = df[column_name].apply(word_tokenize)
    return df
382/246:
#remove urls
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    return text

# Change an abbreviation by its true meaning
def word_abbrev(word):
    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word

# Replace all abbreviations
def replace_abbrev(text):
    string = ""
    for word in text.split():
        string += word_abbrev(word) + " "        
    return string

# Remove @ and mention, replace by USER
def replace_mention(text):
    pat=re.compile(r'@\S+')
    return pat.sub(r'USER',text)

# Remove numbers, replace it by NUMBER
def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r'NUMBER', text)


##remove digits
#def remove_digits(text):
#    text = re.sub(r'[0-9]+', '', text)
#    return text   

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()

#contractions and remowing stopwords and very short words
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 3:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
382/247:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(convert_lower_case)
   # df[column_name] = df[column_name].apply(string_contractions)
   # df['column_token'] = df[column_name].apply(word_tokenize)
    return df
382/248:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(remove_emoji)
    
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(convert_lower_case)
   # df[column_name] = df[column_name].apply(string_contractions)
   # df['column_token'] = df[column_name].apply(word_tokenize)
    return df
382/249:
#remove urls
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    return text

# Change an abbreviation by its true meaning
def word_abbrev(word):
    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word

# Replace all abbreviations
def replace_abbrev(text):
    string = ""
    for word in text.split():
        string += word_abbrev(word) + " "        
    return string

# Remove @ and mention, replace by USER
def replace_mention(text):
    pat=re.compile(r'@\S+')
    return pat.sub(r'USER',text)

# Remove numbers, replace it by NUMBER
def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r'NUMBER', text)


##remove digits
#def remove_digits(text):
#    text = re.sub(r'[0-9]+', '', text)
#    return text   

# Remove all emojis, replace by EMOJI
def remove_emoji(text):
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'EMOJI', text)

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()

#contractions and remowing stopwords and very short words
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 3:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
382/250:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(remove_emoji)
    
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(convert_lower_case)
   # df[column_name] = df[column_name].apply(string_contractions)
   # df['column_token'] = df[column_name].apply(word_tokenize)
    return df
382/251:
#remove urls
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    return text

# Change an abbreviation by its true meaning
def word_abbrev(word):
    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word

# Replace all abbreviations
def replace_abbrev(text):
    string = ""
    for word in text.split():
        string += word_abbrev(word) + " "        
    return string

# Remove @ and mention, replace by USER
def replace_mention(text):
    pat=re.compile(r'@\S+')
    return pat.sub(r'USER',text)

# Remove numbers, replace it by NUMBER
def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r'NUMBER', text)


##remove digits
#def remove_digits(text):
#    text = re.sub(r'[0-9]+', '', text)
#    return text   

# Remove all emojis, replace by EMOJI
def remove_emoji(text):
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'EMOJI', text)

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()

#contractions and remowing stopwords and very short words
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 3:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
382/252:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(remove_emoji)
    
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(convert_lower_case)
   # df[column_name] = df[column_name].apply(string_contractions)
   # df['column_token'] = df[column_name].apply(word_tokenize)
    return df
382/253: df_text = df_train.loc[:, ['id', 'target', 'keyword', 'text']]
382/254: df_text = basic_cleaning(df_text, 'text')
382/255: df_text
382/256: df_text.test.str.contains('no digital')
382/257: df_text.text.str.contains('no digital')
382/258: df_text[df_text.text.str.contains('no digital')]
382/259: df_text[df_text.text.str.contains('mercyofallah ')]
382/260: df_text[df_text.text.str.contains('mercyofallah')]
382/261: df_text[df_text.text.str.contains('bitcoin ')]
382/262: df_text[df_text.text.str.contains('bitcoin')]
382/263: df_train[df_train.text.str.contains('bitcoin')]
382/264:
#remove urls
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    return text

# Change an abbreviation by its true meaning
def word_abbrev(word):
    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word

# Replace all abbreviations
def replace_abbrev(text):
    string = ""
    for word in text.split():
        string += word_abbrev(word) + " "        
    return string

# Remove @ and mention, replace by USER
def replace_mention(text):
    pat=re.compile(r'@\S+')
    return pat.sub(r'USER',text)

# Remove numbers, replace it by NUMBER
def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r'NUMBER', text)


##remove digits
#def remove_digits(text):
#    text = re.sub(r'[0-9]+', '', text)
#    return text   

# Remove all emojis, replace by EMOJI
def remove_emoji(text):
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'EMOJI', text)

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()

#contractions and remowing stopwords and very short words
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 3:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
382/265:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(remove_emoji)
    
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(convert_lower_case)
   # df[column_name] = df[column_name].apply(string_contractions)
   # df['column_token'] = df[column_name].apply(word_tokenize)
    df[column_name] = df[column_name].apply(remove_punct)
    return df
382/266: df_text = df_train.loc[:, ['id', 'target', 'keyword', 'text']]
382/267: df_text = basic_cleaning(df_text, 'text')
382/268: df_text[df_text.text.str.contains('bitcoin')]
382/269: df_train[df_train.text.str.contains('bitcoin')]
382/270: df_train[df_train.id == 4995]
382/271:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
382/272:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(remove_emoji)
    
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(convert_lower_case)
   # df[column_name] = df[column_name].apply(string_contractions)
   # df['column_token'] = df[column_name].apply(word_tokenize)
    df[column_name] = df[column_name].apply(remove_punct)
    return df
382/273: df_text = df_train.loc[:, ['id', 'target', 'keyword', 'text']]
382/274: df_text = basic_cleaning(df_text, 'text')
382/275: df_text[df_text.text.str.contains('bitcoin')]
382/276: df_train[df_train.text.str.contains('bitcoin')]
382/277: df_train[df_train.id == 4995]
382/278: df_train[df_train.text.str.contains('FIFA ')]
382/279: df_train[df_train.text.str.contains('FIFA')]
382/280: df_text[df_text.text.str.contains('fifa')]
382/281: df_text = df_test.loc[:, ['id', 'target', 'keyword', 'text']]
382/282: df_text = df_test.loc[:, ['id', 'keyword', 'text']]
382/283: df_text = basic_cleaning(df_text, 'text')
382/284: df_text[df_text.text.str.contains('fifa')]
382/285: df_test[df_test.text.str.contains('FIFA')]
382/286: df_train[df_train.text.str.contains('FIFA')]
382/287: df_copy = copy.deepcopy(df_train)
382/288:
cols_selected = ['GPE_text_flag', 'count_links', 'count_num_of_chars', 'count_punct', 'count_sent',
                 'count_mentions', 'count_stopwords', 'count_question', 'count_quotation',
                 'count_num_of_capital_chars']

df_copy = copy.deepcopy(df_train)
df_train_text_clean = basic_cleaning(df_copy, 'text')
df_train_text_clean["clean_text"] = df_train_text_clean.keyword.map(str) + " " + df_train_text_clean.text

## new features
df_train_feature_eng = feature_eng_fun(df_copy, 'text')
df_train_feature_eng = df_train_feature_eng[cols_selected]
382/289: df_train_text_clean.head(10)
382/290:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(remove_emoji)
    
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(string_contractions)
   # df['column_token'] = df[column_name].apply(word_tokenize)
    df[column_name] = df[column_name].apply(remove_punct)
    return df
382/291:
cols_selected = ['GPE_text_flag', 'count_links', 'count_num_of_chars', 'count_punct', 'count_sent',
                 'count_mentions', 'count_stopwords', 'count_question', 'count_quotation',
                 'count_num_of_capital_chars']

df_copy = copy.deepcopy(df_train)
df_train_text_clean = basic_cleaning(df_copy, 'text')
df_train_text_clean["clean_text"] = df_train_text_clean.keyword.map(str) + " " + df_train_text_clean.text

## new features
df_train_feature_eng = feature_eng_fun(df_copy, 'text')
df_train_feature_eng = df_train_feature_eng[cols_selected]
382/292: df_train_text_clean.head(10)
382/293: df_train_text_clean.shape
382/294: df_train_feature_eng.shape
382/295:
df_train_full = pd.merge(df_train_text_clean[['target', 'clean_text']], df_train_feature_eng, 
                         left_index=True, right_index=True)
382/296:
## split to test and train
y = df_train_full['target']
X = df_train_full.drop('target', axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
print('X_train shape', X_train.shape)
print('X_test shape', X_test.shape)
382/297:
#max_features=5000

# CountVectorizer
count_vectorizer = CountVectorizer(max_features=max_features)

def count_vector(data):
    count_vectorizer = CountVectorizer()
    vect = count_vectorizer.fit_transform(data)
    return vect, count_vectorizer

def tfidf_vector(data):
    tfidf_vectorizer = TfidfVectorizer()
    vect = tfidf_vectorizer.fit_transform(data)
    return vect, tfidf_vectorizer

def min_max(data):
    min_max_scaler = MinMaxScaler()
    vect = min_max_scaler.fit_transform(data)
    return vect, min_max_scaler

X_train_count, count_vectorizer = count_vector(X_train['clean_text'])
X_train_tfidf, tfidf_vectorizer = tfidf_vector(X_train['clean_text'])

X_train_eng_minmax, minmax_scaler = min_max(X_train[cols_selected])
X_train_eng = X_train[cols_selected].to_numpy()

#### TEST
X_test_count = count_vectorizer.transform(X_test ['clean_text'])                                                     
X_test_tfidf = tfidf_vectorizer.transform(X_test ['clean_text'])

X_test_eng_minmax = minmax_scaler.transform(X_test[cols_selected])
X_test_eng = X_test[cols_selected].to_numpy()

### CONCAT tf-idf + minmax, count_vec + feature_eng

X_train_tfidf_minmax = hstack([X_train_tfidf, X_train_eng_minmax])
X_train_count_eng = hstack([X_train_count, X_train_eng])

X_test_tfidf_minmax = hstack([X_test_tfidf, X_test_eng_minmax])
X_test_count_eng = hstack([X_test_count, X_test_eng])
382/298: X_train_count.shape
382/299: X_train_eng.shape
382/300: X_train_count_eng.shape
382/301:
with open('count_vectorizer.pickle', 'wb') as handle:
    pickle.dump(count_vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
with open('tfidf_vectorizer.pickle', 'wb') as handle:
    pickle.dump(tfidf_vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
with open('minmax_scaler.pickle', 'wb') as handle:
    pickle.dump(minmax_scaler, handle, protocol=pickle.HIGHEST_PROTOCOL)
382/302: metrics = pd.DataFrame(columns=['model' ,'vectoriser', 'f1 score', 'train accuracy','test accuracy'])
382/303:
def fit_and_predict(model, x_train ,x_test, y_train, y_test, vectoriser):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    filename = classifier_name +" using "+ str(vectoriser)+".joblib"
    filename = filename.lower().replace(" ","_")
    print(filename)
    dump(model, filename=filename)
    y_pred = classifier.predict(x_test)
    cmatrix = confusion_matrix(y_test,y_pred)
    
    y_hat = classifier.predict_proba(x_test)[:,1]
    threshold, best_score = best_tresholds(y_hat, y_test)


    f1score = f1_score(y_test,y_pred,average='weighted')
    train_accuracy = round(classifier.score(x_train,y_train)*100)
    test_accuracy =  round(accuracy_score(y_test,y_pred)*100)

    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'f1 score': f1score, 
                              'train accuracy': train_accuracy, 
                              'test accuracy': test_accuracy, 
                              'vectoriser': str(vectoriser),
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using "+ str(vectoriser))
    print('f1 score: {}'.format(f1score))
    print(classification_report(y_test,y_pred))    
    print('Threshold={:.5f}, F-Score={:.5f}'.format(threshold, best_score))
    print('Accuracy of classifier on training set:{}%'.format(train_accuracy))
    print('Accuracy of classifier on test set:{}%' .format(test_accuracy))
382/304: fit_and_predict(rfc, X_train_tfidf, X_test_tfidf, y_train, y_test, 'TfidfVectorizer')
382/305:
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier()
rfc.fit(X_train_tfidf, y_train)
rfc.predict_proba(X_test_tfidf)
382/306:
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier()
rfc.fit(X_train_tfidf, y_train)
pp = rfc.predict_proba(X_test_tfidf)[:, 1]
382/307: pp
382/308: yyyy = (pp >= 0.37200).astype(int)
382/309: yyyy
382/310: f1_score(y_test, yyyy)
382/311: yyyy = (pp >= 0.500).astype(int)
382/312: f1_score(y_test, yyyy)
382/313: confusion_matrix(y_test, yyyy)
382/314:
def get_confusion_matrix_values(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    return(cm[0][0], cm[0][1], cm[1][0], cm[1][1])

TP, FP, FN, TN = get_confusion_matrix_valuesy_test, yyyy)
382/315:
def get_confusion_matrix_values(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    return(cm[0][0], cm[0][1], cm[1][0], cm[1][1])

TP, FP, FN, TN = get_confusion_matrix_values(y_test, yyyy))
382/316:
def get_confusion_matrix_values(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    return(cm[0][0], cm[0][1], cm[1][0], cm[1][1])

TP, FP, FN, TN = get_confusion_matrix_values(y_test, yyyy)
382/317:
prec = TP/(TP + FP)
rec = TP/(TP + FN)
382/318: 2*prec*rec/(prec + rec)
382/319: yyyy = (pp >= 0.500).astype(int)
382/320: f1_score(y_test, yyyy)
382/321:
def get_confusion_matrix_values(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    print(confusion_matrix(y_true, y_pred))
    return(cm[0][0], cm[0][1], cm[1][0], cm[1][1])

TP, FP, FN, TN = get_confusion_matrix_values(y_test, yyyy)
382/322: TP
382/323: FP
382/324: tn, fp, fn, tp = confusion_matrix(y_test, yyyy).ravel()
382/325: tp
382/326: TP
382/327: y_test
382/328: [a == b for (a, b) in itertools.product(yyyy, y_test)]
382/329: import itertools
382/330: [a == b for (a, b) in itertools.product(yyyy, y_test)]
382/331: [a == b for (a, b) in itertools.product(yyyy, y_test)].sum()
382/332: [a == b for (a, b) in itertools.product(yyyy, y_test)]
382/333: [a == b for (a, b) in itertools.product(yyyy, y_test)].astype(int)
382/334: [a == b for (a, b) in itertools.product(yyyy, y_test)]
382/335: sum([a == b for (a, b) in itertools.product(yyyy, y_test)])
382/336: ([a == b for (a, b) in itertools.product(yyyy, y_test)])
382/337: ([a == b for (a, b) in itertools.product(yyyy, y_test)]).astype(int)
382/338: np.array(([a == b for (a, b) in itertools.product(yyyy, y_test)]), dtype=np.float32)
382/339: np.array(([a == b for (a, b) in itertools.product(yyyy, y_test)]), dtype=np.float32).sum()
382/340: np.array(([a == b for (a, b) in itertools.product(yyyy, y_test)]), dtype=np.float32)
382/341: ([a == b for (a, b) in itertools.product(yyyy, y_test)])
382/342: len([a == b for (a, b) in itertools.product(yyyy, y_test)])
382/343:
def perf_measure(y_actual, y_hat):
    TP = 0
    FP = 0
    TN = 0
    FN = 0

    for i in range(len(y_hat)): 
        if y_actual[i]==y_hat[i]==1:
            TP += 1
        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:
            FP += 1
        if y_actual[i]==y_hat[i]==0:
            TN += 1
        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:
            FN += 1

    return(TP, FP, TN, FN)
382/344: perf_measure(y_test, yyyy)
382/345: yyyy = (pp >= 0.500).astype(int)
382/346: f1_score(y_test, yyyy)
382/347: perf_measure(y_test, yyyy)
382/348: y_actual
382/349: y_test
382/350: yyyy
382/351: perf_measure(list(y_test), yyyy)
382/352: fp
382/353: tn, fp, fn, tp = confusion_matrix(y_test, yyyy).ravel()
382/354: TN, FP, FN, TP = confusion_matrix(y_test, yyyy).ravel()
382/355:
prec = TP/(TP + FP)
rec = TP/(TP + FN)
382/356: 2*prec*rec/(prec + rec)
382/357:
def fit_and_predict(model, x_train ,x_test, y_train, y_test, vectoriser):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    filename = classifier_name +" using "+ str(vectoriser)+".joblib"
    filename = filename.lower().replace(" ","_")
    print(filename)
    dump(model, filename=filename)
    y_pred = classifier.predict(x_test)
    cmatrix = confusion_matrix(y_test,y_pred)
    
    y_hat = classifier.predict_proba(x_test)[:,1]
    threshold, best_score = best_tresholds(y_hat, y_test)


    f1score = f1_score(y_test,y_pred)
    train_accuracy = round(classifier.score(x_train,y_train)*100)
    test_accuracy =  round(accuracy_score(y_test,y_pred)*100)

    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'f1 score': f1score, 
                              'train accuracy': train_accuracy, 
                              'test accuracy': test_accuracy, 
                              'vectoriser': str(vectoriser),
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using "+ str(vectoriser))
    print('f1 score: {}'.format(f1score))
    print(classification_report(y_test,y_pred))    
    print('Threshold={:.5f}, F-Score={:.5f}'.format(threshold, best_score))
    print('Accuracy of classifier on training set:{}%'.format(train_accuracy))
    print('Accuracy of classifier on test set:{}%' .format(test_accuracy))
382/358: fit_and_predict(rfc, X_train_tfidf, X_test_tfidf, y_train, y_test, 'TfidfVectorizer')
382/359:
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier()
rfc.fit(X_train_tfidf, y_train)
pp = rfc.predict_proba(X_test_tfidf)[:, 1]
382/360: yyyy = (pp >= 0.500).astype(int)
382/361: f1_score(y_test, yyyy)
382/362: yyyy = (pp > 0.500).astype(int)
382/363: f1_score(y_test, yyyy)
382/364:
def fit_and_predict(model, x_train ,x_test, y_train, y_test, vectoriser):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    filename = classifier_name +" using "+ str(vectoriser)+".joblib"
    filename = filename.lower().replace(" ","_")
    print(filename)
    dump(model, filename=filename)
    y_pred = classifier.predict(x_test)
    cmatrix = confusion_matrix(y_test,y_pred)
    
    y_hat = classifier.predict_proba(x_test)[:,1]
    threshold, best_score = best_tresholds(y_hat, y_test)


    f1score = f1_score(y_test,y_pred)
    train_accuracy = round(classifier.score(x_train,y_train)*100)
    test_accuracy =  round(accuracy_score(y_test,y_pred)*100)

    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'f1 score': f1score, 
                              'train accuracy': train_accuracy, 
                              'test accuracy': test_accuracy, 
                              'vectoriser': str(vectoriser),
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using "+ str(vectoriser))
    print('f1 score for test: {}'.format(f1score))
    print(classification_report(y_test,y_pred))    
    print('Threshold={:.5f}, F-Score={:.5f}'.format(threshold, best_score))
    print('Accuracy of classifier on training set:{}%'.format(train_accuracy))
    print('Accuracy of classifier on test set:{}%' .format(test_accuracy))
382/365: fit_and_predict(rfc, X_train_tfidf, X_test_tfidf, y_train, y_test, 'TfidfVectorizer')
382/366: metrics
382/367:
models=[
        XGBClassifier(max_depth=6, n_estimators=1000),
        LogisticRegression(random_state=random_state),
        SVC(random_state=random_state),
        MultinomialNB(),
        DecisionTreeClassifier(random_state = random_state),
        KNeighborsClassifier(),
        RandomForestClassifier(random_state=random_state),
       ]
382/368: import xgboost as xgb
382/369: !conda install xgboost
382/370: import xgboost as xgb
382/371:
models=[
   #     XGBClassifier(max_depth=6, n_estimators=1000),
        LogisticRegression(random_state=random_state),
        SVC(random_state=random_state),
        MultinomialNB(),
        DecisionTreeClassifier(random_state = random_state),
        KNeighborsClassifier(),
        RandomForestClassifier(random_state=random_state),
       ]
382/372: from sklearn.linear_model import LogisticRegression
382/373:
models=[
   #     XGBClassifier(max_depth=6, n_estimators=1000),
        LogisticRegression(random_state=random_state),
        SVC(random_state=random_state),
        MultinomialNB(),
        DecisionTreeClassifier(random_state = random_state),
        KNeighborsClassifier(),
        RandomForestClassifier(random_state=random_state),
       ]
382/374:
models=[
   #     XGBClassifier(max_depth=6, n_estimators=1000),
        LogisticRegression(random_state=30),
        SVC(random_state=random_state),
        MultinomialNB(),
        DecisionTreeClassifier(random_state = random_state),
        KNeighborsClassifier(),
        RandomForestClassifier(random_state=random_state),
       ]
382/375:
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
382/376:
models=[
   #     XGBClassifier(max_depth=6, n_estimators=1000),
        LogisticRegression(random_state=30),
        SVC(random_state=random_state),
        MultinomialNB(),
        DecisionTreeClassifier(random_state = random_state),
        KNeighborsClassifier(),
        RandomForestClassifier(random_state=random_state),
       ]
382/377:
models=[
   #     XGBClassifier(max_depth=6, n_estimators=1000),
        LogisticRegression(random_state=30),
        SVC(random_state=30),
        MultinomialNB(),
        DecisionTreeClassifier(random_state = random_state),
        KNeighborsClassifier(),
        RandomForestClassifier(random_state=random_state),
       ]
382/378:
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
382/379:
models=[
   #     XGBClassifier(max_depth=6, n_estimators=1000),
        LogisticRegression(random_state=30),
        SVC(random_state=30),
        MultinomialNB(),
        DecisionTreeClassifier(random_state = random_state),
        KNeighborsClassifier(),
        RandomForestClassifier(random_state=random_state),
       ]
382/380:
models=[
   #     XGBClassifier(max_depth=6, n_estimators=1000),
        LogisticRegression(random_state=30),
        SVC(random_state=30),
        MultinomialNB(),
        DecisionTreeClassifier(random_state = 30),
        KNeighborsClassifier(),
        RandomForestClassifier(random_state=30),
       ]
382/381:
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
382/382:
models=[
   #     XGBClassifier(max_depth=6, n_estimators=1000),
        LogisticRegression(random_state=30),
        SVC(random_state=30),
        MultinomialNB(),
        DecisionTreeClassifier(random_state = 30),
        KNeighborsClassifier(),
        RandomForestClassifier(random_state=30),
       ]
382/383: metrics = pd.DataFrame(columns=['model' ,'vectoriser', 'f1 score', 'best_f1', 'train accuracy','test accuracy'])
382/384:
def fit_and_predict(model, x_train ,x_test, y_train, y_test, vectoriser):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    filename = classifier_name +" using "+ str(vectoriser)+".joblib"
    filename = filename.lower().replace(" ","_")
    print(filename)
    dump(model, filename=filename)
    y_pred = classifier.predict(x_test)
    cmatrix = confusion_matrix(y_test,y_pred)
    
    y_hat = classifier.predict_proba(x_test)[:,1]
    threshold, best_score = best_tresholds(y_hat, y_test)


    f1score = f1_score(y_test,y_pred)
    train_accuracy = round(classifier.score(x_train,y_train)*100)
    test_accuracy =  round(accuracy_score(y_test,y_pred)*100)

    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'f1 score': f1score, 
                              'best_f1': best_score
                              'threshold': threshold
                              'train accuracy': train_accuracy, 
                              'test accuracy': test_accuracy, 
                              'vectoriser': str(vectoriser),
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using "+ str(vectoriser))
    print('f1 score for test: {}'.format(f1score))
    print(classification_report(y_test,y_pred))    
    print('Threshold={:.5f}, F-Score={:.5f}'.format(threshold, best_score))
    print('Accuracy of classifier on training set:{}%'.format(train_accuracy))
    print('Accuracy of classifier on test set:{}%' .format(test_accuracy))
382/385:
def fit_and_predict(model, x_train ,x_test, y_train, y_test, vectoriser):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    filename = classifier_name +" using "+ str(vectoriser)+".joblib"
    filename = filename.lower().replace(" ","_")
    print(filename)
    dump(model, filename=filename)
    y_pred = classifier.predict(x_test)
    cmatrix = confusion_matrix(y_test,y_pred)
    
    y_hat = classifier.predict_proba(x_test)[:,1]
    threshold, best_score = best_tresholds(y_hat, y_test)


    f1score = f1_score(y_test,y_pred)
    train_accuracy = round(classifier.score(x_train,y_train)*100)
    test_accuracy =  round(accuracy_score(y_test,y_pred)*100)

    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'f1 score': f1score, 
                              'best_f1': best_score,
                              'threshold': threshold,
                              'train accuracy': train_accuracy, 
                              'test accuracy': test_accuracy, 
                              'vectoriser': str(vectoriser),
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using "+ str(vectoriser))
    print('f1 score for test: {}'.format(f1score))
    print(classification_report(y_test,y_pred))    
    print('Threshold={:.5f}, F-Score={:.5f}'.format(threshold, best_score))
    print('Accuracy of classifier on training set:{}%'.format(train_accuracy))
    print('Accuracy of classifier on test set:{}%' .format(test_accuracy))
382/386:
models=[
   #     XGBClassifier(max_depth=6, n_estimators=1000),
        LogisticRegression(random_state=30),
    #    SVC(random_state=30),
    #    MultinomialNB(),
    #    DecisionTreeClassifier(random_state = 30),
    #    KNeighborsClassifier(),
    #    RandomForestClassifier(random_state=30),
       ]
382/387:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train_count, X_test_count, y_train ,y_test, 'Count vector')
    
    fit_and_predict(model, X_train_tfidf, X_test_tfidf, y_train, y_test, 'Tfidf vector')
    
    ## count with EF
    fit_and_predict(model, X_train_count_eng, X_test_count_eng, y_train, y_test, 'Count vector + FE')
    
    ## TF IDF with EF
    fit_and_predict(model, X_train_tfidf_minmax, X_test_tfidf_minmax, y_train, y_test, 'TF-IDF + FE')
382/388: metrics
382/389:
metrics = pd.DataFrame(columns=['model' ,'vectoriser', 'f1 score', 'best_f1', 'threshold', 
                                'train accuracy','test accuracy'])
382/390:
models=[
   #     XGBClassifier(max_depth=6, n_estimators=1000),
        LogisticRegression(random_state=30),
        SVC(random_state=30),
        MultinomialNB(),
        DecisionTreeClassifier(random_state = 30),
        KNeighborsClassifier(),
        RandomForestClassifier(random_state=30),
       ]
382/391:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train_count, X_test_count, y_train ,y_test, 'Count vector')
    
    fit_and_predict(model, X_train_tfidf, X_test_tfidf, y_train, y_test, 'Tfidf vector')
    
    ## count with EF
    fit_and_predict(model, X_train_count_eng, X_test_count_eng, y_train, y_test, 'Count vector + FE')
    
    ## TF IDF with EF
    fit_and_predict(model, X_train_tfidf_minmax, X_test_tfidf_minmax, y_train, y_test, 'TF-IDF + FE')
382/392: metrics
382/393:
models=[
   #     XGBClassifier(max_depth=6, n_estimators=1000),
        LogisticRegression(random_state=30),
        SVC(random_state=30, probability=True),
        MultinomialNB(),
        DecisionTreeClassifier(random_state = 30),
        KNeighborsClassifier(),
        RandomForestClassifier(random_state=30),
       ]
382/394:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train_count, X_test_count, y_train ,y_test, 'Count vector')
    
    fit_and_predict(model, X_train_tfidf, X_test_tfidf, y_train, y_test, 'Tfidf vector')
    
    ## count with EF
    fit_and_predict(model, X_train_count_eng, X_test_count_eng, y_train, y_test, 'Count vector + FE')
    
    ## TF IDF with EF
    fit_and_predict(model, X_train_tfidf_minmax, X_test_tfidf_minmax, y_train, y_test, 'TF-IDF + FE')
382/395: metrics
382/396: metrics.sort_values('best_f1')
382/397: df_test
382/398:
cols_selected = ['GPE_text_flag', 'count_links', 'count_num_of_chars', 'count_punct', 'count_sent',
                 'count_mentions', 'count_stopwords', 'count_question', 'count_quotation',
                 'count_num_of_capital_chars']

df_copy = copy.deepcopy(df_train)
df_train_text_clean = basic_cleaning(df_copy, 'text')
df_train_text_clean["clean_text"] = df_train_text_clean.keyword.map(str) + " " + df_train_text_clean.text

## new features
df_train_feature_eng = feature_eng_fun(df_copy, 'text')
df_train_feature_eng = df_train_feature_eng[cols_selected]


#### for test
df_test_text_clean_f = basic_cleaning(df_test, 'text')
df_test_text_clean_f["clean_text"] = df_test_text_clean_f.keyword.map(str) + " " + df_test_text_clean_f.text

df_test_feature_eng_f = feature_eng_fun(basic_cleaning, 'text')
df_test_feature_eng_f = df_test_feature_eng_f[cols_selected]
382/399:
cols_selected = ['GPE_text_flag', 'count_links', 'count_num_of_chars', 'count_punct', 'count_sent',
                 'count_mentions', 'count_stopwords', 'count_question', 'count_quotation',
                 'count_num_of_capital_chars']

df_copy = copy.deepcopy(df_train)
df_train_text_clean = basic_cleaning(df_copy, 'text')
df_train_text_clean["clean_text"] = df_train_text_clean.keyword.map(str) + " " + df_train_text_clean.text

## new features
df_train_feature_eng = feature_eng_fun(df_copy, 'text')
df_train_feature_eng = df_train_feature_eng[cols_selected]


#### for test
df_test_text_clean_f = basic_cleaning(df_test, 'text')
df_test_text_clean_f["clean_text"] = df_test_text_clean_f.keyword.map(str) + " " + df_test_text_clean_f.text

df_test_feature_eng_f = feature_eng_fun(df_test, 'text')
df_test_feature_eng_f = df_test_feature_eng_f[cols_selected]
382/400: df_train_text_clean.shape
382/401: df_train_feature_eng.shape
382/402:
df_train_full = pd.merge(df_train_text_clean[['target', 'clean_text']], df_train_feature_eng, 
                         left_index=True, right_index=True)

df_test_full = pd.merge(df_test_text_clean_f[['target', 'clean_text']], df_test_feature_eng_f, 
                         left_index=True, right_index=True)
382/403:
df_train_full = pd.merge(df_train_text_clean[['target', 'clean_text']], df_train_feature_eng, 
                         left_index=True, right_index=True)

df_test_full = pd.merge(df_test_text_clean_f[['clean_text']], df_test_feature_eng_f, 
                         left_index=True, right_index=True)
382/404:
#max_features=5000

# CountVectorizer
count_vectorizer = CountVectorizer(max_features=max_features)

def count_vector(data):
    count_vectorizer = CountVectorizer()
    vect = count_vectorizer.fit_transform(data)
    return vect, count_vectorizer

def tfidf_vector(data):
    tfidf_vectorizer = TfidfVectorizer()
    vect = tfidf_vectorizer.fit_transform(data)
    return vect, tfidf_vectorizer

def min_max(data):
    min_max_scaler = MinMaxScaler()
    vect = min_max_scaler.fit_transform(data)
    return vect, min_max_scaler

X_train_count, count_vectorizer = count_vector(X_train['clean_text'])
X_train_tfidf, tfidf_vectorizer = tfidf_vector(X_train['clean_text'])

X_train_eng_minmax, minmax_scaler = min_max(X_train[cols_selected])
X_train_eng = X_train[cols_selected].to_numpy()

#### TEST
X_test_count = count_vectorizer.transform(X_test ['clean_text'])                                                     
X_test_tfidf = tfidf_vectorizer.transform(X_test ['clean_text'])

X_test_eng_minmax = minmax_scaler.transform(X_test[cols_selected])
X_test_eng = X_test[cols_selected].to_numpy()

### TEST DATASET
X_test_count_f = count_vectorizer.transform(df_test_full['clean_text'])                                                     
X_test_tfidf_f = tfidf_vectorizer.transform(df_test_full['clean_text'])

X_test_eng_minmax_f = minmax_scaler.transform(df_test_full[cols_selected])
X_test_eng_f = df_test_full[cols_selected].to_numpy()


### CONCAT tf-idf + minmax, count_vec + feature_eng

X_train_tfidf_minmax = hstack([X_train_tfidf, X_train_eng_minmax])
X_train_count_eng = hstack([X_train_count, X_train_eng])


X_test_tfidf_minmax = hstack([X_test_tfidf, X_test_eng_minmax])
X_test_count_eng = hstack([X_test_count, X_test_eng])

X_test_tfidf_minmax_f = hstack([X_test_tfidf_f, X_test_eng_minmax_f])
X_test_count_eng_f = hstack([X_test_count_f, X_test_eng_f]
382/405:
#max_features=5000

# CountVectorizer
count_vectorizer = CountVectorizer(max_features=max_features)

def count_vector(data):
    count_vectorizer = CountVectorizer()
    vect = count_vectorizer.fit_transform(data)
    return vect, count_vectorizer

def tfidf_vector(data):
    tfidf_vectorizer = TfidfVectorizer()
    vect = tfidf_vectorizer.fit_transform(data)
    return vect, tfidf_vectorizer

def min_max(data):
    min_max_scaler = MinMaxScaler()
    vect = min_max_scaler.fit_transform(data)
    return vect, min_max_scaler

X_train_count, count_vectorizer = count_vector(X_train['clean_text'])
X_train_tfidf, tfidf_vectorizer = tfidf_vector(X_train['clean_text'])

X_train_eng_minmax, minmax_scaler = min_max(X_train[cols_selected])
X_train_eng = X_train[cols_selected].to_numpy()

#### TEST
X_test_count = count_vectorizer.transform(X_test ['clean_text'])                                                     
X_test_tfidf = tfidf_vectorizer.transform(X_test ['clean_text'])

X_test_eng_minmax = minmax_scaler.transform(X_test[cols_selected])
X_test_eng = X_test[cols_selected].to_numpy()

### TEST DATASET
X_test_count_f = count_vectorizer.transform(df_test_full['clean_text'])                                                     
X_test_tfidf_f = tfidf_vectorizer.transform(df_test_full['clean_text'])

X_test_eng_minmax_f = minmax_scaler.transform(df_test_full[cols_selected])
X_test_eng_f = df_test_full[cols_selected].to_numpy()


### CONCAT tf-idf + minmax, count_vec + feature_eng

X_train_tfidf_minmax = hstack([X_train_tfidf, X_train_eng_minmax])
X_train_count_eng = hstack([X_train_count, X_train_eng])


X_test_tfidf_minmax = hstack([X_test_tfidf, X_test_eng_minmax])
X_test_count_eng = hstack([X_test_count, X_test_eng])

X_test_tfidf_minmax_f = hstack([X_test_tfidf_f, X_test_eng_minmax_f])
X_test_count_eng_f = hstack([X_test_count_f, X_test_eng_f])
382/406: X_train_count.shape
382/407: X_train_eng.shape
382/408: X_train_count_eng.shape
382/409:
lr = LogisticRegression(random_state=30)
lr.fit(X_train_tfidf_minmax, y_train)
382/410: lr.predict(X_test_tfidf_minmax)
382/411: y_predict_ir = lr.predict(X_test_tfidf_minmax)
382/412: f1_score(y_test, y_predict_ir)
382/413:
y_predict_ir = lr.predict(X_test_tfidf_minmax)
y_predprob = lr.predict_proba(X_test_tfidf_minmax_f)
382/414: y_predprob
382/415:
y_predict_ir = lr.predict(X_test_tfidf_minmax)
y_predprob = lr.predict_proba(X_test_tfidf_minmax_f)[:, 1]
382/416: f1_score(y_test, y_predict_ir)
382/417: y_predprob
382/418:
df_t = df_test[['id']]
df_t['target'] = (y_predprob >= 0.450).astype(int)
382/419:
df_t[:] = df_test[['id']]
df_t['target'] = (y_predprob >= 0.450).astype(int)
382/420:
df_t = df_test.loc[:, 'id']
df_t['target'] = (y_predprob >= 0.450).astype(int)
382/421: (y_predprob >= 0.450).astype(int)
382/422:
df_t = df_test.loc[:, 'id']
df_t['target'] = pd.Series((y_predprob >= 0.450).astype(int))
382/423:
df_t = df_test.loc[:, 'id']
df_t['target'] = (y_predprob >= 0.450).astype(int).to_list()
382/424:
df_t = df_test.loc[:, 'id']
df_t['target'] = (y_predprob >= 0.450).astype(int).tolist()
382/425: df_t
382/426:
df_t = df_test.loc[:, 'id']
df_t['target'] = (y_predprob >= 0.450).astype(int)
382/427: df_t
382/428:
df_t['id'] = df_test.loc[:, 'id']
df_t['target'] = (y_predprob >= 0.450).astype(int)
382/429: df_t
382/430:
df_t['id'] = df_test.loc[:, 'id']
#df_t['target'] = (y_predprob >= 0.450).astype(int)
382/431: df_t
382/432:
df_t['id'] = df_test[['id']]
#df_t['target'] = (y_predprob >= 0.450).astype(int)
382/433: df_t
382/434: del df_t
382/435:
df_t['id'] = df_test[['id']]
#df_t['target'] = (y_predprob >= 0.450).astype(int)
382/436: df_t
382/437:
df_t = df_test[['id']]
#df_t['target'] = (y_predprob >= 0.450).astype(int)
382/438: df_t
382/439:
df_t = df_test[['id']]
df_t['target'] = (y_predprob >= 0.450).astype(int)
382/440: df_t
382/441: del df_t
382/442:
df_t = df_test[['id']]
df_t['target'] = (y_predprob >= 0.450).astype(int).tolist()
382/443: del df_t
382/444:
df_t = df_test[['id']]
df_t['target'] = (y_predprob >= 0.419).astype(int)
382/445: df_t
382/446: df_t.to_csv("submission_logit.csv", index=False)
382/447:
df_t = df_test[['id']]
df_t['target'] = (y_predprob >= 0.5.astype(int)
382/448:
df_t = df_test[['id']]
df_t['target'] = (y_predprob >= 0.5).astype(int)
382/449: df_t.to_csv("submission_logit_05.csv", index=False)
382/450:
df_t = df_test[['id']]
df_t['target'] = (y_predprob >= 0.6).astype(int)
382/451: df_t.to_csv("submission_logit_06.csv", index=False)
382/452: df_train[df_train.text.str.contains('#wildfires')]
382/453: df_test[df_text.id == 6]
382/454: df_train_text_clean[df_train_text_clean.id == 6]
382/455:
normal_words =' '.join([text for text in df_location['GPE_flag'][df_location['target'] == 1]]) 
wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words) 
plt.figure(figsize=(10, 7)) 
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()
382/456: df_location
382/457: [text for text in df_location['GPE_flag'][df_location['target'] == 1]]
382/458: [text for text in df_location['GPE_flag']]
384/1:
import pandas as pd
import numpy as np
import re
import copy
from collections import Counter
import string
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline
384/2: pd.set_option('max_colwidth', None)
384/3:
import matplotlib.pyplot as plt
import seaborn as sns
384/4:
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.tokenize.treebank import TreebankWordDetokenizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer

import emoji as emj
import spacy 
## It's important to use _lg for OOV in future
nlp = spacy.load('en_core_web_lg')
384/5:
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import confusion_matrix, classification_report, roc_curve
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
from sklearn.metrics import f1_score, accuracy_score
384/6:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
384/7:
nltk.download()
STOPWORDS = set(stopwords.words('english'))
384/8: df_train.head()
384/9: df_train.isnull().mean()
384/10: df_train.shape, df_test.shape
384/11:
#cardinality check
for col in df_train.columns:
    print('{} has {} unique instances'.format(col, len(df_train[col].unique())))
384/12: df_train.info()
384/13: df_train.keyword.value_counts()
384/14:
plt.figure(figsize=[8, 40])
sns.countplot(y='keyword',
              data=df_train,
              color='lightblue',
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
384/15:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword', 
              hue='target', 
              data=df_train, 
              palette=['lightblue', 'red']
             )
plt.xticks(rotation=90)
plt.legend(loc='upper right')
plt.title("Distribution of Target per Keyword ")
plt.show()
384/16: df_train.keyword = df_train.keyword.str.replace('%20', ' ')
384/17:
# Let's show most common keywords for target = 1
df_train[df_train.target == 1].keyword.value_counts()[:10]
384/18:
# Let's show most common keywords for target = 1
df_train[df_train.target == 0].keyword.value_counts()[:10]
384/19:
df_train_group = df_train.groupby(['keyword', 'target'])['id'].count().unstack(fill_value=0).reset_index()
df_train_group.columns = ['keyword', 'target_0', 'target_1']
df_train_group['prob_real_disasters'] = df_train_group.target_1/(df_train_group.target_0 + df_train_group.target_1)
384/20: df_train_group.sort_values('prob_real_disasters', ascending=False)
384/21: df_train.keyword.nunique()
384/22: df_train.target.value_counts()/len(df_train)
384/23: ## This data set is unbalanced, so we should remember about it later
384/24: df_train.location.nunique()
384/25: df_location = df_train.loc[:, ['id', 'target', 'location', 'text']]
384/26: df_location.location.fillna('no_location', inplace=True)
384/27:
# Write a function to display basic entity info:
def show_ents(doc):
    doc = nlp(doc)
    count_GPE = 0
    if doc.ents:
        for ent in doc.ents:
            if ent.label_ == 'GPE':
                #print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
                count_GPE = 1
                break
    return count_GPE
384/28: df_location['GPE_loc_flag'] = df_location.location.apply(show_ents)
384/29: df_location['GPE_text_flag'] = df_location.text.apply(show_ents)
384/30: df_location['GPE_flag'] = df_location.GPE_loc_flag | df_location.GPE_text_flag
384/31:
f, axes = plt.subplots(1, 3, figsize=(20,6))
sns.countplot(x='GPE_loc_flag', hue='target', data=df_location, ax=axes[0])
sns.countplot(x='GPE_text_flag', hue='target', data=df_location, ax=axes[1])
sns.countplot(x='GPE_flag', hue='target', data=df_location, ax=axes[2])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')
axes[2].set_title('GPE')

#plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
axes[0].legend(labels=['0', '1'])
axes[1].legend(labels=['0', '1'])
axes[2].legend(labels=['0', '1'])

plt.show()
384/32:
# Looks like the location extracted from the tweet will be more informative than the original location feature.
df_location.iloc[:, -4:].corr()
384/33: df_location
384/34: df_punct_fe = df_train.loc[:, ['id', 'target', 'text']]
384/35:
#string.punctuation = '!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
def count_punctuations(text):
    d=dict()
    for i in string.punctuation:
        d[str(i)+' count']=text.count(i)
    return d
384/36:
df_punct_fe['punct_count'] = df_punct_fe["text"].apply(lambda x:count_punctuations(x))
df_punct= pd.DataFrame(list(df_punct_fe.punct_count))
merge_df = pd.merge(df_punct_fe.target,df_punct,left_index=True, right_index=True)
384/37:
merge_df_gr = merge_df.groupby('target').sum().reset_index()
merge_df_gr = merge_df_gr.T
merge_df_gr.columns = ['target_0', 'target_1']
384/38: merge_df_gr
384/39: merge_df_gr['sum_0_1'] = merge_df_gr['target_0'] + merge_df_gr['target_1']
384/40: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
384/41: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
384/42:
def punct_add_exclamation(text):
    return len(re.findall("!", text))
384/43:
def punct_add_question(text):
    return len(re.findall("\?", text))
384/44:
def punct_add_quotation(text):
    return len(re.findall("'", text))
384/45:
df_punct_fe['count_exclamation'] = df_punct_fe.text.apply(punct_add_exclamation)
df_punct_fe['count_question'] = df_punct_fe.text.apply(punct_add_question)
df_punct_fe['count_quotation'] = df_punct_fe.text.apply(punct_add_quotation)
384/46: new_cols = ['count_exclamation', 'count_question', 'count_quotation']
384/47: df_punct_fe.head(2)
384/48:
# count number of characters 
def count_chars(text):
    return len(text)

# count number of words 
def count_words(text):
    return len(text.split())

# count number of capital characters
def count_capital_chars(text):
    count=0
    for i in text:
        if i.isupper():
            count+=1
    return count

# count number of capital words
def count_capital_words(text):
    return sum(map(str.isupper,text.split()))

# count number of punctuations
def count_punctuations(text):
    count = lambda l1,l2: sum([1 for x in l1 if x in l2])
    return count(text,set(string.punctuation))

# count number of words in quotes
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    string_wothout_contractions = string_wothout_contractions.replace('"', '\'')
    x = re.findall("\'(.+?)\'", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
    
# count number of sentences
def count_sent(text):
    return len(nltk.sent_tokenize(text))

# count number of unique words 
def count_unique_words(text):
    return len(set(text.split()))
    
# count of hashtags
def count_htags(text):
    x = re.findall(r'(\#\w[A-Za-z0-9]*)', text)
    return len(x)

# count of mentions
def count_mentions(text):
    x = re.findall(r'(\@\w[A-Za-z0-9]*)', text)
    return len(x)

# count of stopwords
def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)

#count of links
def count_links(text):
    link_list = re.findall(r'(https?://[^\s]+)', text)
    bitly_list = re.findall(r'bit.ly/\S+', text)
    return (len(link_list + bitly_list))

#count digits
def count_digits(text):
    digit_list = re.findall(r'[0-9]+', text)
    return len(digit_list)

# Factorize repeated punctuation, add REPEAT
def count_repeat_punct(text):
    rep_list = re.findall(r'([!?.]){2,}', text)
    return len(rep_list)
384/49:
def feature_eng_fun(df_in, text):
    df = copy.deepcopy(df_in)
    df['count_exclamation'] = df_train[text].apply(punct_add_exclamation)
    df['count_question'] = df[text].apply(punct_add_question)
    df['count_quotation'] = df[text].apply(punct_add_quotation)
    df['count_num_of_chars'] = df[text].apply(count_chars)
    df['count_num_of_words'] = df[text].apply(count_words)
    df['count_num_of_capital_chars'] = df[text].apply(count_capital_chars)
    df['count_num_of_capital_words'] = df[text].apply(count_capital_words)
    df['count_punct'] = df[text].apply(count_punctuations)
    df['count_words_in_quotes'] = df[text].apply(count_words_in_quotes)
    df['count_sent'] = df[text].apply(count_sent)
    df['count_unique_words'] = df[text].apply(count_unique_words)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_mentions'] = df[text].apply(count_mentions)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_stopwords'] = df[text].apply(count_stopwords)
    df['count_links'] = df[text].apply(count_links) 
    df['count_repeat_punct'] = df[text].apply(count_repeat_punct)    
    ## This function from part above
    df['GPE_text_flag'] = df[text].apply(show_ents)    
    return df
384/50:
## .loc will create a new copy, not view
df_feature_eng = df_train.loc[:, ('id', 'text', 'target')]
384/51: df_feature_eng = feature_eng_fun(df_feature_eng, 'text')
384/52: df_feature_eng.drop(['text'], axis=1, inplace=True)
384/53:
plt.figure(figsize=(14,10))
sns.heatmap(df_feature_eng.corr(),annot=True)
384/54: cols_selected = np.abs(df_feature_eng.corr()['target']).sort_values(ascending=False)[1:11].index.to_list()
384/55: df_feature_eng[cols_selected].head(3)
384/56:
# Let's look at some tweets more carefully
df_train[55:60]
384/57:
abbreviations = {
    "$" : " dollar ",
    "‚Ç¨" : " euro ",
    "4ao" : "for adults only",
    "a.m" : "before midday",
    "a3" : "anytime anywhere anyplace",
    "aamof" : "as a matter of fact",
    "acct" : "account",
    "adih" : "another day in hell",
    "afaic" : "as far as i am concerned",
    "afaict" : "as far as i can tell",
    "afaik" : "as far as i know",
    "afair" : "as far as i remember",
    "afk" : "away from keyboard",
    "app" : "application",
    "approx" : "approximately",
    "apps" : "applications",
    "asap" : "as soon as possible",
    "asl" : "age, sex, location",
    "atk" : "at the keyboard",
    "ave." : "avenue",
    "aymm" : "are you my mother",
    "ayor" : "at your own risk", 
    "b&b" : "bed and breakfast",
    "b+b" : "bed and breakfast",
    "b.c" : "before christ",
    "b2b" : "business to business",
    "b2c" : "business to customer",
    "b4" : "before",
    "b4n" : "bye for now",
    "b@u" : "back at you",
    "bae" : "before anyone else",
    "bak" : "back at keyboard",
    "bbbg" : "bye bye be good",
    "bbc" : "british broadcasting corporation",
    "bbias" : "be back in a second",
    "bbl" : "be back later",
    "bbs" : "be back soon",
    "be4" : "before",
    "bfn" : "bye for now",
    "blvd" : "boulevard",
    "bout" : "about",
    "brb" : "be right back",
    "bros" : "brothers",
    "brt" : "be right there",
    "bsaaw" : "big smile and a wink",
    "btw" : "by the way",
    "bwl" : "bursting with laughter",
    "c/o" : "care of",
    "cet" : "central european time",
    "cf" : "compare",
    "cia" : "central intelligence agency",
    "csl" : "can not stop laughing",
    "cu" : "see you",
    "cul8r" : "see you later",
    "cv" : "curriculum vitae",
    "cwot" : "complete waste of time",
    "cya" : "see you",
    "cyt" : "see you tomorrow",
    "dae" : "does anyone else",
    "dbmib" : "do not bother me i am busy",
    "diy" : "do it yourself",
    "dm" : "direct message",
    "dwh" : "during work hours",
    "e123" : "easy as one two three",
    "eet" : "eastern european time",
    "eg" : "example",
    "embm" : "early morning business meeting",
    "encl" : "enclosed",
    "encl." : "enclosed",
    "etc" : "and so on",
    "faq" : "frequently asked questions",
    "fawc" : "for anyone who cares",
    "fb" : "facebook",
    "fc" : "fingers crossed",
    "fig" : "figure",
    "fimh" : "forever in my heart", 
    "ft." : "feet",
    "ft" : "featuring",
    "ftl" : "for the loss",
    "ftw" : "for the win",
    "fwiw" : "for what it is worth",
    "fyi" : "for your information",
    "g9" : "genius",
    "gahoy" : "get a hold of yourself",
    "gal" : "get a life",
    "gcse" : "general certificate of secondary education",
    "gfn" : "gone for now",
    "gg" : "good game",
    "gl" : "good luck",
    "glhf" : "good luck have fun",
    "gmt" : "greenwich mean time",
    "gmta" : "great minds think alike",
    "gn" : "good night",
    "g.o.a.t" : "greatest of all time",
    "goat" : "greatest of all time",
    "goi" : "get over it",
    "gps" : "global positioning system",
    "gr8" : "great",
    "gratz" : "congratulations",
    "gyal" : "girl",
    "h&c" : "hot and cold",
    "hp" : "horsepower",
    "hr" : "hour",
    "hrh" : "his royal highness",
    "ht" : "height",
    "ibrb" : "i will be right back",
    "ic" : "i see",
    "icq" : "i seek you",
    "icymi" : "in case you missed it",
    "idc" : "i do not care",
    "idgadf" : "i do not give a damn fuck",
    "idgaf" : "i do not give a fuck",
    "idk" : "i do not know",
    "ie" : "that is",
    "i.e" : "that is",
    "ifyp" : "i feel your pain",
    "IG" : "instagram",
    "iirc" : "if i remember correctly",
    "ilu" : "i love you",
    "ily" : "i love you",
    "imho" : "in my humble opinion",
    "imo" : "in my opinion",
    "imu" : "i miss you",
    "iow" : "in other words",
    "irl" : "in real life",
    "j4f" : "just for fun",
    "jic" : "just in case",
    "jk" : "just kidding",
    "jsyk" : "just so you know",
    "l8r" : "later",
    "lb" : "pound",
    "lbs" : "pounds",
    "ldr" : "long distance relationship",
    "lmao" : "laugh my ass off",
    "lmfao" : "laugh my fucking ass off",
    "lol" : "laughing out loud",
    "ltd" : "limited",
    "ltns" : "long time no see",
    "m8" : "mate",
    "mf" : "motherfucker",
    "mfs" : "motherfuckers",
    "mfw" : "my face when",
    "mofo" : "motherfucker",
    "mph" : "miles per hour",
    "mr" : "mister",
    "mrw" : "my reaction when",
    "ms" : "miss",
    "mte" : "my thoughts exactly",
    "nagi" : "not a good idea",
    "nbc" : "national broadcasting company",
    "nbd" : "not big deal",
    "nfs" : "not for sale",
    "ngl" : "not going to lie",
    "nhs" : "national health service",
    "nrn" : "no reply necessary",
    "nsfl" : "not safe for life",
    "nsfw" : "not safe for work",
    "nth" : "nice to have",
    "nvr" : "never",
    "nyc" : "new york city",
    "oc" : "original content",
    "og" : "original",
    "ohp" : "overhead projector",
    "oic" : "oh i see",
    "omdb" : "over my dead body",
    "omg" : "oh my god",
    "omw" : "on my way",
    "p.a" : "per annum",
    "p.m" : "after midday",
    "pm" : "prime minister",
    "poc" : "people of color",
    "pov" : "point of view",
    "pp" : "pages",
    "ppl" : "people",
    "prw" : "parents are watching",
    "ps" : "postscript",
    "pt" : "point",
    "ptb" : "please text back",
    "pto" : "please turn over",
    "qpsa" : "what happens", #"que pasa",
    "ratchet" : "rude",
    "rbtl" : "read between the lines",
    "rlrt" : "real life retweet", 
    "rofl" : "rolling on the floor laughing",
    "roflol" : "rolling on the floor laughing out loud",
    "rotflmao" : "rolling on the floor laughing my ass off",
    "rt" : "retweet",
    "ruok" : "are you ok",
    "sfw" : "safe for work",
    "sk8" : "skate",
    "smh" : "shake my head",
    "sq" : "square",
    "srsly" : "seriously", 
    "ssdd" : "same stuff different day",
    "tbh" : "to be honest",
    "tbs" : "tablespooful",
    "tbsp" : "tablespooful",
    "tfw" : "that feeling when",
    "thks" : "thank you",
    "tho" : "though",
    "thx" : "thank you",
    "tia" : "thanks in advance",
    "til" : "today i learned",
    "tl;dr" : "too long i did not read",
    "tldr" : "too long i did not read",
    "tmb" : "tweet me back",
    "tntl" : "trying not to laugh",
    "ttyl" : "talk to you later",
    "u" : "you",
    "u2" : "you too",
    "u4e" : "yours for ever",
    "utc" : "coordinated universal time",
    "w/" : "with",
    "w/o" : "without",
    "w8" : "wait",
    "wassup" : "what is up",
    "wb" : "welcome back",
    "wtf" : "what the fuck",
    "wtg" : "way to go",
    "wtpa" : "where the party at",
    "wuf" : "where are you from",
    "wuzup" : "what is up",
    "wywh" : "wish you were here",
    "yd" : "yard",
    "ygtr" : "you got that right",
    "ynk" : "you never know",
    "zzz" : "sleeping bored and tired"
}
384/58:
#remove urls
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    return text

# Change an abbreviation by its true meaning
def word_abbrev(word):
    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word

# Replace all abbreviations
def replace_abbrev(text):
    string = ""
    for word in text.split():
        string += word_abbrev(word) + " "        
    return string

# Remove @ and mention, replace by USER
def replace_mention(text):
    pat=re.compile(r'@\S+')
    return pat.sub(r'USER',text)

# Remove numbers, replace it by NUMBER
def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r'NUMBER', text)


##remove digits
#def remove_digits(text):
#    text = re.sub(r'[0-9]+', '', text)
#    return text   

# Remove all emojis, replace by EMOJI
def remove_emoji(text):
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'EMOJI', text)

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()

#contractions and remowing stopwords and very short words
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 3:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
384/59:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(remove_emoji)
    
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(string_contractions)
   # df['column_token'] = df[column_name].apply(word_tokenize)
    df[column_name] = df[column_name].apply(remove_punct)
    return df
384/60: df_text = df_train.loc[:, ['id', 'target', 'keyword', 'text']]
384/61: df_text = basic_cleaning(df_text, 'text')
384/62: df_text[df_text.text.str.contains('fifa')]
384/63: df_train[df_train.text.str.contains('FIFA')]
384/64: all_text = ' '.join(df_text['text'].tolist())
384/65:
oov_list = []
def search_oov(text):
    tokens = nlp(text)
    for token in tokens:
        if token.is_oov:
            oov_list.append(token)
384/66: search_oov(all_text)
384/67: oov_list_strings  = [i.text for i in oov_list]
384/68:
counter = Counter(oov_list_strings)
counter.most_common(10)
384/69: sid = SentimentIntensityAnalyzer()
384/70: df_sentiment = df_train.loc[:, ['id', 'target', 'text']]
384/71: df_sentiment['sentiment_compound']  = df_sentiment.text.apply(lambda tweet: sid.polarity_scores(tweet)['compound'])
384/72: df_sentiment['sentiment_compound_score'] = df_sentiment['sentiment_compound'].apply(lambda c: 'pos' if c >=0 else 'neg')
384/73:
plt.figure(figsize=[4, 4])
sns.countplot(x='target', hue='sentiment_compound_score', data=df_sentiment)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
384/74: #not very optimistic result, I won't add a sentiment feature :)
384/75: ## CREATE A MODEL BASED ON TEXT
384/76:
def to_labels(pos_probs, threshold):
    return (pos_probs >= threshold).astype('int')

def best_tresholds(y_pred_proba, y_test):
    # define thresholds
    thresholds = np.arange(0, 1, 0.001)
    # evaluate each threshold
    scores = [f1_score(y_test, to_labels(y_pred_proba, t)) for t in thresholds]
    # get best threshold
    ix = np.argmax(scores)
    #print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
    return thresholds[ix], scores[ix]
384/77: df_copy = copy.deepcopy(df_train)
384/78: df_copy
384/79:
cols_selected = ['GPE_text_flag', 'count_links', 'count_num_of_chars', 'count_punct', 'count_sent',
                 'count_mentions', 'count_stopwords', 'count_question', 'count_quotation',
                 'count_num_of_capital_chars']

df_copy = copy.deepcopy(df_train)
df_train_text_clean = basic_cleaning(df_copy, 'text')
df_train_text_clean["clean_text"] = df_train_text_clean.keyword.map(str) + " " + df_train_text_clean.text

## new features
df_train_feature_eng = feature_eng_fun(df_copy, 'text')
df_train_feature_eng = df_train_feature_eng[cols_selected]


#### for test
df_test_text_clean_f = basic_cleaning(df_test, 'text')
df_test_text_clean_f["clean_text"] = df_test_text_clean_f.keyword.map(str) + " " + df_test_text_clean_f.text

df_test_feature_eng_f = feature_eng_fun(df_test, 'text')
df_test_feature_eng_f = df_test_feature_eng_f[cols_selected]
384/80: df_train_text_clean.shape
384/81: df_train_feature_eng.shape
384/82:
df_train_full = pd.merge(df_train_text_clean[['target', 'clean_text']], df_train_feature_eng, 
                         left_index=True, right_index=True)

df_test_full = pd.merge(df_test_text_clean_f[['clean_text']], df_test_feature_eng_f, 
                         left_index=True, right_index=True)
384/83:
## split to test and train
y = df_train_full['target']
X = df_train_full.drop('target', axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
print('X_train shape', X_train.shape)
print('X_test shape', X_test.shape)
384/84:
#max_features=5000

# CountVectorizer
count_vectorizer = CountVectorizer(max_features=max_features)

def count_vector(data):
    count_vectorizer = CountVectorizer()
    vect = count_vectorizer.fit_transform(data)
    return vect, count_vectorizer

def tfidf_vector(data):
    tfidf_vectorizer = TfidfVectorizer()
    vect = tfidf_vectorizer.fit_transform(data)
    return vect, tfidf_vectorizer

def min_max(data):
    min_max_scaler = MinMaxScaler()
    vect = min_max_scaler.fit_transform(data)
    return vect, min_max_scaler

X_train_count, count_vectorizer = count_vector(X_train['clean_text'])
X_train_tfidf, tfidf_vectorizer = tfidf_vector(X_train['clean_text'])

X_train_eng_minmax, minmax_scaler = min_max(X_train[cols_selected])
X_train_eng = X_train[cols_selected].to_numpy()

#### TEST
X_test_count = count_vectorizer.transform(X_test ['clean_text'])                                                     
X_test_tfidf = tfidf_vectorizer.transform(X_test ['clean_text'])

X_test_eng_minmax = minmax_scaler.transform(X_test[cols_selected])
X_test_eng = X_test[cols_selected].to_numpy()

### TEST DATASET
X_test_count_f = count_vectorizer.transform(df_test_full['clean_text'])                                                     
X_test_tfidf_f = tfidf_vectorizer.transform(df_test_full['clean_text'])

X_test_eng_minmax_f = minmax_scaler.transform(df_test_full[cols_selected])
X_test_eng_f = df_test_full[cols_selected].to_numpy()


### CONCAT tf-idf + minmax, count_vec + feature_eng

X_train_tfidf_minmax = hstack([X_train_tfidf, X_train_eng_minmax])
X_train_count_eng = hstack([X_train_count, X_train_eng])


X_test_tfidf_minmax = hstack([X_test_tfidf, X_test_eng_minmax])
X_test_count_eng = hstack([X_test_count, X_test_eng])

X_test_tfidf_minmax_f = hstack([X_test_tfidf_f, X_test_eng_minmax_f])
X_test_count_eng_f = hstack([X_test_count_f, X_test_eng_f])
384/85:
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
384/86:
models=[
   #     XGBClassifier(max_depth=6, n_estimators=1000),
        LogisticRegression(random_state=30),
        SVC(random_state=30, probability=True),
        MultinomialNB(),
        DecisionTreeClassifier(random_state = 30),
        KNeighborsClassifier(),
        RandomForestClassifier(random_state=30),
       ]
384/87:
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import RandomForestClassifier
384/88:
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
384/89:
models=[
   #     XGBClassifier(max_depth=6, n_estimators=1000),
        LogisticRegression(random_state=30),
        SVC(random_state=30, probability=True),
        MultinomialNB(),
        DecisionTreeClassifier(random_state = 30),
        KNeighborsClassifier(),
        RandomForestClassifier(random_state=30),
       ]
384/90:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train_count, X_test_count, y_train ,y_test, 'Count vector')
    
    fit_and_predict(model, X_train_tfidf, X_test_tfidf, y_train, y_test, 'Tfidf vector')
    
    ## count with EF
    fit_and_predict(model, X_train_count_eng, X_test_count_eng, y_train, y_test, 'Count vector + FE')
    
    ## TF IDF with EF
    fit_and_predict(model, X_train_tfidf_minmax, X_test_tfidf_minmax, y_train, y_test, 'TF-IDF + FE')
384/91:
def fit_and_predict(model, x_train ,x_test, y_train, y_test, vectoriser):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    filename = classifier_name +" using "+ str(vectoriser)+".joblib"
    filename = filename.lower().replace(" ","_")
    print(filename)
    dump(model, filename=filename)
    y_pred = classifier.predict(x_test)
    cmatrix = confusion_matrix(y_test,y_pred)
    
    y_hat = classifier.predict_proba(x_test)[:,1]
    threshold, best_score = best_tresholds(y_hat, y_test)


    f1score = f1_score(y_test,y_pred)
    train_accuracy = round(classifier.score(x_train,y_train)*100)
    test_accuracy =  round(accuracy_score(y_test,y_pred)*100)

    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'f1 score': f1score, 
                              'best_f1': best_score,
                              'threshold': threshold,
                              'train accuracy': train_accuracy, 
                              'test accuracy': test_accuracy, 
                              'vectoriser': str(vectoriser),
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using "+ str(vectoriser))
    print('f1 score for test: {}'.format(f1score))
    print(classification_report(y_test,y_pred))    
    print('Threshold={:.5f}, F-Score={:.5f}'.format(threshold, best_score))
    print('Accuracy of classifier on training set:{}%'.format(train_accuracy))
    print('Accuracy of classifier on test set:{}%' .format(test_accuracy))
384/92:
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
384/93:
models=[
   #     XGBClassifier(max_depth=6, n_estimators=1000),
        LogisticRegression(random_state=30),
        SVC(random_state=30, probability=True),
        MultinomialNB(),
        DecisionTreeClassifier(random_state = 30),
        KNeighborsClassifier(),
        RandomForestClassifier(random_state=30),
       ]
384/94:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train_count, X_test_count, y_train ,y_test, 'Count vector')
    
    fit_and_predict(model, X_train_tfidf, X_test_tfidf, y_train, y_test, 'Tfidf vector')
    
    ## count with EF
    fit_and_predict(model, X_train_count_eng, X_test_count_eng, y_train, y_test, 'Count vector + FE')
    
    ## TF IDF with EF
    fit_and_predict(model, X_train_tfidf_minmax, X_test_tfidf_minmax, y_train, y_test, 'TF-IDF + FE')
384/95:
max_features=5000

# CountVectorizer
count_vectorizer = CountVectorizer(max_features=max_features)

def count_vector(data):
    count_vectorizer = CountVectorizer()
    vect = count_vectorizer.fit_transform(data)
    return vect, count_vectorizer

def tfidf_vector(data):
    tfidf_vectorizer = TfidfVectorizer()
    vect = tfidf_vectorizer.fit_transform(data)
    return vect, tfidf_vectorizer

def min_max(data):
    min_max_scaler = MinMaxScaler()
    vect = min_max_scaler.fit_transform(data)
    return vect, min_max_scaler

X_train_count, count_vectorizer = count_vector(X_train['clean_text'])
X_train_tfidf, tfidf_vectorizer = tfidf_vector(X_train['clean_text'])

X_train_eng_minmax, minmax_scaler = min_max(X_train[cols_selected])
X_train_eng = X_train[cols_selected].to_numpy()

#### TEST
X_test_count = count_vectorizer.transform(X_test ['clean_text'])                                                     
X_test_tfidf = tfidf_vectorizer.transform(X_test ['clean_text'])

X_test_eng_minmax = minmax_scaler.transform(X_test[cols_selected])
X_test_eng = X_test[cols_selected].to_numpy()

### TEST DATASET
X_test_count_f = count_vectorizer.transform(df_test_full['clean_text'])                                                     
X_test_tfidf_f = tfidf_vectorizer.transform(df_test_full['clean_text'])

X_test_eng_minmax_f = minmax_scaler.transform(df_test_full[cols_selected])
X_test_eng_f = df_test_full[cols_selected].to_numpy()


### CONCAT tf-idf + minmax, count_vec + feature_eng

X_train_tfidf_minmax = hstack([X_train_tfidf, X_train_eng_minmax])
X_train_count_eng = hstack([X_train_count, X_train_eng])


X_test_tfidf_minmax = hstack([X_test_tfidf, X_test_eng_minmax])
X_test_count_eng = hstack([X_test_count, X_test_eng])

X_test_tfidf_minmax_f = hstack([X_test_tfidf_f, X_test_eng_minmax_f])
X_test_count_eng_f = hstack([X_test_count_f, X_test_eng_f])
384/96:
max_features=5000

# CountVectorizer
count_vectorizer = CountVectorizer(max_features=max_features)

def count_vector(data):
    count_vectorizer = CountVectorizer()
    vect = count_vectorizer.fit_transform(data)
    return vect, count_vectorizer

def tfidf_vector(data):
    tfidf_vectorizer = TfidfVectorizer()
    vect = tfidf_vectorizer.fit_transform(data)
    return vect, tfidf_vectorizer

def min_max(data):
    min_max_scaler = MinMaxScaler()
    vect = min_max_scaler.fit_transform(data)
    return vect, min_max_scaler

X_train_count, count_vectorizer = count_vector(X_train['clean_text'])
X_train_tfidf, tfidf_vectorizer = tfidf_vector(X_train['clean_text'])

X_train_eng_minmax, minmax_scaler = min_max(X_train[cols_selected])
X_train_eng = X_train[cols_selected].to_numpy()

#### TEST
X_test_count = count_vectorizer.transform(X_test ['clean_text'])                                                     
X_test_tfidf = tfidf_vectorizer.transform(X_test ['clean_text'])

X_test_eng_minmax = minmax_scaler.transform(X_test[cols_selected])
X_test_eng = X_test[cols_selected].to_numpy()

### TEST DATASET
X_test_count_f = count_vectorizer.transform(df_test_full['clean_text'])                                                     
X_test_tfidf_f = tfidf_vectorizer.transform(df_test_full['clean_text'])

X_test_eng_minmax_f = minmax_scaler.transform(df_test_full[cols_selected])
X_test_eng_f = df_test_full[cols_selected].to_numpy()


### CONCAT tf-idf + minmax, count_vec + feature_eng

X_train_tfidf_minmax = np.hstack([X_train_tfidf, X_train_eng_minmax])
X_train_count_eng = hstack([X_train_count, X_train_eng])


X_test_tfidf_minmax = hstack([X_test_tfidf, X_test_eng_minmax])
X_test_count_eng = hstack([X_test_count, X_test_eng])

X_test_tfidf_minmax_f = hstack([X_test_tfidf_f, X_test_eng_minmax_f])
X_test_count_eng_f = hstack([X_test_count_f, X_test_eng_f])
384/97:
max_features=5000

# CountVectorizer
count_vectorizer = CountVectorizer(max_features=max_features)

def count_vector(data):
    count_vectorizer = CountVectorizer()
    vect = count_vectorizer.fit_transform(data)
    return vect, count_vectorizer

def tfidf_vector(data):
    tfidf_vectorizer = TfidfVectorizer()
    vect = tfidf_vectorizer.fit_transform(data)
    return vect, tfidf_vectorizer

def min_max(data):
    min_max_scaler = MinMaxScaler()
    vect = min_max_scaler.fit_transform(data)
    return vect, min_max_scaler

X_train_count, count_vectorizer = count_vector(X_train['clean_text'])
X_train_tfidf, tfidf_vectorizer = tfidf_vector(X_train['clean_text'])

X_train_eng_minmax, minmax_scaler = min_max(X_train[cols_selected])
X_train_eng = X_train[cols_selected].to_numpy()

#### TEST
X_test_count = count_vectorizer.transform(X_test ['clean_text'])                                                     
X_test_tfidf = tfidf_vectorizer.transform(X_test ['clean_text'])

X_test_eng_minmax = minmax_scaler.transform(X_test[cols_selected])
X_test_eng = X_test[cols_selected].to_numpy()

### TEST DATASET
X_test_count_f = count_vectorizer.transform(df_test_full['clean_text'])                                                     
X_test_tfidf_f = tfidf_vectorizer.transform(df_test_full['clean_text'])

X_test_eng_minmax_f = minmax_scaler.transform(df_test_full[cols_selected])
X_test_eng_f = df_test_full[cols_selected].to_numpy()


### CONCAT tf-idf + minmax, count_vec + feature_eng

X_train_tfidf_minmax = hstack([X_train_tfidf, X_train_eng_minmax])
X_train_count_eng = hstack([X_train_count, X_train_eng])


X_test_tfidf_minmax = hstack([X_test_tfidf, X_test_eng_minmax])
X_test_count_eng = hstack([X_test_count, X_test_eng])

X_test_tfidf_minmax_f = hstack([X_test_tfidf_f, X_test_eng_minmax_f])
X_test_count_eng_f = hstack([X_test_count_f, X_test_eng_f])
384/98:
import pandas as pd
import numpy as np
import re
import copy
from collections import Counter
import string
from scipy.sparse import hstack
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline
384/99:
max_features=5000

# CountVectorizer
count_vectorizer = CountVectorizer(max_features=max_features)

def count_vector(data):
    count_vectorizer = CountVectorizer()
    vect = count_vectorizer.fit_transform(data)
    return vect, count_vectorizer

def tfidf_vector(data):
    tfidf_vectorizer = TfidfVectorizer()
    vect = tfidf_vectorizer.fit_transform(data)
    return vect, tfidf_vectorizer

def min_max(data):
    min_max_scaler = MinMaxScaler()
    vect = min_max_scaler.fit_transform(data)
    return vect, min_max_scaler

X_train_count, count_vectorizer = count_vector(X_train['clean_text'])
X_train_tfidf, tfidf_vectorizer = tfidf_vector(X_train['clean_text'])

X_train_eng_minmax, minmax_scaler = min_max(X_train[cols_selected])
X_train_eng = X_train[cols_selected].to_numpy()

#### TEST
X_test_count = count_vectorizer.transform(X_test ['clean_text'])                                                     
X_test_tfidf = tfidf_vectorizer.transform(X_test ['clean_text'])

X_test_eng_minmax = minmax_scaler.transform(X_test[cols_selected])
X_test_eng = X_test[cols_selected].to_numpy()

### TEST DATASET
X_test_count_f = count_vectorizer.transform(df_test_full['clean_text'])                                                     
X_test_tfidf_f = tfidf_vectorizer.transform(df_test_full['clean_text'])

X_test_eng_minmax_f = minmax_scaler.transform(df_test_full[cols_selected])
X_test_eng_f = df_test_full[cols_selected].to_numpy()


### CONCAT tf-idf + minmax, count_vec + feature_eng

X_train_tfidf_minmax = hstack([X_train_tfidf, X_train_eng_minmax])
X_train_count_eng = hstack([X_train_count, X_train_eng])


X_test_tfidf_minmax = hstack([X_test_tfidf, X_test_eng_minmax])
X_test_count_eng = hstack([X_test_count, X_test_eng])

X_test_tfidf_minmax_f = hstack([X_test_tfidf_f, X_test_eng_minmax_f])
X_test_count_eng_f = hstack([X_test_count_f, X_test_eng_f])
384/100: X_train_count.shape
384/101: X_train_eng.shape
384/102:
def fit_and_predict(model, x_train ,x_test, y_train, y_test, vectoriser):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    filename = classifier_name +" using "+ str(vectoriser)+".joblib"
    filename = filename.lower().replace(" ","_")
    print(filename)
    dump(model, filename=filename)
    y_pred = classifier.predict(x_test)
    cmatrix = confusion_matrix(y_test,y_pred)
    
    y_hat = classifier.predict_proba(x_test)[:,1]
    threshold, best_score = best_tresholds(y_hat, y_test)


    f1score = f1_score(y_test,y_pred)
    train_accuracy = round(classifier.score(x_train,y_train)*100)
    test_accuracy =  round(accuracy_score(y_test,y_pred)*100)

    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'f1 score': f1score, 
                              'best_f1': best_score,
                              'threshold': threshold,
                              'train accuracy': train_accuracy, 
                              'test accuracy': test_accuracy, 
                              'vectoriser': str(vectoriser),
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using "+ str(vectoriser))
    print('f1 score for test: {}'.format(f1score))
    print(classification_report(y_test,y_pred))    
    print('Threshold={:.5f}, F-Score={:.5f}'.format(threshold, best_score))
    print('Accuracy of classifier on training set:{}%'.format(train_accuracy))
    print('Accuracy of classifier on test set:{}%' .format(test_accuracy))
384/103:
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
384/104:
models=[
   #     XGBClassifier(max_depth=6, n_estimators=1000),
        LogisticRegression(random_state=30),
        SVC(random_state=30, probability=True),
        MultinomialNB(),
        DecisionTreeClassifier(random_state = 30),
        KNeighborsClassifier(),
        RandomForestClassifier(random_state=30),
       ]
384/105:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train_count, X_test_count, y_train ,y_test, 'Count vector')
    
    fit_and_predict(model, X_train_tfidf, X_test_tfidf, y_train, y_test, 'Tfidf vector')
    
    ## count with EF
    fit_and_predict(model, X_train_count_eng, X_test_count_eng, y_train, y_test, 'Count vector + FE')
    
    ## TF IDF with EF
    fit_and_predict(model, X_train_tfidf_minmax, X_test_tfidf_minmax, y_train, y_test, 'TF-IDF + FE')
384/106:
def fit_and_predict(model, x_train ,x_test, y_train, y_test, vectoriser):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)

    y_pred = classifier.predict(x_test)
    cmatrix = confusion_matrix(y_test,y_pred)
    
    y_hat = classifier.predict_proba(x_test)[:,1]
    threshold, best_score = best_tresholds(y_hat, y_test)


    f1score = f1_score(y_test,y_pred)
    train_accuracy = round(classifier.score(x_train,y_train)*100)
    test_accuracy =  round(accuracy_score(y_test,y_pred)*100)

    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'f1 score': f1score, 
                              'best_f1': best_score,
                              'threshold': threshold,
                              'train accuracy': train_accuracy, 
                              'test accuracy': test_accuracy, 
                              'vectoriser': str(vectoriser),
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using "+ str(vectoriser))
    print('f1 score for test: {}'.format(f1score))
    print(classification_report(y_test,y_pred))    
    print('Threshold={:.5f}, F-Score={:.5f}'.format(threshold, best_score))
    print('Accuracy of classifier on training set:{}%'.format(train_accuracy))
    print('Accuracy of classifier on test set:{}%' .format(test_accuracy))
384/107:
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
384/108:
models=[
   #     XGBClassifier(max_depth=6, n_estimators=1000),
        LogisticRegression(random_state=30),
        SVC(random_state=30, probability=True),
        MultinomialNB(),
        DecisionTreeClassifier(random_state = 30),
        KNeighborsClassifier(),
        RandomForestClassifier(random_state=30),
       ]
384/109:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train_count, X_test_count, y_train ,y_test, 'Count vector')
    
    fit_and_predict(model, X_train_tfidf, X_test_tfidf, y_train, y_test, 'Tfidf vector')
    
    ## count with EF
    fit_and_predict(model, X_train_count_eng, X_test_count_eng, y_train, y_test, 'Count vector + FE')
    
    ## TF IDF with EF
    fit_and_predict(model, X_train_tfidf_minmax, X_test_tfidf_minmax, y_train, y_test, 'TF-IDF + FE')
384/110:
metrics = pd.DataFrame(columns=['model' ,'vectoriser', 'f1 score', 'best_f1', 'threshold', 
                                'train accuracy','test accuracy'])
384/111:
def fit_and_predict(model, x_train ,x_test, y_train, y_test, vectoriser):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)

    y_pred = classifier.predict(x_test)
    cmatrix = confusion_matrix(y_test,y_pred)
    
    y_hat = classifier.predict_proba(x_test)[:,1]
    threshold, best_score = best_tresholds(y_hat, y_test)


    f1score = f1_score(y_test,y_pred)
    train_accuracy = round(classifier.score(x_train,y_train)*100)
    test_accuracy =  round(accuracy_score(y_test,y_pred)*100)

    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'f1 score': f1score, 
                              'best_f1': best_score,
                              'threshold': threshold,
                              'train accuracy': train_accuracy, 
                              'test accuracy': test_accuracy, 
                              'vectoriser': str(vectoriser),
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using "+ str(vectoriser))
    print('f1 score for test: {}'.format(f1score))
    print(classification_report(y_test,y_pred))    
    print('Threshold={:.5f}, F-Score={:.5f}'.format(threshold, best_score))
    print('Accuracy of classifier on training set:{}%'.format(train_accuracy))
    print('Accuracy of classifier on test set:{}%' .format(test_accuracy))
384/112:
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
384/113:
models=[
   #     XGBClassifier(max_depth=6, n_estimators=1000),
        LogisticRegression(random_state=30),
        SVC(random_state=30, probability=True),
        MultinomialNB(),
        DecisionTreeClassifier(random_state = 30),
        KNeighborsClassifier(),
        RandomForestClassifier(random_state=30),
       ]
384/114:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train_count, X_test_count, y_train ,y_test, 'Count vector')
    
    fit_and_predict(model, X_train_tfidf, X_test_tfidf, y_train, y_test, 'Tfidf vector')
    
    ## count with EF
    fit_and_predict(model, X_train_count_eng, X_test_count_eng, y_train, y_test, 'Count vector + FE')
    
    ## TF IDF with EF
    fit_and_predict(model, X_train_tfidf_minmax, X_test_tfidf_minmax, y_train, y_test, 'TF-IDF + FE')
384/115: metrics.sort_values('best_f1')
384/116: max(yhat[:, 1]).astype('int')
384/117:
y_predict_ir = lr.predict(X_test_tfidf_minmax)
y_predprob = lr.predict_proba(X_test_tfidf_minmax_f)[:, 1]
384/118:
lr = LogisticRegression(random_state=30)
lr.fit(X_train_tfidf_minmax, y_train)
y_pred_prob = lr.predict_proba(X_test_tfidf_minmax_f)[:, 1]
384/119:
df_t = df_test[['id']]
df_t['target'] = (y_predprob >= 0.416).astype(int)
384/120:
df_t = df_test[['id']]
df_t['target'] = (y_pred_prob >= 0.416).astype(int)
384/121: df_t.to_csv("submission_logit_treshold_416.csv", index=False)
385/1:
w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
lemmatizer = nltk.stem.WordNetLemmatizer()

def lemmatize_text(text):
    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]

df = pd.DataFrame(['this was cheesy', 'she likes these books', 'wow this is great'], columns=['text'])
df['text_lemmatized'] = df.text.apply(lemmatize_text)
386/1:
import pandas as pd
import numpy as np
import re
import copy
from collections import Counter
import string
from scipy.sparse import hstack
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline
386/2: pd.set_option('max_colwidth', None)
386/3:
import matplotlib.pyplot as plt
import seaborn as sns
386/4:
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.tokenize.treebank import TreebankWordDetokenizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer

import emoji as emj
import spacy 
## It's important to use _lg for OOV in future
nlp = spacy.load('en_core_web_lg')
386/5:
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import confusion_matrix, classification_report, roc_curve
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
from sklearn.metrics import f1_score, accuracy_score
386/6:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
386/7:
nltk.download()
STOPWORDS = set(stopwords.words('english'))
386/8: wnl = WordNetLemmatizer()
386/9:
w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
#lemmatizer = nltk.stem.WordNetLemmatizer()

def lemmatize_text(text):
    return [wnl.lemmatize(w) for w in w_tokenizer.tokenize(text)]

df = pd.DataFrame(['this was cheesy', 'she likes these books', 'wow this is great'], columns=['text'])
df['text_lemmatized'] = df.text.apply(lemmatize_text)
386/10: df
386/11: df_train.head()
386/12: df_train.isnull().mean()
386/13: df_train.shape, df_test.shape
386/14:
#cardinality check
for col in df_train.columns:
    print('{} has {} unique instances'.format(col, len(df_train[col].unique())))
386/15: df_train.info()
386/16: df_train.keyword.value_counts()
386/17:
plt.figure(figsize=[8, 40])
sns.countplot(y='keyword',
              data=df_train,
              color='lightblue',
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
386/18:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword', 
              hue='target', 
              data=df_train, 
              palette=['lightblue', 'red']
             )
plt.xticks(rotation=90)
plt.legend(loc='upper right')
plt.title("Distribution of Target per Keyword ")
plt.show()
386/19: df_train.keyword = df_train.keyword.str.replace('%20', ' ')
386/20:
# Let's show most common keywords for target = 1
df_train[df_train.target == 1].keyword.value_counts()[:10]
386/21:
# Let's show most common keywords for target = 1
df_train[df_train.target == 0].keyword.value_counts()[:10]
386/22:
df_train_group = df_train.groupby(['keyword', 'target'])['id'].count().unstack(fill_value=0).reset_index()
df_train_group.columns = ['keyword', 'target_0', 'target_1']
df_train_group['prob_real_disasters'] = df_train_group.target_1/(df_train_group.target_0 + df_train_group.target_1)
386/23: df_train_group.sort_values('prob_real_disasters', ascending=False)
386/24: df_train.keyword.nunique()
386/25: df_train.target.value_counts()/len(df_train)
386/26: ## This data set is unbalanced, so we should remember about it later
386/27: df_train.location.nunique()
386/28: df_location = df_train.loc[:, ['id', 'target', 'location', 'text']]
386/29: df_location.location.fillna('no_location', inplace=True)
386/30:
# Write a function to display basic entity info:
def show_ents(doc):
    doc = nlp(doc)
    count_GPE = 0
    if doc.ents:
        for ent in doc.ents:
            if ent.label_ == 'GPE':
                #print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
                count_GPE = 1
                break
    return count_GPE
386/31: df_location['GPE_loc_flag'] = df_location.location.apply(show_ents)
386/32: df_location['GPE_text_flag'] = df_location.text.apply(show_ents)
386/33: df_location['GPE_flag'] = df_location.GPE_loc_flag | df_location.GPE_text_flag
386/34:
f, axes = plt.subplots(1, 3, figsize=(20,6))
sns.countplot(x='GPE_loc_flag', hue='target', data=df_location, ax=axes[0])
sns.countplot(x='GPE_text_flag', hue='target', data=df_location, ax=axes[1])
sns.countplot(x='GPE_flag', hue='target', data=df_location, ax=axes[2])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')
axes[2].set_title('GPE')

#plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
axes[0].legend(labels=['0', '1'])
axes[1].legend(labels=['0', '1'])
axes[2].legend(labels=['0', '1'])

plt.show()
386/35:
# Looks like the location extracted from the tweet will be more informative than the original location feature.
df_location.iloc[:, -4:].corr()
386/36: df_location
386/37: df_punct_fe = df_train.loc[:, ['id', 'target', 'text']]
386/38:
#string.punctuation = '!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
def count_punctuations(text):
    d=dict()
    for i in string.punctuation:
        d[str(i)+' count']=text.count(i)
    return d
386/39:
df_punct_fe['punct_count'] = df_punct_fe["text"].apply(lambda x:count_punctuations(x))
df_punct= pd.DataFrame(list(df_punct_fe.punct_count))
merge_df = pd.merge(df_punct_fe.target,df_punct,left_index=True, right_index=True)
386/40:
merge_df_gr = merge_df.groupby('target').sum().reset_index()
merge_df_gr = merge_df_gr.T
merge_df_gr.columns = ['target_0', 'target_1']
386/41: merge_df_gr
386/42: merge_df_gr['sum_0_1'] = merge_df_gr['target_0'] + merge_df_gr['target_1']
386/43: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
386/44: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
386/45:
def punct_add_exclamation(text):
    return len(re.findall("!", text))
386/46:
def punct_add_question(text):
    return len(re.findall("\?", text))
386/47:
def punct_add_quotation(text):
    return len(re.findall("'", text))
386/48:
df_punct_fe['count_exclamation'] = df_punct_fe.text.apply(punct_add_exclamation)
df_punct_fe['count_question'] = df_punct_fe.text.apply(punct_add_question)
df_punct_fe['count_quotation'] = df_punct_fe.text.apply(punct_add_quotation)
386/49: new_cols = ['count_exclamation', 'count_question', 'count_quotation']
386/50: df_punct_fe.head(2)
386/51:
# count number of characters 
def count_chars(text):
    return len(text)

# count number of words 
def count_words(text):
    return len(text.split())

# count number of capital characters
def count_capital_chars(text):
    count=0
    for i in text:
        if i.isupper():
            count+=1
    return count

# count number of capital words
def count_capital_words(text):
    return sum(map(str.isupper,text.split()))

# count number of punctuations
def count_punctuations(text):
    count = lambda l1,l2: sum([1 for x in l1 if x in l2])
    return count(text,set(string.punctuation))

# count number of words in quotes
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    string_wothout_contractions = string_wothout_contractions.replace('"', '\'')
    x = re.findall("\'(.+?)\'", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
    
# count number of sentences
def count_sent(text):
    return len(nltk.sent_tokenize(text))

# count number of unique words 
def count_unique_words(text):
    return len(set(text.split()))
    
# count of hashtags
def count_htags(text):
    x = re.findall(r'(\#\w[A-Za-z0-9]*)', text)
    return len(x)

# count of mentions
def count_mentions(text):
    x = re.findall(r'(\@\w[A-Za-z0-9]*)', text)
    return len(x)

# count of stopwords
def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)

#count of links
def count_links(text):
    link_list = re.findall(r'(https?://[^\s]+)', text)
    bitly_list = re.findall(r'bit.ly/\S+', text)
    return (len(link_list + bitly_list))

#count digits
def count_digits(text):
    digit_list = re.findall(r'[0-9]+', text)
    return len(digit_list)

# Factorize repeated punctuation, add REPEAT
def count_repeat_punct(text):
    rep_list = re.findall(r'([!?.]){2,}', text)
    return len(rep_list)
386/52:
def feature_eng_fun(df_in, text):
    df = copy.deepcopy(df_in)
    df['count_exclamation'] = df_train[text].apply(punct_add_exclamation)
    df['count_question'] = df[text].apply(punct_add_question)
    df['count_quotation'] = df[text].apply(punct_add_quotation)
    df['count_num_of_chars'] = df[text].apply(count_chars)
    df['count_num_of_words'] = df[text].apply(count_words)
    df['count_num_of_capital_chars'] = df[text].apply(count_capital_chars)
    df['count_num_of_capital_words'] = df[text].apply(count_capital_words)
    df['count_punct'] = df[text].apply(count_punctuations)
    df['count_words_in_quotes'] = df[text].apply(count_words_in_quotes)
    df['count_sent'] = df[text].apply(count_sent)
    df['count_unique_words'] = df[text].apply(count_unique_words)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_mentions'] = df[text].apply(count_mentions)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_stopwords'] = df[text].apply(count_stopwords)
    df['count_links'] = df[text].apply(count_links) 
    df['count_repeat_punct'] = df[text].apply(count_repeat_punct)    
    ## This function from part above
    df['GPE_text_flag'] = df[text].apply(show_ents)    
    return df
386/53:
## .loc will create a new copy, not view
df_feature_eng = df_train.loc[:, ('id', 'text', 'target')]
386/54: df_feature_eng = feature_eng_fun(df_feature_eng, 'text')
386/55:
## .loc will create a new copy, not view
df_feature_eng = df_train.loc[:, ('id', 'text', 'target')]
386/56: df_feature_eng = feature_eng_fun(df_feature_eng, 'text')
386/57: df_feature_eng.drop(['text'], axis=1, inplace=True)
386/58:
plt.figure(figsize=(14,10))
sns.heatmap(df_feature_eng.corr(),annot=True)
386/59: cols_selected = np.abs(df_feature_eng.corr()['target']).sort_values(ascending=False)[1:11].index.to_list()
386/60: df_feature_eng[cols_selected].head(3)
386/61:
# Let's look at some tweets more carefully
df_train[55:60]
386/62:
abbreviations = {
    "$" : " dollar ",
    "‚Ç¨" : " euro ",
    "4ao" : "for adults only",
    "a.m" : "before midday",
    "a3" : "anytime anywhere anyplace",
    "aamof" : "as a matter of fact",
    "acct" : "account",
    "adih" : "another day in hell",
    "afaic" : "as far as i am concerned",
    "afaict" : "as far as i can tell",
    "afaik" : "as far as i know",
    "afair" : "as far as i remember",
    "afk" : "away from keyboard",
    "app" : "application",
    "approx" : "approximately",
    "apps" : "applications",
    "asap" : "as soon as possible",
    "asl" : "age, sex, location",
    "atk" : "at the keyboard",
    "ave." : "avenue",
    "aymm" : "are you my mother",
    "ayor" : "at your own risk", 
    "b&b" : "bed and breakfast",
    "b+b" : "bed and breakfast",
    "b.c" : "before christ",
    "b2b" : "business to business",
    "b2c" : "business to customer",
    "b4" : "before",
    "b4n" : "bye for now",
    "b@u" : "back at you",
    "bae" : "before anyone else",
    "bak" : "back at keyboard",
    "bbbg" : "bye bye be good",
    "bbc" : "british broadcasting corporation",
    "bbias" : "be back in a second",
    "bbl" : "be back later",
    "bbs" : "be back soon",
    "be4" : "before",
    "bfn" : "bye for now",
    "blvd" : "boulevard",
    "bout" : "about",
    "brb" : "be right back",
    "bros" : "brothers",
    "brt" : "be right there",
    "bsaaw" : "big smile and a wink",
    "btw" : "by the way",
    "bwl" : "bursting with laughter",
    "c/o" : "care of",
    "cet" : "central european time",
    "cf" : "compare",
    "cia" : "central intelligence agency",
    "csl" : "can not stop laughing",
    "cu" : "see you",
    "cul8r" : "see you later",
    "cv" : "curriculum vitae",
    "cwot" : "complete waste of time",
    "cya" : "see you",
    "cyt" : "see you tomorrow",
    "dae" : "does anyone else",
    "dbmib" : "do not bother me i am busy",
    "diy" : "do it yourself",
    "dm" : "direct message",
    "dwh" : "during work hours",
    "e123" : "easy as one two three",
    "eet" : "eastern european time",
    "eg" : "example",
    "embm" : "early morning business meeting",
    "encl" : "enclosed",
    "encl." : "enclosed",
    "etc" : "and so on",
    "faq" : "frequently asked questions",
    "fawc" : "for anyone who cares",
    "fb" : "facebook",
    "fc" : "fingers crossed",
    "fig" : "figure",
    "fimh" : "forever in my heart", 
    "ft." : "feet",
    "ft" : "featuring",
    "ftl" : "for the loss",
    "ftw" : "for the win",
    "fwiw" : "for what it is worth",
    "fyi" : "for your information",
    "g9" : "genius",
    "gahoy" : "get a hold of yourself",
    "gal" : "get a life",
    "gcse" : "general certificate of secondary education",
    "gfn" : "gone for now",
    "gg" : "good game",
    "gl" : "good luck",
    "glhf" : "good luck have fun",
    "gmt" : "greenwich mean time",
    "gmta" : "great minds think alike",
    "gn" : "good night",
    "g.o.a.t" : "greatest of all time",
    "goat" : "greatest of all time",
    "goi" : "get over it",
    "gps" : "global positioning system",
    "gr8" : "great",
    "gratz" : "congratulations",
    "gyal" : "girl",
    "h&c" : "hot and cold",
    "hp" : "horsepower",
    "hr" : "hour",
    "hrh" : "his royal highness",
    "ht" : "height",
    "ibrb" : "i will be right back",
    "ic" : "i see",
    "icq" : "i seek you",
    "icymi" : "in case you missed it",
    "idc" : "i do not care",
    "idgadf" : "i do not give a damn fuck",
    "idgaf" : "i do not give a fuck",
    "idk" : "i do not know",
    "ie" : "that is",
    "i.e" : "that is",
    "ifyp" : "i feel your pain",
    "IG" : "instagram",
    "iirc" : "if i remember correctly",
    "ilu" : "i love you",
    "ily" : "i love you",
    "imho" : "in my humble opinion",
    "imo" : "in my opinion",
    "imu" : "i miss you",
    "iow" : "in other words",
    "irl" : "in real life",
    "j4f" : "just for fun",
    "jic" : "just in case",
    "jk" : "just kidding",
    "jsyk" : "just so you know",
    "l8r" : "later",
    "lb" : "pound",
    "lbs" : "pounds",
    "ldr" : "long distance relationship",
    "lmao" : "laugh my ass off",
    "lmfao" : "laugh my fucking ass off",
    "lol" : "laughing out loud",
    "ltd" : "limited",
    "ltns" : "long time no see",
    "m8" : "mate",
    "mf" : "motherfucker",
    "mfs" : "motherfuckers",
    "mfw" : "my face when",
    "mofo" : "motherfucker",
    "mph" : "miles per hour",
    "mr" : "mister",
    "mrw" : "my reaction when",
    "ms" : "miss",
    "mte" : "my thoughts exactly",
    "nagi" : "not a good idea",
    "nbc" : "national broadcasting company",
    "nbd" : "not big deal",
    "nfs" : "not for sale",
    "ngl" : "not going to lie",
    "nhs" : "national health service",
    "nrn" : "no reply necessary",
    "nsfl" : "not safe for life",
    "nsfw" : "not safe for work",
    "nth" : "nice to have",
    "nvr" : "never",
    "nyc" : "new york city",
    "oc" : "original content",
    "og" : "original",
    "ohp" : "overhead projector",
    "oic" : "oh i see",
    "omdb" : "over my dead body",
    "omg" : "oh my god",
    "omw" : "on my way",
    "p.a" : "per annum",
    "p.m" : "after midday",
    "pm" : "prime minister",
    "poc" : "people of color",
    "pov" : "point of view",
    "pp" : "pages",
    "ppl" : "people",
    "prw" : "parents are watching",
    "ps" : "postscript",
    "pt" : "point",
    "ptb" : "please text back",
    "pto" : "please turn over",
    "qpsa" : "what happens", #"que pasa",
    "ratchet" : "rude",
    "rbtl" : "read between the lines",
    "rlrt" : "real life retweet", 
    "rofl" : "rolling on the floor laughing",
    "roflol" : "rolling on the floor laughing out loud",
    "rotflmao" : "rolling on the floor laughing my ass off",
    "rt" : "retweet",
    "ruok" : "are you ok",
    "sfw" : "safe for work",
    "sk8" : "skate",
    "smh" : "shake my head",
    "sq" : "square",
    "srsly" : "seriously", 
    "ssdd" : "same stuff different day",
    "tbh" : "to be honest",
    "tbs" : "tablespooful",
    "tbsp" : "tablespooful",
    "tfw" : "that feeling when",
    "thks" : "thank you",
    "tho" : "though",
    "thx" : "thank you",
    "tia" : "thanks in advance",
    "til" : "today i learned",
    "tl;dr" : "too long i did not read",
    "tldr" : "too long i did not read",
    "tmb" : "tweet me back",
    "tntl" : "trying not to laugh",
    "ttyl" : "talk to you later",
    "u" : "you",
    "u2" : "you too",
    "u4e" : "yours for ever",
    "utc" : "coordinated universal time",
    "w/" : "with",
    "w/o" : "without",
    "w8" : "wait",
    "wassup" : "what is up",
    "wb" : "welcome back",
    "wtf" : "what the fuck",
    "wtg" : "way to go",
    "wtpa" : "where the party at",
    "wuf" : "where are you from",
    "wuzup" : "what is up",
    "wywh" : "wish you were here",
    "yd" : "yard",
    "ygtr" : "you got that right",
    "ynk" : "you never know",
    "zzz" : "sleeping bored and tired"
}
386/63:
#remove not ascii
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    return text

# Change an abbreviation by its true meaning
def word_abbrev(word):
    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word

# Replace all abbreviations
def replace_abbrev(text):
    string = ""
    for word in text.split():
        string += word_abbrev(word) + " "        
    return string

# Remove @ and mention, replace by USER
def replace_mention(text):
    pat=re.compile(r'@\S+')
    return pat.sub(r'USER',text)

# Remove numbers, replace it by NUMBER
def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r'NUMBER', text)


##remove digits
#def remove_digits(text):
#    text = re.sub(r'[0-9]+', '', text)
#    return text   

# Remove all emojis, replace by EMOJI (I didn't find any)
#def remove_emoji(text):
#    emoji_pattern = re.compile("["
#                           u"\U0001F600-\U0001F64F"  # emoticons
#                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
#                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
#                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
#                           u"\U00002702-\U000027B0"
#                           u"\U000024C2-\U0001F251"
#                           "]+", flags=re.UNICODE)
#    return emoji_pattern.sub(r'EMOJI', text)

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()

#contractions and remowing stopwords and very short words
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 3:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
386/64:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(remove_emoji)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(string_contractions)
   # df['column_token'] = df[column_name].apply(word_tokenize)
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(clean_text)
    return df
386/65: df_text = df_train.loc[:, ['id', 'target', 'keyword', 'text']]
386/66: df_text = basic_cleaning(df_text, 'text')
386/67:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(remove_number)
#    df[column_name] = df[column_name].apply(remove_emoji)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(string_contractions)
   # df['column_token'] = df[column_name].apply(word_tokenize)
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(clean_text)
    return df
386/68: df_text = df_train.loc[:, ['id', 'target', 'keyword', 'text']]
386/69: df_text = basic_cleaning(df_text, 'text')
386/70: df_text.head(3)
386/71: all_text = ' '.join(df_text['text'].tolist())
386/72:
oov_list = []
def search_oov(text):
    tokens = nlp(text)
    for token in tokens:
        if token.is_oov:
            oov_list.append(token)
386/73: search_oov(all_text)
386/74: oov_list_strings  = [i.text for i in oov_list]
386/75:
counter = Counter(oov_list_strings)
counter.most_common(10)
386/76:
#remove not ascii
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    return text

# Change an abbreviation by its true meaning
def word_abbrev(word):
    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word

# Replace all abbreviations
def replace_abbrev(text):
    string = ""
    for word in text.split():
        string += word_abbrev(word) + " "        
    return string

# Remove @ and mention, replace by USER
def replace_mention(text):
    pat=re.compile(r'@\S+')
    return pat.sub(r'USER',text)

# Remove numbers, replace it by NUMBER
def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r' NUMBER ', text)


##remove digits
#def remove_digits(text):
#    text = re.sub(r'[0-9]+', '', text)
#    return text   

# Remove all emojis, replace by EMOJI (I didn't find any)
#def remove_emoji(text):
#    emoji_pattern = re.compile("["
#                           u"\U0001F600-\U0001F64F"  # emoticons
#                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
#                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
#                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
#                           u"\U00002702-\U000027B0"
#                           u"\U000024C2-\U0001F251"
#                           "]+", flags=re.UNICODE)
#    return emoji_pattern.sub(r'EMOJI', text)

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()

#contractions and remowing stopwords and very short words
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 3:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
386/77:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(remove_number)
#    df[column_name] = df[column_name].apply(remove_emoji)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(string_contractions)
   # df['column_token'] = df[column_name].apply(word_tokenize)
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(clean_text)
    return df
386/78: df_text = df_train.loc[:, ['id', 'target', 'keyword', 'text']]
386/79: df_text = basic_cleaning(df_text, 'text')
386/80: df_text.head(3)
386/81: all_text = ' '.join(df_text['text'].tolist())
386/82:
oov_list = []
def search_oov(text):
    tokens = nlp(text)
    for token in tokens:
        if token.is_oov:
            oov_list.append(token)
386/83: search_oov(all_text)
386/84: oov_list_strings  = [i.text for i in oov_list]
386/85:
counter = Counter(oov_list_strings)
counter.most_common(10)
386/86: sid = SentimentIntensityAnalyzer()
386/87: df_sentiment = df_train.loc[:, ['id', 'target', 'text']]
386/88: df_sentiment['sentiment_compound']  = df_sentiment.text.apply(lambda tweet: sid.polarity_scores(tweet)['compound'])
386/89: df_sentiment['sentiment_compound_score'] = df_sentiment['sentiment_compound'].apply(lambda c: 'pos' if c >=0 else 'neg')
386/90:
plt.figure(figsize=[4, 4])
sns.countplot(x='target', hue='sentiment_compound_score', data=df_sentiment)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
386/91: #not very optimistic result, I won't add a sentiment feature :)
386/92: ## CREATE A MODEL BASED ON TEXT
386/93:
def to_labels(pos_probs, threshold):
    return (pos_probs >= threshold).astype('int')

def best_tresholds(y_pred_proba, y_test):
    # define thresholds
    thresholds = np.arange(0, 1, 0.001)
    # evaluate each threshold
    scores = [f1_score(y_test, to_labels(y_pred_proba, t)) for t in thresholds]
    # get best threshold
    ix = np.argmax(scores)
    #print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
    return thresholds[ix], scores[ix]
386/94: ### with eng features
386/95: df_copy = copy.deepcopy(df_train)
386/96:
cols_selected = ['GPE_text_flag', 'count_links', 'count_num_of_chars', 'count_punct', 'count_sent',
                 'count_mentions', 'count_stopwords', 'count_question', 'count_quotation',
                 'count_num_of_capital_chars']

df_copy = copy.deepcopy(df_train)
df_train_text_clean = basic_cleaning(df_copy, 'text')
df_train_text_clean["clean_text"] = df_train_text_clean.keyword.map(str) + " " + df_train_text_clean.text

## new features
df_train_feature_eng = feature_eng_fun(df_copy, 'text')
df_train_feature_eng = df_train_feature_eng[cols_selected]


#### for test
df_test_text_clean_f = basic_cleaning(df_test, 'text')
df_test_text_clean_f["clean_text"] = df_test_text_clean_f.keyword.map(str) + " " + df_test_text_clean_f.text

df_test_feature_eng_f = feature_eng_fun(df_test, 'text')
df_test_feature_eng_f = df_test_feature_eng_f[cols_selected]
386/97: df_train_text_clean.shape
386/98: df_train_feature_eng.shape
386/99: df_train_text_clean.head(5)
386/100:
## ADD LEMMATIZATION
w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
def lemmatize_text(text):
    return [wnl.lemmatize(w) for w in w_tokenizer.tokenize(text)]

df_train_text_clean['text_lemmatized'] = df_train_text_clean.clean_text.apply(lemmatize_text)
386/101: df_train_text_clean
386/102:
## ADD LEMMATIZATION
w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
def lemmatize_text(text):
    return [wnl.lemmatize(w) for w in w_tokenizer.tokenize(text)]

df_train_text_clean['text_lemmatized'] = df_train_text_clean.clean_text.apply(lemmatize_text)
df_train_text_clean['all_text'] = df_train_text_clean['text_lemmatized'].apply(lambda x: " ".join(x))
386/103: df_train_text_clean
386/104:
## ADD LEMMATIZATION
w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
def lemmatize_text(text):
    return [wnl.lemmatize(w) for w in w_tokenizer.tokenize(text)]

df_train_text_clean['text_lemmatized'] = df_train_text_clean.clean_text.apply(lemmatize_text)
df_train_text_clean['all_text'] = df_train_text_clean['text_lemmatized'].apply(lambda x: " ".join(x))
386/105: df_train_text_clean.shape
386/106: df_train_feature_eng.shape
386/107:
df_train_full = pd.merge(df_train_text_clean[['target', 'clean_text']], df_train_feature_eng, 
                         left_index=True, right_index=True)

df_test_full = pd.merge(df_test_text_clean_f[['all_text']], df_test_feature_eng_f, 
                         left_index=True, right_index=True)
386/108:
def to_labels(pos_probs, threshold):
    return (pos_probs >= threshold).astype('int')

def best_tresholds(y_pred_proba, y_test):
    # define thresholds
    thresholds = np.arange(0, 1, 0.001)
    # evaluate each threshold
    scores = [f1_score(y_test, to_labels(y_pred_proba, t)) for t in thresholds]
    # get best threshold
    ix = np.argmax(scores)
    #print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
    return thresholds[ix], scores[ix]
386/109: ### with eng features
386/110: df_copy = copy.deepcopy(df_train)
386/111:
cols_selected = ['GPE_text_flag', 'count_links', 'count_num_of_chars', 'count_punct', 'count_sent',
                 'count_mentions', 'count_stopwords', 'count_question', 'count_quotation',
                 'count_num_of_capital_chars']

df_copy = copy.deepcopy(df_train)
df_train_text_clean = basic_cleaning(df_copy, 'text')
df_train_text_clean["clean_text"] = df_train_text_clean.keyword.map(str) + " " + df_train_text_clean.text

## new features
df_train_feature_eng = feature_eng_fun(df_copy, 'text')
df_train_feature_eng = df_train_feature_eng[cols_selected]


#### for test
df_test_text_clean_f = basic_cleaning(df_test, 'text')
df_test_text_clean_f["clean_text"] = df_test_text_clean_f.keyword.map(str) + " " + df_test_text_clean_f.text

df_test_feature_eng_f = feature_eng_fun(df_test, 'text')
df_test_feature_eng_f = df_test_feature_eng_f[cols_selected]
386/112: df_train_text_clean.head(5)
386/113:
## ADD LEMMATIZATION
w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
def lemmatize_text(text):
    return [wnl.lemmatize(w) for w in w_tokenizer.tokenize(text)]

df_train_text_clean['text_lemmatized'] = df_train_text_clean.clean_text.apply(lemmatize_text)
df_train_text_clean['all_text'] = df_train_text_clean['text_lemmatized'].apply(lambda x: " ".join(x))
386/114: df_train_text_clean.shape
386/115: df_train_feature_eng.shape
386/116:
## ADD LEMMATIZATION
w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
def lemmatize_text(text):
    return [wnl.lemmatize(w) for w in w_tokenizer.tokenize(text)]

df_train_text_clean['text_lemmatized'] = df_train_text_clean.clean_text.apply(lemmatize_text)
df_train_text_clean['all_text'] = df_train_text_clean['text_lemmatized'].apply(lambda x: " ".join(x))

df_test_text_clean_f['text_lemmatized'] = df_test_text_clean_f.clean_text.apply(lemmatize_text)
df_test_text_clean_f['all_text'] = df_test_text_clean_f['text_lemmatized'].apply(lambda x: " ".join(x))
386/117: df_train_text_clean.shape
386/118: df_train_feature_eng.shape
386/119:
df_train_full = pd.merge(df_train_text_clean[['target', 'all_text']], df_train_feature_eng, 
                         left_index=True, right_index=True)

df_test_full = pd.merge(df_test_text_clean_f[['all_text']], df_test_feature_eng_f, 
                         left_index=True, right_index=True)
386/120:
## split to test and train
y = df_train_full['target']
X = df_train_full.drop('target', axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
print('X_train shape', X_train.shape)
print('X_test shape', X_test.shape)
386/121:
max_features=15000

# CountVectorizer
count_vectorizer = CountVectorizer(max_features=max_features)

def count_vector(data):
    count_vectorizer = CountVectorizer()
    vect = count_vectorizer.fit_transform(data)
    return vect, count_vectorizer

def tfidf_vector(data):
    tfidf_vectorizer = TfidfVectorizer()
    vect = tfidf_vectorizer.fit_transform(data)
    return vect, tfidf_vectorizer

def min_max(data):
    min_max_scaler = MinMaxScaler()
    vect = min_max_scaler.fit_transform(data)
    return vect, min_max_scaler

X_train_count, count_vectorizer = count_vector(X_train['clean_text'])
X_train_tfidf, tfidf_vectorizer = tfidf_vector(X_train['clean_text'])

X_train_eng_minmax, minmax_scaler = min_max(X_train[cols_selected])
X_train_eng = X_train[cols_selected].to_numpy()

#### TEST
X_test_count = count_vectorizer.transform(X_test ['clean_text'])                                                     
X_test_tfidf = tfidf_vectorizer.transform(X_test ['clean_text'])

X_test_eng_minmax = minmax_scaler.transform(X_test[cols_selected])
X_test_eng = X_test[cols_selected].to_numpy()

### TEST DATASET
X_test_count_f = count_vectorizer.transform(df_test_full['clean_text'])                                                     
X_test_tfidf_f = tfidf_vectorizer.transform(df_test_full['clean_text'])

X_test_eng_minmax_f = minmax_scaler.transform(df_test_full[cols_selected])
X_test_eng_f = df_test_full[cols_selected].to_numpy()


### CONCAT tf-idf + minmax, count_vec + feature_eng

X_train_tfidf_minmax = hstack([X_train_tfidf, X_train_eng_minmax])
X_train_count_eng = hstack([X_train_count, X_train_eng])


X_test_tfidf_minmax = hstack([X_test_tfidf, X_test_eng_minmax])
X_test_count_eng = hstack([X_test_count, X_test_eng])

X_test_tfidf_minmax_f = hstack([X_test_tfidf_f, X_test_eng_minmax_f])
X_test_count_eng_f = hstack([X_test_count_f, X_test_eng_f])
386/122: text_column = 'all_text'
386/123:
max_features=15000

# CountVectorizer
count_vectorizer = CountVectorizer(max_features=max_features)

def count_vector(data):
    count_vectorizer = CountVectorizer()
    vect = count_vectorizer.fit_transform(data)
    return vect, count_vectorizer

def tfidf_vector(data):
    tfidf_vectorizer = TfidfVectorizer()
    vect = tfidf_vectorizer.fit_transform(data)
    return vect, tfidf_vectorizer

def min_max(data):
    min_max_scaler = MinMaxScaler()
    vect = min_max_scaler.fit_transform(data)
    return vect, min_max_scaler

X_train_count, count_vectorizer = count_vector(X_train[text_column])
X_train_tfidf, tfidf_vectorizer = tfidf_vector(X_train[text_column])

X_train_eng_minmax, minmax_scaler = min_max(X_train[cols_selected])
X_train_eng = X_train[cols_selected].to_numpy()

#### TEST
X_test_count = count_vectorizer.transform(X_test [text_column])                                                     
X_test_tfidf = tfidf_vectorizer.transform(X_test [text_column])

X_test_eng_minmax = minmax_scaler.transform(X_test[cols_selected])
X_test_eng = X_test[cols_selected].to_numpy()

### TEST DATASET
X_test_count_f = count_vectorizer.transform(df_test_full[text_column])                                                     
X_test_tfidf_f = tfidf_vectorizer.transform(df_test_full[text_column])

X_test_eng_minmax_f = minmax_scaler.transform(df_test_full[cols_selected])
X_test_eng_f = df_test_full[cols_selected].to_numpy()


### CONCAT tf-idf + minmax, count_vec + feature_eng

X_train_tfidf_minmax = hstack([X_train_tfidf, X_train_eng_minmax])
X_train_count_eng = hstack([X_train_count, X_train_eng])


X_test_tfidf_minmax = hstack([X_test_tfidf, X_test_eng_minmax])
X_test_count_eng = hstack([X_test_count, X_test_eng])

X_test_tfidf_minmax_f = hstack([X_test_tfidf_f, X_test_eng_minmax_f])
X_test_count_eng_f = hstack([X_test_count_f, X_test_eng_f])
386/124:
#import pickle
#from joblib import dump
#
#with open('count_vectorizer.pickle', 'wb') as handle:
#    pickle.dump(count_vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
#with open('tfidf_vectorizer.pickle', 'wb') as handle:
#    pickle.dump(tfidf_vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
#with open('minmax_scaler.pickle', 'wb') as handle:
#    pickle.dump(minmax_scaler, handle, protocol=pickle.HIGHEST_PROTOCOL)
386/125:
metrics = pd.DataFrame(columns=['model' ,'vectoriser', 'f1 score', 'best_f1', 'threshold', 
                                'train accuracy','test accuracy'])
386/126:
def fit_and_predict(model, x_train ,x_test, y_train, y_test, vectoriser):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)

    y_pred = classifier.predict(x_test)
    cmatrix = confusion_matrix(y_test,y_pred)
    
    y_hat = classifier.predict_proba(x_test)[:,1]
    threshold, best_score = best_tresholds(y_hat, y_test)


    f1score = f1_score(y_test,y_pred)
    train_accuracy = round(classifier.score(x_train,y_train)*100)
    test_accuracy =  round(accuracy_score(y_test,y_pred)*100)

    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'f1 score': f1score, 
                              'best_f1': best_score,
                              'threshold': threshold,
                              'train accuracy': train_accuracy, 
                              'test accuracy': test_accuracy, 
                              'vectoriser': str(vectoriser),
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using "+ str(vectoriser))
    print('f1 score for test: {}'.format(f1score))
    print(classification_report(y_test,y_pred))    
    print('Threshold={:.5f}, F-Score={:.5f}'.format(threshold, best_score))
    print('Accuracy of classifier on training set:{}%'.format(train_accuracy))
    print('Accuracy of classifier on test set:{}%' .format(test_accuracy))
386/127:
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
386/128:
models=[
   #     XGBClassifier(max_depth=6, n_estimators=1000),
        LogisticRegression(random_state=30),
        SVC(random_state=30, probability=True),
        MultinomialNB(),
        DecisionTreeClassifier(random_state = 30),
        KNeighborsClassifier(),
        RandomForestClassifier(random_state=30),
       ]
386/129:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train_count, X_test_count, y_train ,y_test, 'Count vector')
    
    fit_and_predict(model, X_train_tfidf, X_test_tfidf, y_train, y_test, 'Tfidf vector')
    
    ## count with EF
    fit_and_predict(model, X_train_count_eng, X_test_count_eng, y_train, y_test, 'Count vector + FE')
    
    ## TF IDF with EF
    fit_and_predict(model, X_train_tfidf_minmax, X_test_tfidf_minmax, y_train, y_test, 'TF-IDF + FE')
386/130: metrics.sort_values('best_f1')
386/131:
lr = LogisticRegression(random_state=30)
lr.fit(X_train_tfidf_minmax, y_train)
y_pred_prob = lr.predict_proba(X_test_tfidf_minmax_f)[:, 1]
386/132:
RBF = RandomForestClassifier(random_state=30)
RBF.fit(X_train_tfidf_minmax, y_train)
386/133: RBF
386/134: RBF.get_params
386/135: RBF.get_params()
386/136:
models=[
   #     XGBClassifier(max_depth=6, n_estimators=1000),
        LogisticRegression(random_state=30),
        SVC(random_state=30, probability=True),
        MultinomialNB(),
        DecisionTreeClassifier(random_state = 30),
        KNeighborsClassifier(),
        RandomForestClassifier(random_state=30, max_depth=6),
       ]
386/137:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train_count, X_test_count, y_train ,y_test, 'Count vector')
    
    fit_and_predict(model, X_train_tfidf, X_test_tfidf, y_train, y_test, 'Tfidf vector')
    
    ## count with EF
    fit_and_predict(model, X_train_count_eng, X_test_count_eng, y_train, y_test, 'Count vector + FE')
    
    ## TF IDF with EF
    fit_and_predict(model, X_train_tfidf_minmax, X_test_tfidf_minmax, y_train, y_test, 'TF-IDF + FE')
386/138:
models=[
   #     XGBClassifier(max_depth=6, n_estimators=1000),
        LogisticRegression(random_state=30),
        SVC(random_state=30, probability=True),
        MultinomialNB(),
        DecisionTreeClassifier(random_state = 30),
        KNeighborsClassifier(),
        RandomForestClassifier(random_state=30, max_depth=4),
       ]
386/139:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train_count, X_test_count, y_train ,y_test, 'Count vector')
    
    fit_and_predict(model, X_train_tfidf, X_test_tfidf, y_train, y_test, 'Tfidf vector')
    
    ## count with EF
    fit_and_predict(model, X_train_count_eng, X_test_count_eng, y_train, y_test, 'Count vector + FE')
    
    ## TF IDF with EF
    fit_and_predict(model, X_train_tfidf_minmax, X_test_tfidf_minmax, y_train, y_test, 'TF-IDF + FE')
386/140: metrics.sort_values('best_f1')
386/141: metrics.sort_values('best_f1')
386/142:
metrics = pd.DataFrame(columns=['model' ,'vectoriser', 'f1 score', 'best_f1', 'threshold', 
                                'train accuracy','test accuracy'])
386/143:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train_count, X_test_count, y_train ,y_test, 'Count vector')
    
    fit_and_predict(model, X_train_tfidf, X_test_tfidf, y_train, y_test, 'Tfidf vector')
    
    ## count with EF
    fit_and_predict(model, X_train_count_eng, X_test_count_eng, y_train, y_test, 'Count vector + FE')
    
    ## TF IDF with EF
    fit_and_predict(model, X_train_tfidf_minmax, X_test_tfidf_minmax, y_train, y_test, 'TF-IDF + FE')
386/144: metrics.sort_values('best_f1')
386/145: param_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}
386/146: from sklearn.model_selection import GridSearchCV
386/147:
from sklearn.model_selection import StratifiedKFold
StratifiedKFold(n_splits=3, random_state=None, shuffle=False)
386/148:
grid = GridSearchCV(SVC(),param_grid,verbose=2,scoring=f1_score)
grid.fit(X_train_tfidf_minmax,y_train)
386/149:
grid = GridSearchCV(SVC(),param_grid,verbose=2,scoring=f1)
grid.fit(X_train_tfidf_minmax,y_train)
386/150:
grid = GridSearchCV(SVC(),param_grid,verbose=2,scoring='f1')
grid.fit(X_train_tfidf_minmax,y_train)
386/151: param_grid = {'C': [0.1,1, 10], 'gamma': [0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}
386/152:
grid = GridSearchCV(SVC(),param_grid,verbose=2,scoring='f1')
grid.fit(X_train_tfidf_minmax,y_train)
386/153:
grid = GridSearchCV(SVC(),param_grid,cv=StratifiedKFold(),verbose=2,scoring='f1')
grid.fit(X_train_tfidf_minmax,y_train)
386/154: grid.best_params_
386/155: grid.best_score_
386/156: grid.best_estimator_
386/157: param_grid = {'C': [1, 7, 9, 10, 11, 13], 'gamma': [0.15, 0.1,0.05,0.01],'kernel': ['rbf', 'poly', 'sigmoid']}
386/158:
from sklearn.model_selection import StratifiedKFold
StratifiedKFold(n_splits=3, random_state=None, shuffle=False)
386/159:
grid = GridSearchCV(SVC(),param_grid,cv=StratifiedKFold(),verbose=2,scoring='f1')
grid.fit(X_train_tfidf_minmax,y_train)
386/160: grid.best_estimator_
386/161:
svc_best = grid.best_estimator_
y_predict_proba_svc = svc_best.predict_proba(X_test_tfidf_minmax)
386/162: param_grid = {'C': [11, 12, 13, 15, 17], 'gamma': [0.07,0.05,0.03],'kernel': ['rbf', 'poly', 'sigmoid']}
386/163: grid.best_params_
386/164: param_grid = {'C': [11, 12, 13, 15, 17], 'gamma': [0.07,0.05,0.03],'kernel': ['rbf']}
386/165:
from sklearn.model_selection import StratifiedKFold
StratifiedKFold(n_splits=3, random_state=None, shuffle=False)
386/166:
grid = GridSearchCV(SVC(probability=True),param_grid,cv=StratifiedKFold(),verbose=2,scoring='f1')
grid.fit(X_train_tfidf_minmax,y_train)
386/167: grid.best_estimator_
386/168:
svc_best = grid.best_estimator_
y_predict_proba_svc = svc_best.predict_proba(X_test_tfidf_minmax)
386/169: from sklearn.metrics import roc_curve
386/170:
thresholds = arange(0, 1, 0.001)
fpr, tpr, thresh = roc_curve(y_test, y_predict_proba_svc[:,1], pos_label=1)
gmeans = np.sqrt(tpr * (1-fpr))
ix = argmax(gmeans)
print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
386/171:
thresholds = np.arange(0, 1, 0.001)
fpr, tpr, thresh = roc_curve(y_test, y_predict_proba_svc[:,1], pos_label=1)
gmeans = np.sqrt(tpr * (1-fpr))
ix = argmax(gmeans)
print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
386/172:
thresholds = np.arange(0, 1, 0.001)
fpr, tpr, thresh = roc_curve(y_test, y_predict_proba_svc[:,1], pos_label=1)
gmeans = np.sqrt(tpr * (1-fpr))
ix = np.argmax(gmeans)
print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
386/173: grid.best_score_
386/174:
svc_best = grid.best_estimator_
y_predict_proba_svc = svc_best.predict_proba(X_test_tfidf_minmax)
y_pred_svc = svc_best.predict(X_test_tfidf_minmax)
print(f1_score(y_test, y_pred_svc))
386/175:
sns.lineplot(x=fpr, y=tpr, linestyle='--')
plt.scatter(fpr[ix], tpr[ix], marker='o', color='r', label='Best')
#plt.plot(fpr1, tpr1, linestyle='--',color='orange', label='mnb')
# title
plt.title('ROC curve')
# x label
plt.xlabel('False Positive Rate')
# y label
plt.ylabel('True Positive rate')
plt.savefig('ROC',dpi=300)
plt.grid(linestyle='--')
plt.show();
386/176:
thresholds = np.arange(0, 1, 0.001)
fpr, tpr, thresh = roc_curve(y_test, y_predict_proba_svc[:,1], pos_label=1)
gmeans = np.sqrt(tpr * (1-fpr))
ix = np.argmax(gmeans)
print('Best Threshold={:.3f}, G-Mean=%.3f'.format(thresholds[ix], gmeans[ix]))
386/177:
thresholds = np.arange(0, 1, 0.001)
fpr, tpr, thresh = roc_curve(y_test, y_predict_proba_svc[:,1], pos_label=1)
gmeans = np.sqrt(tpr * (1-fpr))
ix = np.argmax(gmeans)
print('Best Threshold={:.4f}, G-Mean=%.3f'.format(thresholds[ix], gmeans[ix]))
386/178:
thresholds = np.arange(0, 1, 0.001)
fpr, tpr, thresh = roc_curve(y_test, y_predict_proba_svc[:,1], pos_label=1)
gmeans = np.sqrt(tpr * (1-fpr))
ix = np.argmax(gmeans)
print('Best Threshold={:.4f}, G-Mean={:.3}'.format(thresholds[ix], gmeans[ix]))
386/179: from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
386/180:
lda = LinearDiscriminantAnalysis(n_components = 1000)
X_train_lda = lda.fit_transform(X_train_tfidf_minmax,y_train)
X_test_lda = lda.predict(X_test_eng_minmax)
386/181:
lda = LinearDiscriminantAnalysis(n_components = 1000)
lda.fit(X_train_tfidf_minmax,y_train)
X_train_lda = lda.predict(X_train_tfidf_minmax)
X_test_lda = lda.predict(X_test_eng_minmax)
386/182:
lda = LinearDiscriminantAnalysis(n_components = 1000)
lda.fit(X_train_tfidf_minmax.todense(), y_train)
X_train_lda = lda.predict(X_train_tfidf_minmax)
X_test_lda = lda.predict(X_test_eng_minmax)
386/183: X_train_tfidf_minmax.todense()
386/184: X_train_tfidf_minmax.todense().shape
386/185: X_train_tfidf_minmax.shape
386/186: X_train_tfidf_minmax.todense().shape
386/187:
svd = TruncatedSVD(n_components = 1000)
svd.fit(X_train_tfidf_minmax.todense(), y_train)
X_train_svd = svd.predict(X_train_tfidf_minmax)
X_test_svd = svd.predict(X_test_eng_minmax)
386/188: from sklearn.decomposition import TruncatedSVD
386/189:
svd = TruncatedSVD(n_components = 1000)
svd.fit(X_train_tfidf_minmax.todense(), y_train)
X_train_svd = svd.predict(X_train_tfidf_minmax)
X_test_svd = svd.predict(X_test_eng_minmax)
386/190:
svc_best = grid.best_estimator_
y_predict_proba_svc = svc_best.predict_proba(X_test_tfidf_minmax)
y_predict_proba_train_svc = svc_best.predict_proba(X_train_tfidf_minmax)

y_pred_svc = svc_best.predict(X_test_tfidf_minmax)
print(f1_score(y_test, y_pred_svc))

y_predict_proba_svc_f = svc_best.predict_proba(X_test_tfidf_minmax_f)[:,1]
386/191:
thresholds = np.arange(0, 1, 0.001)
fpr, tpr, thresh = roc_curve(y_train, y_predict_proba_train_svc[:,1], pos_label=1)
gmeans = np.sqrt(tpr * (1-fpr))
ix = np.argmax(gmeans)
print('Best Threshold Train ={:.4f}, G-Mean={:.3}'.format(thresholds[ix], gmeans[ix]))
fpr, tpr, thresh = roc_curve(y_test, y_predict_proba_svc[:,1], pos_label=1)
gmeans = np.sqrt(tpr * (1-fpr))
ix = np.argmax(gmeans)
print('Best Threshold Test ={:.4f}, G-Mean={:.3}'.format(thresholds[ix], gmeans[ix]))
386/192:
df_t = df_test[['id']]
df_t['target'] = (y_predict_proba_svc_f >= 0.29).astype(int)
df_t.to_csv("submission_svm_029.csv", index=False)
386/193: df_t
386/194:
svd = TruncatedSVD(n_components = 1000)
svd.fit(X_train_tfidf_minmax.todense(), y_train)
X_train_svd = svd.predict(X_train_tfidf_minmax)
X_test_svd = svd.predict(X_test_eng_minmax)
386/195: svd
386/196:
X_train_svd = svd.transform(X_train_tfidf_minmax)
X_test_svd = svd.transform(X_test_eng_minmax)
386/197: X_test_eng_minmax.shape
386/198:
max_features=15000

# CountVectorizer
count_vectorizer = CountVectorizer(max_features=max_features)

def count_vector(data):
    count_vectorizer = CountVectorizer()
    vect = count_vectorizer.fit_transform(data)
    return vect, count_vectorizer

def tfidf_vector(data):
    tfidf_vectorizer = TfidfVectorizer()
    vect = tfidf_vectorizer.fit_transform(data)
    return vect, tfidf_vectorizer

def min_max(data):
    min_max_scaler = MinMaxScaler()
    vect = min_max_scaler.fit_transform(data)
    return vect, min_max_scaler

X_train_count, count_vectorizer = count_vector(X_train[text_column])
X_train_tfidf, tfidf_vectorizer = tfidf_vector(X_train[text_column])

X_train_eng_minmax, minmax_scaler = min_max(X_train[cols_selected])
X_train_eng = X_train[cols_selected].to_numpy()

#### TEST
X_test_count = count_vectorizer.transform(X_test [text_column])                                                     
X_test_tfidf = tfidf_vectorizer.transform(X_test [text_column])

X_test_eng_minmax = minmax_scaler.transform(X_test[cols_selected])
X_test_eng = X_test[cols_selected].to_numpy()

### TEST DATASET
X_test_count_f = count_vectorizer.transform(df_test_full[text_column])                                                     
X_test_tfidf_f = tfidf_vectorizer.transform(df_test_full[text_column])

X_test_eng_minmax_f = minmax_scaler.transform(df_test_full[cols_selected])
X_test_eng_f = df_test_full[cols_selected].to_numpy()


### CONCAT tf-idf + minmax, count_vec + feature_eng

X_train_tfidf_minmax = hstack([X_train_tfidf, X_train_eng_minmax])
X_train_count_eng = hstack([X_train_count, X_train_eng])


X_test_tfidf_minmax = hstack([X_test_tfidf, X_test_eng_minmax])
X_test_count_eng = hstack([X_test_count, X_test_eng])

X_test_tfidf_minmax_f = hstack([X_test_tfidf_f, X_test_eng_minmax_f])
X_test_count_eng_f = hstack([X_test_count_f, X_test_eng_f])
386/199: X_test_eng_minmax.shape
386/200:
X_train_svd = svd.transform(X_train_tfidf_minmax)
X_test_svd = svd.transform(X_test_tfidf_minmax)
386/201: X_train_svd.shape
386/202: X_train_svd
386/203:
svc_best = grid.best_estimator_
svc_best.fit(X_train_svd, y_train)

y_predict_proba_svc_svd = svc_best.predict_proba(X_test_svd)
y_pred_svc_svd = svc_best.predict(X_test_svd)
print(f1_score(y_test, y_pred_svc))
386/204:
X_train_svd = svd.transform(X_train_tfidf_minmax)
X_test_svd = svd.transform(X_test_tfidf_minmax)
X_test_svd_f = svd.transform(X_test_tfidf_minmax_ff)
386/205:
#X_train_svd = svd.transform(X_train_tfidf_minmax)
#X_test_svd = svd.transform(X_test_tfidf_minmax)
X_test_svd_f = svd.transform(X_test_tfidf_minmax_f)
386/206: X_test_svd_f.shape
386/207:  1 / (1000 * X_train_svd.var())
386/208: param_grid = {'C': [1,5,10], 'gamma': [1, 0.6, 0.1],'kernel': ['rbf', 'poly', 'sigmoid']}
386/209:  #1 / (1000 * X_train_svd.var())
386/210:
grid_svd = GridSearchCV(SVC(probability=True),param_grid,cv=StratifiedKFold(),verbose=2,scoring='f1')
grid_svd.fit(X_train_svd,y_train)
386/211:
import pickle
from joblib import dump
#
#with open('count_vectorizer.pickle', 'wb') as handle:
#    pickle.dump(count_vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
#with open('tfidf_vectorizer.pickle', 'wb') as handle:
#    pickle.dump(tfidf_vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
#with open('minmax_scaler.pickle', 'wb') as handle:
#    pickle.dump(minmax_scaler, handle, protocol=pickle.HIGHEST_PROTOCOL)
386/212:
with open('TruncatedSVD.pickle', 'wb') as handle:
    pickle.dump(svd, handle, protocol=pickle.HIGHEST_PROTOCOL)
386/213: param_grid = {'C': [1,5,10], 'gamma': [1, 0.6, 0.1],'kernel': ['rbf']}
386/214:  #1 / (1000 * X_train_svd.var())
386/215:
grid_svd = GridSearchCV(SVC(probability=True),param_grid,cv=StratifiedKFold(),verbose=2,scoring='f1')
grid_svd.fit(X_train_svd,y_train)
386/216: grid_svd.best_params_
386/217:
y_pred_svc_svd = grid_svd.predict(X_test_svd)
print(f1_score(y_test, y_pred_svc))
386/218: grid_svd.best_estimator_
386/219: svd_svc = grid_svd.best_estimator_
386/220:
y_pred_svc_svd = svd_svc.predict(X_test_svd)
y_predict_proba_svc_svd = svd_svc.predict_proba(X_test_svd)[:,1]
print(f1_score(y_test, y_pred_svc_svd))
386/221:
thresholds = np.arange(0, 1, 0.001)
fpr, tpr, thresh = roc_curve(y_train, y_predict_proba_svc_svd, pos_label=1)
gmeans = np.sqrt(tpr * (1-fpr))
ix = np.argmax(gmeans)
print('Best Threshold Train ={:.4f}, G-Mean={:.3}'.format(thresholds[ix], gmeans[ix]))
386/222: y_predict_proba_svc_svd
386/223:
thresholds = np.arange(0, 1, 0.001)
fpr, tpr, thresh = roc_curve(y_train, y_pred_svc_svd, pos_label=1)
gmeans = np.sqrt(tpr * (1-fpr))
ix = np.argmax(gmeans)
print('Best Threshold Train ={:.4f}, G-Mean={:.3}'.format(thresholds[ix], gmeans[ix]))
386/224:
thresholds = np.arange(0, 1, 0.001)
fpr, tpr, thresh = roc_curve(y_test, y_pred_svc_svd, pos_label=1)
gmeans = np.sqrt(tpr * (1-fpr))
ix = np.argmax(gmeans)
print('Best Threshold Train ={:.4f}, G-Mean={:.3}'.format(thresholds[ix], gmeans[ix]))
386/225: y_predict_proba_svc_svd_f = svd_svc.predict_proba(X_test_svd_f)[:,1]
386/226:
df_t = df_test[['id']]
df_t['target'] = (y_predict_proba_svc_f >= 0.001).astype(int)
#df_t.to_csv("submission_svm_029.csv", index=False)
386/227: df_t
386/228: df_t.describe()
386/229:
df_t = df_test[['id']]
df_t['target'] = (y_predict_proba_svc_f >= 0.5).astype(int)
#df_t.to_csv("submission_svm_029.csv", index=False)
386/230: df_t.describe()
386/231: grid_svd.best_score_
386/232:
thresholds = np.arange(0, 1, 0.001)
fpr, tpr, thresh = roc_curve(y_test, y_predict_proba_svc_svd, pos_label=1)
gmeans = np.sqrt(tpr * (1-fpr))
ix = np.argmax(gmeans)
print('Best Threshold Train ={:.4f}, G-Mean={:.3}'.format(thresholds[ix], gmeans[ix]))
386/233: (y_predict_proba_svc_svd >= 0.22).astype(int)
386/234: f1_score(y_test(y_predict_proba_svc_svd >= 0.22).astype(int))
386/235: f1_score(y_test, (y_predict_proba_svc_svd >= 0.22).astype(int))
386/236: f1_score(y_test, (y_predict_proba_svc_svd >= 0.5).astype(int))
386/237: f1_score(y_test, (y_predict_proba_svc_svd >= 0.6).astype(int))
386/238:
df_t = df_test[['id']]
df_t['target'] = (y_predict_proba_svc_f >= 0.5).astype(int)
#df_t.to_csv("submission_svm_029.csv", index=False)
386/239: df_t.describe()
386/240:
df_t = df_test[['id']]
df_t['target'] = (y_predict_proba_svc_f >= 0.5).astype(int)
df_t.to_csv("submission_svm_svd.csv", index=False)
386/241:
y_pred_svc_svd = svd_svc.predict(X_test_svd)
y_predict_proba_svc_svd = svd_svc.predict_proba(X_test_tfidf_minmax)[:,1]
print(f1_score(y_test, y_pred_svc))
386/242:
y_pred_svc_svd = svd_svc.predict(X_test_svd)
y_predict_proba_svc_svd = svd_svc.predict_proba(X_test_svd)[:,1]
print(f1_score(y_test, y_pred_svc))
386/243: best_tresholds(y_predict_proba_svc_svd, y_test)
386/244:
df_t = df_test[['id']]
df_t['target'] = (y_predict_proba_svc_f >= 0.534).astype(int)
#df_t.to_csv("submission_svm_svd.csv", index=False)
386/245: df_t.describe()
386/246:
df_t = df_test[['id']]
df_t['target'] = (y_predict_proba_svc_f >= 0.534).astype(int)
df_t.to_csv("submission_svm_svd_0534.csv", index=False)
386/247:
df_t = df_test[['id']]
df_t['target'] = (y_predict_proba_svc_f >= 0.514).astype(int)
df_t.to_csv("submission_svm_svd_0534.csv", index=False)
386/248: df_t.describe()
386/249: grid_svd.best_params_
386/250: param_grid = {'C': [9, 10, 12, 13], 'gamma': [0.3, 0.1, 0.07],'kernel': ['rbf']}
386/251: svd
386/252: del svd
386/253: svd
386/254: svd
386/255: param_grid = {'C': [9, 10, 12, 13], 'gamma': [0.3, 0.1, 0.07],'kernel': ['rbf']}
386/256:  #1 / (1000 * X_train_svd.var())
386/257:
grid_svd = GridSearchCV(SVC(probability=True),param_grid,cv=StratifiedKFold(),verbose=2,scoring='f1')
grid_svd.fit(X_train_svd,y_train)
386/258: grid_svd.best_params_
386/259: svd_svc = grid_svd.best_estimator_
386/260: grid_svd.best_score_
386/261:
y_pred_svc_svd = svd_svc.predict(X_test_svd)
y_predict_proba_svc_svd = svd_svc.predict_proba(X_test_svd)[:,1]
print(f1_score(y_test, y_pred_svc))
386/262: best_tresholds(y_predict_proba_svc_svd, y_test)
386/263: y_predict_proba_svc_svd_f = svd_svc.predict_proba(X_test_svd_f)[:,1]
386/264:
df_t = df_test[['id']]
df_t['target'] = (y_predict_proba_svc_f >= 0.514).astype(int)
df_t.to_csv("submission_svm_svd_0514.csv", index=False)
386/265: df_t.describe()
386/266: param_grid = {'C': [10, 15], 'gamma': [0.3, 0.1, 0.07],'kernel': ['rbf']}
386/267:  #1 / (1000 * X_train_svd.var())
386/268:
grid_svd = GridSearchCV(SVC(probability=True),param_grid,cv=StratifiedKFold(),verbose=2,scoring='f1')
grid_svd.fit(X_train_svd,y_train)
388/1:
import pandas as pd
import numpy as np
import re
import copy
from collections import Counter
import string
from scipy.sparse import hstack
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline
388/2: pd.set_option('max_colwidth', None)
388/3:
import matplotlib.pyplot as plt
import seaborn as sns
388/4:
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.tokenize.treebank import TreebankWordDetokenizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer

import emoji as emj
import spacy 
## It's important to use _lg for OOV in future
nlp = spacy.load('en_core_web_lg')
388/5:
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report, roc_curve
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
from sklearn.metrics import f1_score, accuracy_score
388/6:
import pickle
from joblib import dump
388/7:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
388/8:
nltk.download()
STOPWORDS = set(stopwords.words('english'))
388/9: wnl = WordNetLemmatizer()
388/10: df_train.head()
388/11: df_train.isnull().mean()
388/12: df_train.shape, df_test.shape
388/13:
#cardinality check
for col in df_train.columns:
    print('{} has {} unique instances'.format(col, len(df_train[col].unique())))
388/14: df_train.info()
388/15: df_train.keyword.value_counts()
388/16:
plt.figure(figsize=[8, 40])
sns.countplot(y='keyword',
              data=df_train,
              color='lightblue',
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
388/17:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword', 
              hue='target', 
              data=df_train, 
              palette=['lightblue', 'red']
             )
plt.xticks(rotation=90)
plt.legend(loc='upper right')
plt.title("Distribution of Target per Keyword ")
plt.show()
388/18: df_train.keyword = df_train.keyword.str.replace('%20', ' ')
388/19:
# Let's show most common keywords for target = 1
df_train[df_train.target == 1].keyword.value_counts()[:10]
388/20:
# Let's show most common keywords for target = 1
df_train[df_train.target == 0].keyword.value_counts()[:10]
388/21:
df_train_group = df_train.groupby(['keyword', 'target'])['id'].count().unstack(fill_value=0).reset_index()
df_train_group.columns = ['keyword', 'target_0', 'target_1']
df_train_group['prob_real_disasters'] = df_train_group.target_1/(df_train_group.target_0 + df_train_group.target_1)
388/22: df_train_group.sort_values('prob_real_disasters', ascending=False)
388/23: df_train.keyword.nunique()
388/24: df_train.target.value_counts()/len(df_train)
388/25: ## This data set is unbalanced, so we should remember about it later
388/26: df_train.location.nunique()
388/27: df_location = df_train.loc[:, ['id', 'target', 'location', 'text']]
388/28: df_location.location.fillna('no_location', inplace=True)
388/29:
# Write a function to display basic entity info:
def show_ents(doc):
    doc = nlp(doc)
    count_GPE = 0
    if doc.ents:
        for ent in doc.ents:
            if ent.label_ == 'GPE':
                #print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
                count_GPE = 1
                break
    return count_GPE
388/30: df_location['GPE_loc_flag'] = df_location.location.apply(show_ents)
388/31: df_location['GPE_text_flag'] = df_location.text.apply(show_ents)
388/32: df_location['GPE_flag'] = df_location.GPE_loc_flag | df_location.GPE_text_flag
388/33:
f, axes = plt.subplots(1, 3, figsize=(20,6))
sns.countplot(x='GPE_loc_flag', hue='target', data=df_location, ax=axes[0])
sns.countplot(x='GPE_text_flag', hue='target', data=df_location, ax=axes[1])
sns.countplot(x='GPE_flag', hue='target', data=df_location, ax=axes[2])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')
axes[2].set_title('GPE')

#plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
axes[0].legend(labels=['0', '1'])
axes[1].legend(labels=['0', '1'])
axes[2].legend(labels=['0', '1'])

plt.show()
388/34:
# Looks like the location extracted from the tweet will be more informative than the original location feature.
df_location.iloc[:, -4:].corr()
388/35: df_punct_fe = df_train.loc[:, ['id', 'target', 'text']]
388/36:
#string.punctuation = '!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
def count_punctuations(text):
    d=dict()
    for i in string.punctuation:
        d[str(i)+' count']=text.count(i)
    return d
388/37:
df_punct_fe['punct_count'] = df_punct_fe["text"].apply(lambda x:count_punctuations(x))
df_punct= pd.DataFrame(list(df_punct_fe.punct_count))
merge_df = pd.merge(df_punct_fe.target,df_punct,left_index=True, right_index=True)
388/38:
merge_df_gr = merge_df.groupby('target').sum().reset_index()
merge_df_gr = merge_df_gr.T
merge_df_gr.columns = ['target_0', 'target_1']
388/39: merge_df_gr
388/40: merge_df_gr['sum_0_1'] = merge_df_gr['target_0'] + merge_df_gr['target_1']
388/41: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
388/42: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
388/43:
def punct_add_exclamation(text):
    return len(re.findall("!", text))
388/44:
def punct_add_question(text):
    return len(re.findall("\?", text))
388/45:
def punct_add_quotation(text):
    return len(re.findall("'", text))
388/46:
df_punct_fe['count_exclamation'] = df_punct_fe.text.apply(punct_add_exclamation)
df_punct_fe['count_question'] = df_punct_fe.text.apply(punct_add_question)
df_punct_fe['count_quotation'] = df_punct_fe.text.apply(punct_add_quotation)
388/47: new_cols = ['count_exclamation', 'count_question', 'count_quotation']
388/48: df_punct_fe.head(2)
388/49:
# count number of characters 
def count_chars(text):
    return len(text)

# count number of words 
def count_words(text):
    return len(text.split())

# count number of capital characters
def count_capital_chars(text):
    count=0
    for i in text:
        if i.isupper():
            count+=1
    return count

# count number of capital words
def count_capital_words(text):
    return sum(map(str.isupper,text.split()))

# count number of punctuations
def count_punctuations(text):
    count = lambda l1,l2: sum([1 for x in l1 if x in l2])
    return count(text,set(string.punctuation))

# count number of words in quotes
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    string_wothout_contractions = string_wothout_contractions.replace('"', '\'')
    x = re.findall("\'(.+?)\'", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
    
# count number of sentences
def count_sent(text):
    return len(nltk.sent_tokenize(text))

# count number of unique words 
def count_unique_words(text):
    return len(set(text.split()))
    
# count of hashtags
def count_htags(text):
    x = re.findall(r'(\#\w[A-Za-z0-9]*)', text)
    return len(x)

# count of mentions
def count_mentions(text):
    x = re.findall(r'(\@\w[A-Za-z0-9]*)', text)
    return len(x)

# count of stopwords
def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)

#count of links
def count_links(text):
    link_list = re.findall(r'(https?://[^\s]+)', text)
    bitly_list = re.findall(r'bit.ly/\S+', text)
    return (len(link_list + bitly_list))

#count digits
def count_digits(text):
    digit_list = re.findall(r'[0-9]+', text)
    return len(digit_list)

# Factorize repeated punctuation, add REPEAT
def count_repeat_punct(text):
    rep_list = re.findall(r'([!?.]){2,}', text)
    return len(rep_list)
388/50:
def feature_eng_fun(df_in, text):
    df = copy.deepcopy(df_in)
    df['count_exclamation'] = df_train[text].apply(punct_add_exclamation)
    df['count_question'] = df[text].apply(punct_add_question)
    df['count_quotation'] = df[text].apply(punct_add_quotation)
    df['count_num_of_chars'] = df[text].apply(count_chars)
    df['count_num_of_words'] = df[text].apply(count_words)
    df['count_num_of_capital_chars'] = df[text].apply(count_capital_chars)
    df['count_num_of_capital_words'] = df[text].apply(count_capital_words)
    df['count_punct'] = df[text].apply(count_punctuations)
    df['count_words_in_quotes'] = df[text].apply(count_words_in_quotes)
    df['count_sent'] = df[text].apply(count_sent)
    df['count_unique_words'] = df[text].apply(count_unique_words)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_mentions'] = df[text].apply(count_mentions)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_stopwords'] = df[text].apply(count_stopwords)
    df['count_links'] = df[text].apply(count_links) 
    df['count_repeat_punct'] = df[text].apply(count_repeat_punct)    
    ## This function from part above
    df['GPE_text_flag'] = df[text].apply(show_ents)    
    return df
388/51:
## .loc will create a new copy, not view
df_feature_eng = df_train.loc[:, ('id', 'text', 'target')]
388/52: df_feature_eng = feature_eng_fun(df_feature_eng, 'text')
388/53: df_feature_eng.drop(['text'], axis=1, inplace=True)
388/54:
plt.figure(figsize=(14,10))
sns.heatmap(df_feature_eng.corr(),annot=True)
388/55: cols_selected = np.abs(df_feature_eng.corr()['target']).sort_values(ascending=False)[1:11].index.to_list()
388/56: df_feature_eng[cols_selected].head(3)
388/57:
# Let's look at some tweets more carefully
df_train[55:60]
388/58:
abbreviations = {
    "$" : " dollar ",
    "‚Ç¨" : " euro ",
    "4ao" : "for adults only",
    "a.m" : "before midday",
    "a3" : "anytime anywhere anyplace",
    "aamof" : "as a matter of fact",
    "acct" : "account",
    "adih" : "another day in hell",
    "afaic" : "as far as i am concerned",
    "afaict" : "as far as i can tell",
    "afaik" : "as far as i know",
    "afair" : "as far as i remember",
    "afk" : "away from keyboard",
    "app" : "application",
    "approx" : "approximately",
    "apps" : "applications",
    "asap" : "as soon as possible",
    "asl" : "age, sex, location",
    "atk" : "at the keyboard",
    "ave." : "avenue",
    "aymm" : "are you my mother",
    "ayor" : "at your own risk", 
    "b&b" : "bed and breakfast",
    "b+b" : "bed and breakfast",
    "b.c" : "before christ",
    "b2b" : "business to business",
    "b2c" : "business to customer",
    "b4" : "before",
    "b4n" : "bye for now",
    "b@u" : "back at you",
    "bae" : "before anyone else",
    "bak" : "back at keyboard",
    "bbbg" : "bye bye be good",
    "bbc" : "british broadcasting corporation",
    "bbias" : "be back in a second",
    "bbl" : "be back later",
    "bbs" : "be back soon",
    "be4" : "before",
    "bfn" : "bye for now",
    "blvd" : "boulevard",
    "bout" : "about",
    "brb" : "be right back",
    "bros" : "brothers",
    "brt" : "be right there",
    "bsaaw" : "big smile and a wink",
    "btw" : "by the way",
    "bwl" : "bursting with laughter",
    "c/o" : "care of",
    "cet" : "central european time",
    "cf" : "compare",
    "cia" : "central intelligence agency",
    "csl" : "can not stop laughing",
    "cu" : "see you",
    "cul8r" : "see you later",
    "cv" : "curriculum vitae",
    "cwot" : "complete waste of time",
    "cya" : "see you",
    "cyt" : "see you tomorrow",
    "dae" : "does anyone else",
    "dbmib" : "do not bother me i am busy",
    "diy" : "do it yourself",
    "dm" : "direct message",
    "dwh" : "during work hours",
    "e123" : "easy as one two three",
    "eet" : "eastern european time",
    "eg" : "example",
    "embm" : "early morning business meeting",
    "encl" : "enclosed",
    "encl." : "enclosed",
    "etc" : "and so on",
    "faq" : "frequently asked questions",
    "fawc" : "for anyone who cares",
    "fb" : "facebook",
    "fc" : "fingers crossed",
    "fig" : "figure",
    "fimh" : "forever in my heart", 
    "ft." : "feet",
    "ft" : "featuring",
    "ftl" : "for the loss",
    "ftw" : "for the win",
    "fwiw" : "for what it is worth",
    "fyi" : "for your information",
    "g9" : "genius",
    "gahoy" : "get a hold of yourself",
    "gal" : "get a life",
    "gcse" : "general certificate of secondary education",
    "gfn" : "gone for now",
    "gg" : "good game",
    "gl" : "good luck",
    "glhf" : "good luck have fun",
    "gmt" : "greenwich mean time",
    "gmta" : "great minds think alike",
    "gn" : "good night",
    "g.o.a.t" : "greatest of all time",
    "goat" : "greatest of all time",
    "goi" : "get over it",
    "gps" : "global positioning system",
    "gr8" : "great",
    "gratz" : "congratulations",
    "gyal" : "girl",
    "h&c" : "hot and cold",
    "hp" : "horsepower",
    "hr" : "hour",
    "hrh" : "his royal highness",
    "ht" : "height",
    "ibrb" : "i will be right back",
    "ic" : "i see",
    "icq" : "i seek you",
    "icymi" : "in case you missed it",
    "idc" : "i do not care",
    "idgadf" : "i do not give a damn fuck",
    "idgaf" : "i do not give a fuck",
    "idk" : "i do not know",
    "ie" : "that is",
    "i.e" : "that is",
    "ifyp" : "i feel your pain",
    "IG" : "instagram",
    "iirc" : "if i remember correctly",
    "ilu" : "i love you",
    "ily" : "i love you",
    "imho" : "in my humble opinion",
    "imo" : "in my opinion",
    "imu" : "i miss you",
    "iow" : "in other words",
    "irl" : "in real life",
    "j4f" : "just for fun",
    "jic" : "just in case",
    "jk" : "just kidding",
    "jsyk" : "just so you know",
    "l8r" : "later",
    "lb" : "pound",
    "lbs" : "pounds",
    "ldr" : "long distance relationship",
    "lmao" : "laugh my ass off",
    "lmfao" : "laugh my fucking ass off",
    "lol" : "laughing out loud",
    "ltd" : "limited",
    "ltns" : "long time no see",
    "m8" : "mate",
    "mf" : "motherfucker",
    "mfs" : "motherfuckers",
    "mfw" : "my face when",
    "mofo" : "motherfucker",
    "mph" : "miles per hour",
    "mr" : "mister",
    "mrw" : "my reaction when",
    "ms" : "miss",
    "mte" : "my thoughts exactly",
    "nagi" : "not a good idea",
    "nbc" : "national broadcasting company",
    "nbd" : "not big deal",
    "nfs" : "not for sale",
    "ngl" : "not going to lie",
    "nhs" : "national health service",
    "nrn" : "no reply necessary",
    "nsfl" : "not safe for life",
    "nsfw" : "not safe for work",
    "nth" : "nice to have",
    "nvr" : "never",
    "nyc" : "new york city",
    "oc" : "original content",
    "og" : "original",
    "ohp" : "overhead projector",
    "oic" : "oh i see",
    "omdb" : "over my dead body",
    "omg" : "oh my god",
    "omw" : "on my way",
    "p.a" : "per annum",
    "p.m" : "after midday",
    "pm" : "prime minister",
    "poc" : "people of color",
    "pov" : "point of view",
    "pp" : "pages",
    "ppl" : "people",
    "prw" : "parents are watching",
    "ps" : "postscript",
    "pt" : "point",
    "ptb" : "please text back",
    "pto" : "please turn over",
    "qpsa" : "what happens", #"que pasa",
    "ratchet" : "rude",
    "rbtl" : "read between the lines",
    "rlrt" : "real life retweet", 
    "rofl" : "rolling on the floor laughing",
    "roflol" : "rolling on the floor laughing out loud",
    "rotflmao" : "rolling on the floor laughing my ass off",
    "rt" : "retweet",
    "ruok" : "are you ok",
    "sfw" : "safe for work",
    "sk8" : "skate",
    "smh" : "shake my head",
    "sq" : "square",
    "srsly" : "seriously", 
    "ssdd" : "same stuff different day",
    "tbh" : "to be honest",
    "tbs" : "tablespooful",
    "tbsp" : "tablespooful",
    "tfw" : "that feeling when",
    "thks" : "thank you",
    "tho" : "though",
    "thx" : "thank you",
    "tia" : "thanks in advance",
    "til" : "today i learned",
    "tl;dr" : "too long i did not read",
    "tldr" : "too long i did not read",
    "tmb" : "tweet me back",
    "tntl" : "trying not to laugh",
    "ttyl" : "talk to you later",
    "u" : "you",
    "u2" : "you too",
    "u4e" : "yours for ever",
    "utc" : "coordinated universal time",
    "w/" : "with",
    "w/o" : "without",
    "w8" : "wait",
    "wassup" : "what is up",
    "wb" : "welcome back",
    "wtf" : "what the fuck",
    "wtg" : "way to go",
    "wtpa" : "where the party at",
    "wuf" : "where are you from",
    "wuzup" : "what is up",
    "wywh" : "wish you were here",
    "yd" : "yard",
    "ygtr" : "you got that right",
    "ynk" : "you never know",
    "zzz" : "sleeping bored and tired"
}
388/59:
#remove not ascii
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    return text

# Change an abbreviation by its true meaning
def word_abbrev(word):
    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word

# Replace all abbreviations
def replace_abbrev(text):
    string = ""
    for word in text.split():
        string += word_abbrev(word) + " "        
    return string

# Remove @ and mention, replace by USER
def replace_mention(text):
    pat=re.compile(r'@\S+')
    return pat.sub(r'USER',text)

# Remove numbers, replace it by NUMBER
def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r' NUMBER ', text)

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()

#contractions and remowing stopwords and very short words
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 3:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
388/60:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(string_contractions)
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(clean_text)
    return df
388/61: df_text = df_train.loc[:, ['id', 'target', 'keyword', 'text']]
388/62: df_text = basic_cleaning(df_text, 'text')
388/63: df_text.head(3)
388/64: all_text = ' '.join(df_text['text'].tolist())
388/65:
oov_list = []
def search_oov(text):
    tokens = nlp(text)
    for token in tokens:
        if token.is_oov:
            oov_list.append(token)
388/66: search_oov(all_text)
388/67: oov_list_strings  = [i.text for i in oov_list]
388/68:
counter = Counter(oov_list_strings)
counter.most_common(10)
388/69: # I don't see a critical oov words, so I won't replace anything
388/70: sid = SentimentIntensityAnalyzer()
388/71: df_sentiment = df_train.loc[:, ['id', 'target', 'text']]
388/72: df_sentiment['sentiment_compound']  = df_sentiment.text.apply(lambda tweet: sid.polarity_scores(tweet)['compound'])
388/73: df_sentiment['sentiment_compound_score'] = df_sentiment['sentiment_compound'].apply(lambda c: 'pos' if c >=0 else 'neg')
388/74:
plt.figure(figsize=[4, 4])
sns.countplot(x='target', hue='sentiment_compound_score', data=df_sentiment)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
388/75: #not very optimistic result, I won't add a sentiment feature :)
388/76: ## CREATE A MODEL BASED ON TEXT
388/77: ### with eng features
388/78: df_copy = copy.deepcopy(df_train)
388/79:
cols_selected = ['GPE_text_flag', 'count_links', 'count_num_of_chars', 'count_punct', 'count_sent',
                 'count_mentions', 'count_stopwords', 'count_question', 'count_quotation',
                 'count_num_of_capital_chars']

df_copy = copy.deepcopy(df_train)
df_train_text_clean = basic_cleaning(df_copy, 'text')
df_train_text_clean["clean_text"] = df_train_text_clean.keyword.map(str) + " " + df_train_text_clean.text

## new features
df_train_feature_eng = feature_eng_fun(df_copy, 'text')
df_train_feature_eng = df_train_feature_eng[cols_selected]


#### for test
df_test_text_clean_f = basic_cleaning(df_test, 'text')
df_test_text_clean_f["clean_text"] = df_test_text_clean_f.keyword.map(str) + " " + df_test_text_clean_f.text

df_test_feature_eng_f = feature_eng_fun(df_test, 'text')
df_test_feature_eng_f = df_test_feature_eng_f[cols_selected]
388/80: text_column = 'all_text'
388/81:
## ADD LEMMATIZATION
w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
def lemmatize_text(text):
    return [wnl.lemmatize(w) for w in w_tokenizer.tokenize(text)]

df_train_text_clean['text_lemmatized'] = df_train_text_clean.clean_text.apply(lemmatize_text)
df_train_text_clean['all_text'] = df_train_text_clean['text_lemmatized'].apply(lambda x: " ".join(x))

df_test_text_clean_f['text_lemmatized'] = df_test_text_clean_f.clean_text.apply(lemmatize_text)
df_test_text_clean_f['all_text'] = df_test_text_clean_f['text_lemmatized'].apply(lambda x: " ".join(x))
388/82:
df_train_full = pd.merge(df_train_text_clean[['target', text_column]], df_train_feature_eng, 
                         left_index=True, right_index=True)

df_test_full = pd.merge(df_test_text_clean_f[[text_column]], df_test_feature_eng_f, 
                         left_index=True, right_index=True)
388/83: df_train_full
388/84:
## split to test and train
y = df_train_full['target']
X = df_train_full.drop('target', axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
print('X_train shape', X_train.shape)
print('X_test shape', X_test.shape)
388/85: y_train
388/86: sum(y_train)
388/87: sum(y_train)/len(y_train)
388/88: sum(y_test)/len(y_test)
388/89:
## split to test and train
y = df_train_full['target']
X = df_train_full.drop('target', axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)
print('X_train shape', X_train.shape)
print('X_test shape', X_test.shape)
388/90: sum(y_train)/len(y_train)
388/91: sum(y_test)/len(y_test)
388/92:
## split to test and train
y = df_train_full['target']
X = df_train_full.drop('target', axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, shuffle=True)
print('X_train shape', X_train.shape)
print('X_test shape', X_test.shape)
388/93: sum(y_train)/len(y_train)
388/94: sum(y_test)/len(y_test)
388/95:
max_features=15000

# CountVectorizer
count_vectorizer = CountVectorizer(max_features=max_features)

def count_vector(data):
    count_vectorizer = CountVectorizer()
    vect = count_vectorizer.fit_transform(data)
    return vect, count_vectorizer

def tfidf_vector(data):
    tfidf_vectorizer = TfidfVectorizer()
    vect = tfidf_vectorizer.fit_transform(data)
    return vect, tfidf_vectorizer

def min_max(data):
    min_max_scaler = MinMaxScaler()
    vect = min_max_scaler.fit_transform(data)
    return vect, min_max_scaler

# TRAIN
X_train_count, count_vectorizer = count_vector(X_train[text_column])
X_train_tfidf, tfidf_vectorizer = tfidf_vector(X_train[text_column])

X_train_eng_minmax, minmax_scaler = min_max(X_train[cols_selected])
X_train_eng = X_train[cols_selected].to_numpy()

#### TEST
X_test_count = count_vectorizer.transform(X_test [text_column])                                                     
X_test_tfidf = tfidf_vectorizer.transform(X_test [text_column])

X_test_eng_minmax = minmax_scaler.transform(X_test[cols_selected])
X_test_eng = X_test[cols_selected].to_numpy()

### VALIDATION DATASET
X_test_count_f = count_vectorizer.transform(df_test_full[text_column])                                                     
X_test_tfidf_f = tfidf_vectorizer.transform(df_test_full[text_column])

X_test_eng_minmax_f = minmax_scaler.transform(df_test_full[cols_selected])
X_test_eng_f = df_test_full[cols_selected].to_numpy()


### CONCAT tf-idf + minmax, count_vec + feature_eng

X_train_tfidf_minmax = hstack([X_train_tfidf, X_train_eng_minmax])
X_train_count_eng = hstack([X_train_count, X_train_eng])


X_test_tfidf_minmax = hstack([X_test_tfidf, X_test_eng_minmax])
X_test_count_eng = hstack([X_test_count, X_test_eng])

### FOR VALIDATION DATASET
X_test_tfidf_minmax_f = hstack([X_test_tfidf_f, X_test_eng_minmax_f])
X_test_count_eng_f = hstack([X_test_count_f, X_test_eng_f])
388/96:
#create a df for saving results of each model
metrics = pd.DataFrame(columns=['model' ,'vectoriser', 'f1 score', 'best_f1', 'threshold', 
                                'train accuracy','test accuracy'])
388/97:
def fit_and_predict(model, x_train ,x_test, y_train, y_test, vectoriser):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)

    y_pred = classifier.predict(x_test)
    cmatrix = confusion_matrix(y_test,y_pred)
    
    y_hat = classifier.predict_proba(x_test)[:,1]
    threshold, best_score = best_tresholds(y_hat, y_test)


    f1score = f1_score(y_test,y_pred)
    train_accuracy = round(classifier.score(x_train,y_train)*100)
    test_accuracy =  round(accuracy_score(y_test,y_pred)*100)

    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'f1 score': f1score, 
                              'best_f1': best_score,
                              'threshold': threshold,
                              'train accuracy': train_accuracy, 
                              'test accuracy': test_accuracy, 
                              'vectoriser': str(vectoriser),
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using "+ str(vectoriser))
    print('f1 score for test: {}'.format(f1score))
    print(classification_report(y_test,y_pred))    
    print('Threshold={:.5f}, F-Score={:.5f}'.format(threshold, best_score))
    print('Accuracy of classifier on training set:{}%'.format(train_accuracy))
    print('Accuracy of classifier on test set:{}%' .format(test_accuracy))
388/98:
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
388/99:
models=[
        LogisticRegression(random_state=30),
        SVC(random_state=30, probability=True),
        MultinomialNB(),
        DecisionTreeClassifier(random_state = 30),
        KNeighborsClassifier(),
        RandomForestClassifier(random_state=30, max_depth=4),
       ]
388/100:
#functions for best threshold searching
def to_labels(pos_probs, threshold):
    return (pos_probs >= threshold).astype('int')

def best_tresholds(y_pred_proba, y_test):
    # define thresholds
    thresholds = np.arange(0, 1, 0.001)
    # evaluate each threshold
    scores = [f1_score(y_test, to_labels(y_pred_proba, t)) for t in thresholds]
    # get best threshold
    ix = np.argmax(scores)
    #print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
    return thresholds[ix], scores[ix]
388/101:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train_count, X_test_count, y_train ,y_test, 'Count vector')
    
    fit_and_predict(model, X_train_tfidf, X_test_tfidf, y_train, y_test, 'Tfidf vector')
    
    ## count with EF
    fit_and_predict(model, X_train_count_eng, X_test_count_eng, y_train, y_test, 'Count vector + FE')
    
    ## TF IDF with EF
    fit_and_predict(model, X_train_tfidf_minmax, X_test_tfidf_minmax, y_train, y_test, 'TF-IDF + FE')
388/102: metrics.sort_values('best_f1')
388/103: ### LogReg and SVC show a good result. I want to tune the hyperparameters for the SVC to improve the quality of the classification.
388/104:
param_grid = {'C': [1, 5, 10, 15], 
              'gamma': [0.3, 0.1, 0.07],
              'kernel': ['linear', 'rbf', 'poly', 'sigmoid']}
388/105:
from sklearn.model_selection import StratifiedKFold
StratifiedKFold(n_splits=3, random_state=None, shuffle=False)
388/106:
import pandas as pd
import numpy as np
import re
import copy
from collections import Counter
import string
import sys
from scipy.sparse import hstack
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline
388/107:
### CREATE A FILE WITH THE RESULTS OF EACH MODEL IN GRIDSEARCHCV
old_stdout = sys.stdout
log_file = open("SVC_count_vect.log","w")
sys.stdout = log_file
388/108:
grid = GridSearchCV(SVC(probability=True),param_grid,cv=StratifiedKFold(),verbose=10,scoring='f1')
grid.fit(X_train_count,y_train)
388/109:
sys.stdout = old_stdout
log_file.close()
388/110: grid.best_score_
388/111:
param_grid = {'C': [15, 17, 21], 
              'gamma': [0.07, 0.05],
              'kernel': ['rbf']}
388/112:
from sklearn.model_selection import StratifiedKFold
StratifiedKFold(n_splits=3, random_state=None, shuffle=False)
388/113:
### CREATE A FILE WITH THE RESULTS OF EACH MODEL IN GRIDSEARCHCV
old_stdout = sys.stdout
log_file = open("SVC_count_vect_2.log","w")
sys.stdout = log_file
388/114:
grid = GridSearchCV(SVC(probability=True),param_grid,cv=StratifiedKFold(),verbose=10,scoring='f1')
grid.fit(X_train_count,y_train)
388/115:
sys.stdout = old_stdout
log_file.close()
388/116: grid.best_score_
388/117:
param_grid = {'C': [1, 5, 10, 15], 
              'gamma': [0.3, 0.1, 0.07],
              'kernel': ['rbf', 'poly', 'sigmoid']}
388/118:
from sklearn.model_selection import StratifiedKFold
StratifiedKFold(n_splits=3, random_state=None, shuffle=False)
388/119:
### CREATE A FILE WITH THE RESULTS OF EACH MODEL IN GRIDSEARCHCV
old_stdout = sys.stdout
log_file = open("SVC_tfidf_vect_1.log","w")
sys.stdout = log_file
388/120:
grid = GridSearchCV(SVC(probability=True),param_grid,cv=StratifiedKFold(),verbose=10,scoring='f1')
grid.fit(X_train_count,y_train)
388/121:
param_grid = {'C': [1, 5, 10, 15], 
              'gamma': [0.3, 0.1, 0.07],
              'kernel': ['rbf', 'poly', 'sigmoid']}
388/122:
from sklearn.model_selection import StratifiedKFold
StratifiedKFold(n_splits=3, random_state=None, shuffle=False)
388/123:
### CREATE A FILE WITH THE RESULTS OF EACH MODEL IN GRIDSEARCHCV
old_stdout = sys.stdout
log_file = open("SVC_tfidf_vect_1.log","w")
sys.stdout = log_file
388/124:
grid = GridSearchCV(SVC(probability=True),param_grid,cv=StratifiedKFold(),verbose=10,scoring='f1')
grid.fit(X_train_tfidf_minmax,y_train)
388/125:
sys.stdout = old_stdout
log_file.close()
388/126: grid.best_score_
388/127: grid.best_estimator_
388/128:
param_grid = {'C': [13, 15, 17], 
              'gamma': [0.1, 0.07, 0.05, .01],
              'kernel': ['rbf']}
388/129:
from sklearn.model_selection import StratifiedKFold
StratifiedKFold(n_splits=3, random_state=None, shuffle=False)
388/130:
### CREATE A FILE WITH THE RESULTS OF EACH MODEL IN GRIDSEARCHCV
old_stdout = sys.stdout
log_file = open("SVC_tfidf_vect_2.log","w")
sys.stdout = log_file
388/131:
grid = GridSearchCV(SVC(probability=True),param_grid,cv=StratifiedKFold(),verbose=10,scoring='f1')
grid.fit(X_train_tfidf_minmax,y_train)
388/132:
sys.stdout = old_stdout
log_file.close()
388/133: grid.best_estimator_
388/134: grid.best_score_
388/135: grid.best_estimator_
388/136:
param_grid = {'C': [12, 13, 14, 15], 
              'gamma': [0.1, 0.07, 0.06, .005],
              'kernel': ['rbf']}
388/137:
from sklearn.model_selection import StratifiedKFold
StratifiedKFold(n_splits=3, random_state=None, shuffle=False)
388/138:
### CREATE A FILE WITH THE RESULTS OF EACH MODEL IN GRIDSEARCHCV
old_stdout = sys.stdout
log_file = open("SVC_tfidf_vect_3.log","w")
sys.stdout = log_file
388/139:
grid = GridSearchCV(SVC(probability=True),param_grid,cv=StratifiedKFold(),verbose=10,scoring='f1')
grid.fit(X_train_tfidf_minmax,y_train)
388/140:
sys.stdout = old_stdout
log_file.close()
388/141: grid.best_score_
388/142:
svd = TruncatedSVD(n_components = 500)
svd.fit(X_train_tfidf_minmax.todense(), y_train)
X_train_svd = svd.transform(X_train_tfidf_minmax)
X_test_svd = svd.transform(X_test_eng_minmax)
388/143: from sklearn.decomposition import TruncatedSVD
388/144:
svd = TruncatedSVD(n_components = 500)
svd.fit(X_train_tfidf_minmax.todense(), y_train)
X_train_svd = svd.transform(X_train_tfidf_minmax)
X_test_svd = svd.transform(X_test_eng_minmax)
388/145:
with open('TruncatedSVD_500.pickle', 'wb') as handle:
    pickle.dump(svd, handle, protocol=pickle.HIGHEST_PROTOCOL)
388/146:
svd = TruncatedSVD(n_components = 100)
svd.fit(X_train_tfidf_minmax.todense(), y_train)
#X_train_svd = svd.transform(X_train_tfidf_minmax)
#X_test_svd = svd.transform(X_test_eng_minmax)
388/147:
with open('TruncatedSVD_100.pickle', 'wb') as handle:
    pickle.dump(svd, handle, protocol=pickle.HIGHEST_PROTOCOL)
388/148: #In .log files are results of Gridsearchcv. I tried different hyperparameters and datasets and this score is the best one.
388/149:
print('Best hyperparameters: ', grid.best_params_)
print('Best score: ', grid.best_score_)
388/150: grid.best_score_
388/151: grid.best_params_
388/152:
print('Best Score: %s' % result.best_score_)
print('Best Hyperparameters: %s' % result.best_params_)
388/153:
print('Best Score: %s' % grid.best_score_)
print('Best Hyperparameters: %s' % grid.best_params_)
388/154:
sys.stdout = old_stdout
log_file.close()
388/155:
print('Best Score: %s' % grid.best_score_)
print('Best Hyperparameters: %s' % grid.best_params_)
389/1:
import pandas as pd
import numpy as np
import re
import copy
from collections import Counter
import string
import sys
from scipy.sparse import hstack
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline
389/2: pd.set_option('max_colwidth', None)
389/3:
import matplotlib.pyplot as plt
import seaborn as sns
389/4:
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.tokenize.treebank import TreebankWordDetokenizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer

import emoji as emj
import spacy 
## It's important to use _lg for OOV in future
nlp = spacy.load('en_core_web_lg')
389/5:
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report, roc_curve
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
from sklearn.metrics import f1_score, accuracy_score
389/6:
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
389/7:
import pickle
from joblib import dump
389/8:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
389/9:
nltk.download()
STOPWORDS = set(stopwords.words('english'))
389/10: wnl = WordNetLemmatizer()
389/11: df_train.head()
389/12: df_train.isnull().mean()
389/13: df_train.shape, df_test.shape
389/14:
#cardinality check
for col in df_train.columns:
    print('{} has {} unique instances'.format(col, len(df_train[col].unique())))
389/15: df_train.info()
389/16: df_train.keyword.value_counts()
389/17:
plt.figure(figsize=[8, 40])
sns.countplot(y='keyword',
              data=df_train,
              color='lightblue',
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
389/18:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword', 
              hue='target', 
              data=df_train, 
              palette=['lightblue', 'red']
             )
plt.xticks(rotation=90)
plt.legend(loc='upper right')
plt.title("Distribution of Target per Keyword ")
plt.show()
389/19: df_train.keyword = df_train.keyword.str.replace('%20', ' ')
389/20:
# Let's show most common keywords for target = 1
df_train[df_train.target == 1].keyword.value_counts()[:10]
389/21:
# Let's show most common keywords for target = 1
df_train[df_train.target == 0].keyword.value_counts()[:10]
389/22:
df_train_group = df_train.groupby(['keyword', 'target'])['id'].count().unstack(fill_value=0).reset_index()
df_train_group.columns = ['keyword', 'target_0', 'target_1']
df_train_group['prob_real_disasters'] = df_train_group.target_1/(df_train_group.target_0 + df_train_group.target_1)
389/23: df_train_group.sort_values('prob_real_disasters', ascending=False)
389/24: df_train.keyword.nunique()
389/25: df_train.target.value_counts()/len(df_train)
389/26: ## This data set is unbalanced, so we should remember about it later
389/27: df_train.location.nunique()
389/28: df_location = df_train.loc[:, ['id', 'target', 'location', 'text']]
389/29: df_location.location.fillna('no_location', inplace=True)
389/30:
# Write a function to display basic entity info:
def show_ents(doc):
    doc = nlp(doc)
    count_GPE = 0
    if doc.ents:
        for ent in doc.ents:
            if ent.label_ == 'GPE':
                #print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
                count_GPE = 1
                break
    return count_GPE
389/31: df_location['GPE_loc_flag'] = df_location.location.apply(show_ents)
389/32: df_location['GPE_text_flag'] = df_location.text.apply(show_ents)
389/33: df_location['GPE_flag'] = df_location.GPE_loc_flag | df_location.GPE_text_flag
389/34:
f, axes = plt.subplots(1, 3, figsize=(20,6))
sns.countplot(x='GPE_loc_flag', hue='target', data=df_location, ax=axes[0])
sns.countplot(x='GPE_text_flag', hue='target', data=df_location, ax=axes[1])
sns.countplot(x='GPE_flag', hue='target', data=df_location, ax=axes[2])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')
axes[2].set_title('GPE')

#plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
axes[0].legend(labels=['0', '1'])
axes[1].legend(labels=['0', '1'])
axes[2].legend(labels=['0', '1'])

plt.show()
389/35:
# Looks like the location extracted from the tweet will be more informative than the original location feature.
df_location.iloc[:, -4:].corr()
389/36: df_punct_fe = df_train.loc[:, ['id', 'target', 'text']]
389/37:
#string.punctuation = '!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
def count_punctuations(text):
    d=dict()
    for i in string.punctuation:
        d[str(i)+' count']=text.count(i)
    return d
389/38:
df_punct_fe['punct_count'] = df_punct_fe["text"].apply(lambda x:count_punctuations(x))
df_punct= pd.DataFrame(list(df_punct_fe.punct_count))
merge_df = pd.merge(df_punct_fe.target,df_punct,left_index=True, right_index=True)
389/39:
merge_df_gr = merge_df.groupby('target').sum().reset_index()
merge_df_gr = merge_df_gr.T
merge_df_gr.columns = ['target_0', 'target_1']
389/40: merge_df_gr
389/41: merge_df_gr['sum_0_1'] = merge_df_gr['target_0'] + merge_df_gr['target_1']
389/42: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
389/43: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
389/44:
def punct_add_exclamation(text):
    return len(re.findall("!", text))
389/45:
def punct_add_question(text):
    return len(re.findall("\?", text))
389/46:
def punct_add_quotation(text):
    return len(re.findall("'", text))
389/47:
df_punct_fe['count_exclamation'] = df_punct_fe.text.apply(punct_add_exclamation)
df_punct_fe['count_question'] = df_punct_fe.text.apply(punct_add_question)
df_punct_fe['count_quotation'] = df_punct_fe.text.apply(punct_add_quotation)
389/48: new_cols = ['count_exclamation', 'count_question', 'count_quotation']
389/49: df_punct_fe.head(2)
389/50:
# count number of characters 
def count_chars(text):
    return len(text)

# count number of words 
def count_words(text):
    return len(text.split())

# count number of capital characters
def count_capital_chars(text):
    count=0
    for i in text:
        if i.isupper():
            count+=1
    return count

# count number of capital words
def count_capital_words(text):
    return sum(map(str.isupper,text.split()))

# count number of punctuations
def count_punctuations(text):
    count = lambda l1,l2: sum([1 for x in l1 if x in l2])
    return count(text,set(string.punctuation))

# count number of words in quotes
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    string_wothout_contractions = string_wothout_contractions.replace('"', '\'')
    x = re.findall("\'(.+?)\'", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
    
# count number of sentences
def count_sent(text):
    return len(nltk.sent_tokenize(text))

# count number of unique words 
def count_unique_words(text):
    return len(set(text.split()))
    
# count of hashtags
def count_htags(text):
    x = re.findall(r'(\#\w[A-Za-z0-9]*)', text)
    return len(x)

# count of mentions
def count_mentions(text):
    x = re.findall(r'(\@\w[A-Za-z0-9]*)', text)
    return len(x)

# count of stopwords
def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)

#count of links
def count_links(text):
    link_list = re.findall(r'(https?://[^\s]+)', text)
    bitly_list = re.findall(r'bit.ly/\S+', text)
    return (len(link_list + bitly_list))

#count digits
def count_digits(text):
    digit_list = re.findall(r'[0-9]+', text)
    return len(digit_list)

# Factorize repeated punctuation, add REPEAT
def count_repeat_punct(text):
    rep_list = re.findall(r'([!?.]){2,}', text)
    return len(rep_list)
389/51:
def feature_eng_fun(df_in, text):
    df = copy.deepcopy(df_in)
    df['count_exclamation'] = df_train[text].apply(punct_add_exclamation)
    df['count_question'] = df[text].apply(punct_add_question)
    df['count_quotation'] = df[text].apply(punct_add_quotation)
    df['count_num_of_chars'] = df[text].apply(count_chars)
    df['count_num_of_words'] = df[text].apply(count_words)
    df['count_num_of_capital_chars'] = df[text].apply(count_capital_chars)
    df['count_num_of_capital_words'] = df[text].apply(count_capital_words)
    df['count_punct'] = df[text].apply(count_punctuations)
    df['count_words_in_quotes'] = df[text].apply(count_words_in_quotes)
    df['count_sent'] = df[text].apply(count_sent)
    df['count_unique_words'] = df[text].apply(count_unique_words)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_mentions'] = df[text].apply(count_mentions)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_stopwords'] = df[text].apply(count_stopwords)
    df['count_links'] = df[text].apply(count_links) 
    df['count_repeat_punct'] = df[text].apply(count_repeat_punct)    
    ## This function from part above
    df['GPE_text_flag'] = df[text].apply(show_ents)    
    return df
389/52:
## .loc will create a new copy, not view
df_feature_eng = df_train.loc[:, ('id', 'text', 'target')]
389/53: df_feature_eng = feature_eng_fun(df_feature_eng, 'text')
389/54: df_feature_eng.drop(['text'], axis=1, inplace=True)
389/55:
plt.figure(figsize=(14,10))
sns.heatmap(df_feature_eng.corr(),annot=True)
389/56: cols_selected = np.abs(df_feature_eng.corr()['target']).sort_values(ascending=False)[1:11].index.to_list()
389/57: df_feature_eng[cols_selected].head(3)
389/58:
# Let's look at some tweets more carefully
df_train[55:60]
389/59:
abbreviations = {
    "$" : " dollar ",
    "‚Ç¨" : " euro ",
    "4ao" : "for adults only",
    "a.m" : "before midday",
    "a3" : "anytime anywhere anyplace",
    "aamof" : "as a matter of fact",
    "acct" : "account",
    "adih" : "another day in hell",
    "afaic" : "as far as i am concerned",
    "afaict" : "as far as i can tell",
    "afaik" : "as far as i know",
    "afair" : "as far as i remember",
    "afk" : "away from keyboard",
    "app" : "application",
    "approx" : "approximately",
    "apps" : "applications",
    "asap" : "as soon as possible",
    "asl" : "age, sex, location",
    "atk" : "at the keyboard",
    "ave." : "avenue",
    "aymm" : "are you my mother",
    "ayor" : "at your own risk", 
    "b&b" : "bed and breakfast",
    "b+b" : "bed and breakfast",
    "b.c" : "before christ",
    "b2b" : "business to business",
    "b2c" : "business to customer",
    "b4" : "before",
    "b4n" : "bye for now",
    "b@u" : "back at you",
    "bae" : "before anyone else",
    "bak" : "back at keyboard",
    "bbbg" : "bye bye be good",
    "bbc" : "british broadcasting corporation",
    "bbias" : "be back in a second",
    "bbl" : "be back later",
    "bbs" : "be back soon",
    "be4" : "before",
    "bfn" : "bye for now",
    "blvd" : "boulevard",
    "bout" : "about",
    "brb" : "be right back",
    "bros" : "brothers",
    "brt" : "be right there",
    "bsaaw" : "big smile and a wink",
    "btw" : "by the way",
    "bwl" : "bursting with laughter",
    "c/o" : "care of",
    "cet" : "central european time",
    "cf" : "compare",
    "cia" : "central intelligence agency",
    "csl" : "can not stop laughing",
    "cu" : "see you",
    "cul8r" : "see you later",
    "cv" : "curriculum vitae",
    "cwot" : "complete waste of time",
    "cya" : "see you",
    "cyt" : "see you tomorrow",
    "dae" : "does anyone else",
    "dbmib" : "do not bother me i am busy",
    "diy" : "do it yourself",
    "dm" : "direct message",
    "dwh" : "during work hours",
    "e123" : "easy as one two three",
    "eet" : "eastern european time",
    "eg" : "example",
    "embm" : "early morning business meeting",
    "encl" : "enclosed",
    "encl." : "enclosed",
    "etc" : "and so on",
    "faq" : "frequently asked questions",
    "fawc" : "for anyone who cares",
    "fb" : "facebook",
    "fc" : "fingers crossed",
    "fig" : "figure",
    "fimh" : "forever in my heart", 
    "ft." : "feet",
    "ft" : "featuring",
    "ftl" : "for the loss",
    "ftw" : "for the win",
    "fwiw" : "for what it is worth",
    "fyi" : "for your information",
    "g9" : "genius",
    "gahoy" : "get a hold of yourself",
    "gal" : "get a life",
    "gcse" : "general certificate of secondary education",
    "gfn" : "gone for now",
    "gg" : "good game",
    "gl" : "good luck",
    "glhf" : "good luck have fun",
    "gmt" : "greenwich mean time",
    "gmta" : "great minds think alike",
    "gn" : "good night",
    "g.o.a.t" : "greatest of all time",
    "goat" : "greatest of all time",
    "goi" : "get over it",
    "gps" : "global positioning system",
    "gr8" : "great",
    "gratz" : "congratulations",
    "gyal" : "girl",
    "h&c" : "hot and cold",
    "hp" : "horsepower",
    "hr" : "hour",
    "hrh" : "his royal highness",
    "ht" : "height",
    "ibrb" : "i will be right back",
    "ic" : "i see",
    "icq" : "i seek you",
    "icymi" : "in case you missed it",
    "idc" : "i do not care",
    "idgadf" : "i do not give a damn fuck",
    "idgaf" : "i do not give a fuck",
    "idk" : "i do not know",
    "ie" : "that is",
    "i.e" : "that is",
    "ifyp" : "i feel your pain",
    "IG" : "instagram",
    "iirc" : "if i remember correctly",
    "ilu" : "i love you",
    "ily" : "i love you",
    "imho" : "in my humble opinion",
    "imo" : "in my opinion",
    "imu" : "i miss you",
    "iow" : "in other words",
    "irl" : "in real life",
    "j4f" : "just for fun",
    "jic" : "just in case",
    "jk" : "just kidding",
    "jsyk" : "just so you know",
    "l8r" : "later",
    "lb" : "pound",
    "lbs" : "pounds",
    "ldr" : "long distance relationship",
    "lmao" : "laugh my ass off",
    "lmfao" : "laugh my fucking ass off",
    "lol" : "laughing out loud",
    "ltd" : "limited",
    "ltns" : "long time no see",
    "m8" : "mate",
    "mf" : "motherfucker",
    "mfs" : "motherfuckers",
    "mfw" : "my face when",
    "mofo" : "motherfucker",
    "mph" : "miles per hour",
    "mr" : "mister",
    "mrw" : "my reaction when",
    "ms" : "miss",
    "mte" : "my thoughts exactly",
    "nagi" : "not a good idea",
    "nbc" : "national broadcasting company",
    "nbd" : "not big deal",
    "nfs" : "not for sale",
    "ngl" : "not going to lie",
    "nhs" : "national health service",
    "nrn" : "no reply necessary",
    "nsfl" : "not safe for life",
    "nsfw" : "not safe for work",
    "nth" : "nice to have",
    "nvr" : "never",
    "nyc" : "new york city",
    "oc" : "original content",
    "og" : "original",
    "ohp" : "overhead projector",
    "oic" : "oh i see",
    "omdb" : "over my dead body",
    "omg" : "oh my god",
    "omw" : "on my way",
    "p.a" : "per annum",
    "p.m" : "after midday",
    "pm" : "prime minister",
    "poc" : "people of color",
    "pov" : "point of view",
    "pp" : "pages",
    "ppl" : "people",
    "prw" : "parents are watching",
    "ps" : "postscript",
    "pt" : "point",
    "ptb" : "please text back",
    "pto" : "please turn over",
    "qpsa" : "what happens", #"que pasa",
    "ratchet" : "rude",
    "rbtl" : "read between the lines",
    "rlrt" : "real life retweet", 
    "rofl" : "rolling on the floor laughing",
    "roflol" : "rolling on the floor laughing out loud",
    "rotflmao" : "rolling on the floor laughing my ass off",
    "rt" : "retweet",
    "ruok" : "are you ok",
    "sfw" : "safe for work",
    "sk8" : "skate",
    "smh" : "shake my head",
    "sq" : "square",
    "srsly" : "seriously", 
    "ssdd" : "same stuff different day",
    "tbh" : "to be honest",
    "tbs" : "tablespooful",
    "tbsp" : "tablespooful",
    "tfw" : "that feeling when",
    "thks" : "thank you",
    "tho" : "though",
    "thx" : "thank you",
    "tia" : "thanks in advance",
    "til" : "today i learned",
    "tl;dr" : "too long i did not read",
    "tldr" : "too long i did not read",
    "tmb" : "tweet me back",
    "tntl" : "trying not to laugh",
    "ttyl" : "talk to you later",
    "u" : "you",
    "u2" : "you too",
    "u4e" : "yours for ever",
    "utc" : "coordinated universal time",
    "w/" : "with",
    "w/o" : "without",
    "w8" : "wait",
    "wassup" : "what is up",
    "wb" : "welcome back",
    "wtf" : "what the fuck",
    "wtg" : "way to go",
    "wtpa" : "where the party at",
    "wuf" : "where are you from",
    "wuzup" : "what is up",
    "wywh" : "wish you were here",
    "yd" : "yard",
    "ygtr" : "you got that right",
    "ynk" : "you never know",
    "zzz" : "sleeping bored and tired"
}
389/60:
#remove not ascii
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    return text

# Change an abbreviation by its true meaning
def word_abbrev(word):
    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word

# Replace all abbreviations
def replace_abbrev(text):
    string = ""
    for word in text.split():
        string += word_abbrev(word) + " "        
    return string

# Remove @ and mention, replace by USER
def replace_mention(text):
    pat=re.compile(r'@\S+')
    return pat.sub(r'USER',text)

# Remove numbers, replace it by NUMBER
def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r' NUMBER ', text)

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()

#contractions and remowing stopwords and very short words
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 3:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
389/61:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(string_contractions)
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(clean_text)
    return df
389/62: df_text = df_train.loc[:, ['id', 'target', 'keyword', 'text']]
389/63: df_text = basic_cleaning(df_text, 'text')
389/64: df_text.head(3)
389/65: all_text = ' '.join(df_text['text'].tolist())
389/66:
oov_list = []
def search_oov(text):
    tokens = nlp(text)
    for token in tokens:
        if token.is_oov:
            oov_list.append(token)
389/67: search_oov(all_text)
389/68: oov_list_strings  = [i.text for i in oov_list]
389/69:
counter = Counter(oov_list_strings)
counter.most_common(10)
389/70: # I don't see a critical oov words, so I won't replace anything
389/71: sid = SentimentIntensityAnalyzer()
389/72: df_sentiment = df_train.loc[:, ['id', 'target', 'text']]
389/73: df_sentiment['sentiment_compound']  = df_sentiment.text.apply(lambda tweet: sid.polarity_scores(tweet)['compound'])
389/74: df_sentiment['sentiment_compound_score'] = df_sentiment['sentiment_compound'].apply(lambda c: 'pos' if c >=0 else 'neg')
389/75:
plt.figure(figsize=[4, 4])
sns.countplot(x='target', hue='sentiment_compound_score', data=df_sentiment)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
389/76: #not very optimistic result, I won't add a sentiment feature :)
389/77: ## CREATE A MODEL BASED ON TEXT
389/78: ### with eng features
389/79: df_copy = copy.deepcopy(df_train)
389/80:
cols_selected = ['GPE_text_flag', 'count_links', 'count_num_of_chars', 'count_punct', 'count_sent',
                 'count_mentions', 'count_stopwords', 'count_question', 'count_quotation',
                 'count_num_of_capital_chars']

df_copy = copy.deepcopy(df_train)
df_train_text_clean = basic_cleaning(df_copy, 'text')
df_train_text_clean["clean_text"] = df_train_text_clean.keyword.map(str) + " " + df_train_text_clean.text

## new features
df_train_feature_eng = feature_eng_fun(df_copy, 'text')
df_train_feature_eng = df_train_feature_eng[cols_selected]


#### for test
df_test_text_clean_f = basic_cleaning(df_test, 'text')
df_test_text_clean_f["clean_text"] = df_test_text_clean_f.keyword.map(str) + " " + df_test_text_clean_f.text

df_test_feature_eng_f = feature_eng_fun(df_test, 'text')
df_test_feature_eng_f = df_test_feature_eng_f[cols_selected]
389/81:
## ADD LEMMATIZATION
w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
def lemmatize_text(text):
    return [wnl.lemmatize(w) for w in w_tokenizer.tokenize(text)]

df_train_text_clean['text_lemmatized'] = df_train_text_clean.clean_text.apply(lemmatize_text)
df_train_text_clean['all_text'] = df_train_text_clean['text_lemmatized'].apply(lambda x: " ".join(x))

df_test_text_clean_f['text_lemmatized'] = df_test_text_clean_f.clean_text.apply(lemmatize_text)
df_test_text_clean_f['all_text'] = df_test_text_clean_f['text_lemmatized'].apply(lambda x: " ".join(x))
389/82: text_column = 'all_text'
389/83:
df_train_full = pd.merge(df_train_text_clean[['target', text_column]], df_train_feature_eng, 
                         left_index=True, right_index=True)

df_test_full = pd.merge(df_test_text_clean_f[[text_column]], df_test_feature_eng_f, 
                         left_index=True, right_index=True)
389/84:
## split to test and train
y = df_train_full['target']
X = df_train_full.drop('target', axis=1)
# This dataset is unbalanced so add stratify parameter for splitting
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, shuffle=True)
print('X_train shape', X_train.shape)
print('X_test shape', X_test.shape)
389/85:
max_features=15000

# CountVectorizer
count_vectorizer = CountVectorizer(max_features=max_features)

def count_vector(data):
    count_vectorizer = CountVectorizer()
    vect = count_vectorizer.fit_transform(data)
    return vect, count_vectorizer

def tfidf_vector(data):
    tfidf_vectorizer = TfidfVectorizer()
    vect = tfidf_vectorizer.fit_transform(data)
    return vect, tfidf_vectorizer

def min_max(data):
    min_max_scaler = MinMaxScaler()
    vect = min_max_scaler.fit_transform(data)
    return vect, min_max_scaler

# TRAIN
X_train_count, count_vectorizer = count_vector(X_train[text_column])
X_train_tfidf, tfidf_vectorizer = tfidf_vector(X_train[text_column])

X_train_eng_minmax, minmax_scaler = min_max(X_train[cols_selected])
X_train_eng = X_train[cols_selected].to_numpy()

#### TEST
X_test_count = count_vectorizer.transform(X_test [text_column])                                                     
X_test_tfidf = tfidf_vectorizer.transform(X_test [text_column])

X_test_eng_minmax = minmax_scaler.transform(X_test[cols_selected])
X_test_eng = X_test[cols_selected].to_numpy()

### VALIDATION DATASET
X_test_count_f = count_vectorizer.transform(df_test_full[text_column])                                                     
X_test_tfidf_f = tfidf_vectorizer.transform(df_test_full[text_column])

X_test_eng_minmax_f = minmax_scaler.transform(df_test_full[cols_selected])
X_test_eng_f = df_test_full[cols_selected].to_numpy()


### CONCAT tf-idf + minmax, count_vec + feature_eng

X_train_tfidf_minmax = hstack([X_train_tfidf, X_train_eng_minmax])
X_train_count_eng = hstack([X_train_count, X_train_eng])


X_test_tfidf_minmax = hstack([X_test_tfidf, X_test_eng_minmax])
X_test_count_eng = hstack([X_test_count, X_test_eng])

### FOR VALIDATION DATASET
X_test_tfidf_minmax_f = hstack([X_test_tfidf_f, X_test_eng_minmax_f])
X_test_count_eng_f = hstack([X_test_count_f, X_test_eng_f])
389/86:
#
#with open('count_vectorizer.pickle', 'wb') as handle:
#    pickle.dump(count_vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
#with open('tfidf_vectorizer.pickle', 'wb') as handle:
#    pickle.dump(tfidf_vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
#with open('minmax_scaler.pickle', 'wb') as handle:
#    pickle.dump(minmax_scaler, handle, protocol=pickle.HIGHEST_PROTOCOL)
389/87:
#functions for searching best threshold
def to_labels(pos_probs, threshold):
    return (pos_probs >= threshold).astype('int')

def best_tresholds(y_pred_proba, y_test):
    # define thresholds
    thresholds = np.arange(0, 1, 0.001)
    # evaluate each threshold
    scores = [f1_score(y_test, to_labels(y_pred_proba, t)) for t in thresholds]
    # get best threshold
    ix = np.argmax(scores)
    #print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
    return thresholds[ix], scores[ix]
389/88:
#create a df for saving results of each model
metrics = pd.DataFrame(columns=['model' ,'vectoriser', 'f1 score', 'best_f1', 'threshold', 
                                'train accuracy','test accuracy'])
389/89:
def fit_and_predict(model, x_train ,x_test, y_train, y_test, vectoriser):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)

    y_pred = classifier.predict(x_test)
    cmatrix = confusion_matrix(y_test,y_pred)
    
    y_hat = classifier.predict_proba(x_test)[:,1]
    threshold, best_score = best_tresholds(y_hat, y_test)


    f1score = f1_score(y_test,y_pred)
    train_accuracy = round(classifier.score(x_train,y_train)*100)
    test_accuracy =  round(accuracy_score(y_test,y_pred)*100)

    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'f1 score': f1score, 
                              'best_f1': best_score,
                              'threshold': threshold,
                              'train accuracy': train_accuracy, 
                              'test accuracy': test_accuracy, 
                              'vectoriser': str(vectoriser),
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using "+ str(vectoriser))
    print('f1 score for test: {}'.format(f1score))
    print(classification_report(y_test,y_pred))    
    print('Threshold={:.5f}, F-Score={:.5f}'.format(threshold, best_score))
    print('Accuracy of classifier on training set:{}%'.format(train_accuracy))
    print('Accuracy of classifier on test set:{}%' .format(test_accuracy))
389/90:
models=[
        LogisticRegression(random_state=30),
        SVC(random_state=30, probability=True),
        MultinomialNB(),
        DecisionTreeClassifier(random_state = 30),
        KNeighborsClassifier(),
        RandomForestClassifier(random_state=30, max_depth=4),
       ]
389/91:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train_count, X_test_count, y_train ,y_test, 'Count vector')
    
    fit_and_predict(model, X_train_tfidf, X_test_tfidf, y_train, y_test, 'Tfidf vector')
    
    ## count with EF
    fit_and_predict(model, X_train_count_eng, X_test_count_eng, y_train, y_test, 'Count vector + FE')
    
    ## TF IDF with EF
    fit_and_predict(model, X_train_tfidf_minmax, X_test_tfidf_minmax, y_train, y_test, 'TF-IDF + FE')
389/92: metrics.sort_values('best_f1')
389/93: metrics.to_csv('classification_models_score.csv')
389/94: ### LogReg and SVC show a good result. I want to tune the hyperparameters for the SVC to improve the quality of the classification.
389/95:
param_grid = {'C': [12, 13, 14], 
              'gamma': [0.1, 0.07, 0.05],
              'kernel': ['rbf']}
389/96:
from sklearn.model_selection import StratifiedKFold
StratifiedKFold(n_splits=3, random_state=None, shuffle=False)
389/97:
### CREATE A FILE WITH THE RESULTS OF EACH MODEL IN GRIDSEARCHCV
#old_stdout = sys.stdout
#log_file = open("SVC_tfidf_vect_3.log","w")
#sys.stdout = log_file
389/98:
grid = GridSearchCV(SVC(probability=True),param_grid,cv=StratifiedKFold(),verbose=0,scoring='f1')
grid.fit(X_train_tfidf_minmax,y_train)
389/99:
#sys.stdout = old_stdout
#log_file.close()
389/100:
#In .log files are results of Gridsearchcv. I tried different hyperparameters and datasets 
#and this score is the best one.
389/101:
print('Best hyperparameters: ', grid.best_params_)
print('Best score: ', grid.best_score_)
389/102:
svc_best = grid.best_estimator_
y_predict_proba_svc = svc_best.predict_proba(X_test_tfidf_minmax)
y_predict_proba_train_svc = svc_best.predict_proba(X_train_tfidf_minmax)

y_pred_svc = svc_best.predict(X_test_tfidf_minmax)
print(f1_score(y_test, y_pred_svc))

y_predict_proba_svc_f = svc_best.predict_proba(X_test_tfidf_minmax_f)[:,1]
389/103: best_tresholds(y_predict_proba_train_svc, y_train)
389/104: best_tresholds(y_predict_proba_train_svc[:, 1], y_train)
389/105: best_tresholds(y_predict_proba_svc[:, 1], y_train)
389/106: best_tresholds(y_predict_proba_svc[:, 1], y_test)
389/107: best_tresholds(y_predict_proba_train_svc[:, 1], y_train)
389/108: best_tresholds(y_predict_proba_svc[:, 1], y_test)
389/109:
df_t = df_test[['id']]
df_t['target'] = (y_predict_proba_svc_f >= 0.38).astype(int)
df_t.to_csv("submission_svm_038.csv", index=False)
389/110: df_t
389/111: df_t.describe()
389/112:
df_t = df_test[['id']]
df_t['target'] = (y_predict_proba_svc_f >= 0.5).astype(int)
df_t.to_csv("submission_svm_038.csv", index=False)
389/113: df_t.describe()
389/114:
df_t = df_test[['id']]
df_t['target'] = (y_predict_proba_svc_f >= 0.38).astype(int)
df_t.to_csv("submission_svm_038.csv", index=False)
389/115: df_t.describe()
389/116:
df_t = df_test[['id']]
df_t['target'] = (y_predict_proba_svc_f >= 0.36).astype(int)
df_t.to_csv("submission_svm_038.csv", index=False)
389/117: df_t.describe()
389/118:
df_t = df_test[['id']]
df_t['target'] = (y_predict_proba_svc_f >= 0.364).astype(int)
df_t.to_csv("submission_svm_038.csv", index=False)
389/119: df_t.describe()
389/120:
df_t = df_test[['id']]
df_t['target'] = (y_predict_proba_svc_f >= 0.5).astype(int)
df_t.to_csv("submission_svm_05.csv", index=False)
389/121: df_t.describe()
389/122: from sklearn.decomposition import TruncatedSVD
389/123: svd = pickle.load(open('TruncatedSVD_500.pickle', 'rb'))
389/124:
X_train_svd = svd.transform(X_train_tfidf_minmax)
X_test_svd = svd.transform(X_test_eng_minmax
389/125:
X_train_svd = svd.transform(X_train_tfidf_minmax)
X_test_svd = svd.transform(X_test_eng_minmax)
389/126:
X_train_svd = svd.transform(X_train_tfidf_minmax)
X_test_svd = svd.transform(X_test_tfidf_minmax)
389/127:
X_train_svd = svd.transform(X_train_tfidf_minmax)
X_test_svd = svd.transform(X_test_tfidf)
389/128:
X_train_svd = svd.transform(X_train_tfidf)
X_test_svd = svd.transform(X_test_tfidf)
389/129:
X_train_svd = svd.transform(X_train_tfidf_minmax)
X_test_svd = svd.transform(X_test_tfidf)
389/130:
X_train_svd = svd.transform(X_train_tfidf_minmax.todense())
X_test_svd = svd.transform(X_test_tfidf)
389/131:
svd = TruncatedSVD(n_components = 1000)
svd.fit(X_train_tfidf_minmax.todense(), y_train)
389/132:
### SAVE ALL DF
X_train_count.to_csv("../data/X_train_count.csv")
X_train_tfidf.to_csv("../data/X_train_tfidf.csv")
X_test_count.to_csv("../data/X_test_count.csv")
X_test_tfidf.to_csv("../data/X_test_tfidf.csv")
X_test_count_f.to_csv("../data/X_test_count_f.csv")
X_test_count_f.to_csv("../data/X_test_count_f.csv")
#########
X_train_tfidf_minmax.to_csv("../data/X_train_tfidf_minmax.csv")
X_train_count_eng.to_csv("../data/X_train_count_eng.csv")
X_test_tfidf_minmax.to_csv("../data/X_test_tfidf_minmax.csv")
X_test_count_eng.to_csv("../data/X_test_count_eng.csv")
X_test_tfidf_minmax_f.to_csv("../data/X_test_tfidf_minmax_f.csv")
X_test_count_eng_f.to_csv("../data/X_test_count_eng_f.csv")
389/133: X_train_count
389/134:
from scipy import sparse
### SAVE ALL DF
X_train_count.save_npz("../data/X_train_count.npz", your_matrix)
X_train_tfidf.save_npz("../data/X_train_tfidf.npz", your_matrix)

X_test_count.save_npz("../data/X_test_count.npz", your_matrix)
X_test_tfidf.save_npz("../data/X_test_tfidf.npz", your_matrix)

X_train_tfidf_minmax.save_npz("../data/X_train_tfidf_minmax.npz", your_matrix)
X_train_count_eng.save_npz("../data/X_train_count_eng.npz", your_matrix)

X_test_tfidf_minmax.save_npz("../data/X_test_tfidf_minmax.npz", your_matrix)
X_test_count_eng.save_npz("../data/X_test_count_eng.npz", your_matrix)

#########
X_test_count_f.save_npz("../data/X_test_count_f.npz", your_matrix)
X_test_tfidf_f.save_npz("../data/X_test_tfidf_f.npz", your_matrix)
X_test_count_eng_f.save_npz("../data/X_test_count_eng_f.npz", your_matrix)
X_test_tfidf_minmax_f.save_npz("../data/X_test_tfidf_minmax_f.npz", your_matrix)
389/135:
from scipy import sparse
### SAVE ALL DF
sparse.save_npz("../data/X_train_count.npz", X_train_count)
sparse.save_npz("../data/X_train_tfidf.npz", X_train_tfidf)

sparse.save_npz("../data/X_test_count.npz", X_test_count)
sparse.save_npz("../data/X_test_tfidf.npz", X_test_tfidf)

sparse.save_npz("../data/X_train_tfidf_minmax.npz", X_train_tfidf_minmax)
sparse.save_npz("../data/X_train_count_eng.npz", X_train_count_eng)

sparse.save_npz("../data/X_test_tfidf_minmax.npz", X_test_tfidf_minmax)
sparse.save_npz("../data/X_test_count_eng.npz", X_test_count_eng)

#########
sparse.save_npz("../data/X_test_count_f.npz", X_test_count_f)
sparse.save_npz("../data/X_test_tfidf_f.npz", X_test_tfidf_f)
sparse.save_npz("../data/X_test_count_eng_f.npz", X_test_count_eng_f)
sparse.save_npz("../data/X_test_tfidf_minmax_f.npz", X_test_tfidf_minmax_f)
389/136: svd = pickle.load(open('TruncatedSVD_500.pickle', 'rb'))
389/137:
with open('TruncatedSVD_1000.pickle', 'wb') as handle:
    pickle.dump(svd, handle, protocol=pickle.HIGHEST_PROTOCOL)
389/138:
X_train_svd = svd.transform(X_train_tfidf_minmax.todense())
X_test_svd = svd.transform(X_test_tfidf)
389/139:
X_train_svd = svd.transform(X_train_tfidf_minmax)
X_test_svd = svd.transform(X_test_tfidf)
389/140:
svd = TruncatedSVD(n_components = 1000)
svd.fit(X_train_tfidf_minmax)
389/141:
X_train_svd = svd.transform(X_train_tfidf_minmax)
X_test_svd = svd.transform(X_test_tfidf)
389/142:
X_train_svd = svd.transform(X_train_tfidf_minmax)
X_test_svd = svd.transform(X_test_tfidf_minmax)
389/143: X_train_svd
389/144: X_test_svd.shape
389/145:
svc_best = grid.best_estimator_
svc_best.fit(X_train_svd, y_train)

y_predict_proba_svc_svd = svc_best.predict_proba(X_test_svd)
y_pred_svc_svd = svc_best.predict(X_test_svd)
print(f1_score(y_test, y_pred_svc))
389/146: print(X_train_svd)
389/147: print(X_train_svd.shape)
389/148: svc_best
389/149:
svd = TruncatedSVD(n_components = 500)
svd.fit(X_train_tfidf_minmax)
389/150:
with open('TruncatedSVD_500.pickle', 'wb') as handle:
    pickle.dump(svd, handle, protocol=pickle.HIGHEST_PROTOCOL)
389/151: #svd = pickle.load(open('TruncatedSVD_500.pickle', 'rb'))
389/152:
X_train_svd = svd.transform(X_train_tfidf_minmax)
X_test_svd = svd.transform(X_test_tfidf_minmax)
X_test_svd_f = svd.transform(X_test_tfidf_minmax_f)
389/153: print(X_train_svd.shape)
389/154:
svc_best = grid.best_estimator_
svc_best.fit(X_train_svd, y_train)

y_predict_proba_svc_svd = svc_best.predict_proba(X_test_svd)
y_pred_svc_svd = svc_best.predict(X_test_svd)
print(f1_score(y_test, y_pred_svc))
389/155:
svc_best = grid.best_estimator_
svc_best.fit(X_train_svd, y_train)

y_predict_proba_svc_svd = svc_best.predict_proba(X_test_svd)
y_pred_svc_svd = svc_best.predict(X_test_svd)
print(f1_score(y_test, y_pred_svc_svd))
389/156:
svd = TruncatedSVD(n_components = 1000)
svd.fit(X_train_tfidf_minmax)
389/157:
with open('TruncatedSVD_1000.pickle', 'wb') as handle:
    pickle.dump(svd, handle, protocol=pickle.HIGHEST_PROTOCOL)
389/158: #svd = pickle.load(open('TruncatedSVD_500.pickle', 'rb'))
389/159:
X_train_svd = svd.transform(X_train_tfidf_minmax)
X_test_svd = svd.transform(X_test_tfidf_minmax)
X_test_svd_f = svd.transform(X_test_tfidf_minmax_f)
389/160: print(X_train_svd.shape)
389/161:
svc_best = grid.best_estimator_
svc_best.fit(X_train_svd, y_train)

y_predict_proba_svc_svd = svc_best.predict_proba(X_test_svd)
y_pred_svc_svd = svc_best.predict(X_test_svd)
print(f1_score(y_test, y_pred_svc_svd))
389/162:
svd = TruncatedSVD(n_components = 1500)
svd.fit(X_train_tfidf_minmax)
389/163:
with open('TruncatedSVD_1500.pickle', 'wb') as handle:
    pickle.dump(svd, handle, protocol=pickle.HIGHEST_PROTOCOL)
389/164:
X_train_svd = svd.transform(X_train_tfidf_minmax)
X_test_svd = svd.transform(X_test_tfidf_minmax)
X_test_svd_f = svd.transform(X_test_tfidf_minmax_f)
389/165: print(X_train_svd.shape)
389/166:
svc_best = grid.best_estimator_
svc_best.fit(X_train_svd, y_train)

y_predict_proba_svc_svd = svc_best.predict_proba(X_test_svd)
y_pred_svc_svd = svc_best.predict(X_test_svd)
print(f1_score(y_test, y_pred_svc_svd))
389/167:
svd = TruncatedSVD(n_components = 1000)
svd.fit(X_train_tfidf_minmax)
389/168:
with open('TruncatedSVD_1000.pickle', 'wb') as handle:
    pickle.dump(svd, handle, protocol=pickle.HIGHEST_PROTOCOL)
389/169: #svd = pickle.load(open('TruncatedSVD_500.pickle', 'rb'))
389/170:
X_train_svd = svd.transform(X_train_tfidf_minmax)
X_test_svd = svd.transform(X_test_tfidf_minmax)
X_test_svd_f = svd.transform(X_test_tfidf_minmax_f)
389/171: print(X_train_svd.shape)
389/172:
svc_best = grid.best_estimator_
svc_best.fit(X_train_svd, y_train)

y_predict_proba_svc_svd = svc_best.predict_proba(X_test_svd)
y_pred_svc_svd = svc_best.predict(X_test_svd)
print(f1_score(y_test, y_pred_svc_svd))
389/173:
svd = TruncatedSVD(n_components = 750)
svd.fit(X_train_tfidf_minmax)
389/174:
with open('TruncatedSVD_750.pickle', 'wb') as handle:
    pickle.dump(svd, handle, protocol=pickle.HIGHEST_PROTOCOL)
389/175: #svd = pickle.load(open('TruncatedSVD_500.pickle', 'rb'))
389/176:
X_train_svd = svd.transform(X_train_tfidf_minmax)
X_test_svd = svd.transform(X_test_tfidf_minmax)
X_test_svd_f = svd.transform(X_test_tfidf_minmax_f)
389/177: print(X_train_svd.shape)
389/178:
svc_best = grid.best_estimator_
svc_best.fit(X_train_svd, y_train)

y_predict_proba_svc_svd = svc_best.predict_proba(X_test_svd)
y_pred_svc_svd = svc_best.predict(X_test_svd)
print(f1_score(y_test, y_pred_svc_svd))
389/179:
svd = TruncatedSVD(n_components = 1000)
svd.fit(X_train_tfidf_minmax)
389/180:
with open('TruncatedSVD_1000.pickle', 'wb') as handle:
    pickle.dump(svd, handle, protocol=pickle.HIGHEST_PROTOCOL)
389/181: #svd = pickle.load(open('TruncatedSVD_500.pickle', 'rb'))
389/182:
X_train_svd = svd.transform(X_train_tfidf_minmax)
X_test_svd = svd.transform(X_test_tfidf_minmax)
X_test_svd_f = svd.transform(X_test_tfidf_minmax_f)
389/183:
svc_best = grid.best_estimator_
svc_best.fit(X_train_svd, y_train)

y_predict_proba_svc_svd = svc_best.predict_proba(X_test_svd)
y_pred_svc_svd = svc_best.predict(X_test_svd)
print(f1_score(y_test, y_pred_svc_svd))
389/184:  #1 / (1000 * X_train_svd.var())
389/185: X_train_svd.shape
389/186: from sklearn.decomposition import TruncatedSVD
389/187:
svd = TruncatedSVD(n_components = 1000)
svd.fit(X_train_tfidf_minmax)
389/188:
with open('TruncatedSVD_1000.pickle', 'wb') as handle:
    pickle.dump(svd, handle, protocol=pickle.HIGHEST_PROTOCOL)
389/189: #svd = pickle.load(open('TruncatedSVD_500.pickle', 'rb'))
389/190:
X_train_svd = svd.transform(X_train_tfidf_minmax)
X_test_svd = svd.transform(X_test_tfidf_minmax)
X_test_svd_f = svd.transform(X_test_tfidf_minmax_f)
389/191: X_train_svd.shape
389/192:
svc_best = grid.best_estimator_
svc_best.fit(X_train_svd, y_train)

y_predict_proba_svc_svd = svc_best.predict_proba(X_test_svd)
y_pred_svc_svd = svc_best.predict(X_test_svd)
print(f1_score(y_test, y_pred_svc_svd))
389/193:
svd = TruncatedSVD(n_components = 1000)
svd.fit(X_train_tfidf_minmax)
389/194:
with open('TruncatedSVD_1000.pickle', 'wb') as handle:
    pickle.dump(svd, handle, protocol=pickle.HIGHEST_PROTOCOL)
389/195: #svd = pickle.load(open('TruncatedSVD_500.pickle', 'rb'))
389/196:
X_train_svd = svd.transform(X_train_tfidf_minmax)
X_test_svd = svd.transform(X_test_tfidf_minmax)
X_test_svd_f = svd.transform(X_test_tfidf_minmax_f)
389/197: X_train_svd.shape
389/198:
svc_best = grid.best_estimator_
svc_best.fit(X_train_svd, y_train)

y_predict_proba_svc_svd = svc_best.predict_proba(X_test_svd)
y_pred_svc_svd = svc_best.predict(X_test_svd)
print(f1_score(y_test, y_pred_svc_svd))
389/199: param_grid = {'C': [1, 5, 10, 15], 'gamma': [0.3, 0.1, 0.05],'kernel': ['rbf']}
389/200:
grid_svd = GridSearchCV(SVC(probability=True),param_grid,cv=StratifiedKFold(),verbose=2,scoring='f1')
grid_svd.fit(X_train_svd,y_train)
389/201:
grid_svd = GridSearchCV(SVC(probability=True),param_grid,cv=StratifiedKFold(),verbose=10,scoring='f1')
grid_svd.fit(X_train_svd,y_train)
390/1:
import pickle
import numpy as np
390/2:
with open("train_qa.txt", "rb") as fp:   # Unpickling
    train_data =  pickle.load(fp)
390/3:
with open("test_qa.txt", "rb") as fp:   # Unpickling
    test_data =  pickle.load(fp)
390/4: type(test_data)
390/5: type(train_data)
390/6: len(test_data)
390/7: len(train_data)
390/8: train_data[0]
390/9: train_data
390/10: train_data.shape
390/11: train_data[0]
390/12: train_data[0].dtype
390/13: train_data[0]
390/14: train_data[0]
390/15: ' '.join(train_data[0][0])
390/16: ' '.join(train_data[0][1])
390/17: train_data[0][2]
390/18:
# Create a set that holds the vocab words
vocab = set()
390/19: all_data = test_data + train_data
390/20: all_data
390/21:
for story, question , answer in all_data:
    # In case you don't know what a union of sets is:
    # https://www.programiz.com/python-programming/methods/set/union
    vocab = vocab.union(set(story))
    vocab = vocab.union(set(question))
390/22:
vocab.add('no')
vocab.add('yes')
390/23: vocab
390/24: vocab_len = len(vocab) + 1 #we add an extra space to hold a 0 for Keras's pad_sequences
390/25: max_story_len = max([len(data[0]) for data in all_data])
390/26: max_story_len
390/27: max_question_len = max([len(data[1]) for data in all_data])
390/28: max_question_len
390/29: vocab_len
390/30: vocab
390/31:
# Reserve 0 for pad_sequences
vocab_size = len(vocab) + 1
390/32:
from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer
390/33:
# integer encode sequences of words
tokenizer = Tokenizer(filters=[])
tokenizer.fit_on_texts(vocab)
390/34: tokenizer.word_index
390/35:
train_story_text = []
train_question_text = []
train_answers = []

for story,question,answer in train_data:
    train_story_text.append(story)
    train_question_text.append(question)
390/36: train_story_seq = tokenizer.texts_to_sequences(train_story_text)
390/37: train_story_seq
390/38: train_story_seq.shape
390/39: train_story_seq[0].shape
390/40: train_story_seq[0]
390/41: len(train_story_text)
390/42: len(train_story_seq)
390/43: # word_index = tokenizer.word_index
390/44:
def vectorize_stories(data, word_index=tokenizer.word_index, max_story_len=max_story_len,max_question_len=max_question_len):
    '''
    INPUT: 
    
    data: consisting of Stories,Queries,and Answers
    word_index: word index dictionary from tokenizer
    max_story_len: the length of the longest story (used for pad_sequences function)
    max_question_len: length of the longest question (used for pad_sequences function)


    OUTPUT:
    
    Vectorizes the stories,questions, and answers into padded sequences. We first loop for every story, query , and
    answer in the data. Then we convert the raw words to an word index value. Then we append each set to their appropriate
    output list. Then once we have converted the words to numbers, we pad the sequences so they are all of equal length.
    
    Returns this in the form of a tuple (X,Xq,Y) (padded based on max lengths)
    '''
    
    
    # X = STORIES
    X = []
    # Xq = QUERY/QUESTION
    Xq = []
    # Y = CORRECT ANSWERysw
    Y = []
    
    
    for story, query, answer in data:
        
        # Grab the word index for every word in story
        x = [word_index[word.lower()] for word in story]
        # Grab the word index for every word in query
        xq = [word_index[word.lower()] for word in query]
        
        # Grab the Answers (either Yes/No so we don't need to use list comprehension here)
        # Index 0 is reserved so we're going to use + 1
        y = np.zeros(len(word_index) + 1)
        
        # Now that y is all zeros and we know its just Yes/No , we can use numpy logic to create this assignment
        #
        y[word_index[answer]] = 1
        
        # Append each set of story,query, and answer to their respective holding lists
        X.append(x)
        Xq.append(xq)
        Y.append(y)
        
    # Finally, pad the sequences based on their max length so the RNN can be trained on uniformly long sequences.
        
    # RETURN TUPLE FOR UNPACKING
    return (pad_sequences(X, maxlen=max_story_len),pad_sequences(Xq, maxlen=max_question_len), np.array(Y))
390/45: tokenizer.word_index['apple']
389/202: grid_svd.best_score_
389/203: grid_svd.best_params_
389/204: param_grid = {'C': [13, 15, 17], 'gamma': [0.07, 0.05, 0.03],'kernel': ['rbf']}
389/205:  #1 / (1000 * X_train_svd.var())
389/206:
grid_svd = GridSearchCV(SVC(probability=True),param_grid,cv=StratifiedKFold(),verbose=10,scoring='f1')
grid_svd.fit(X_train_svd,y_train)
390/46: inputs_train, queries_train, answers_train = vectorize_stories(train_data)
390/47: inputs_test, queries_test, answers_test = vectorize_stories(test_data)
390/48: inputs_test
390/49: queries_test
390/50: answers_test
390/51: sum(answers_test)
390/52: tokenizer.word_index['yes']
390/53: tokenizer.word_index['no']
389/207: grid_svd.best_params_
389/208: svd_svc = grid_svd.best_estimator_
389/209: grid_svd.best_score_
389/210:
y_pred_svc_svd = svd_svc.predict(X_test_svd)
y_predict_proba_svc_svd = svd_svc.predict_proba(X_test_svd)[:,1]
print(f1_score(y_test, y_pred_svc))
389/211: best_tresholds(y_predict_proba_svc_svd, y_test)
389/212:
df_t = df_test[['id']]
df_t['target'] = (y_predict_proba_svc_f >= 0.500).astype(int)
df_t.to_csv("submission_svm_svd_05.csv", index=False)
389/213: df_t.describe()
389/214:
df_t = df_test[['id']]
df_t['target'] = (y_predict_proba_svc_f >= 0.395).astype(int)
df_t.to_csv("submission_svm_svd_395.csv", index=False)
389/215: df_t.describe()
389/216:
df_t = df_test[['id']]
df_t['target'] = (y_predict_proba_svc_f >= 0.395).astype(int)
df_t.to_csv("submission_svm_svd_55.csv", index=False)
389/217: df_t.describe()
389/218:
df_t = df_test[['id']]
df_t['target'] = (y_predict_proba_svc_f >= 0.55).astype(int)
df_t.to_csv("submission_svm_svd_55.csv", index=False)
389/219: df_t.describe()
389/220: y_predict_proba_svc_svd_f = svd_svc.predict_proba(X_test_svd_f)[:,1]
390/54:
from keras.models import Sequential, Model
from keras.layers.embeddings import Embedding
from keras.layers import Input, Activation, Dense, Permute, Dropout
from keras.layers import add, dot, concatenate
from keras.layers import LSTM
390/55:
input_sequence = Input((max_story_len,))
question = Input((max_question_len,))
390/56:
# Input gets embedded to a sequence of vectors
input_encoder_m = Sequential()
input_encoder_m.add(Embedding(input_dim=vocab_size,output_dim=64))
input_encoder_m.add(Dropout(0.3))

# This encoder will output:
# (samples, story_maxlen, embedding_dim)
390/57:
# embed the input into a sequence of vectors of size query_maxlen
input_encoder_c = Sequential()
input_encoder_c.add(Embedding(input_dim=vocab_size,output_dim=max_question_len))
input_encoder_c.add(Dropout(0.3))
# output: (samples, story_maxlen, query_maxlen)
390/58:
# embed the question into a sequence of vectors
question_encoder = Sequential()
question_encoder.add(Embedding(input_dim=vocab_size,
                               output_dim=64,
                               input_length=max_question_len))
question_encoder.add(Dropout(0.3))
# output: (samples, query_maxlen, embedding_dim)
390/59:
# Input gets embedded to a sequence of vectors
input_encoder_m = Sequential()
input_encoder_m.add(Embedding(input_dim=vocab_size,output_dim=64))
input_encoder_m.add(Dropout(0.3))

# This encoder will output:
# (samples, story_maxlen, embedding_dim)
390/60:
# embed the input into a sequence of vectors of size query_maxlen
input_encoder_c = Sequential()
input_encoder_c.add(Embedding(input_dim=vocab_size,output_dim=max_question_len))
input_encoder_c.add(Dropout(0.3))
# output: (samples, story_maxlen, query_maxlen)
390/61:
# embed the input into a sequence of vectors of size query_maxlen
input_encoder_c = Sequential()
input_encoder_c.add(Embedding(input_dim=vocab_size,output_dim=max_question_len))
input_encoder_c.add(Dropout(0.3))
# output: (samples, story_maxlen, query_maxlen)
390/62:
# embed the question into a sequence of vectors
question_encoder = Sequential()
question_encoder.add(Embedding(input_dim=vocab_size,
                               output_dim=64,
                               input_length=max_question_len))
question_encoder.add(Dropout(0.3))
# output: (samples, query_maxlen, embedding_dim)
390/63:
# encode input sequence and questions (which are indices)
# to sequences of dense vectors
input_encoded_m = input_encoder_m(input_sequence)
input_encoded_c = input_encoder_c(input_sequence)
question_encoded = question_encoder(question)
390/64:
# shape: `(samples, story_maxlen, query_maxlen)`
match = dot([input_encoded_m, question_encoded], axes=(2, 2))
match = Activation('softmax')(match)
390/65:
# add the match matrix with the second input vector sequence
response = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)
response = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)
390/66:
# concatenate the match matrix with the question vector sequence
answer = concatenate([response, question_encoded])
390/67: answer
390/68:
# Reduce with RNN (LSTM)
answer = LSTM(32)(answer)  # (samples, 32)
390/69:
# Regularization with Dropout
answer = Dropout(0.5)(answer)
answer = Dense(vocab_size)(answer)  # (samples, vocab_size)
390/70:
# we output a probability distribution over the vocabulary
answer = Activation('softmax')(answer)

# build the final model
model = Model([input_sequence, question], answer)
model.compile(optimizer='rmsprop', loss='categorical_crossentropy',
              metrics=['accuracy'])
390/71: model.summary()
390/72:
# train
history = model.fit([inputs_train, queries_train], answers_train,batch_size=32,epochs=120,validation_data=([inputs_test, queries_test], answers_test))
390/73:
import matplotlib.pyplot as plt
%matplotlib inline
print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
390/74: print(history.history.keys())
390/75:
import matplotlib.pyplot as plt
%matplotlib inline
print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
390/76:
import matplotlib.pyplot as plt
%matplotlib inline
print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
390/77:
model.load_weights(filename)
pred_results = model.predict(([inputs_test, queries_test]))
390/78: test_data[0][0]
390/79:
#model.load_weights(filename)
pred_results = model.predict(([inputs_test, queries_test]))
390/80: test_data[0][0]
390/81:
story =' '.join(word for word in test_data[0][0])
print(story)
390/82:
query = ' '.join(word for word in test_data[0][1])
print(query)
390/83: print("True Test Answer from Data is:",test_data[0][2])
390/84: pred_results[0]
390/85: np.argmax(pred_results[0])
390/86: tokenizer
390/87: tokenizer.word_index
390/88: tokenizer.word_index.values().index(31)
390/89: (tokenizer.word_index.values()).index(31)
390/90: (tokenizer.word_index.values())
390/91: vocab
390/92:
# Note the whitespace of the periods
my_story = "John left the kitchen . Sandra dropped the football in the garden ."
my_story.split()
390/93: my_question = "Is the football in the garden ?"
390/94: my_question.split()
390/95: mydata = [(my_story.split(),my_question.split(),'yes')]
390/96: my_story,my_ques,my_ans = vectorize_stories(mydata)
390/97: pred_results = model.predict(([ my_story, my_ques]))
390/98:
#Generate prediction from model
val_max = np.argmax(pred_results[0])

for key, val in tokenizer.word_index.items():
    if val == val_max:
        k = key

print("Predicted answer is: ", k)
print("Probability of certainty was: ", pred_results[0][val_max])
390/99:
#Generate prediction from model
val_max = np.argmax(pred_results[0])

for key, val in tokenizer.word_index.items():
    if val == val_max:
        k = key

print("Predicted answer is: ", k)
print("Probability of certainty was: ", pred_results[0][val_max])
393/1: import pandas as pd
393/2: npr = pd.read_csv('npr.csv')
393/3: npr.head()
393/4: from sklearn.decomposition import NMF
393/5: nmf_model = NMF(n_components=7,random_state=42)
393/6:
# This can take awhile, we're dealing with a large amount of documents!
nmf_model.fit(dtm)
393/7: tfidf = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')
393/8: import pandas as pd
393/9: npr = pd.read_csv('npr.csv')
393/10: npr.head()
393/11: from sklearn.feature_extraction.text import TfidfVectorizer
393/12: tfidf = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')
393/13: dtm = tfidf.fit_transform(npr['Article'])
393/14: dtm
393/15: from sklearn.decomposition import NMF
393/16: nmf_model = NMF(n_components=7,random_state=42)
393/17:
# This can take awhile, we're dealing with a large amount of documents!
nmf_model.fit(dtm)
393/18: len(tfidf.get_feature_names())
393/19: import random
393/20:
for i in range(10):
    random_word_id = random.randint(0,54776)
    print(tfidf.get_feature_names()[random_word_id])
393/21:
for index,topic in enumerate(nmf_model.components_):
    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')
    print([tfidf.get_feature_names()[i] for i in topic.argsort()[-15:]])
    print('\n')
393/22: dtm
393/23: dtm.shape
393/24: len(npr)
393/25: topic_results = nmf_model.transform(dtm)
393/26: topic_results.shape
393/27: topic_results[0]
393/28: topic_results[0].round(2)
393/29: topic_results[0].argmax()
393/30: npr.head()
393/31: topic_results.argmax(axis=1)
393/32: npr['Topic'] = topic_results.argmax(axis=1)
393/33: npr.head(10)
393/34: topic_results
393/35: topic_results.shape
397/1:
# RUN THIS CELL to perform standard imports:
import spacy
nlp = spacy.load('en_core_web_sm')
from spacy import displacy
397/2: doc
397/3:
with open('../TextFiles/peterrabbit.txt') as f:
    doc = nlp(f.read())
397/4: doc
397/5: doc[0]
397/6: doc.sents
397/7: doc.sents[0]
397/8: POS_counts
397/9: POS_counts = doc.count_by(spacy.attrs.POS)
397/10: POS_counts
397/11: POS_counts.items()
397/12: doc.vocab()
397/13:
for tokens in list(doc.sents)[2]:
    print(f'{tokens.text:{7}}\
            {tokens.pos_:{6}}\
            {tokens.tag_:{5}}\
            {spacy.explain(tokens.tag_)}')
397/14: POS_counts = doc.count_by(spacy.attrs.POS)
397/15: POS_counts
397/16: POS_counts.items()
397/17: doc.vocab()
397/18:
for k, v in sorted(POS_counts.items()):
    print(f'{k:>{3}}. {doc.vocab[k].text:{8}} : {v}')
397/19: doc.vocab[1]
397/20: doc.vocab[1].text
397/21: doc.vocab[2].text
397/22: doc.vocab[3].text
397/23: doc.vocab[85].text
397/24:
for k, v in sorted(POS_counts.items()):
    print(f'{k:>{3}}. {doc.vocab[k].text:{8}} : {v}')
397/25: print(f'176/1258 = {POS_counts[91]/len(doc)*100:.2f}%')
397/26: POS_counts
397/27: POS_counts[90]
397/28: POS_counts[91]
397/29: doc.vocab[91].text
397/30: print(f'176/1258 = {POS_counts[90]/len(doc)*100:.2f}%')
397/31: print(f'170/1258 = {POS_counts[92]/len(doc)*100:.2f}%')
397/32: displacy.render(list(doc.sents)[2])
397/33: displacy.render(list(doc.sents)[2], style='dep', jupyter=True, options={'distance': 70})
397/34: displacy.render(list(doc.sents)[2], style='ent', jupyter=True, options={'distance': 70})
397/35: displacy.render(list(doc.sents)[2], style='ent', jupyter=True)
397/36: displacy.render(list(doc.sents)[2], style='dep', jupyter=True, options={'distance': 70})
397/37: displacy.render(list(doc.sents)[2], style='dep', jupyter=True, options={'distance': 20})
397/38: displacy.render(list(doc.sents)[2], style='dep', jupyter=True, options={'distance': 60})
397/39: displacy.render(list(doc.sents)[2], style='dep', jupyter=True, options={'distance': 70})
397/40: doc
397/41: doc.ents
397/42:
for ent in doc.ents[:2]:
    print ent.text
397/43:
for ent in doc.ents[:2]:
    print (ent.text)
397/44:
for ent in doc.ents[:2]:
    print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
397/45:
for ent in doc.ents[:2]:
    print (ent.text, ent.label_)
397/46:
for ent in doc.ents[:2]:
    print (ent.text, ent.label)
397/47:
for ent in doc.ents[:2]:
    print (ent.text, ent.label_)
397/48: len(doc)
397/49: len(doc.sents)
397/50: len([sent in doc.sents])
397/51: len([for sent in doc.sents])
397/52: len([sent for sent in doc.sents])
397/53: doc.ents
397/54: [nlp(sent) for sent in doc.sents]
397/55: [nlp(sent.text) for sent in doc.sents]
397/56: list_sent = [nlp(sent.text) for sent in doc.sents]
397/57: list_sent
397/58: list_sent[0]
397/59:
list_sent = [nlp(sent.text) for sent in doc.sents]
[doc for doc in list_sent if doc.ents]
397/60:
list_sent = [nlp(sent.text) for sent in doc.sents]
list_ents = [doc for doc in list_sent if doc.ents]
397/61: list_ents[0]
397/62: len(list_ents)
397/63:
list_of_sents = [nlp(sent.text) for sent in doc.sents]
list_of_ners = [doc for doc in list_of_sents if doc.ents]
len(list_of_ners)
397/64:
colors = {'ORG': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'PRODUCT': 'linear-gradient(to right, rgba(255,0,0,0), rgba(255,0,0,1))'}

options = {'ents': ['ORG', 'PRODUCT'], 'colors':colors}

displacy.render(list_of_sents[0], style='ent', jupyter=True, options=options)
397/65: displacy.render(list_of_sents[0], style='ent', jupyter=True, options=options)
397/66: displacy.render(list_of_sents[0], style='ent', jupyter=True)
397/67:
colors = {'ORG': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'PRODUCT': 'linear-gradient(to right, rgba(255,0,0,0), rgba(255,0,0,1))'}

options = {'ents': ['ORG', 'PRODUCT'], 'colors':colors}

displacy.render(list_of_sents[1], style='ent', jupyter=True, options=options)
397/68:
colors = {'ORG': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'PRODUCT': 'linear-gradient(to right, rgba(255,0,0,0), rgba(255,0,0,1))'}

options = {'ents': ['ORG', 'PRODUCT'], 'colors':colors}

displacy.render(list_of_sents[2], style='ent', jupyter=True, options=options)
397/69:
colors = {'ORG': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'PRODUCT': 'linear-gradient(to right, rgba(255,0,0,0), rgba(255,0,0,1))'}

options = {'ents': ['ORG', 'PRODUCT'], 'colors':colors}

displacy.render(list_of_sents[3], style='ent', jupyter=True, options=options)
397/70:
colors = {'ORG': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'PRODUCT': 'linear-gradient(to right, rgba(255,0,0,0), rgba(255,0,0,1))'}

options = {'ents': ['ORG', 'PRODUCT'], 'colors':colors}

displacy.render(list_of_sents[4], style='ent', jupyter=True, options=options)
397/71:
colors = {'ORG': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'PRODUCT': 'linear-gradient(to right, rgba(255,0,0,0), rgba(255,0,0,1))'}

options = {'ents': ['ORG', 'PRODUCT'], 'colors':colors}

displacy.render(list_of_sents[2], style='ent', jupyter=True, options=options)
397/72:
colors = {'ORG': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'PRODUCT': 'linear-gradient(to right, rgba(255,0,0,0), rgba(255,0,0,1))'}

options = {'ents': ['ORG', 'PRODUCT'], 'colors':colors}

displacy.render(list_of_sents[1], style='ent', jupyter=True, options=options)
397/73:
colors = {'ORG': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'PRODUCT': 'linear-gradient(to right, rgba(255,0,0,0), rgba(255,0,0,1))'}

options = {'ents': ['ORG', 'PRODUCT'], 'colors':colors}

displacy.render(list_ents[1], style='ent', jupyter=True, options=options)
397/74:
colors = {'ORG': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'PRODUCT': 'linear-gradient(to right, rgba(255,0,0,0), rgba(255,0,0,1))'}

options = {'ents': ['ORG', 'PRODUCT'], 'colors':colors}

displacy.render(list_ents[0], style='ent', jupyter=True, options=options)
397/75:
colors = {'PERSON': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'PRODUCT': 'linear-gradient(to right, rgba(255,0,0,0), rgba(255,0,0,1))'}

options = {'ents': ['ORG', 'PRODUCT'], 'colors':colors}

displacy.render(list_ents[0], style='ent', jupyter=True, options=options)
397/76:
colors = {'PERSON': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'PRODUCT': 'linear-gradient(to right, rgba(255,0,0,0), rgba(255,0,0,1))'}

options = {'ents': ['PERSON', 'PRODUCT'], 'colors':colors}

displacy.render(list_ents[0], style='ent', jupyter=True, options=options)
397/77:
colors = {'PERSON': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'DATE': 'linear-gradient(to right, rgba(255,0,0,0), rgba(255,0,0,1))'}

options = {'ents': ['PERSON', 'DATE'], 'colors':colors}

displacy.render(list_ents[0], style='ent', jupyter=True, options=options)
397/78:
colors = {'PERSON': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'DATE': 'lightgreen'}

options = {'ents': ['PERSON', 'DATE'], 'colors':colors}

displacy.render(list_ents[0], style='ent', jupyter=True, options=options)
397/79:
colors = {'PERSON': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'DATE': 'mint'}

options = {'ents': ['PERSON', 'DATE'], 'colors':colors}

displacy.render(list_ents[0], style='ent', jupyter=True, options=options)
397/80:
colors = {'PERSON': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'DATE': 'rgba(255,0,0,1)'}

options = {'ents': ['PERSON', 'DATE'], 'colors':colors}

displacy.render(list_ents[0], style='ent', jupyter=True, options=options)
397/81:
colors = {'PERSON': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'DATE': 'rgba(222,222,222)'}

options = {'ents': ['PERSON', 'DATE'], 'colors':colors}

displacy.render(list_ents[0], style='ent', jupyter=True, options=options)
397/82:
colors = {'PERSON': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'DATE': 'rgba(222,111,222)'}

options = {'ents': ['PERSON', 'DATE'], 'colors':colors}

displacy.render(list_ents[0], style='ent', jupyter=True, options=options)
397/83:
colors = {'PERSON': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'DATE': 'rgba(222,3,222)'}

options = {'ents': ['PERSON', 'DATE'], 'colors':colors}

displacy.render(list_ents[0], style='ent', jupyter=True, options=options)
397/84:
colors = {'PERSON': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'DATE': 'rgba(222,3,66)'}

options = {'ents': ['PERSON', 'DATE'], 'colors':colors}

displacy.render(list_ents[0], style='ent', jupyter=True, options=options)
397/85:
colors = {'PERSON': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'DATE': 'rgba(2,3,66)'}

options = {'ents': ['PERSON', 'DATE'], 'colors':colors}

displacy.render(list_ents[0], style='ent', jupyter=True, options=options)
397/86:
colors = {'PERSON': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'DATE': 'rgba(200,3,66)'}

options = {'ents': ['PERSON', 'DATE'], 'colors':colors}

displacy.render(list_ents[0], style='ent', jupyter=True, options=options)
397/87:
colors = {'PERSON': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'DATE': 'rgba(200,3,66, 100)'}

options = {'ents': ['PERSON', 'DATE'], 'colors':colors}

displacy.render(list_ents[0], style='ent', jupyter=True, options=options)
397/88:
colors = {'PERSON': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'DATE': 'rgba(200,3,66, 300)'}

options = {'ents': ['PERSON', 'DATE'], 'colors':colors}

displacy.render(list_ents[0], style='ent', jupyter=True, options=options)
397/89:
colors = {'PERSON': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'DATE': 'rgba(200,3,66, 50)'}

options = {'ents': ['PERSON', 'DATE'], 'colors':colors}

displacy.render(list_ents[0], style='ent', jupyter=True, options=options)
397/90:
colors = {'PERSON': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'DATE': 'rgba(11,122,66, 50)'}

options = {'ents': ['PERSON', 'DATE'], 'colors':colors}

displacy.render(list_ents[0], style='ent', jupyter=True, options=options)
397/91:
colors = {'PERSON': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'DATE': 'rgba(147, 217, 139, 1)'}

options = {'ents': ['PERSON', 'DATE'], 'colors':colors}

displacy.render(list_ents[0], style='ent', jupyter=True, options=options)
397/92:
colors = {'PERSON': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'DATE': 'rgba(153, 222, 232, 1)'}

options = {'ents': ['PERSON', 'DATE'], 'colors':colors}

displacy.render(list_ents[0], style='ent', jupyter=True, options=options)
397/93:
colors = {'PERSON': 'linear-gradient(180deg, #aa9cfc, #fc9ce7)', 'DATE': 'rgba(153, 222, 232, 1)'}

options = {'ents': ['PERSON', 'DATE'], 'colors':colors}

displacy.render(list_ents[0], style='ent', jupyter=True, options=options)
397/94:
colors = {'PERSON': 'linear-gradient(33, #aa9cfc, #fc9ce7)', 'DATE': 'rgba(153, 222, 232, 1)'}

options = {'ents': ['PERSON', 'DATE'], 'colors':colors}

displacy.render(list_ents[0], style='ent', jupyter=True, options=options)
397/95:
colors = {'PERSON': 'linear-gradient(33deg, #aa9cfc, #fc9ce7)', 'DATE': 'rgba(153, 222, 232, 1)'}

options = {'ents': ['PERSON', 'DATE'], 'colors':colors}

displacy.render(list_ents[0], style='ent', jupyter=True, options=options)
399/1:
# Perform standard imports
import spacy
nlp = spacy.load('en_core_web_sm')

# Import the game script
import game
399/2:
# Enter your text here:
text = u"The quick brown fox jumped over the lazy dog's back."
399/3:
# Make your Doc object and pass it into the scorer:
doc = nlp(text)
print(game.scorer(doc))
399/4:
# For practice, visualize your fine-grained POS tags (shown in the third column):
print(f"{'TOKEN':{10}} {'COARSE':{8}} {'FINE':{6}} {'DESCRIPTION'}")
print(f"{'-----':{10}} {'------':{8}} {'----':{6}} {'-----------'}")

for token in doc:
    print(f'{token.text:{10}} {token.pos_:{8}} {token.tag_:{6}} {spacy.explain(token.tag_)}')
401/1:
import numpy as np
import pandas as pd

df = pd.read_csv('../TextFiles/moviereviews.tsv', sep='\t')
df.head()
401/2: len(df)
401/3: print(df['review'][0])
401/4:
from IPython.display import Markdown, display
display(Markdown('> '+df['review'][0]))
401/5:
# Check for the existence of NaN values in a cell:
df.isnull().sum()
401/6: my_str = ''
401/7: my_str.isspace()
401/8: my_str = '   '
401/9: my_str.isspace()
401/10:
blanks = []
for i, lb, rv in df.itertuples():
    if str(rv).isspace():
        blanks.append(i)
401/11: blanks
401/12: df[1993]
401/13: df.iloc[1993, :]
401/14: df.head(2)
401/15: df.drop(blanks, inplace=True)
401/16: df.shape
401/17:


len(df)
401/18:
blanks = []  # start with an empty list

for i,lb,rv in df.itertuples():  # iterate over the DataFrame
    if type(rv)==str:            # avoid NaN values
        if rv.isspace():         # test 'review' for whitespace
            blanks.append(i)     # add matching index numbers to the list
        
print(len(blanks), 'blanks: ', blanks)
401/19: df.isnull().mean()
401/20: df.dropna(inplace=True)
401/21: df.isnull().mean()
401/22: df['label'].value_counts()
401/23:
from sklearn.model_selection import train_test_split

X = df['review']
y = df['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
401/24:
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC

# Na√Øve Bayes:
text_clf_nb = Pipeline([('tfidf', TfidfVectorizer()),
                     ('clf', MultinomialNB()),
])

# Linear SVC:
text_clf_lsvc = Pipeline([('tfidf', TfidfVectorizer()),
                     ('clf', LinearSVC()),
])
401/25: text_clf_nb.fit(X_train, y_train)
401/26:
# Form a prediction set
predictions = text_clf_nb.predict(X_test)
401/27:
# Report the confusion matrix
from sklearn import metrics
print(metrics.confusion_matrix(y_test,predictions))
401/28:
# Print a classification report
print(metrics.classification_report(y_test,predictions))
401/29:
# Print the overall accuracy
print(metrics.accuracy_score(y_test,predictions))
401/30: text_clf_lsvc.fit(X_train, y_train)
401/31:
# Form a prediction set
predictions = text_clf_lsvc.predict(X_test)
401/32:
# Report the confusion matrix
from sklearn import metrics
print(metrics.confusion_matrix(y_test,predictions))
401/33:
# Print a classification report
print(metrics.classification_report(y_test,predictions))
401/34:
# Print the overall accuracy
print(metrics.accuracy_score(y_test,predictions))
401/35:
stopwords = ['a', 'about', 'an', 'and', 'are', 'as', 'at', 'be', 'been', 'but', 'by', 'can', \
             'even', 'ever', 'for', 'from', 'get', 'had', 'has', 'have', 'he', 'her', 'hers', 'his', \
             'how', 'i', 'if', 'in', 'into', 'is', 'it', 'its', 'just', 'me', 'my', 'of', 'on', 'or', \
             'see', 'seen', 'she', 'so', 'than', 'that', 'the', 'their', 'there', 'they', 'this', \
             'to', 'was', 'we', 'were', 'what', 'when', 'which', 'who', 'will', 'with', 'you']
401/36:
# YOU DO NOT NEED TO RUN THIS CELL UNLESS YOU HAVE
# RECENTLY OPENED THIS NOTEBOOK OR RESTARTED THE KERNEL:

import numpy as np
import pandas as pd

df = pd.read_csv('../TextFiles/moviereviews.tsv', sep='\t')
df.dropna(inplace=True)
blanks = []
for i,lb,rv in df.itertuples():
    if type(rv)==str:
        if rv.isspace():
            blanks.append(i)
df.drop(blanks, inplace=True)
from sklearn.model_selection import train_test_split
X = df['review']
y = df['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn import metrics
401/37:
# RUN THIS CELL TO ADD STOPWORDS TO THE LINEAR SVC PIPELINE:
text_clf_lsvc2 = Pipeline([('tfidf', TfidfVectorizer(stop_words=stopwords)),
                     ('clf', LinearSVC()),
])
text_clf_lsvc2.fit(X_train, y_train)
401/38:
predictions = text_clf_lsvc2.predict(X_test)
print(metrics.confusion_matrix(y_test,predictions))
401/39: print(metrics.classification_report(y_test,predictions))
401/40: print(metrics.accuracy_score(y_test,predictions))
401/41:
myreview = "A movie I really wanted to love was terrible. \
I'm sure the producers had the best intentions, but the execution was lacking."
401/42:
# Use this space to write your own review. Experiment with different lengths and writing styles.
myreview = 'Terrible movie. The worst i"ve ever seen'
401/43: print(text_clf_nb.predict([myreview]))  # be sure to put "myreview" inside square brackets
401/44:
# Use this space to write your own review. Experiment with different lengths and writing styles.
myreview = 'The best movie in my live.'
401/45: print(text_clf_nb.predict([myreview]))  # be sure to put "myreview" inside square brackets
401/46:
# Use this space to write your own review. Experiment with different lengths and writing styles.
myreview = 'The best movie in my live. Fantastic! Adorable!'
401/47: print(text_clf_nb.predict([myreview]))  # be sure to put "myreview" inside square brackets
401/48: print(text_clf_lsvc.predict([myreview]))
402/1:
# Import spaCy and load the language library. Remember to use a larger model!
import spacy
nlp = spacy.load('en_core_web_md')
402/2: import numpy as np
402/3:
# Choose the words you wish to compare, and obtain their vectors
np.linalg.norm(nlp(u'cat').vector)
402/4:
# Import spatial and define a cosine_similarity function
def cos_simularity(vec_a, vec_b):
    return np.dot(vec_a, vec_b)/(np.linalg.norm(vec_a)*np.linalg.norm(vec_b)
)
402/5: cos_simularity(nlp(u'cat').vector, nlp(u'table').vector)
402/6: cos_simularity(nlp(u'cat').vector, nlp(u'cat').vector)
402/7: cos_simularity(nlp(u'cat').vector, nlp(u'dog').vector)
402/8: cos_similarity(nlp(u'cat').vector, nlp(u'Nastya').vector)
402/9: cos_simularity(nlp(u'cat').vector, nlp(u'dog').vector)
402/10: cos_simularity(nlp(u'cat').vector, nlp(u'Nastya').vector)
402/11:
from scipy import spatial
cos_similarity = lambda vec1, vec2: 1 - spatial.distance.cosine(vec1, vec2)
402/12:
pet = nlp.vocab['pet'].vector
chicken = nlp.vocab['chicken'].vector
wild = nlp.vocab['wild'].vector
402/13:
# Write an expression for vector arithmetic
# For example: new_vector = word1 - word2 + word3
new_vec = pet - chicken + wild
402/14:
# List the top ten closest vectors in the vocabulary to the result of the expression above
cos_sim_list = []

for word in nlp.vocab:
    if word.has_vector:
        if word.is_lower:
            if word.is_alpha:
                similarity = cos_similarity(new_vec, word.vector)
                cos_sim_list.append((word, similarity))
402/15: cos_sim_list = sorted(cos_sim_list, key=lambda item: -item[1])
402/16: sorted([3, 33, 1, -3])
402/17: sorted([3, 33, 1, -3], reverse)
402/18: sorted([3, 33, 1, -3], reverse=reverse)
402/19: sorted([3, 33, 1, -3], reverse=True)
402/20:
def vector_math(a,b,c):
    a = nlp(a).vector
    print(a)
402/21:
# Test the function on known words:
vector_math('king','man','woman')
402/22: cos_sim_list
402/23: cos_sim_list[0]
402/24: cos_sim_list
402/25: cos_sim_list[:,1]
402/26: cos_sim_list[0]
402/27: cos_sim_list[0].text
402/28: cos_sim_list[0][0].text
402/29:
# Import spatial and define a cosine_similarity function
def cos_simularity(vec_a, vec_b):
    return np.dot(vec_a, vec_b)/(np.linalg.norm(vec_a)*np.linalg.norm(vec_b)
)
402/30: cos_sim_list = sorted(cos_sim_list, reverse=True)
402/31: print([t[0].text for t in cos_sim_list[:10]])
402/32: cos_sim_list = sorted(cos_sim_list, key=lambda item: -item[1])
402/33: print([t[0].text for t in cos_sim_list[:10]])
402/34:
def vector_math(a,b,c):
    a = nlp(a).vector
    b = nlp(b).vector
    c = nlp(c).vector
    result_string = a-b+c
    simularity_list = []
    for word in nlp.vocab:
        if word.has_vector:
            if word.is_alpha:
                if word.is_lower:
                    cos_sim = cos_simularity(word, result_string)
                    simularity_list.append(word, cos_sim)
    sort_list = sorted(simularity_list, key=lambda item: -item[1])
    print(sort_list[:10])
402/35:
# Test the function on known words:
vector_math('king','man','woman')
402/36:
def vector_math(a,b,c):
    a = nlp(a).vector
    b = nlp(b).vector
    c = nlp(c).vector
    result_string = a-b+c
    simularity_list = []
    print(result_string)
    for word in nlp.vocab:
        if word.has_vector:
            if word.is_alpha:
                if word.is_lower:
                    cos_sim = cos_simularity(word, result_string)
                    simularity_list.append(word, cos_sim)
    sort_list = sorted(simularity_list, key=lambda item: -item[1])
    print(sort_list[:10])
402/37:
# Test the function on known words:
vector_math('king','man','woman')
402/38:
def vector_math(a,b,c):
    a = nlp(a).vector
    b = nlp(b).vector
    c = nlp(c).vector
    result_string = a-b+c
    simularity_list = []
    for word in nlp.vocab:
        if word.has_vector:
            if word.is_alpha:
                if word.is_lower:
                    cos_sim = cos_simularity(word.vector, result_string)
                    simularity_list.append(word, cos_sim)
    sort_list = sorted(simularity_list, key=lambda item: -item[1])
    print(sort_list[:10])
402/39:
# Test the function on known words:
vector_math('king','man','woman')
402/40:
def vector_math(a,b,c):
    a = nlp(a).vector
    b = nlp(b).vector
    c = nlp(c).vector
    result_string = a-b+c
    simularity_list = []
    for word in nlp.vocab:
        if word.has_vector:
            if word.is_alpha:
                if word.is_lower:
                    cos_sim = cos_simularity(word.vector, result_string)
                    simularity_list.append((word, cos_sim))
    sort_list = sorted(simularity_list, key=lambda item: -item[1])
    print(sort_list[:10])
402/41:
# Test the function on known words:
vector_math('king','man','woman')
402/42:
def vector_math(a,b,c):
    a = nlp(a).vector
    b = nlp(b).vector
    c = nlp(c).vector
    result_string = a-b+c
    simularity_list = []
    for word in nlp.vocab:
        if word.has_vector:
            if word.is_alpha:
                if word.is_lower:
                    cos_sim = cos_simularity(word.vector, result_string)
                    simularity_list.append((word, cos_sim))
    sort_list = sorted(simularity_list, key=lambda item: -item[1])
    print(sort_list[:10].text)
402/43:
# Test the function on known words:
vector_math('king','man','woman')
402/44:
def vector_math(a,b,c):
    a = nlp(a).vector
    b = nlp(b).vector
    c = nlp(c).vector
    result_string = a-b+c
    simularity_list = []
    for word in nlp.vocab:
        if word.has_vector:
            if word.is_alpha:
                if word.is_lower:
                    cos_sim = cos_simularity(word.vector, result_string)
                    simularity_list.append((word, cos_sim))
    sort_list = sorted(simularity_list, key=lambda item: -item[1])
    print(t[0].text for t in sort_list)
402/45:
# Test the function on known words:
vector_math('king','man','woman')
402/46:
def vector_math(a,b,c):
    a = nlp(a).vector
    b = nlp(b).vector
    c = nlp(c).vector
    result_string = a-b+c
    simularity_list = []
    for word in nlp.vocab:
        if word.has_vector:
            if word.is_alpha:
                if word.is_lower:
                    cos_sim = cos_simularity(word.vector, result_string)
                    simularity_list.append((word, cos_sim))
    sort_list = sorted(simularity_list, key=lambda item: -item[1])
    print(t[0].text for t in sort_list[:10])
402/47:
# Test the function on known words:
vector_math('king','man','woman')
402/48:
def vector_math(a,b,c):
    a = nlp(a).vector
    b = nlp(b).vector
    c = nlp(c).vector
    result_string = a-b+c
    simularity_list = []
    for word in nlp.vocab:
        if word.has_vector:
            if word.is_alpha:
                if word.is_lower:
                    cos_sim = cos_simularity(word.vector, result_string)
                    simularity_list.append((word, cos_sim))
    sort_list = sorted(simularity_list, key=lambda item: -item[1])
    print([t[0].text for t in sort_list[:10]])
402/49:
# Test the function on known words:
vector_math('king','man','woman')
402/50:
def vector_math(a,b,c):
    a = nlp[a].vector
    b = nlp[b].vector
    c = nlp[c].vector
    result_string = a-b+c
    simularity_list = []
    for word in nlp.vocab:
        if word.has_vector:
            if word.is_alpha:
                if word.is_lower:
                    cos_sim = cos_simularity(word.vector, result_string)
                    simularity_list.append((word, cos_sim))
    sort_list = sorted(simularity_list, key=lambda item: -item[1])
    print([t[0].text for t in sort_list[:10]])
402/51:
# Test the function on known words:
vector_math('king','man','woman')
402/52:
def vector_math(a,b,c):
    new_vector = nlp.vocab[a].vector - nlp.vocab[b].vector + nlp.vocab[c].vector
    computed_similarities = []

    for word in nlp.vocab:
        if word.has_vector:
            if word.is_lower:
                if word.is_alpha:
                    similarity = cosine_similarity(new_vector, word.vector)
                    computed_similarities.append((word, similarity))

    computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])

    return [w[0].text for w in computed_similarities[:10]]
402/53:
# Test the function on known words:
vector_math('king','man','woman')
402/54:
def vector_math(a,b,c):
    new_vector = nlp.vocab[a].vector - nlp.vocab[b].vector + nlp.vocab[c].vector
    computed_similarities = []

    for word in nlp.vocab:
        if word.has_vector:
            if word.is_lower:
                if word.is_alpha:
                    similarity = cos_similarity(new_vector, word.vector)
                    computed_similarities.append((word, similarity))

    computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])

    return [w[0].text for w in computed_similarities[:10]]
402/55:
# Test the function on known words:
vector_math('king','man','woman')
402/56:
# Import spaCy and load the language library. Remember to use a larger model!
import spacy
nlp = spacy.load('en_core_web_lg')
402/57:
def vector_math(a,b,c):
    a = nlp[a].vector
    b = nlp[b].vector
    c = nlp[c].vector
    result_string = a-b+c
    simularity_list = []
    for word in nlp.vocab:
        if word.has_vector:
            if word.is_alpha:
                if word.is_lower:
                    cos_sim = cos_simularity(word.vector, result_string)
                    simularity_list.append((word, cos_sim))
    sort_list = sorted(simularity_list, key=lambda item: -item[1])
    print([t[0].text for t in sort_list[:10]])
402/58:
# Test the function on known words:
vector_math('king','man','woman')
402/59:
def vector_math(a,b,c):
    a = nlp(a).vector
    b = nlp(b).vector
    c = nlp(c).vector
    result_string = a-b+c
    simularity_list = []
    for word in nlp.vocab:
        if word.has_vector:
            if word.is_alpha:
                if word.is_lower:
                    cos_sim = cos_simularity(word.vector, result_string)
                    simularity_list.append((word, cos_sim))
    sort_list = sorted(simularity_list, key=lambda item: -item[1])
    print([t[0].text for t in sort_list[:10]])
402/60:
# Test the function on known words:
vector_math('king','man','woman')
402/61:
# Import SentimentIntensityAnalyzer and create an sid object

from nltk.sentiment.vader import SentimentIntensityAnalyzer

sid = SentimentIntensityAnalyzer()
402/62:
# Write a review as one continuous string (multiple sentences are ok)
review = 'The best cartoon. Perfect!'
402/63:
# Obtain the sid scores for your review
sid.polarity_scores(review)
402/64:
def review_rating(string):
    score = sid.polarity_scores(string)
    if score == 0:
        return 'Neutral'
    elif score>0:
        return 'Positive'
    else:
        return 'Negative'
402/65:
# Test the function on your review above:
review_rating(review)
402/66:
# Obtain the sid scores for your review
sid.polarity_scores(review)['compound']
402/67:
def review_rating(string):
    score = sid.polarity_scores(string)['compound']
    if score == 0:
        return 'Neutral'
    elif score>0:
        return 'Positive'
    else:
        return 'Negative'
402/68:
# Test the function on your review above:
review_rating(review)
396/1: import pandas as pd
396/2: df = pd.read_csv('quora_questions.csv')
396/3: df.head()
396/4:
from sklearn.feature_extraction.text import TfidfVectorizer
tf_vectorizer = TfidfVectorizer(min_df=4, max_df=.8)
396/5: tf_vectorizer.fit(df.Question)
396/6: df_tf_idf = tf_vectorizer.transform(tf_vectorizer)
396/7: df_tf_idf = tf_vectorizer.transform(df.Question)
396/8: df_tf_idf
396/9: from sklearn.decomposition import NMF
396/10: nmf = NMF(n_components=20, random_state=42)
396/11: nmf
396/12: nmf.fit(df_tf_idf)
396/13: nmf
396/14: nmf.components_
396/15: nmf.components_.shape
396/16:
for index,topic in enumerate(nmf_model.components_):
    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')
    print([tfidf.get_feature_names()[i] for i in topic.argsort()[-15:]])
    print('\n')
396/17:
for index,topic in enumerate(nmf.components_):
    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')
    print([tf_vectorizer.get_feature_names()[i] for i in topic.argsort()[-15:]])
    print('\n')
407/1:
import pandas as pd
import numpy as np
407/2: from scipy import sparse
407/3:
X_train_tfidf_minmax = sparse.load_npz("../data/X_train_tfidf_minmax.npz")
X_test_tfidf_minmax = sparse.load_npz("../data/X_test_tfidf_minmax.npz")
#####
X_test_tfidf_minmax_f = sparse.load_npz("../data/X_test_tfidf_minmax_f.npz")
407/4: print(X_train_tfidf_minmax.shape)
407/5:
print(X_train_tfidf_minmax.shape)
print(X_test_tfidf_minmax.shape)
print(X_test_tfidf_minmax_f.shape)
407/6:
import pandas as pd
import numpy as np
import catboost
import optuna
407/7: X_test_tfidf_minmax
408/1:
import pandas as pd
import numpy as np
import re
import copy
from collections import Counter
import string
import sys
from scipy.sparse import hstack
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline
408/2: pd.set_option('max_colwidth', None)
408/3:
import matplotlib.pyplot as plt
import seaborn as sns
408/4:
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.tokenize.treebank import TreebankWordDetokenizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer

import emoji as emj
import spacy 
## It's important to use _lg for OOV in future
nlp = spacy.load('en_core_web_lg')
408/5:
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report, roc_curve
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
from sklearn.metrics import f1_score, accuracy_score
408/6:
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
408/7:
import pickle
from joblib import dump
408/8:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
408/9:
nltk.download()
STOPWORDS = set(stopwords.words('english'))
408/10: wnl = WordNetLemmatizer()
408/11: df_train.head()
408/12: df_train.isnull().mean()
408/13: df_train.shape, df_test.shape
408/14:
#cardinality check
for col in df_train.columns:
    print('{} has {} unique instances'.format(col, len(df_train[col].unique())))
408/15: df_train.info()
409/1:
merge_df_gr = merge_df.groupby('target').sum().reset_index()
merge_df_gr = merge_df_gr.T
merge_df_gr.columns = ['target_0', 'target_1']
410/1:
import pandas as pd
import numpy as np
import re
import copy
from collections import Counter
import string
import sys
from scipy.sparse import hstack
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline
410/2: pd.set_option('max_colwidth', None)
410/3:
import matplotlib.pyplot as plt
import seaborn as sns
410/4:
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.tokenize.treebank import TreebankWordDetokenizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer

import emoji as emj
import spacy 
## It's important to use _lg for OOV in future
nlp = spacy.load('en_core_web_lg')
410/5:
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report, roc_curve
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
from sklearn.metrics import f1_score, accuracy_score
410/6:
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
410/7:
import pickle
from joblib import dump
410/8:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
410/9:
nltk.download()
STOPWORDS = set(stopwords.words('english'))
410/10: wnl = WordNetLemmatizer()
410/11: df_train.head()
410/12: df_train.isnull().mean()
410/13: df_train.shape, df_test.shape
410/14:
#cardinality check
for col in df_train.columns:
    print('{} has {} unique instances'.format(col, len(df_train[col].unique())))
410/15: df_train.info()
410/16: df_train.keyword.value_counts()
410/17:
plt.figure(figsize=[8, 40])
sns.countplot(y='keyword',
              data=df_train,
              color='lightblue',
              order=df_train['keyword'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Keyword Count")
plt.show()
410/18:
plt.figure(figsize=[10, 60])
sns.countplot(y='keyword', 
              hue='target', 
              data=df_train, 
              palette=['lightblue', 'red']
             )
plt.xticks(rotation=90)
plt.legend(loc='upper right')
plt.title("Distribution of Target per Keyword ")
plt.show()
410/19: df_train.keyword = df_train.keyword.str.replace('%20', ' ')
410/20:
# Let's show most common keywords for target = 1
df_train[df_train.target == 1].keyword.value_counts()[:10]
410/21:
# Let's show most common keywords for target = 1
df_train[df_train.target == 0].keyword.value_counts()[:10]
410/22:
df_train_group = df_train.groupby(['keyword', 'target'])['id'].count().unstack(fill_value=0).reset_index()
df_train_group.columns = ['keyword', 'target_0', 'target_1']
df_train_group['prob_real_disasters'] = df_train_group.target_1/(df_train_group.target_0 + df_train_group.target_1)
410/23: df_train_group.sort_values('prob_real_disasters', ascending=False)
410/24: df_train.keyword.nunique()
410/25: df_train.target.value_counts()/len(df_train)
410/26: ## This data set is unbalanced, so we should remember about it later
410/27: df_train.location.nunique()
410/28: df_location = df_train.loc[:, ['id', 'target', 'location', 'text']]
410/29: df_location.location.fillna('no_location', inplace=True)
410/30:
# Write a function to display basic entity info:
def show_ents(doc):
    doc = nlp(doc)
    count_GPE = 0
    if doc.ents:
        for ent in doc.ents:
            if ent.label_ == 'GPE':
                #print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
                count_GPE = 1
                break
    return count_GPE
410/31: df_location['GPE_loc_flag'] = df_location.location.apply(show_ents)
410/32: df_location['GPE_text_flag'] = df_location.text.apply(show_ents)
410/33: df_location['GPE_flag'] = df_location.GPE_loc_flag | df_location.GPE_text_flag
410/34:
f, axes = plt.subplots(1, 3, figsize=(20,6))
sns.countplot(x='GPE_loc_flag', hue='target', data=df_location, ax=axes[0])
sns.countplot(x='GPE_text_flag', hue='target', data=df_location, ax=axes[1])
sns.countplot(x='GPE_flag', hue='target', data=df_location, ax=axes[2])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')
axes[2].set_title('GPE')

#plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
axes[0].legend(labels=['0', '1'])
axes[1].legend(labels=['0', '1'])
axes[2].legend(labels=['0', '1'])

plt.show()
410/35:
# Looks like the location extracted from the tweet will be more informative than the original location feature.
df_location.iloc[:, -4:].corr()
410/36: df_punct_fe = df_train.loc[:, ['id', 'target', 'text']]
410/37:
#string.punctuation = '!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
def count_punctuations(text):
    d=dict()
    for i in string.punctuation:
        d[str(i)+' count']=text.count(i)
    return d
410/38:
df_punct_fe['punct_count'] = df_punct_fe["text"].apply(lambda x:count_punctuations(x))
df_punct= pd.DataFrame(list(df_punct_fe.punct_count))
merge_df = pd.merge(df_punct_fe.target,df_punct,left_index=True, right_index=True)
410/39:
merge_df_gr = merge_df.groupby('target').sum().reset_index()
merge_df_gr = merge_df_gr.T
merge_df_gr.columns = ['target_0', 'target_1']
410/40: merge_df_gr
410/41: merge_df_gr['sum_0_1'] = merge_df_gr['target_0'] + merge_df_gr['target_1']
410/42: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
410/43: merge_df_gr[merge_df_gr.sum_0_1 > 1000]
410/44:
def punct_add_exclamation(text):
    return len(re.findall("!", text))
410/45:
def punct_add_question(text):
    return len(re.findall("\?", text))
410/46:
def punct_add_quotation(text):
    return len(re.findall("'", text))
410/47:
df_punct_fe['count_exclamation'] = df_punct_fe.text.apply(punct_add_exclamation)
df_punct_fe['count_question'] = df_punct_fe.text.apply(punct_add_question)
df_punct_fe['count_quotation'] = df_punct_fe.text.apply(punct_add_quotation)
410/48: new_cols = ['count_exclamation', 'count_question', 'count_quotation']
410/49: df_punct_fe.head(2)
410/50:
# count number of characters 
def count_chars(text):
    return len(text)

# count number of words 
def count_words(text):
    return len(text.split())

# count number of capital characters
def count_capital_chars(text):
    count=0
    for i in text:
        if i.isupper():
            count+=1
    return count

# count number of capital words
def count_capital_words(text):
    return sum(map(str.isupper,text.split()))

# count number of punctuations
def count_punctuations(text):
    count = lambda l1,l2: sum([1 for x in l1 if x in l2])
    return count(text,set(string.punctuation))

# count number of words in quotes
def count_words_in_quotes(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    string_wothout_contractions = string_wothout_contractions.replace('"', '\'')
    x = re.findall("\'(.+?)\'", string_wothout_contractions)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count
    
# count number of sentences
def count_sent(text):
    return len(nltk.sent_tokenize(text))

# count number of unique words 
def count_unique_words(text):
    return len(set(text.split()))
    
# count of hashtags
def count_htags(text):
    x = re.findall(r'(\#\w[A-Za-z0-9]*)', text)
    return len(x)

# count of mentions
def count_mentions(text):
    x = re.findall(r'(\@\w[A-Za-z0-9]*)', text)
    return len(x)

# count of stopwords
def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)

#count of links
def count_links(text):
    link_list = re.findall(r'(https?://[^\s]+)', text)
    bitly_list = re.findall(r'bit.ly/\S+', text)
    return (len(link_list + bitly_list))

#count digits
def count_digits(text):
    digit_list = re.findall(r'[0-9]+', text)
    return len(digit_list)

# Factorize repeated punctuation, add REPEAT
def count_repeat_punct(text):
    rep_list = re.findall(r'([!?.]){2,}', text)
    return len(rep_list)
410/51:
def feature_eng_fun(df_in, text):
    df = copy.deepcopy(df_in)
    df['count_exclamation'] = df_train[text].apply(punct_add_exclamation)
    df['count_question'] = df[text].apply(punct_add_question)
    df['count_quotation'] = df[text].apply(punct_add_quotation)
    df['count_num_of_chars'] = df[text].apply(count_chars)
    df['count_num_of_words'] = df[text].apply(count_words)
    df['count_num_of_capital_chars'] = df[text].apply(count_capital_chars)
    df['count_num_of_capital_words'] = df[text].apply(count_capital_words)
    df['count_punct'] = df[text].apply(count_punctuations)
    df['count_words_in_quotes'] = df[text].apply(count_words_in_quotes)
    df['count_sent'] = df[text].apply(count_sent)
    df['count_unique_words'] = df[text].apply(count_unique_words)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_mentions'] = df[text].apply(count_mentions)
    df['count_htags'] = df[text].apply(count_htags)
    df['count_stopwords'] = df[text].apply(count_stopwords)
    df['count_links'] = df[text].apply(count_links) 
    df['count_repeat_punct'] = df[text].apply(count_repeat_punct)    
    ## This function from part above
    df['GPE_text_flag'] = df[text].apply(show_ents)    
    return df
410/52:
## .loc will create a new copy, not view
df_feature_eng = df_train.loc[:, ('id', 'text', 'target')]
410/53: df_feature_eng = feature_eng_fun(df_feature_eng, 'text')
410/54: df_feature_eng.drop(['text'], axis=1, inplace=True)
410/55:
plt.figure(figsize=(14,10))
sns.heatmap(df_feature_eng.corr(),annot=True)
410/56: cols_selected = np.abs(df_feature_eng.corr()['target']).sort_values(ascending=False)[1:11].index.to_list()
410/57: df_feature_eng[cols_selected].head(3)
410/58:
# Let's look at some tweets more carefully
df_train[55:60]
410/59:
abbreviations = {
    "$" : " dollar ",
    "‚Ç¨" : " euro ",
    "4ao" : "for adults only",
    "a.m" : "before midday",
    "a3" : "anytime anywhere anyplace",
    "aamof" : "as a matter of fact",
    "acct" : "account",
    "adih" : "another day in hell",
    "afaic" : "as far as i am concerned",
    "afaict" : "as far as i can tell",
    "afaik" : "as far as i know",
    "afair" : "as far as i remember",
    "afk" : "away from keyboard",
    "app" : "application",
    "approx" : "approximately",
    "apps" : "applications",
    "asap" : "as soon as possible",
    "asl" : "age, sex, location",
    "atk" : "at the keyboard",
    "ave." : "avenue",
    "aymm" : "are you my mother",
    "ayor" : "at your own risk", 
    "b&b" : "bed and breakfast",
    "b+b" : "bed and breakfast",
    "b.c" : "before christ",
    "b2b" : "business to business",
    "b2c" : "business to customer",
    "b4" : "before",
    "b4n" : "bye for now",
    "b@u" : "back at you",
    "bae" : "before anyone else",
    "bak" : "back at keyboard",
    "bbbg" : "bye bye be good",
    "bbc" : "british broadcasting corporation",
    "bbias" : "be back in a second",
    "bbl" : "be back later",
    "bbs" : "be back soon",
    "be4" : "before",
    "bfn" : "bye for now",
    "blvd" : "boulevard",
    "bout" : "about",
    "brb" : "be right back",
    "bros" : "brothers",
    "brt" : "be right there",
    "bsaaw" : "big smile and a wink",
    "btw" : "by the way",
    "bwl" : "bursting with laughter",
    "c/o" : "care of",
    "cet" : "central european time",
    "cf" : "compare",
    "cia" : "central intelligence agency",
    "csl" : "can not stop laughing",
    "cu" : "see you",
    "cul8r" : "see you later",
    "cv" : "curriculum vitae",
    "cwot" : "complete waste of time",
    "cya" : "see you",
    "cyt" : "see you tomorrow",
    "dae" : "does anyone else",
    "dbmib" : "do not bother me i am busy",
    "diy" : "do it yourself",
    "dm" : "direct message",
    "dwh" : "during work hours",
    "e123" : "easy as one two three",
    "eet" : "eastern european time",
    "eg" : "example",
    "embm" : "early morning business meeting",
    "encl" : "enclosed",
    "encl." : "enclosed",
    "etc" : "and so on",
    "faq" : "frequently asked questions",
    "fawc" : "for anyone who cares",
    "fb" : "facebook",
    "fc" : "fingers crossed",
    "fig" : "figure",
    "fimh" : "forever in my heart", 
    "ft." : "feet",
    "ft" : "featuring",
    "ftl" : "for the loss",
    "ftw" : "for the win",
    "fwiw" : "for what it is worth",
    "fyi" : "for your information",
    "g9" : "genius",
    "gahoy" : "get a hold of yourself",
    "gal" : "get a life",
    "gcse" : "general certificate of secondary education",
    "gfn" : "gone for now",
    "gg" : "good game",
    "gl" : "good luck",
    "glhf" : "good luck have fun",
    "gmt" : "greenwich mean time",
    "gmta" : "great minds think alike",
    "gn" : "good night",
    "g.o.a.t" : "greatest of all time",
    "goat" : "greatest of all time",
    "goi" : "get over it",
    "gps" : "global positioning system",
    "gr8" : "great",
    "gratz" : "congratulations",
    "gyal" : "girl",
    "h&c" : "hot and cold",
    "hp" : "horsepower",
    "hr" : "hour",
    "hrh" : "his royal highness",
    "ht" : "height",
    "ibrb" : "i will be right back",
    "ic" : "i see",
    "icq" : "i seek you",
    "icymi" : "in case you missed it",
    "idc" : "i do not care",
    "idgadf" : "i do not give a damn fuck",
    "idgaf" : "i do not give a fuck",
    "idk" : "i do not know",
    "ie" : "that is",
    "i.e" : "that is",
    "ifyp" : "i feel your pain",
    "IG" : "instagram",
    "iirc" : "if i remember correctly",
    "ilu" : "i love you",
    "ily" : "i love you",
    "imho" : "in my humble opinion",
    "imo" : "in my opinion",
    "imu" : "i miss you",
    "iow" : "in other words",
    "irl" : "in real life",
    "j4f" : "just for fun",
    "jic" : "just in case",
    "jk" : "just kidding",
    "jsyk" : "just so you know",
    "l8r" : "later",
    "lb" : "pound",
    "lbs" : "pounds",
    "ldr" : "long distance relationship",
    "lmao" : "laugh my ass off",
    "lmfao" : "laugh my fucking ass off",
    "lol" : "laughing out loud",
    "ltd" : "limited",
    "ltns" : "long time no see",
    "m8" : "mate",
    "mf" : "motherfucker",
    "mfs" : "motherfuckers",
    "mfw" : "my face when",
    "mofo" : "motherfucker",
    "mph" : "miles per hour",
    "mr" : "mister",
    "mrw" : "my reaction when",
    "ms" : "miss",
    "mte" : "my thoughts exactly",
    "nagi" : "not a good idea",
    "nbc" : "national broadcasting company",
    "nbd" : "not big deal",
    "nfs" : "not for sale",
    "ngl" : "not going to lie",
    "nhs" : "national health service",
    "nrn" : "no reply necessary",
    "nsfl" : "not safe for life",
    "nsfw" : "not safe for work",
    "nth" : "nice to have",
    "nvr" : "never",
    "nyc" : "new york city",
    "oc" : "original content",
    "og" : "original",
    "ohp" : "overhead projector",
    "oic" : "oh i see",
    "omdb" : "over my dead body",
    "omg" : "oh my god",
    "omw" : "on my way",
    "p.a" : "per annum",
    "p.m" : "after midday",
    "pm" : "prime minister",
    "poc" : "people of color",
    "pov" : "point of view",
    "pp" : "pages",
    "ppl" : "people",
    "prw" : "parents are watching",
    "ps" : "postscript",
    "pt" : "point",
    "ptb" : "please text back",
    "pto" : "please turn over",
    "qpsa" : "what happens", #"que pasa",
    "ratchet" : "rude",
    "rbtl" : "read between the lines",
    "rlrt" : "real life retweet", 
    "rofl" : "rolling on the floor laughing",
    "roflol" : "rolling on the floor laughing out loud",
    "rotflmao" : "rolling on the floor laughing my ass off",
    "rt" : "retweet",
    "ruok" : "are you ok",
    "sfw" : "safe for work",
    "sk8" : "skate",
    "smh" : "shake my head",
    "sq" : "square",
    "srsly" : "seriously", 
    "ssdd" : "same stuff different day",
    "tbh" : "to be honest",
    "tbs" : "tablespooful",
    "tbsp" : "tablespooful",
    "tfw" : "that feeling when",
    "thks" : "thank you",
    "tho" : "though",
    "thx" : "thank you",
    "tia" : "thanks in advance",
    "til" : "today i learned",
    "tl;dr" : "too long i did not read",
    "tldr" : "too long i did not read",
    "tmb" : "tweet me back",
    "tntl" : "trying not to laugh",
    "ttyl" : "talk to you later",
    "u" : "you",
    "u2" : "you too",
    "u4e" : "yours for ever",
    "utc" : "coordinated universal time",
    "w/" : "with",
    "w/o" : "without",
    "w8" : "wait",
    "wassup" : "what is up",
    "wb" : "welcome back",
    "wtf" : "what the fuck",
    "wtg" : "way to go",
    "wtpa" : "where the party at",
    "wuf" : "where are you from",
    "wuzup" : "what is up",
    "wywh" : "wish you were here",
    "yd" : "yard",
    "ygtr" : "you got that right",
    "ynk" : "you never know",
    "zzz" : "sleeping bored and tired"
}
410/60:
#remove not ascii
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    return text

# Change an abbreviation by its true meaning
def word_abbrev(word):
    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word

# Replace all abbreviations
def replace_abbrev(text):
    string = ""
    for word in text.split():
        string += word_abbrev(word) + " "        
    return string

# Remove @ and mention, replace by USER
def replace_mention(text):
    pat=re.compile(r'@\S+')
    return pat.sub(r'USER',text)

# Remove numbers, replace it by NUMBER
def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r' NUMBER ', text)

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()

#contractions and remowing stopwords and very short words
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 3:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
410/61:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(string_contractions)
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(clean_text)
    return df
410/62: df_text = df_train.loc[:, ['id', 'target', 'keyword', 'text']]
410/63: df_text = basic_cleaning(df_text, 'text')
410/64: df_text.head(3)
410/65: all_text = ' '.join(df_text['text'].tolist())
410/66:
oov_list = []
def search_oov(text):
    tokens = nlp(text)
    for token in tokens:
        if token.is_oov:
            oov_list.append(token)
410/67: search_oov(all_text)
410/68: oov_list_strings  = [i.text for i in oov_list]
410/69:
counter = Counter(oov_list_strings)
counter.most_common(10)
410/70: # I don't see a critical oov words, so I won't replace anything
410/71: sid = SentimentIntensityAnalyzer()
410/72: df_sentiment = df_train.loc[:, ['id', 'target', 'text']]
410/73: df_sentiment['sentiment_compound']  = df_sentiment.text.apply(lambda tweet: sid.polarity_scores(tweet)['compound'])
410/74: df_sentiment['sentiment_compound_score'] = df_sentiment['sentiment_compound'].apply(lambda c: 'pos' if c >=0 else 'neg')
410/75:
plt.figure(figsize=[4, 4])
sns.countplot(x='target', hue='sentiment_compound_score', data=df_sentiment)
plt.title('Sentiment Analysis')
plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
plt.xlabel('Target')
plt.ylabel('Counts')
plt.legend(labels=['Negative', 'Positive'])
plt.show()
410/76: #not very optimistic result, I won't add a sentiment feature :)
410/77: ## CREATE A MODEL BASED ON TEXT
410/78: ### with eng features
410/79: df_copy = copy.deepcopy(df_train)
410/80:
cols_selected = ['GPE_text_flag', 'count_links', 'count_num_of_chars', 'count_punct', 'count_sent',
                 'count_mentions', 'count_stopwords', 'count_question', 'count_quotation',
                 'count_num_of_capital_chars']

df_copy = copy.deepcopy(df_train)
df_train_text_clean = basic_cleaning(df_copy, 'text')
df_train_text_clean["clean_text"] = df_train_text_clean.keyword.map(str) + " " + df_train_text_clean.text

## new features
df_train_feature_eng = feature_eng_fun(df_copy, 'text')
df_train_feature_eng = df_train_feature_eng[cols_selected]


#### for test
df_test_text_clean_f = basic_cleaning(df_test, 'text')
df_test_text_clean_f["clean_text"] = df_test_text_clean_f.keyword.map(str) + " " + df_test_text_clean_f.text

df_test_feature_eng_f = feature_eng_fun(df_test, 'text')
df_test_feature_eng_f = df_test_feature_eng_f[cols_selected]
410/81:
## ADD LEMMATIZATION
w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
def lemmatize_text(text):
    return [wnl.lemmatize(w) for w in w_tokenizer.tokenize(text)]

df_train_text_clean['text_lemmatized'] = df_train_text_clean.clean_text.apply(lemmatize_text)
df_train_text_clean['all_text'] = df_train_text_clean['text_lemmatized'].apply(lambda x: " ".join(x))

df_test_text_clean_f['text_lemmatized'] = df_test_text_clean_f.clean_text.apply(lemmatize_text)
df_test_text_clean_f['all_text'] = df_test_text_clean_f['text_lemmatized'].apply(lambda x: " ".join(x))
410/82: text_column = 'all_text'
410/83:
df_train_full = pd.merge(df_train_text_clean[['target', text_column]], df_train_feature_eng, 
                         left_index=True, right_index=True)

df_test_full = pd.merge(df_test_text_clean_f[[text_column]], df_test_feature_eng_f, 
                         left_index=True, right_index=True)
410/84:
## split to test and train
y = df_train_full['target']
X = df_train_full.drop('target', axis=1)
# This dataset is unbalanced so add stratify parameter for splitting
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, shuffle=True)
print('X_train shape', X_train.shape)
print('X_test shape', X_test.shape)
410/85:
max_features=15000

# CountVectorizer
count_vectorizer = CountVectorizer(max_features=max_features)

def count_vector(data):
    count_vectorizer = CountVectorizer()
    vect = count_vectorizer.fit_transform(data)
    return vect, count_vectorizer

def tfidf_vector(data):
    tfidf_vectorizer = TfidfVectorizer()
    vect = tfidf_vectorizer.fit_transform(data)
    return vect, tfidf_vectorizer

def min_max(data):
    min_max_scaler = MinMaxScaler()
    vect = min_max_scaler.fit_transform(data)
    return vect, min_max_scaler

# TRAIN
X_train_count, count_vectorizer = count_vector(X_train[text_column])
X_train_tfidf, tfidf_vectorizer = tfidf_vector(X_train[text_column])

X_train_eng_minmax, minmax_scaler = min_max(X_train[cols_selected])
X_train_eng = X_train[cols_selected].to_numpy()

#### TEST
X_test_count = count_vectorizer.transform(X_test [text_column])                                                     
X_test_tfidf = tfidf_vectorizer.transform(X_test [text_column])

X_test_eng_minmax = minmax_scaler.transform(X_test[cols_selected])
X_test_eng = X_test[cols_selected].to_numpy()

### VALIDATION DATASET
X_test_count_f = count_vectorizer.transform(df_test_full[text_column])                                                     
X_test_tfidf_f = tfidf_vectorizer.transform(df_test_full[text_column])

X_test_eng_minmax_f = minmax_scaler.transform(df_test_full[cols_selected])
X_test_eng_f = df_test_full[cols_selected].to_numpy()


### CONCAT tf-idf + minmax, count_vec + feature_eng

X_train_tfidf_minmax = hstack([X_train_tfidf, X_train_eng_minmax])
X_train_count_eng = hstack([X_train_count, X_train_eng])


X_test_tfidf_minmax = hstack([X_test_tfidf, X_test_eng_minmax])
X_test_count_eng = hstack([X_test_count, X_test_eng])

### FOR VALIDATION DATASET
X_test_tfidf_minmax_f = hstack([X_test_tfidf_f, X_test_eng_minmax_f])
X_test_count_eng_f = hstack([X_test_count_f, X_test_eng_f])
410/86: X_train_count
410/87:
from scipy import sparse
### SAVE ALL DF
sparse.save_npz("../data/X_train_count.npz", X_train_count)
sparse.save_npz("../data/X_train_tfidf.npz", X_train_tfidf)

sparse.save_npz("../data/X_test_count.npz", X_test_count)
sparse.save_npz("../data/X_test_tfidf.npz", X_test_tfidf)

sparse.save_npz("../data/X_train_tfidf_minmax.npz", X_train_tfidf_minmax)
sparse.save_npz("../data/X_train_count_eng.npz", X_train_count_eng)

sparse.save_npz("../data/X_test_tfidf_minmax.npz", X_test_tfidf_minmax)
sparse.save_npz("../data/X_test_count_eng.npz", X_test_count_eng)

#########
sparse.save_npz("../data/X_test_count_f.npz", X_test_count_f)
sparse.save_npz("../data/X_test_tfidf_f.npz", X_test_tfidf_f)
sparse.save_npz("../data/X_test_count_eng_f.npz", X_test_count_eng_f)
sparse.save_npz("../data/X_test_tfidf_minmax_f.npz", X_test_tfidf_minmax_f)
410/88: y_train
410/89: y_train.dtype
410/90: y_train
410/91: y_train.sum
410/92: y_train
410/93:
np.save('../data/y_train.npy', y_train)
np.save('../data/y_test.npy', y_test)
407/8: np.load('../data/y_train.npy')
410/94: y_train
407/9:
y_train = np.load('../data/y_train.npy')
y_test = np.load('../data/y_test.npy')
410/95:
#np.save('../data/y_train.npy', y_train)
#np.save('../data/y_test.npy', y_test)
410/96:

with open('count_vectorizer.pickle', 'wb') as handle:
    pickle.dump(count_vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
with open('tfidf_vectorizer.pickle', 'wb') as handle:
    pickle.dump(tfidf_vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
with open('minmax_scaler.pickle', 'wb') as handle:
    pickle.dump(minmax_scaler, handle, protocol=pickle.HIGHEST_PROTOCOL)
410/97:
#functions for searching best threshold
def to_labels(pos_probs, threshold):
    return (pos_probs >= threshold).astype('int')

def best_tresholds(y_pred_proba, y_test):
    # define thresholds
    thresholds = np.arange(0, 1, 0.001)
    # evaluate each threshold
    scores = [f1_score(y_test, to_labels(y_pred_proba, t)) for t in thresholds]
    # get best threshold
    ix = np.argmax(scores)
    #print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
    return thresholds[ix], scores[ix]
410/98:
#create a df for saving results of each model
metrics = pd.DataFrame(columns=['model' ,'vectoriser', 'f1 score', 'best_f1', 'threshold', 
                                'train accuracy','test accuracy'])
410/99:
def fit_and_predict(model, x_train ,x_test, y_train, y_test, vectoriser):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)

    y_pred = classifier.predict(x_test)
    cmatrix = confusion_matrix(y_test,y_pred)
    
    y_hat = classifier.predict_proba(x_test)[:,1]
    threshold, best_score = best_tresholds(y_hat, y_test)


    f1score = f1_score(y_test,y_pred)
    train_accuracy = round(classifier.score(x_train,y_train)*100)
    test_accuracy =  round(accuracy_score(y_test,y_pred)*100)

    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'f1 score': f1score, 
                              'best_f1': best_score,
                              'threshold': threshold,
                              'train accuracy': train_accuracy, 
                              'test accuracy': test_accuracy, 
                              'vectoriser': str(vectoriser),
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using "+ str(vectoriser))
    print('f1 score for test: {}'.format(f1score))
    print(classification_report(y_test,y_pred))    
    print('Threshold={:.5f}, F-Score={:.5f}'.format(threshold, best_score))
    print('Accuracy of classifier on training set:{}%'.format(train_accuracy))
    print('Accuracy of classifier on test set:{}%' .format(test_accuracy))
410/100:
models=[
        LogisticRegression(random_state=30),
        SVC(random_state=30, probability=True),
        MultinomialNB(),
        DecisionTreeClassifier(random_state = 30),
        KNeighborsClassifier(),
        RandomForestClassifier(random_state=30, max_depth=4),
       ]
410/101:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train_count, X_test_count, y_train ,y_test, 'Count vector')
    
    fit_and_predict(model, X_train_tfidf, X_test_tfidf, y_train, y_test, 'Tfidf vector')
    
    ## count with EF
    fit_and_predict(model, X_train_count_eng, X_test_count_eng, y_train, y_test, 'Count vector + FE')
    
    ## TF IDF with EF
    fit_and_predict(model, X_train_tfidf_minmax, X_test_tfidf_minmax, y_train, y_test, 'TF-IDF + FE')
407/10: from sklearn.naive_bayes import MultinomialNB
407/11:
mnb = MultinomialNB()
mnb.fit(X_train_tfidf_minmax, y_train)
407/12: mnb.predict(X_test_tfidf_minmax)
407/13:
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import f1_score
407/14: y_pred = mnb.predict(X_test_tfidf_minmax)
407/15: f1_score(y_test, y_pred)
410/102: metrics.sort_values('best_f1')
410/103:
mnb = MultinomialNB()
mnb.fit(X_train_tfidf_minmax, y_train)
y_pred = mnb.predict(X_test_tfidf_minmax)
f1_score(y_test, y_pred)
407/16: X_train_tfidf_minmax[0]
407/17: X_train_tfidf_minmax
410/104: X_train_tfidf_minmax
410/105: X_train_tfidf_minmax[0]
410/106: X_train_tfidf_minmax.shape
407/18:
X_train_tfidf_minmax = sparse.load_npz("../data/X_train_tfidf_minmax.npz")
X_test_tfidf_minmax = sparse.load_npz("../data/X_test_tfidf_minmax.npz")
#####
X_test_tfidf_minmax_f = sparse.load_npz("../data/X_test_tfidf_minmax_f.npz")
407/19:
print(X_train_tfidf_minmax.shape)
print(X_test_tfidf_minmax.shape)
print(X_test_tfidf_minmax_f.shape)
407/20:
y_train = np.load('../data/y_train.npy')
y_test = np.load('../data/y_test.npy')
407/21:
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import f1_score
407/22:
mnb = MultinomialNB()
mnb.fit(X_train_tfidf_minmax, y_train)
407/23: y_pred = mnb.predict(X_test_tfidf_minmax)
407/24: f1_score(y_test, y_pred)
410/107: metrics.to_csv('classification_models_score.csv')
410/108: ### LogReg and SVC show a good result. I want to tune the hyperparameters for the SVC to improve the quality of the classification.
410/109:
param_grid = {'C': [12, 13, 14], 
              'gamma': [0.1, 0.07, 0.05],
              'kernel': ['rbf']}
410/110:
from sklearn.model_selection import StratifiedKFold
StratifiedKFold(n_splits=3, random_state=None, shuffle=False)
410/111:
### CREATE A FILE WITH THE RESULTS OF EACH MODEL IN GRIDSEARCHCV
#In .log files are results of Gridsearchcv. I tried different hyperparameters and datasets 
#and this score is the best one.

old_stdout = sys.stdout
log_file = open("SVC_tfidf_vect_3.log","w")
sys.stdout = log_file
410/112:
grid = GridSearchCV(SVC(probability=True),param_grid,cv=StratifiedKFold(),verbose=0,scoring='f1')
grid.fit(X_train_tfidf_minmax,y_train)
411/1:
import pandas as pd
import numpy as np
import catboost
import optuna
411/2: from scipy import sparse
411/3:
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import f1_score
411/4:
X_train_tfidf_minmax = sparse.load_npz("../data/X_train_tfidf_minmax.npz")
X_test_tfidf_minmax = sparse.load_npz("../data/X_test_tfidf_minmax.npz")
#####
X_test_tfidf_minmax_f = sparse.load_npz("../data/X_test_tfidf_minmax_f.npz")
#####
y_train = np.load('../data/y_train.npy')
y_test = np.load('../data/y_test.npy')
411/5:
print(X_train_tfidf_minmax.shape)
print(X_test_tfidf_minmax.shape)
print(X_test_tfidf_minmax_f.shape)
411/6:
mnb = MultinomialNB()
mnb.fit(X_train_tfidf_minmax, y_train)
y_pred = mnb.predict(X_test_tfidf_minmax)
f1_score(y_test, y_pred)
411/7:
from catboost import CatBoostClassifier, Pool
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
411/8:
def objective(trial):
    global gbm
    X_train = X_train_tfidf_minmax
    X_test = X_test_tfidf_minmax
    y_train = y_train
    y_test = y_test
    
    param = {
       "objective": trial.suggest_categorical("objective", ["Logloss"]),
       "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.01, 0.1),
       "depth": trial.suggest_int("depth", 3, 7),
       'scale_pos_weight': trial.suggest_int('scale_pos_weight', 80, 110),
       'eta': trial.suggest_loguniform('eta', 1e-3, 1.0),
       'n_estimators': trial.suggest_int('n_estimators', 1000, 1500),
  #      'gamma': trial.suggest_loguniform('gamma', 1e-3, 1.0),
        'subsample': trial.suggest_loguniform('subsample', 0.6, 1.0),
  #      'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.6, 1.0),
  #      'min_child_weight': trial.suggest_int('min_child_weight', 1, 9)
  #      'auto_class_weights': 'Balanced'
    }

    gbm = CatBoostClassifier(**param)

    gbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0, early_stopping_rounds=100)
    trial.set_user_attr(key="best_booster", value=gbm)
    preds = gbm.predict(X_test)
    pred_labels = np.rint(preds)
    accuracy = accuracy_score(y_test, pred_labels)
    f1 = metrics.f1_score(pred_labels, y_test)
    return f1
411/9:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
411/10:
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=500, timeout=1000, callbacks=[callback])
411/11:
X_train_tfidf_minmax = sparse.load_npz("../data/X_train_tfidf_minmax.npz")
X_test_tfidf_minmax = sparse.load_npz("../data/X_test_tfidf_minmax.npz")
#####
X_test_tfidf_minmax_f = sparse.load_npz("../data/X_test_tfidf_minmax_f.npz")
#####
y_train = np.load('../data/y_train.npy')
y_test = np.load('../data/y_test.npy')
411/12:
X_train_tfidf_minmax = sparse.load_npz("../data/X_train_tfidf_minmax.npz")
X_test_tfidf_minmax = sparse.load_npz("../data/X_test_tfidf_minmax.npz")
#####
X_test_tfidf_minmax_f = sparse.load_npz("../data/X_test_tfidf_minmax_f.npz")
#####
y_train_np = np.load('../data/y_train.npy')
y_test_np = np.load('../data/y_test.npy')
411/13:
def objective(trial):
    global gbm
    X_train = X_train_tfidf_minmax
    X_test = X_test_tfidf_minmax
    y_train = y_train_np
    y_test = y_test_np
    
    param = {
       "objective": trial.suggest_categorical("objective", ["Logloss"]),
       "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.01, 0.1),
       "depth": trial.suggest_int("depth", 3, 7),
       'scale_pos_weight': trial.suggest_int('scale_pos_weight', 80, 110),
       'eta': trial.suggest_loguniform('eta', 1e-3, 1.0),
       'n_estimators': trial.suggest_int('n_estimators', 1000, 1500),
  #      'gamma': trial.suggest_loguniform('gamma', 1e-3, 1.0),
        'subsample': trial.suggest_loguniform('subsample', 0.6, 1.0),
  #      'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.6, 1.0),
  #      'min_child_weight': trial.suggest_int('min_child_weight', 1, 9)
  #      'auto_class_weights': 'Balanced'
    }

    gbm = CatBoostClassifier(**param)

    gbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0, early_stopping_rounds=100)
    trial.set_user_attr(key="best_booster", value=gbm)
    preds = gbm.predict(X_test)
    pred_labels = np.rint(preds)
    accuracy = accuracy_score(y_test, pred_labels)
    f1 = metrics.f1_score(pred_labels, y_test)
    return f1
411/14:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
411/15:
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=500, timeout=1000, callbacks=[callback])
411/16:
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import f1_score, accuracy_score
411/17:
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=500, timeout=1000, callbacks=[callback])
411/18:
def objective(trial):
    global gbm
    X_train = X_train_tfidf_minmax
    X_test = X_test_tfidf_minmax
    y_train = y_train_np
    y_test = y_test_np
    
    param = {
       "objective": trial.suggest_categorical("objective", ["Logloss"]),
       "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.01, 0.1),
       "depth": trial.suggest_int("depth", 3, 7),
       'scale_pos_weight': trial.suggest_int('scale_pos_weight', 80, 110),
       'eta': trial.suggest_loguniform('eta', 1e-3, 1.0),
       'n_estimators': trial.suggest_int('n_estimators', 1000, 1500),
  #      'gamma': trial.suggest_loguniform('gamma', 1e-3, 1.0),
        'subsample': trial.suggest_loguniform('subsample', 0.6, 1.0),
  #      'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.6, 1.0),
  #      'min_child_weight': trial.suggest_int('min_child_weight', 1, 9)
  #      'auto_class_weights': 'Balanced'
    }

    gbm = CatBoostClassifier(**param)

    gbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0, early_stopping_rounds=100)
    trial.set_user_attr(key="best_booster", value=gbm)
    preds = gbm.predict(X_test)
    pred_labels = np.rint(preds)
    accuracy = accuracy_score(y_test, pred_labels)
    f1 = f1_score(pred_labels, y_test)
    return f1
411/19:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
411/20:
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=500, timeout=1000, callbacks=[callback])
410/113:
sys.stdout = old_stdout
log_file.close()
410/114:
print('Best hyperparameters: ', grid.best_params_)
print('Best score: ', grid.best_score_)
410/115:
svc_best = grid.best_estimator_
### test df
y_predict_proba_svc = svc_best.predict_proba(X_test_tfidf_minmax)
y_pred_svc = svc_best.predict(X_test_tfidf_minmax)
print(f1_score(y_test, y_pred_svc))

### validation df
y_predict_proba_svc_f = svc_best.predict_proba(X_test_tfidf_minmax_f)[:,1]
410/116:
#So, on Kaggle the better result was shown with treshold = 0.5 --> Score: 0.80018
df_t = df_test[['id']]
df_t['target'] = (y_predict_proba_svc_f >= 0.5).astype(int)
df_t.to_csv("submission_svm_05.csv", index=False)
410/117: df_t.describe()
411/21: best_model_2=study.user_attrs["best_booster"]
411/22:
f_i_2 = pd.DataFrame(
    {'feature_names': best_model_2.feature_names_,
     'feature_importances': best_model_2.feature_importances_
    })
411/23: best_model_2.get_params()
411/24:
def objective(trial):
    global gbm
    X_train = X_train_tfidf_minmax
    X_test = X_test_tfidf_minmax
    y_train = y_train_np
    y_test = y_test_np
    
    param = {
       "objective": trial.suggest_categorical("objective", ["Logloss"]),
       "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.01, 0.1),
       "depth": trial.suggest_int("depth", 2, 7),
#       'scale_pos_weight': trial.suggest_int('scale_pos_weight', 80, 110),
#       'eta': trial.suggest_loguniform('eta', 1e-3, 1.0),
       'n_estimators': trial.suggest_int('n_estimators', 100, 150),
  #      'gamma': trial.suggest_loguniform('gamma', 1e-3, 1.0),
        'subsample': trial.suggest_loguniform('subsample', 0.6, 1.0),
  #      'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.6, 1.0),
  #      'min_child_weight': trial.suggest_int('min_child_weight', 1, 9)
  #      'auto_class_weights': 'Balanced'
    }

    gbm = CatBoostClassifier(**param)

    gbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0, early_stopping_rounds=100)
    trial.set_user_attr(key="best_booster", value=gbm)
    preds = gbm.predict(X_test)
    pred_labels = np.rint(preds)
    accuracy = accuracy_score(y_test, pred_labels)
    f1 = f1_score(pred_labels, y_test)
    return f1
411/25:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
411/26:
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=500, timeout=1000, callbacks=[callback])
411/27: study
411/28: best_model_2=study.user_attrs["best_booster"]
411/29:
f_i_2 = pd.DataFrame(
    {'feature_names': best_model_2.feature_names_,
     'feature_importances': best_model_2.feature_importances_
    })
411/30: best_model_2.get_params()
411/31: study.best_trial
411/32: best_model_2
411/33: study.user_attrs
411/34: best_model_2.predict(X_test_tfidf_minmax)
411/35: f1_score(y_test, best_model_2.predict(X_test_tfidf_minmax))
411/36:
def objective(trial):
    global gbm
    X_train = X_train_tfidf_minmax
    X_test = X_test_tfidf_minmax
    y_train = y_train_np
    y_test = y_test_np
    
    param = {
       "objective": trial.suggest_categorical("objective", ["Logloss"]),
       "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.01, 0.1),
       "depth": trial.suggest_int("depth", 2, 5),
#       'scale_pos_weight': trial.suggest_int('scale_pos_weight', 80, 110),
#       'eta': trial.suggest_loguniform('eta', 1e-3, 1.0),
       'n_estimators': trial.suggest_int('n_estimators', 100, 150),
  #      'gamma': trial.suggest_loguniform('gamma', 1e-3, 1.0),
        'subsample': trial.suggest_loguniform('subsample', 0.6, 1.0),
  #      'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.6, 1.0),
  #      'min_child_weight': trial.suggest_int('min_child_weight', 1, 9)
  #      'auto_class_weights': 'Balanced'
    }

    gbm = CatBoostClassifier(**param)

    gbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0, early_stopping_rounds=100)
    trial.set_user_attr(key="best_booster", value=gbm)
    preds = gbm.predict(X_test)
    pred_labels = np.rint(preds)
    f1 = f1_score(y_test, pred_labels)
    return f1
411/37:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
411/38:
def objective(trial):
    global gbm
    X_train = X_train_tfidf_minmax
    X_test = X_test_tfidf_minmax
    y_train = y_train_np
    y_test = y_test_np
    
    param = {
       "objective": trial.suggest_categorical("objective", ["Logloss"]),
       "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.01, 0.1),
       "depth": trial.suggest_int("depth", 2, 5),
#       'scale_pos_weight': trial.suggest_int('scale_pos_weight', 80, 110),
#       'eta': trial.suggest_loguniform('eta', 1e-3, 1.0),
       'n_estimators': trial.suggest_int('n_estimators', 100, 150),
  #      'gamma': trial.suggest_loguniform('gamma', 1e-3, 1.0),
  #      'subsample': trial.suggest_float('subsample', 0.6, 1.0),
  #      'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.6, 1.0),
  #      'min_child_weight': trial.suggest_int('min_child_weight', 1, 9)
  #      'auto_class_weights': 'Balanced'
    }

    gbm = CatBoostClassifier(**param)

    gbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0, early_stopping_rounds=100)
    trial.set_user_attr(key="best_booster", value=gbm)
    preds = gbm.predict(X_test)
    pred_labels = np.rint(preds)
    f1 = f1_score(y_test, pred_labels)
    return f1
411/39:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
411/40:
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=500, timeout=1000, callbacks=[callback])
410/118: from sklearn.decomposition import TruncatedSVD
410/119:
svd = TruncatedSVD(n_components = 1000)
svd.fit(X_train_tfidf_minmax)
410/120:
with open('TruncatedSVD_1000.pickle', 'wb') as handle:
    pickle.dump(svd, handle, protocol=pickle.HIGHEST_PROTOCOL)
410/121:
svd = TruncatedSVD(n_components = 500)
svd.fit(X_train_tfidf_minmax)
410/122:
with open('TruncatedSVD_500.pickle', 'wb') as handle:
    pickle.dump(svd, handle, protocol=pickle.HIGHEST_PROTOCOL)
405/1: best_model_2.get_params()
405/2: best_model_2=study.user_attrs["best_booster"]
411/41: study.best_trial
411/42: best_model_2=study.user_attrs["best_booster"]
411/43:
f_i_2 = pd.DataFrame(
    {'feature_names': best_model_2.feature_names_,
     'feature_importances': best_model_2.feature_importances_
    })
411/44: best_model_2.get_params()
411/45: best_model_2
411/46: f1_score(y_test, best_model_2.predict(X_test_tfidf_minmax))
411/47:
### SVD
svd = pickle.load(open('../classical_ML/TruncatedSVD_1000.pickle', 'rb'))
411/48:
import pandas as pd
import numpy as np
import catboost
import optuna
import pickle
411/49:
### SVD
svd = pickle.load(open('../classical_ML/TruncatedSVD_1000.pickle', 'rb'))
411/50:
X_train_svd = svd.transform(X_train_tfidf_minmax)
X_test_svd = svd.transform(X_test_tfidf_minmax)
X_test_svd_f = svd.transform(X_test_tfidf_minmax_f)
411/51:
print(X_train_svd.shape)
print(X_test_svd.shape)
print(X_test_svd_f.shape)
411/52:
mnb = MultinomialNB()
mnb.fit(X_train_svd, y_train)
y_pred = mnb.predict(X_test_svd)
f1_score(y_test, y_pred)
411/53:
#mnb = MultinomialNB()
#mnb.fit(X_train_svd, y_train)
#y_pred = mnb.predict(X_test_svd)
#f1_score(y_test, y_pred)
411/54:
def objective(trial):
    global gbm
    X_train = X_train_tfidf_minmax
    X_test = X_test_tfidf_minmax
    y_train = y_train_np
    y_test = y_test_np
    
    param = {
       "objective": trial.suggest_categorical("objective", ["Logloss"]),
       "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.01, 0.1),
       "depth": trial.suggest_int("depth", 2, 5),
#       'scale_pos_weight': trial.suggest_int('scale_pos_weight', 80, 110),
#       'eta': trial.suggest_loguniform('eta', 1e-3, 1.0),
       'n_estimators': trial.suggest_int('n_estimators', 100, 150),
  #      'gamma': trial.suggest_loguniform('gamma', 1e-3, 1.0),
  #      'subsample': trial.suggest_float('subsample', 0.6, 1.0),
  #      'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.6, 1.0),
  #      'min_child_weight': trial.suggest_int('min_child_weight', 1, 9)
  #      'auto_class_weights': 'Balanced'
    }

    gbm = CatBoostClassifier(**param)

    gbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0, early_stopping_rounds=100)
    trial.set_user_attr(key="best_booster", value=gbm)
    preds = gbm.predict(X_test)
    pred_labels = np.rint(preds)
    f1 = f1_score(y_test, pred_labels)
    return f1
411/55:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
411/56:
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=100, timeout=1000, callbacks=[callback])
410/123:
X_train_svd = svd.transform(X_train_tfidf_minmax)
X_test_svd = svd.transform(X_test_tfidf_minmax)
X_test_svd_f = svd.transform(X_test_tfidf_minmax_f)
410/124: X_train_svd.shape
410/125:
svc_best = grid.best_estimator_
svc_best.fit(X_train_svd, y_train)

y_predict_proba_svc_svd = svc_best.predict_proba(X_test_svd)
y_pred_svc_svd = svc_best.predict(X_test_svd)
print(f1_score(y_test, y_pred_svc_svd))
410/126: grid.best_estimator_
410/127: param_grid = {'C': [11, 12, 13], 'gamma': [0.07, 0.06, 0.08],'kernel': ['rbf']}
410/128:  #1 / (1000 * X_train_svd.var())
410/129:
grid_svd = GridSearchCV(SVC(probability=True),param_grid,cv=StratifiedKFold(),verbose=10,scoring='f1')
grid_svd.fit(X_train_svd,y_train)
411/57: study.best_trial
411/58: best_model_2=study.user_attrs["best_booster"]
411/59:
f_i_2 = pd.DataFrame(
    {'feature_names': best_model_2.feature_names_,
     'feature_importances': best_model_2.feature_importances_
    })
411/60: best_model_2.get_params()
411/61: best_model_2
411/62: f1_score(y_test, best_model_2.predict(X_test_tfidf_minmax))
411/63:
def objective(trial):
    global gbm
    X_train = X_train_tfidf_minmax
    X_test = X_test_tfidf_minmax
    y_train = y_train_np
    y_test = y_test_np
    
    param = {
       "objective": trial.suggest_categorical("objective", ["Logloss", "CrossEntropy"]),
       "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.01, 0.1),
       "depth": trial.suggest_int("depth", 1, 10),
       "boosting_type": trial.suggest_categorical("boosting_type", ["Ordered", "Plain"]),
#       'scale_pos_weight': trial.suggest_int('scale_pos_weight', 80, 110),
#       'eta': trial.suggest_loguniform('eta', 1e-3, 1.0),
  #     'n_estimators': trial.suggest_int('n_estimators', 500, 150),
  #      'gamma': trial.suggest_loguniform('gamma', 1e-3, 1.0),
  #      'subsample': trial.suggest_float('subsample', 0.6, 1.0),
  #      'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.6, 1.0),
  #      'min_child_weight': trial.suggest_int('min_child_weight', 1, 9)
  #      'auto_class_weights': 'Balanced'
    }

    gbm = CatBoostClassifier(**param)

    gbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0, early_stopping_rounds=100)
    trial.set_user_attr(key="best_booster", value=gbm)
    preds = gbm.predict(X_test)
    pred_labels = np.rint(preds)
    f1 = f1_score(y_test, pred_labels)
    return f1
411/64:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
411/65:
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=50, timeout=600, callbacks=[callback])
411/66:
### SVD
svd = pickle.load(open('../classical_ML/TruncatedSVD_500.pickle', 'rb'))
411/67:
X_train_svd = svd.transform(X_train_tfidf_minmax)
X_test_svd = svd.transform(X_test_tfidf_minmax)
X_test_svd_f = svd.transform(X_test_tfidf_minmax_f)
411/68:
print(X_train_svd.shape)
print(X_test_svd.shape)
print(X_test_svd_f.shape)
411/69:
#mnb = MultinomialNB()
#mnb.fit(X_train_svd, y_train)
#y_pred = mnb.predict(X_test_svd)
#f1_score(y_test, y_pred)
411/70:
def objective(trial):
    global gbm
    X_train = X_train_tfidf_minmax
    X_test = X_test_tfidf_minmax
    y_train = y_train_np
    y_test = y_test_np
    
    param = {
       "objective": trial.suggest_categorical("objective", ["Logloss", "CrossEntropy"]),
       "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.01, 0.1),
       "depth": trial.suggest_int("depth", 1, 10),
       "boosting_type": trial.suggest_categorical("boosting_type", ["Ordered", "Plain"]),
#       'scale_pos_weight': trial.suggest_int('scale_pos_weight', 80, 110),
#       'eta': trial.suggest_loguniform('eta', 1e-3, 1.0),
  #     'n_estimators': trial.suggest_int('n_estimators', 500, 150),
  #      'gamma': trial.suggest_loguniform('gamma', 1e-3, 1.0),
  #      'subsample': trial.suggest_float('subsample', 0.6, 1.0),
  #      'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.6, 1.0),
  #      'min_child_weight': trial.suggest_int('min_child_weight', 1, 9)
  #      'auto_class_weights': 'Balanced'
    }

    gbm = CatBoostClassifier(**param)

    gbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0, early_stopping_rounds=100)
    trial.set_user_attr(key="best_booster", value=gbm)
    preds = gbm.predict(X_test)
    pred_labels = np.rint(preds)
    f1 = f1_score(y_test, pred_labels)
    return f1
411/71:
def objective(trial):
    global gbm
    X_train = X_train_tfidf_minmax
    X_test = X_test_tfidf_minmax
    y_train = y_train_np
    y_test = y_test_np
    
    param = {
       "objective": trial.suggest_categorical("objective", ["Logloss", "CrossEntropy"]),
       "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.01, 0.1),
       "depth": trial.suggest_int("depth", 1, 9),
       "boosting_type": trial.suggest_categorical("boosting_type", ["Ordered", "Plain"]),
#       'scale_pos_weight': trial.suggest_int('scale_pos_weight', 80, 110),
#       'eta': trial.suggest_loguniform('eta', 1e-3, 1.0),
  #     'n_estimators': trial.suggest_int('n_estimators', 500, 150),
  #      'gamma': trial.suggest_loguniform('gamma', 1e-3, 1.0),
  #      'subsample': trial.suggest_float('subsample', 0.6, 1.0),
  #      'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.6, 1.0),
  #      'min_child_weight': trial.suggest_int('min_child_weight', 1, 9)
  #      'auto_class_weights': 'Balanced'
    }

    gbm = CatBoostClassifier(**param)

    gbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0, early_stopping_rounds=100)
    trial.set_user_attr(key="best_booster", value=gbm)
    preds = gbm.predict(X_test)
    pred_labels = np.rint(preds)
    f1 = f1_score(y_test, pred_labels)
    return f1
411/72:
def objective(trial):
    global gbm
    X_train = X_train_svd
    X_test = X_test_svd
    y_train = y_train_np
    y_test = y_test_np
    
    param = {
       "objective": trial.suggest_categorical("objective", ["Logloss", "CrossEntropy"]),
       "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.01, 0.1),
       "depth": trial.suggest_int("depth", 1, 9),
       "boosting_type": trial.suggest_categorical("boosting_type", ["Ordered", "Plain"]),
#       'scale_pos_weight': trial.suggest_int('scale_pos_weight', 80, 110),
#       'eta': trial.suggest_loguniform('eta', 1e-3, 1.0),
  #     'n_estimators': trial.suggest_int('n_estimators', 500, 150),
  #      'gamma': trial.suggest_loguniform('gamma', 1e-3, 1.0),
  #      'subsample': trial.suggest_float('subsample', 0.6, 1.0),
  #      'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.6, 1.0),
  #      'min_child_weight': trial.suggest_int('min_child_weight', 1, 9)
  #      'auto_class_weights': 'Balanced'
    }

    gbm = CatBoostClassifier(**param)

    gbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0, early_stopping_rounds=100)
    trial.set_user_attr(key="best_booster", value=gbm)
    preds = gbm.predict(X_test)
    pred_labels = np.rint(preds)
    f1 = f1_score(y_test, pred_labels)
    return f1
411/73:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
411/74:
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=50, timeout=600, callbacks=[callback])
411/75: study.best_trial
411/76: best_model_2=study.user_attrs["best_booster"]
411/77:
f_i_2 = pd.DataFrame(
    {'feature_names': best_model_2.feature_names_,
     'feature_importances': best_model_2.feature_importances_
    })
411/78: best_model_2.get_params()
411/79: best_model_2
411/80: f1_score(y_test, best_model_2.predict(X_test_tfidf_minmax))
411/81:  best_model_2.predict(X_test_svd)
411/82: f1_score(y_test,  best_model_2.predict(X_test_svd))
411/83:
def objective(trial):
    global gbm
    X_train = X_train_svd
    X_test = X_test_svd
    y_train = y_train_np
    y_test = y_test_np
    
    param = {
       "objective": trial.suggest_categorical("objective", ["Logloss", "CrossEntropy"]),
       "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.01, 0.1),
       "depth": trial.suggest_int("depth", 1, 8),
       "boosting_type": trial.suggest_categorical("boosting_type", ["Ordered", "Plain"]),
    }

    gbm = CatBoostClassifier(**param)

    gbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0, early_stopping_rounds=150)
    trial.set_user_attr(key="best_booster", value=gbm)
    preds = gbm.predict(X_test)
    f1 = f1_score(y_test, preds)
    return f1
411/84:
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=100, timeout=600, callbacks=[callback])
410/130: grid_svd.best_params_
410/131: svd_svc = grid_svd.best_estimator_
410/132: grid_svd.best_score_
411/85:
#def callback(study, trial):
#    if study.best_trial.number == trial.number:
#        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
411/86: study.best_trial
411/87: best_model_2=study.user_attrs["best_booster"]
411/88: best_model_2.get_params()
411/89: f1_score(y_test,  best_model_2.predict(X_test_svd))
410/133:
y_pred_svc_svd = svd_svc.predict(X_test_svd)
y_predict_proba_svc_svd = svd_svc.predict_proba(X_test_svd)[:,1]
print(f1_score(y_test, y_pred_svc))
410/134: best_tresholds(y_predict_proba_svc_svd, y_test)
410/135: y_predict_proba_svc_svd_f = svd_svc.predict_proba(X_test_svd_f)[:,1]
410/136:
df_t = df_test[['id']]
df_t['target'] = (y_predict_proba_svc_svd_f >= 0.5).astype(int)
df_t.to_csv("submission_svm_svd_500_05.csv", index=False)
410/137: df_t.describe()
410/138:
df_t = df_test[['id']]
df_t['target'] = (y_predict_proba_svc_svd_f >= 0.46).astype(int)
df_t.to_csv("submission_svm_svd_500_046.csv", index=False)
410/139: df_t.describe()
410/140:
df_t = df_test[['id']]
df_t['target'] = (y_predict_proba_svc_svd_f >= 0.55).astype(int)
df_t.to_csv("submission_svm_svd_500_055.csv", index=False)
410/141: df_t.describe()
411/90: f_i_2
411/91: f_i_2.sort_values('feature_importances', ascending=False)
411/92: f_i_2.sort_values('feature_importances', ascending=False)[:20]
411/93:
from catboost import CatBoostClassifier, Pool
from optuna.integration import CatBoostPruningCallback
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
411/94:
def objective(trial):
    global gbm
    train_x = X_train_svd
    valid_x = X_test_svd
    train_y = y_train_np
    valid_y = y_test_np
    
    param = {
        "objective": trial.suggest_categorical("objective", ["Logloss", "CrossEntropy"]),
        "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.01, 0.1, log=True),
        "depth": trial.suggest_int("depth", 1, 9),
        "boosting_type": trial.suggest_categorical("boosting_type", ["Ordered", "Plain"]),
        "bootstrap_type": trial.suggest_categorical(
            "bootstrap_type", ["Bayesian", "Bernoulli", "MVS"]
        ),
        "eval_metric": "F1",
    }

    if param["bootstrap_type"] == "Bayesian":
        param["bagging_temperature"] = trial.suggest_float("bagging_temperature", 0, 10)
    elif param["bootstrap_type"] == "Bernoulli":
        param["subsample"] = trial.suggest_float("subsample", 0.1, 1, log=True)
    
    gbm = CatBoostClassifier(**param)

    pruning_callback = CatBoostPruningCallback(trial, "F1")
    
    gbm.fit(
        train_x,
        train_y,
        eval_set=[(valid_x, valid_y)],
        verbose=0,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    preds = gbm.predict(X_test)
    f1 = f1_score(y_test, preds)
    return f1
411/95:
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=100, timeout=600, callbacks=[callback])
411/96:
def objective(trial):
    global gbm
    train_x = X_train_svd
    valid_x = X_test_svd
    train_y = y_train_np
    valid_y = y_test_np
    
    param = {
        "objective": trial.suggest_categorical("objective", ["Logloss", "CrossEntropy"]),
        "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.01, 0.1, log=True),
        "depth": trial.suggest_int("depth", 1, 9),
        "boosting_type": trial.suggest_categorical("boosting_type", ["Ordered", "Plain"]),
        "bootstrap_type": trial.suggest_categorical(
            "bootstrap_type", ["Bayesian", "Bernoulli", "MVS"]
        ),
        "eval_metric": "F1",
    }

    if param["bootstrap_type"] == "Bayesian":
        param["bagging_temperature"] = trial.suggest_float("bagging_temperature", 0, 10)
    elif param["bootstrap_type"] == "Bernoulli":
        param["subsample"] = trial.suggest_float("subsample", 0.1, 1, log=True)
    
    gbm = CatBoostClassifier(**param)

    pruning_callback = CatBoostPruningCallback(trial, "F1")
    
    gbm.fit(
        train_x,
        train_y,
        eval_set=[(valid_x, valid_y)],
        verbose=0,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    preds = gbm.predict(valid_x)
    f1 = f1_score(valid_y, preds)
    return f1
411/97:
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=100, timeout=600, callbacks=[callback])
411/98:
def objective(trial):
    global gbm
    train_x = X_train_svd
    valid_x = X_test_svd
    train_y = y_train_np
    valid_y = y_test_np
    
    param = {
        "objective": trial.suggest_categorical("objective", ["Logloss", "CrossEntropy"]),
        "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.01, 0.1, log=True),
        "depth": trial.suggest_int("depth", 1, 9),
        "boosting_type": trial.suggest_categorical("boosting_type", ["Ordered", "Plain"]),
        "bootstrap_type": trial.suggest_categorical(
            "bootstrap_type", ["Bayesian", "Bernoulli", "MVS"]
        ),
        "eval_metric": "F1",
    }

    if param["bootstrap_type"] == "Bayesian":
        param["bagging_temperature"] = trial.suggest_float("bagging_temperature", 0, 10)
    elif param["bootstrap_type"] == "Bernoulli":
        param["subsample"] = trial.suggest_float("subsample", 0.1, 1, log=True)
    
    gbm = CatBoostClassifier(**param)

    pruning_callback = CatBoostPruningCallback(trial, "F1")
    
    gbm.fit(
        train_x,
        train_y,
        eval_set=[(valid_x, valid_y)],
        verbose=0,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=gbm)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    preds = gbm.predict(valid_x)
    f1 = f1_score(valid_y, preds)
    return f1
411/99:
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=100, timeout=600, callbacks=[callback])
411/100:
def objective(trial):
    global gbm
    train_x = X_train_svd
    valid_x = X_test_svd
    train_y = y_train_np
    valid_y = y_test_np
    
    param = {
        "objective": trial.suggest_categorical("objective", ["Logloss", "CrossEntropy"]),
        "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.01, 0.1, log=True),
        "depth": trial.suggest_int("depth", 1, 9),
        "boosting_type": trial.suggest_categorical("boosting_type", ["Ordered", "Plain"]),
        "bootstrap_type": trial.suggest_categorical(
            "bootstrap_type", ["Bayesian", "Bernoulli", "MVS"]
        ),
        "eval_metric": "F1",
    }

    if param["bootstrap_type"] == "Bayesian":
        param["bagging_temperature"] = trial.suggest_float("bagging_temperature", 0, 10)
    elif param["bootstrap_type"] == "Bernoulli":
        param["subsample"] = trial.suggest_float("subsample", 0.1, 1, log=True)
    
    gbm = CatBoostClassifier(**param)

    pruning_callback = CatBoostPruningCallback(trial, "F1")
    
    gbm.fit(
        train_x,
        train_y,
        eval_set=[(valid_x, valid_y)],
        verbose=0,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=gbm)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    preds = gbm.predict(valid_x)
    f1 = f1_score(valid_y, preds)
    return f1
411/101:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="maximize")
study.optimize(objective, n_trials=100, timeout=600, callbacks=[callback])
411/102: best_model=study.user_attrs["best_booster"]
411/103: best_model.get_params()
411/104: f1_score(y_test,  best_model.predict(X_test_svd))
411/105:
df_features_importance = pd.DataFrame(
    {'feature_names': best_model_2.feature_names_,
     'feature_importances': best_model_2.feature_importances_
    })
411/106: df_features_importance.sort_values('feature_importances', ascending=False)[:10]
411/107: df_test_valid = pd.read_csv("../data/test.csv")
411/108:  best_model.predict(X_test_svd_f)
411/109: y_predict_val = best_model.predict(X_test_svd_f)
411/110:
df_t = df_test[['id']]
df_t['target'] = y_predict_val
df_t.to_csv("catboost_1.csv", index=False)
411/111: df_test_valid = pd.read_csv("../data/test.csv")
411/112:
df_t = df_test_valid[['id']]
df_t['target'] = y_predict_val
df_t.to_csv("catboost_1.csv", index=False)
411/113: df_t
411/114: df_t.describe
411/115: df_t.describe()
411/116:
def objective(trial):
    global gbm
    train_x = X_train_svd
    valid_x = X_test_svd
    train_y = y_train_np
    valid_y = y_test_np
    
    param = {
        "objective": trial.suggest_categorical("objective", ["Logloss", "CrossEntropy"]),
        "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.01, 0.1, log=True),
        "depth": trial.suggest_int("depth", 1, 6),
        "boosting_type": trial.suggest_categorical("boosting_type", ["Ordered", "Plain"]),
        "bootstrap_type": trial.suggest_categorical(
            "bootstrap_type", ["Bayesian", "Bernoulli", "MVS"]
        ),
        "eval_metric": "F1",
    }

    if param["bootstrap_type"] == "Bayesian":
        param["bagging_temperature"] = trial.suggest_float("bagging_temperature", 0, 10)
    elif param["bootstrap_type"] == "Bernoulli":
        param["subsample"] = trial.suggest_float("subsample", 0.1, 1, log=True)
    
    gbm = CatBoostClassifier(**param)

    pruning_callback = CatBoostPruningCallback(trial, "F1")
    
    gbm.fit(
        train_x,
        train_y,
        eval_set=[(valid_x, valid_y)],
        verbose=0,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=gbm)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    preds = gbm.predict(valid_x)
    f1 = f1_score(valid_y, preds)
    return f1
411/117:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="maximize")
study.optimize(objective, n_trials=100, timeout=600, callbacks=[callback])
411/118: best_model=study.user_attrs["best_booster"]
411/119: best_model.get_params()
411/120: f1_score(y_test,  best_model.predict(X_test_svd))
411/121:
df_features_importance = pd.DataFrame(
    {'feature_names': best_model_2.feature_names_,
     'feature_importances': best_model_2.feature_importances_
    })
411/122: df_features_importance.sort_values('feature_importances', ascending=False)[:10]
411/123:
df_features_importance = pd.DataFrame(
    {'feature_names': best_model.feature_names_,
     'feature_importances': best_model.feature_importances_
    })
411/124: df_features_importance.sort_values('feature_importances', ascending=False)[:10]
411/125:
def objective(trial):
    global gbm
    train_x = X_train_svd
    valid_x = X_test_svd
    train_y = y_train_np
    valid_y = y_test_np
    
    param = {
        "objective": trial.suggest_categorical("objective", ["Logloss", "CrossEntropy"]),
        "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.01, 0.1, log=True),
        "depth": trial.suggest_int("depth", 1, 6),
        "boosting_type": trial.suggest_categorical("boosting_type", ["Ordered", "Plain"]),
        "bootstrap_type": trial.suggest_categorical(
            "bootstrap_type", ["Bayesian", "Bernoulli", "MVS"]
        ),
        "eval_metric": "Accuracy",
    }

    if param["bootstrap_type"] == "Bayesian":
        param["bagging_temperature"] = trial.suggest_float("bagging_temperature", 0, 10)
    elif param["bootstrap_type"] == "Bernoulli":
        param["subsample"] = trial.suggest_float("subsample", 0.1, 1, log=True)
    
    gbm = CatBoostClassifier(**param)

    pruning_callback = CatBoostPruningCallback(trial, "Accuracy")
    
    gbm.fit(
        train_x,
        train_y,
        eval_set=[(valid_x, valid_y)],
        verbose=0,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=gbm)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    preds = gbm.predict(valid_x)
    acc = accuracy_score(valid_y, preds)
    return f1
411/126:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="maximize")
study.optimize(objective, n_trials=100, timeout=600, callbacks=[callback])
411/127:
def objective(trial):
    global gbm
    train_x = X_train_svd
    valid_x = X_test_svd
    train_y = y_train_np
    valid_y = y_test_np
    
    param = {
        "objective": trial.suggest_categorical("objective", ["Logloss", "CrossEntropy"]),
        "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.01, 0.1, log=True),
        "depth": trial.suggest_int("depth", 1, 6),
        "boosting_type": trial.suggest_categorical("boosting_type", ["Ordered", "Plain"]),
        "bootstrap_type": trial.suggest_categorical(
            "bootstrap_type", ["Bayesian", "Bernoulli", "MVS"]
        ),
        "eval_metric": "Accuracy",
    }

    if param["bootstrap_type"] == "Bayesian":
        param["bagging_temperature"] = trial.suggest_float("bagging_temperature", 0, 10)
    elif param["bootstrap_type"] == "Bernoulli":
        param["subsample"] = trial.suggest_float("subsample", 0.1, 1, log=True)
    
    gbm = CatBoostClassifier(**param)

    pruning_callback = CatBoostPruningCallback(trial, "Accuracy")
    
    gbm.fit(
        train_x,
        train_y,
        eval_set=[(valid_x, valid_y)],
        verbose=0,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=gbm)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    preds = gbm.predict(valid_x)
    acc = accuracy_score(valid_y, preds)
    return acc
411/128:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="maximize")
study.optimize(objective, n_trials=100, timeout=600, callbacks=[callback])
411/129: best_model=study.user_attrs["best_booster"]
411/130: best_model.get_params()
411/131: f1_score(y_test,  best_model.predict(X_test_svd))
411/132: accuracy_score(y_test,  best_model.predict(X_test_svd))
411/133:
df_features_importance = pd.DataFrame(
    {'feature_names': best_model.feature_names_,
     'feature_importances': best_model.feature_importances_
    })
411/134: df_features_importance.sort_values('feature_importances', ascending=False)[:10]
411/135: df_test_valid = pd.read_csv("../data/test.csv")
411/136: y_predict_val = best_model.predict(X_test_svd_f)
411/137:
df_t = df_test_valid[['id']]
df_t['target'] = y_predict_val
df_t.to_csv("catboost_2.csv", index=False)
411/138: df_t.describe()
411/139:
def objective(trial):
    global gbm
    train_x = X_train_svd
    valid_x = X_test_svd
    train_y = y_train_np
    valid_y = y_test_np
    
    param = {
        "objective": trial.suggest_categorical("objective", ["Logloss", "CrossEntropy"]),
        "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.01, 0.1, log=True),
        "depth": trial.suggest_int("depth", 1, 6),
        "boosting_type": trial.suggest_categorical("boosting_type", ["Ordered", "Plain"]),
        "bootstrap_type": trial.suggest_categorical(
            "bootstrap_type", ["Bayesian", "Bernoulli", "MVS"]
        ),
        "eval_metric": "F1",
    }

    if param["bootstrap_type"] == "Bayesian":
        param["bagging_temperature"] = trial.suggest_float("bagging_temperature", 0, 10)
    elif param["bootstrap_type"] == "Bernoulli":
        param["subsample"] = trial.suggest_float("subsample", 0.1, 1, log=True)
    
    gbm = CatBoostClassifier(**param)

    pruning_callback = CatBoostPruningCallback(trial, "F1")
    
    gbm.fit(
        train_x,
        train_y,
        eval_set=[(valid_x, valid_y)],
        verbose=0,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=gbm)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    preds = gbm.predict(valid_x)
    f1 = f1_score(valid_y, preds)
    return f1
411/140:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="maximize")
study.optimize(objective, n_trials=150, timeout=600, callbacks=[callback])
411/141: best_model=study.user_attrs["best_booster"]
411/142: best_model.get_params()
411/143: f1_score(y_test,  best_model.predict(X_test_svd))
411/144: accuracy_score(y_test,  best_model.predict(X_test_svd))
411/145:
## —Å—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫ prec\rec
def get_scores(y_true, y_pred, arange = np.arange(1, 100, 1), mode = True ):
    perc_lst = [np.percentile(y_pred, x) for x in arange]
    prec_lst = [precision_score(y_true, [1 if _ > x else 0 for _ in y_pred]) for x in perc_lst]
    recall_lst = [recall_score(y_true, [1 if _ > x else 0 for _ in y_pred]) for x in perc_lst]
    if mode == 'prob':
        graph_1 = []
        graph_1.append(go.Scatter(x = perc_lst, y = prec_lst, name = 'precision'))
        graph_1.append(go.Scatter(x = perc_lst, y = recall_lst, name = 'recall'))
        layout = (go.Layout(title = 'different metrics', xaxis = dict(title = 'probability_threshold'), 
                            yaxis = dict(title = 'Scores'))
                 )

        fig = go.Figure(data = graph_1, layout = layout)
        iplot(fig)

    else:

        graph_1 = []
        graph_1.append(go.Scatter(x = arange, y = prec_lst, name = 'precision'))
        graph_1.append(go.Scatter(x = arange, y = recall_lst, name = 'recall'))
        #graph_1.append(go.Scatter(x = arange, y = list(np.ones(len(arange))) * prob, name = 'random_precision'))
        layout = (go.Layout(title = 'different metrics', xaxis = dict(title = 'quantilites_threshold'), 

                            yaxis = dict(title = 'Scores'))

                 )

        fig = go.Figure(data = graph_1, layout = layout)
        iplot(fig) 
    return perc_lst, prec_lst, recall_lst
411/146:
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode, iplot
init_notebook_mode(connected = True)
411/147: get_scores(y_test, best_model.predict(X_test_svd))
411/148: get_scores(y_test, best_model.predict(X_test_svd), mode='prob')
411/149: best_model.predict(X_test_svd)
411/150: y = best_model.predict(X_test_svd)
411/151: get_scores(y_test, y, mode='prob')
411/152:
import os
import matplotlib.pyplot as plt#visualization
from PIL import  Image
%matplotlib inline
import pandas as pd
import seaborn as sns#visualization
import itertools
import warnings
warnings.filterwarnings("ignore")
import io
import plotly.offline as py#visualization
py.init_notebook_mode(connected=True)#visualization
import plotly.graph_objs as go#visualization
import plotly.tools as tls#visualization
import plotly.figure_factory as ff
411/153:
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode, iplot
init_notebook_mode(connected = True)
411/154:
## —Å—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫ prec\rec
def get_scores(y_true, y_pred, arange = np.arange(1, 100, 1), mode = True ):
    perc_lst = [np.percentile(y_pred, x) for x in arange]
    prec_lst = [precision_score(y_true, [1 if _ > x else 0 for _ in y_pred]) for x in perc_lst]
    recall_lst = [recall_score(y_true, [1 if _ > x else 0 for _ in y_pred]) for x in perc_lst]
    if mode == 'prob':
        graph_1 = []
        graph_1.append(go.Scatter(x = perc_lst, y = prec_lst, name = 'precision'))
        graph_1.append(go.Scatter(x = perc_lst, y = recall_lst, name = 'recall'))
        layout = (go.Layout(title = 'different metrics', xaxis = dict(title = 'probability_threshold'), 
                            yaxis = dict(title = 'Scores'))
                 )

        fig = go.Figure(data = graph_1, layout = layout)
        iplot(fig)

    else:

        graph_1 = []
        graph_1.append(go.Scatter(x = arange, y = prec_lst, name = 'precision'))
        graph_1.append(go.Scatter(x = arange, y = recall_lst, name = 'recall'))
        #graph_1.append(go.Scatter(x = arange, y = list(np.ones(len(arange))) * prob, name = 'random_precision'))
        layout = (go.Layout(title = 'different metrics', xaxis = dict(title = 'quantilites_threshold'), 

                            yaxis = dict(title = 'Scores'))

                 )

        fig = go.Figure(data = graph_1, layout = layout)
        iplot(fig) 
    return perc_lst, prec_lst, recall_lst
411/155: y = best_model.predict(X_test_svd)
411/156: get_scores(y_test, y, mode='prob')
411/157: get_scores(y_test, y)
411/158:
y = best_model.predict_proba
(X_test_svd)
411/159: y = best_model.predict_proba(X_test_svd)
411/160: get_scores(y_test, y)
411/161: y
411/162: y[:,1]
411/163: get_scores(y_test, y[:,1])
411/164: get_scores(y_test, y[:,1], mode='prob')
411/165: y = best_model.predict(X_test_svd)
411/166: precision_score(y_test, y)
411/167: recall_score(y_test, y)
411/168: get_scores(y_test, y[:,1])
411/169: y = best_model.predict_proba(X_test_svd)
411/170: y[:,1]
411/171: get_scores(y_test, y[:,1])
411/172: get_scores(y_test, y[:,1], mode='prob')
411/173: y = best_model.predict_proba(X_test_svd)
411/174: y[:,1]
411/175: y_04 = (y >= 0.4).astype(int)
411/176: recall_score(y_test, y_04)
411/177: y_04
411/178: y_04 = (y[:,1] >= 0.4).astype(int)
411/179: y_04
411/180: recall_score(y_test, y_04)
411/181: y_04 = (y[:,1] >= 0.1).astype(int)
411/182: y_04
411/183: recall_score(y_test, y_04)
411/184: precision_score(y_test, y)
411/185: precision_score(y_test, y_04)
411/186: y_04 = (y[:,1] >= 0.8).astype(int)
411/187: precision_score(y_test, y_04)
411/188: recall_score(y_test, y_04)
411/189:
#functions for searching best threshold
def to_labels(pos_probs, threshold):
    return (pos_probs >= threshold).astype('int')

def best_tresholds(y_pred_proba, y_test):
    # define thresholds
    thresholds = np.arange(0, 1, 0.001)
    # evaluate each threshold
    scores = [f1_score(y_test, to_labels(y_pred_proba, t)) for t in thresholds]
    # get best threshold
    ix = np.argmax(scores)
    #print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
    return thresholds[ix], scores[ix]
411/190: y
411/191: best_tresholds(y[:,1], y_test)
412/1:
import pandas as pd
import numpy as np
import catboost
import optuna
import pickle
import os
from scipy import sparse
412/2:
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import f1_score, accuracy_score
412/3:
import pandas as pd
import numpy as np
import catboost
import optuna
import pickle
import os
from scipy import sparse
413/1:
import pandas as pd
import numpy as np
import catboost
import optuna
import pickle
import os
from scipy import sparse
413/2:
import pandas as pd
import numpy as np
import catboost
import optuna
import pickle
import os
from scipy import sparse
413/3:
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import f1_score, accuracy_score
413/4:
from catboost import CatBoostClassifier, Pool
from optuna.integration import CatBoostPruningCallback
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
413/5:
import os
import matplotlib.pyplot as plt#visualization
#from PIL import  Image
%matplotlib inline
import seaborn as sns#visualization
#import itertools
#import warnings
#warnings.filterwarnings("ignore")
#import io
import plotly.offline as py #visualization
py.init_notebook_mode(connected=True)#visualization
import plotly.graph_objs as go#visualization
import plotly.tools as tls#visualization
import plotly.figure_factory as ff
413/6:
X_train_tfidf_minmax = sparse.load_npz("../data/X_train_tfidf_minmax.npz")
X_test_tfidf_minmax = sparse.load_npz("../data/X_test_tfidf_minmax.npz")
#####
X_test_tfidf_minmax_f = sparse.load_npz("../data/X_test_tfidf_minmax_f.npz")
#####
y_train_np = np.load('../data/y_train.npy')
y_test_np = np.load('../data/y_test.npy')
413/7:
### SVD
svd = pickle.load(open('../classical_ML/TruncatedSVD_500.pickle', 'rb'))
413/8:
X_train_svd = svd.transform(X_train_tfidf_minmax)
X_test_svd = svd.transform(X_test_tfidf_minmax)
X_test_svd_f = svd.transform(X_test_tfidf_minmax_f)
413/9:
print(X_train_svd.shape)
print(X_test_svd.shape)
print(X_test_svd_f.shape)
413/10:
def objective(trial):
    global gbm
    train_x = X_train_svd
    valid_x = X_test_svd
    train_y = y_train_np
    valid_y = y_test_np
    
    param = {
        "objective": trial.suggest_categorical("objective", ["Logloss", "CrossEntropy"]),
        "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.01, 0.1, log=True),
        "depth": trial.suggest_int("depth", 1, 6),
        "boosting_type": trial.suggest_categorical("boosting_type", ["Ordered", "Plain"]),
        "bootstrap_type": trial.suggest_categorical(
            "bootstrap_type", ["Bayesian", "Bernoulli", "MVS"]
        ),
        "eval_metric": "F1",
    }

    if param["bootstrap_type"] == "Bayesian":
        param["bagging_temperature"] = trial.suggest_float("bagging_temperature", 0, 10)
    elif param["bootstrap_type"] == "Bernoulli":
        param["subsample"] = trial.suggest_float("subsample", 0.1, 1, log=True)
    
    gbm = CatBoostClassifier(**param)

    pruning_callback = CatBoostPruningCallback(trial, "F1")
    
    gbm.fit(
        train_x,
        train_y,
        eval_set=[(valid_x, valid_y)],
        verbose=0,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=gbm)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    preds = gbm.predict(valid_x)
    f1 = f1_score(valid_y, preds)
    return f1
413/11:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="maximize")
study.optimize(objective, n_trials=150, timeout=600, callbacks=[callback])
413/12:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="maximize")
study.optimize(objective, n_trials=150, timeout=600)
413/13: y_predict_val = best_model.predict(X_test_svd_f)
413/14:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
413/15: study.user_attrs
413/16:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
413/17:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="maximize")
study.optimize(objective, n_trials=150, timeout=600, callbacks=[callback])
413/18:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
413/19:
print('F1 score: ', f1_score(y_test, best_model.predict(X_test_svd)))
print('Accuracy: ', accuracy_score(y_test, best_model.predict(X_test_svd)))
413/20:
print('F1 score: ', f1_score(y_test_np, best_model.predict(X_test_svd)))
print('Accuracy: ', accuracy_score(y_test_np, best_model.predict(X_test_svd)))
413/21:
# Create df with all features and importance of them
df_features_importance = pd.DataFrame(
    {'feature_names': best_model.feature_names_,
     'feature_importances': best_model.feature_importances_
    })
413/22:
#Check features with the highest importance. If one feature is more important than others it could be a leak
df_features_importance.sort_values('feature_importances', ascending=False)[:10]
413/23: y_predict_prob = best_model.predict_prob(X_test_svd)
413/24: y_predict_prob = best_model.predict_proba(X_test_svd)
413/25:
## Chart of precision and recall for quantiles
def get_scores(y_true, y_pred, arange = np.arange(1, 100, 1), mode = True ):
    perc_lst = [np.percentile(y_pred, x) for x in arange]
    prec_lst = [precision_score(y_true, [1 if _ > x else 0 for _ in y_pred]) for x in perc_lst]
    recall_lst = [recall_score(y_true, [1 if _ > x else 0 for _ in y_pred]) for x in perc_lst]
    if mode == 'prob':
        graph_1 = []
        graph_1.append(go.Scatter(x = perc_lst, y = prec_lst, name = 'precision'))
        graph_1.append(go.Scatter(x = perc_lst, y = recall_lst, name = 'recall'))
        layout = (go.Layout(title = 'different metrics', xaxis = dict(title = 'probability_threshold'), 
                            yaxis = dict(title = 'Scores'))
                 )

        fig = go.Figure(data = graph_1, layout = layout)
        iplot(fig)

    else:

        graph_1 = []
        graph_1.append(go.Scatter(x = arange, y = prec_lst, name = 'precision'))
        graph_1.append(go.Scatter(x = arange, y = recall_lst, name = 'recall'))
        #graph_1.append(go.Scatter(x = arange, y = list(np.ones(len(arange))) * prob, name = 'random_precision'))
        layout = (go.Layout(title = 'different metrics', xaxis = dict(title = 'quantilites_threshold'), 

                            yaxis = dict(title = 'Scores'))

                 )

        fig = go.Figure(data = graph_1, layout = layout)
        iplot(fig) 
    return perc_lst, prec_lst, recall_lst
413/26:
#functions for searching best threshold
def to_labels(pos_probs, threshold):
    return (pos_probs >= threshold).astype('int')

def best_tresholds(y_pred_proba, y_test):
    # define thresholds
    thresholds = np.arange(0, 1, 0.001)
    # evaluate each threshold
    scores = [f1_score(y_test, to_labels(y_pred_proba, t)) for t in thresholds]
    # get best threshold
    ix = np.argmax(scores)
    #print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
    return thresholds[ix], scores[ix]
413/27:
## Let's find the best treshold
best_tresholds(y_predict_prob[:,1], y_test)
413/28:
## Let's find the best treshold
best_tresholds(y_predict_prob[:,1], y_test_np)
413/29:
## Also we can check the threshold on the chart. 
# The point where precision and recall curves are crossed is the best threshold
get_scores(y_test, y_predict_prob[:,1], mode='prob')
413/30:
## Also we can check the threshold on the chart. 
# The point where precision and recall curves are crossed is the best threshold
get_scores(y_test_np, y_predict_prob[:,1], mode='prob')
413/31:
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score
413/32:
## Also we can check the threshold on the chart. 
# The point where precision and recall curves are crossed is the best threshold
get_scores(y_test_np, y_predict_prob[:,1], mode='prob')
413/33:
import os
import matplotlib.pyplot as plt#visualization
#from PIL import  Image
%matplotlib inline
import seaborn as sns#visualization
#import itertools
#import warnings
#warnings.filterwarnings("ignore")
#import io
import plotly.offline as py #visualization
py.init_notebook_mode(connected=True)#visualization
import plotly.graph_objs as go#visualization
import plotly.tools as tls#visualization
import plotly.figure_factory as ff
from plotly.offline import init_notebook_mode, iplot
413/34:
## Also we can check the threshold on the chart. 
# The point where precision and recall curves are crossed is the best threshold
get_scores(y_test_np, y_predict_prob[:,1], mode='prob')
413/35:
best_treshold=0.453
y_val = best_model.predict_proba()[:,1]
413/36:
best_treshold=0.453
y_val = best_model.predict_proba(X_test_svd_f)[:,1]
413/37:
### for kaggle
df_test_valid = pd.read_csv("../data/test.csv")
y_predict_val = best_model.predict(X_test_svd_f)
df_t = df_test[['id']]
df_t['target'] = (y_val >= best_treshold).astype(int)
df_t.to_csv("catboost_3.csv", index=False)
413/38:
### for kaggle
df_test_valid = pd.read_csv("../data/test.csv")
y_predict_val = best_model.predict(X_test_svd_f)
df_t = df_test_valid[['id']]
df_t['target'] = (y_val >= best_treshold).astype(int)
df_t.to_csv("catboost_3.csv", index=False)
413/39: df_t.describe()
413/40:
### for kaggle
df_test_valid = pd.read_csv("../data/test.csv")
y_predict_val = best_model.predict(X_test_svd_f)
df_t = df_test_valid[['id']]
df_t['target'] = (y_val >= .5).astype(int)
df_t.to_csv("catboost_4.csv", index=False)
413/41: df_t.describe()
413/42:
## Also we can check the threshold on the chart. 
# The point where precision and recall curves are crossed is close to the best threshold
perc_lst, prec_lst, recall_lst = get_scores(y_test_np, y_predict_prob[:,1], mode='prob')
416/1:
import tensorflow_hub as hub
import tokenization
module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'
bert_layer = hub.KerasLayer(module_url, trainable=True)
416/2:
import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow_hub as hub
import logging
logging.basicConfig(level=logging.INFO)
416/3:
import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow_hub as hub
import logging
logging.basicConfig(level=logging.INFO)
416/4:
#import tensorflow_hub as hub
import tokenization
module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'
bert_layer = hub.KerasLayer(module_url, trainable=True)
416/5: !wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py
416/6: !get --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py
416/7: !pip install tokenization
416/8:
#import tensorflow_hub as hub
import tokenization
module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'
bert_layer = hub.KerasLayer(module_url, trainable=True)
416/9: tokenization
416/10: train=pd.read_json("embold_train.json").reset_index(drop=True)
416/11: train=pd.read_json("embold_train.json").reset_index(drop=True)
416/12:
train=pd.read_json("embold_train.json").reset_index(drop=True)
test=pd.read_json("embold_test.json").reset_index(drop=True)
416/13: train
416/14: train.label.value_counts()
416/15: train.head(3)
416/16: train['Review'] = (train['title'].map(str) +' '+ train['body']).apply(lambda row: row.strip())
416/17: train
416/18: test['Review'] = (test['title'].map(str) +' '+ test['body']).apply(lambda row: row.strip())
416/19: train
416/20:
vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)
416/21: from bert import tokenization
416/22: !pip install bert-tensorflow
416/23: from bert import tokenization
416/24:
vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)
416/25:
def bert_encode(texts, tokenizer, max_len=512):
    all_tokens = []
    all_masks = []
    all_segments = []
416/26:
def bert_encode(texts, tokenizer, max_len=512):
    all_tokens = []
    all_masks = []
    all_segments = []
    
    for text in texts:
        text = tokenizer.tokenize(text)
            
        text = text[:max_len-2]
        input_sequence = ["[CLS]"] + text + ["[SEP]"]
        pad_len = max_len - len(input_sequence)
        
        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len
        pad_masks = [1] * len(input_sequence) + [0] * pad_len
        segment_ids = [0] * max_len
        
        all_tokens.append(tokens)
        all_masks.append(pad_masks)
        all_segments.append(segment_ids)
    
    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)
416/27:
def build_model(bert_layer, max_len=512):
    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_word_ids")
    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_mask")
    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="segment_ids")

    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])
    clf_output = sequence_output[:, 0, :]
    net = tf.keras.layers.Dense(64, activation='relu')(clf_output)
    net = tf.keras.layers.Dropout(0.2)(net)
    net = tf.keras.layers.Dense(32, activation='relu')(net)
    net = tf.keras.layers.Dropout(0.2)(net)
    out = tf.keras.layers.Dense(5, activation='softmax')(net)
    
    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)
    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])
    
    return model
416/28:
max_len = 150
train_input = bert_encode(train.Review.values, tokenizer, max_len=max_len)
test_input = bert_encode(test.Review.values, tokenizer, max_len=max_len)
train_labels = tf.onclick="parent.postMessage({'referent':'.tensorflow.keras'}, '*')">keras.utils.to_categorical(train.label.values, num_classes=3)
416/29: !pip install bert-tensorflow==1.0.1
416/30:
max_len = 150
train_input = bert_encode(train.Review.values, tokenizer, max_len=max_len)
test_input = bert_encode(test.Review.values, tokenizer, max_len=max_len)
train_labels = tf.onclick="parent.postMessage({'referent':'.tensorflow.keras'}, '*')">keras.utils.to_categorical(train.label.values, num_classes=3)
416/31: from bert import tokenization
416/32:
max_len = 150
train_input = bert_encode(train.Review.values, tokenizer, max_len=max_len)
test_input = bert_encode(test.Review.values, tokenizer, max_len=max_len)
train_labels = tf.onclick="parent.postMessage({'referent':'.tensorflow.keras'}, '*')">keras.utils.to_categorical(train.label.values, num_classes=3)
416/33: !pip install --upgrade tb-nightly
416/34: from bert import tokenization
416/35:
import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow_hub as hub
import logging
logging.basicConfig(level=logging.INFO)
416/36:
#import tensorflow_hub as hub
#import tokenization
module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'
bert_layer = hub.KerasLayer(module_url, trainable=True)
416/37: tokenization
416/38:
train=pd.read_json("embold_train.json").reset_index(drop=True)
test=pd.read_json("embold_test.json").reset_index(drop=True)
416/39: train.label.value_counts()
416/40: train.head(3)
416/41: train['Review'] = (train['title'].map(str) +' '+ train['body']).apply(lambda row: row.strip())
416/42: test['Review'] = (test['title'].map(str) +' '+ test['body']).apply(lambda row: row.strip())
416/43: train
416/44:
vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)
416/45:
def bert_encode(texts, tokenizer, max_len=512):
    all_tokens = []
    all_masks = []
    all_segments = []
    
    for text in texts:
        text = tokenizer.tokenize(text)
            
        text = text[:max_len-2]
        input_sequence = ["[CLS]"] + text + ["[SEP]"]
        pad_len = max_len - len(input_sequence)
        
        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len
        pad_masks = [1] * len(input_sequence) + [0] * pad_len
        segment_ids = [0] * max_len
        
        all_tokens.append(tokens)
        all_masks.append(pad_masks)
        all_segments.append(segment_ids)
    
    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)
416/46:
def build_model(bert_layer, max_len=512):
    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_word_ids")
    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_mask")
    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="segment_ids")

    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])
    clf_output = sequence_output[:, 0, :]
    net = tf.keras.layers.Dense(64, activation='relu')(clf_output)
    net = tf.keras.layers.Dropout(0.2)(net)
    net = tf.keras.layers.Dense(32, activation='relu')(net)
    net = tf.keras.layers.Dropout(0.2)(net)
    out = tf.keras.layers.Dense(5, activation='softmax')(net)
    
    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)
    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])
    
    return model
416/47:
max_len = 150
train_input = bert_encode(train.Review.values, tokenizer, max_len=max_len)
test_input = bert_encode(test.Review.values, tokenizer, max_len=max_len)
train_labels = tf.onclick="parent.postMessage({'referent':'.tensorflow.keras'}, '*')">keras.utils.to_categorical(train.label.values, num_classes=3)
416/48: !wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py
417/1:
import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow_hub as hub
import logging
logging.basicConfig(level=logging.INFO)
417/2: from bert import tokenization
417/3:
#import tensorflow_hub as hub
#import tokenization
module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'
bert_layer = hub.KerasLayer(module_url, trainable=True)
417/4: !wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py
417/5: tokenization
417/6:
train=pd.read_json("embold_train.json").reset_index(drop=True)
test=pd.read_json("embold_test.json").reset_index(drop=True)
417/7: train.label.value_counts()
417/8: train.head(3)
417/9: train['Review'] = (train['title'].map(str) +' '+ train['body']).apply(lambda row: row.strip())
417/10: test['Review'] = (test['title'].map(str) +' '+ test['body']).apply(lambda row: row.strip())
417/11: train
417/12:
vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)
417/13:
max_len = 150
train_input = bert_encode(train.Review.values, tokenizer, max_len=max_len)
test_input = bert_encode(test.Review.values, tokenizer, max_len=max_len)
train_labels = tf.onclick="parent.postMessage({'referent':'.tensorflow.keras'}, '*')">keras.utils.to_categorical(train.label.values, num_classes=3)
417/14:
vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)
418/1:
import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow_hub as hub
import logging
logging.basicConfig(level=logging.INFO)
418/2: !wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py
418/3:
#import tensorflow_hub as hub
#import tokenization
import tensorflow_hub as hub
import tokenization
module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'
bert_layer = hub.KerasLayer(module_url, trainable=True)
418/4: #tokenization
418/5:
train=pd.read_json("embold_train.json").reset_index(drop=True)
test=pd.read_json("embold_test.json").reset_index(drop=True)
418/6: train.label.value_counts()
418/7: train.head(3)
418/8: train['Review'] = (train['title'].map(str) +' '+ train['body']).apply(lambda row: row.strip())
418/9: test['Review'] = (test['title'].map(str) +' '+ test['body']).apply(lambda row: row.strip())
418/10: train
418/11:
vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)
418/12: !wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py
418/13: !wget https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py
418/14:
#import tensorflow_hub as hub
#import tokenization
import tensorflow_hub as hub
import tokenization
module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'
bert_layer = hub.KerasLayer(module_url, trainable=True)
418/15:
train=pd.read_json("embold_train.json").reset_index(drop=True)
test=pd.read_json("embold_test.json").reset_index(drop=True)
418/16: train.label.value_counts()
418/17: train.head(3)
418/18: train['Review'] = (train['title'].map(str) +' '+ train['body']).apply(lambda row: row.strip())
418/19: test['Review'] = (test['title'].map(str) +' '+ test['body']).apply(lambda row: row.strip())
418/20: train
418/21:
vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)
419/1:
import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow_hub as hub
import logging
logging.basicConfig(level=logging.INFO)
419/2:
#import tensorflow_hub as hub
#import tokenization
import tensorflow_hub as hub
import tokenization
module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'
bert_layer = hub.KerasLayer(module_url, trainable=True)
419/3: tokenization
419/4:
train=pd.read_json("embold_train.json").reset_index(drop=True)
test=pd.read_json("embold_test.json").reset_index(drop=True)
419/5: train.label.value_counts()
419/6: train.head(3)
419/7: train['Review'] = (train['title'].map(str) +' '+ train['body']).apply(lambda row: row.strip())
419/8: test['Review'] = (test['title'].map(str) +' '+ test['body']).apply(lambda row: row.strip())
419/9: train
419/10:
vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)
419/11:
max_len = 150
train_input = bert_encode(train.Review.values, tokenizer, max_len=max_len)
test_input = bert_encode(test.Review.values, tokenizer, max_len=max_len)
train_labels = tf.onclick="parent.postMessage({'referent':'.tensorflow.keras'}, '*')">keras.utils.to_categorical(train.label.values, num_classes=3)
420/1:
import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow_hub as hub
import logging
logging.basicConfig(level=logging.INFO)
420/2:
#import tensorflow_hub as hub
#import tokenization
import tensorflow_hub as hub
import tokenization
module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'
bert_layer = hub.KerasLayer(module_url, trainable=True)
420/3:
train=pd.read_json("embold_train.json").reset_index(drop=True)
test=pd.read_json("embold_test.json").reset_index(drop=True)
420/4: train.label.value_counts()
420/5: train.head(3)
420/6: train['Review'] = (train['title'].map(str) +' '+ train['body']).apply(lambda row: row.strip())
420/7: test['Review'] = (test['title'].map(str) +' '+ test['body']).apply(lambda row: row.strip())
420/8: train
420/9:
vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)
420/10:
def bert_encode(texts, tokenizer, max_len=512):
    all_tokens = []
    all_masks = []
    all_segments = []
    
    for text in texts:
        text = tokenizer.tokenize(text)
            
        text = text[:max_len-2]
        input_sequence = ["[CLS]"] + text + ["[SEP]"]
        pad_len = max_len - len(input_sequence)
        
        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len
        pad_masks = [1] * len(input_sequence) + [0] * pad_len
        segment_ids = [0] * max_len
        
        all_tokens.append(tokens)
        all_masks.append(pad_masks)
        all_segments.append(segment_ids)
    
    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)
420/11:
def build_model(bert_layer, max_len=512):
    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_word_ids")
    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_mask")
    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="segment_ids")

    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])
    clf_output = sequence_output[:, 0, :]
    net = tf.keras.layers.Dense(64, activation='relu')(clf_output)
    net = tf.keras.layers.Dropout(0.2)(net)
    net = tf.keras.layers.Dense(32, activation='relu')(net)
    net = tf.keras.layers.Dropout(0.2)(net)
    out = tf.keras.layers.Dense(5, activation='softmax')(net)
    
    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)
    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])
    
    return model
420/12:
max_len = 150
train_input = bert_encode(train.Review.values, tokenizer, max_len=max_len)
test_input = bert_encode(test.Review.values, tokenizer, max_len=max_len)
train_labels = tf.onclick="parent.postMessage({'referent':'.tensorflow.keras'}, '*')">keras.utils.to_categorical(train.label.values, num_classes=3)
420/13: import keras
420/14:
max_len = 150
train_input = bert_encode(train.Review.values, tokenizer, max_len=max_len)
test_input = bert_encode(test.Review.values, tokenizer, max_len=max_len)
train_labels = tf.onclick="parent.postMessage({'referent':'.tensorflow.keras'}, '*')">keras.utils.to_categorical(train.label.values, num_classes=3)
421/1:
import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow_hub as hub
import logging
logging.basicConfig(level=logging.INFO)
421/2: #!wget https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py
421/3: #from bert import tokenization
421/4: #!pip install bert-tensorflow
421/5:
#import tensorflow_hub as hub
#import tokenization
import tensorflow_hub as hub
import tokenization
module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'
bert_layer = hub.KerasLayer(module_url, trainable=True)
421/6:
train=pd.read_json("embold_train.json").reset_index(drop=True)
test=pd.read_json("embold_test.json").reset_index(drop=True)
421/7: train.label.value_counts()
421/8: train.head(3)
421/9: train['Review'] = (train['title'].map(str) +' '+ train['body']).apply(lambda row: row.strip())
421/10: test['Review'] = (test['title'].map(str) +' '+ test['body']).apply(lambda row: row.strip())
421/11: train
421/12:
vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)
421/13:
def bert_encode(texts, tokenizer, max_len=512):
    all_tokens = []
    all_masks = []
    all_segments = []
    
    for text in texts:
        text = tokenizer.tokenize(text)
            
        text = text[:max_len-2]
        input_sequence = ["[CLS]"] + text + ["[SEP]"]
        pad_len = max_len - len(input_sequence)
        
        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len
        pad_masks = [1] * len(input_sequence) + [0] * pad_len
        segment_ids = [0] * max_len
        
        all_tokens.append(tokens)
        all_masks.append(pad_masks)
        all_segments.append(segment_ids)
    
    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)
421/14:
def build_model(bert_layer, max_len=512):
    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_word_ids")
    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_mask")
    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="segment_ids")

    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])
    clf_output = sequence_output[:, 0, :]
    net = tf.keras.layers.Dense(64, activation='relu')(clf_output)
    net = tf.keras.layers.Dropout(0.2)(net)
    net = tf.keras.layers.Dense(32, activation='relu')(net)
    net = tf.keras.layers.Dropout(0.2)(net)
    out = tf.keras.layers.Dense(5, activation='softmax')(net)
    
    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)
    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])
    
    return model
421/15:
import keras
from keras.utils import np_utils
421/16:
max_len = 150
train_input = bert_encode(train.Review.values, tokenizer, max_len=max_len)
test_input = bert_encode(test.Review.values, tokenizer, max_len=max_len)
train_labels = tf.onclick="parent.postMessage({'referent':'.tensorflow.keras'}, '*')">keras.utils.np_utils.to_categorical(train.label.values, num_classes=3)
421/17:
max_len = 150
train_input = bert_encode(train.Review.values, tokenizer, max_len=max_len)
test_input = bert_encode(test.Review.values, tokenizer, max_len=max_len)
train_labels = keras.utils.np_utils.to_categorical(train.label.values, num_classes=3)
421/18:
model = build_model(bert_layer, max_len=max_len)
model.summary()
421/19:
checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)
earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)

train_history = model.fit(
    train_input, train_labels, 
    validation_split=0.2,
    epochs=3,
    callbacks=[checkpoint, earlystopping],
    batch_size=32,
    verbose=1
)
421/20: train_labels
421/21: train_labels
421/22: train_labels.shape
421/23:
def build_model(bert_layer, max_len=512):
    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_word_ids")
    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_mask")
    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="segment_ids")

    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])
    clf_output = sequence_output[:, 0, :]
    net = tf.keras.layers.Dense(64, activation='relu')(clf_output)
    net = tf.keras.layers.Dropout(0.2)(net)
    net = tf.keras.layers.Dense(32, activation='relu')(net)
    net = tf.keras.layers.Dropout(0.2)(net)
    out = tf.keras.layers.Dense(3, activation='softmax')(net)
    
    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)
    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])
    
    return model
421/24:
checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)
earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)

train_history = model.fit(
    train_input, train_labels, 
    validation_split=0.2,
    epochs=3,
    callbacks=[checkpoint, earlystopping],
    batch_size=32,
    verbose=1
)
421/25:
model = build_model(bert_layer, max_len=max_len)
model.summary()
421/26: train_labels.shape
421/27:
checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)
earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)

train_history = model.fit(
    train_input, train_labels, 
    validation_split=0.2,
    epochs=3,
    callbacks=[checkpoint, earlystopping],
    batch_size=32,
    verbose=1
)
421/28:
checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)
earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)

train_history = model.fit(
    train_input, train_labels, 
    validation_split=0.2,
    epochs=3,
    callbacks=[checkpoint, earlystopping],
    batch_size=16,
    verbose=1
)
421/29:
checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)
earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)

train_history = model.fit(
    train_input, train_labels, 
    validation_split=0.2,
    epochs=3,
    callbacks=[checkpoint, earlystopping],
    batch_size=32,
    verbose=1
)
421/30: train_labels[:1500].shape
421/31:
checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)
earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)

train_history = model.fit(
    train_input[:1500], train_labels[:1500], 
    validation_split=0.2,
    epochs=3,
    callbacks=[checkpoint, earlystopping],
    batch_size=32,
    verbose=1
)
421/32: train_short = train[:1500]
421/33: test.shape
421/34:
train_short = train[:1500]
test_short = test[:300]
421/35:
max_len = 150
train_input = bert_encode(train_short.Review.values, tokenizer, max_len=max_len)
test_input = bert_encode(test_short.Review.values, tokenizer, max_len=max_len)
train_labels = keras.utils.np_utils.to_categorical(train_short.label.values, num_classes=3)
421/36:
model = build_model(bert_layer, max_len=max_len)
model.summary()
422/1:
import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow_hub as hub
import logging
logging.basicConfig(level=logging.INFO)
422/2: #!wget https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py
422/3: #from bert import tokenization
422/4: #!pip install bert-tensorflow
422/5:
#import tensorflow_hub as hub
#import tokenization
import tensorflow_hub as hub
import tokenization
module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'
bert_layer = hub.KerasLayer(module_url, trainable=True)
422/6:
train=pd.read_json("embold_train.json").reset_index(drop=True)
test=pd.read_json("embold_test.json").reset_index(drop=True)
422/7: train.label.value_counts()
422/8: test.shape
422/9: train['Review'] = (train['title'].map(str) +' '+ train['body']).apply(lambda row: row.strip())
422/10: test['Review'] = (test['title'].map(str) +' '+ test['body']).apply(lambda row: row.strip())
422/11:
train_short = train[:15000]
test_short = test[:300]
422/12: train_short
422/13: train_short.label.value_counts()
422/14:
vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)
422/15:
def bert_encode(texts, tokenizer, max_len=512):
    all_tokens = []
    all_masks = []
    all_segments = []
    
    for text in texts:
        text = tokenizer.tokenize(text)
            
        text = text[:max_len-2]
        input_sequence = ["[CLS]"] + text + ["[SEP]"]
        pad_len = max_len - len(input_sequence)
        
        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len
        pad_masks = [1] * len(input_sequence) + [0] * pad_len
        segment_ids = [0] * max_len
        
        all_tokens.append(tokens)
        all_masks.append(pad_masks)
        all_segments.append(segment_ids)
    
    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)
422/16:
def build_model(bert_layer, max_len=512):
    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_word_ids")
    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_mask")
    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="segment_ids")

    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])
    clf_output = sequence_output[:, 0, :]
    net = tf.keras.layers.Dense(64, activation='relu')(clf_output)
    net = tf.keras.layers.Dropout(0.2)(net)
    net = tf.keras.layers.Dense(32, activation='relu')(net)
    net = tf.keras.layers.Dropout(0.2)(net)
    out = tf.keras.layers.Dense(3, activation='softmax')(net)
    
    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)
    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])
    
    return model
422/17:
import keras
from keras.utils import np_utils
422/18:
max_len = 150
train_input = bert_encode(train_short.Review.values, tokenizer, max_len=max_len)
test_input = bert_encode(test_short.Review.values, tokenizer, max_len=max_len)
train_labels = keras.utils.np_utils.to_categorical(train_short.label.values, num_classes=3)
422/19:
model = build_model(bert_layer, max_len=max_len)
model.summary()
422/20:
checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)
earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)

train_history = model.fit(
    train_input, train_labels, 
    validation_split=0.2,
    epochs=3,
    callbacks=[checkpoint, earlystopping],
    batch_size=32,
    verbose=1
)
423/1:
import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow_hub as hub
import logging
logging.basicConfig(level=logging.INFO)
423/2: #!wget https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py
423/3: #from bert import tokenization
423/4: #!pip install bert-tensorflow
423/5:
#import tensorflow_hub as hub
#import tokenization
import tensorflow_hub as hub
import tokenization
module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'
bert_layer = hub.KerasLayer(module_url, trainable=True)
423/6:
train=pd.read_json("embold_train.json").reset_index(drop=True)
test=pd.read_json("embold_test.json").reset_index(drop=True)
423/7: train.label.value_counts()
423/8: test.shape
423/9: train['Review'] = (train['title'].map(str) +' '+ train['body']).apply(lambda row: row.strip())
423/10: test['Review'] = (test['title'].map(str) +' '+ test['body']).apply(lambda row: row.strip())
423/11:
train_short = train[:15000]
test_short = test[:300]
423/12:
vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)
423/13:
def bert_encode(texts, tokenizer, max_len=512):
    all_tokens = []
    all_masks = []
    all_segments = []
    
    for text in texts:
        text = tokenizer.tokenize(text)
            
        text = text[:max_len-2]
        input_sequence = ["[CLS]"] + text + ["[SEP]"]
        pad_len = max_len - len(input_sequence)
        
        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len
        pad_masks = [1] * len(input_sequence) + [0] * pad_len
        segment_ids = [0] * max_len
        
        all_tokens.append(tokens)
        all_masks.append(pad_masks)
        all_segments.append(segment_ids)
    
    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)
423/14:
def build_model(bert_layer, max_len=512):
    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_word_ids")
    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_mask")
    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="segment_ids")

    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])
    clf_output = sequence_output[:, 0, :]
    net = tf.keras.layers.Dense(64, activation='relu')(clf_output)
    net = tf.keras.layers.Dropout(0.2)(net)
    net = tf.keras.layers.Dense(32, activation='relu')(net)
    net = tf.keras.layers.Dropout(0.2)(net)
    out = tf.keras.layers.Dense(3, activation='softmax')(net)
    
    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)
    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])
    
    return model
423/15:
import keras
from keras.utils import np_utils
423/16:
max_len = 150
train_input = bert_encode(train_short.Review.values, tokenizer, max_len=max_len)
test_input = bert_encode(test_short.Review.values, tokenizer, max_len=max_len)
train_labels = keras.utils.np_utils.to_categorical(train_short.label.values, num_classes=3)
423/17:
model = build_model(bert_layer, max_len=max_len)
model.summary()
423/18: train_labels
423/19:
checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)
earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)

train_history = model.fit(
    train_input, train_labels, 
    validation_split=0.2,
    epochs=3,
    callbacks=[checkpoint, earlystopping],
    batch_size=32,
    verbose=1
)
423/20: tf.config.list_physical_devices('GPU')
423/21:
checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)
earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)

train_history = model.fit(
    train_input, train_labels, 
    validation_split=0.2,
    epochs=3,
    callbacks=[checkpoint, earlystopping],
    batch_size=32,
    verbose=1
)
423/22:
physical_device = tf.config.experimental.list_physical_devices('GPU')
print(f'Device found : {physical_device}')
423/23: tf.config.list_physical_devices('CPU')
423/24:
physical_device = tf.config.experimental.list_physical_devices('GPU')
print(f'Device found : {physical_device}')
423/25:
physical_device = tf.config.experimental.list_physical_devices('CPU')
print(f'Device found : {physical_device}')
423/26:
checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)
earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)

train_history = model.fit(
    train_input, train_labels, 
    validation_split=0.2,
    epochs=3,
    callbacks=[checkpoint, earlystopping],
    batch_size=32,
    verbose=1
)
423/27:
checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)
earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)

train_history = model.fit(
    train_input, train_labels, 
    validation_split=0.2,
    epochs=3,
    callbacks=[checkpoint, earlystopping],
    batch_size=8,
    verbose=1
)
423/28: tf.config.list_physical_devices('GPU')
423/29:
checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)
earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)

train_history = model.fit(
    train_input, train_labels, 
    validation_split=0.2,
    epochs=1,
    callbacks=[checkpoint, earlystopping],
    batch_size=4,
    verbose=1
)
423/30:
train=pd.read_json("embold_train.json").reset_index(drop=True)
test=pd.read_json("embold_test.json").reset_index(drop=True)
423/31: train
423/32: train.to_csv('embold_train.csv')
423/33:
train.to_csv('embold_train.csv')
test.to_csv('embold_test.csv')
425/1: # https://www.tensorflow.org/text/tutorials/classify_text_with_bert
425/2:
import os
import shutil

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
from official.nlp import optimization  # to create AdamW optimizer

import matplotlib.pyplot as plt

tf.get_logger().setLevel('ERROR')
425/3: !conda install -q -U "tensorflow-text==2.8.*"
425/4: !conda install -q "tensorflow-text==2.8.*"
425/5: !conda install -q tf-models-official==2.7.0
425/6: !pip install -q "tensorflow-text==2.8.*"
425/7: !pip install -q tf-models-official==2.7.0
425/8: !pip install tensorflow-text==2.8.*
425/9: !pip install tensorflow-text==2.8.*
425/10: !pip install -q tf-models-official==2.7.0
425/11:
#!pip install tensorflow-text==2.8.*
#!pip install -q tf-models-official==2.7.0
425/12:
import os
import shutil

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
from official.nlp import optimization  # to create AdamW optimizer

import matplotlib.pyplot as plt

tf.get_logger().setLevel('ERROR')
427/1:
import os
import shutil

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
from official.nlp import optimization  # to create AdamW optimizer

import matplotlib.pyplot as plt

tf.get_logger().setLevel('ERROR')
428/1:
import os
import shutil

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
from official.nlp import optimization  # to create AdamW optimizer

import matplotlib.pyplot as plt

tf.get_logger().setLevel('ERROR')
428/2:
import os
import shutil

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
#from official.nlp import optimization  # to create AdamW optimizer
from tensorflow.keras.optimizers import Adam

import matplotlib.pyplot as plt

tf.get_logger().setLevel('ERROR')
428/3: bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)
428/4: bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'
428/5:
tfhub_handle_encoder = map_name_to_handle[bert_model_name]
tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]

print(f'BERT model selected           : {tfhub_handle_encoder}')
print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')
428/6:
tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'
tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'

print(f'BERT model selected           : {tfhub_handle_encoder}')
print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')
428/7: bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)
428/8: bert_model = hub.KerasLayer(tfhub_handle_encoder)
428/9:
def build_classifier_model():
    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')
    encoder_inputs = preprocessing_layer(text_input)
    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')
    outputs = encoder(encoder_inputs)
    net = outputs['pooled_output']
    net = tf.keras.layers.Dropout(0.1)(net)
    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)
    return tf.keras.Model(text_input, net)
428/10:
classifier_model = build_classifier_model()
bert_raw_result = classifier_model(tf.constant(text_test))
print(tf.sigmoid(bert_raw_result))
428/11: text_test = 'Test text.'
428/12:
def build_classifier_model():
    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')
    encoder_inputs = preprocessing_layer(text_input)
    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')
    outputs = encoder(encoder_inputs)
    net = outputs['pooled_output']
    net = tf.keras.layers.Dropout(0.1)(net)
    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)
    return tf.keras.Model(text_input, net)
428/13:
classifier_model = build_classifier_model()
bert_raw_result = classifier_model(tf.constant(text_test))
print(tf.sigmoid(bert_raw_result))
428/14:
text_test = ['this is such an amazing movie!']
text_preprocessed = bert_preprocess_model(text_test)
428/15: text_preprocessed
428/16:
def build_classifier_model():
    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')
    encoder_inputs = preprocessing_layer(text_input)
    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')
    outputs = encoder(encoder_inputs)
    net = outputs['pooled_output']
    net = tf.keras.layers.Dropout(0.1)(net)
    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)
    return tf.keras.Model(text_input, net)
428/17:
classifier_model = build_classifier_model()
bert_raw_result = classifier_model(tf.constant(text_test))
print(tf.sigmoid(bert_raw_result))
428/18: tf.keras.utils.plot_model(classifier_model)
428/19: !pip install pydot
428/20: tf.keras.utils.plot_model(classifier_model)
428/21: !conda install -c anaconda graphviz
428/22: tf.keras.utils.plot_model(classifier_model)
429/1:
import os
import shutil

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
#from official.nlp import optimization  # to create AdamW optimizer
from tensorflow.keras.optimizers import Adam

import matplotlib.pyplot as plt

tf.get_logger().setLevel('ERROR')
429/2: bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'
429/3:
tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'
tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'

print(f'BERT model selected           : {tfhub_handle_encoder}')
print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')
429/4:
bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)
bert_model = hub.KerasLayer(tfhub_handle_encoder)
429/5:
text_test = ['this is such an amazing movie!']
text_preprocessed = bert_preprocess_model(text_test)
429/6:
def build_classifier_model():
    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')
    encoder_inputs = preprocessing_layer(text_input)
    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')
    outputs = encoder(encoder_inputs)
    net = outputs['pooled_output']
    net = tf.keras.layers.Dropout(0.1)(net)
    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)
    return tf.keras.Model(text_input, net)
429/7:
classifier_model = build_classifier_model()
bert_raw_result = classifier_model(tf.constant(text_test))
print(tf.sigmoid(bert_raw_result))
429/8: #!pip install pydot
429/9: tf.keras.utils.plot_model(classifier_model)
429/10: tf.keras.utils.plot_model(classifier_model, show_shapes=True)
431/1: # https://www.tensorflow.org/text/tutorials/classify_text_with_bert
431/2:
#!pip install tensorflow-text==2.8.*
#!pip install -q tf-models-official==2.7.0
#!pip install tf-models-official
431/3: #!conda install -c anaconda graphviz
431/4:
import os
import shutil

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
#from official.nlp import optimization  # to create AdamW optimizer
from tensorflow.keras.optimizers import Adam

import matplotlib.pyplot as plt

tf.get_logger().setLevel('ERROR')
431/5: bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'
431/6:
tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'
tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'

print(f'BERT model selected           : {tfhub_handle_encoder}')
print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')
431/7:
train_data = pd.read_csv('train.csv',usecols=['id','text','target'])
test_data = pd.read_csv('test.csv',usecols=['id','text'])
431/8: import pandas as pd
431/9:
train_data = pd.read_csv('train.csv',usecols=['id','text','target'])
test_data = pd.read_csv('test.csv',usecols=['id','text'])
431/10:
bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)
bert_model = hub.KerasLayer(tfhub_handle_encoder)
431/11:
text_test = ['this is such an amazing movie!']
text_preprocessed = bert_preprocess_model(text_test)
431/12: print("max len of tweets",max([len(x.split()) for x in train_data.text]))
431/13: text_preprocessed
431/14:
def build_classifier_model():
    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')
    encoder_inputs = preprocessing_layer(text_input)
    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')
    outputs = encoder(encoder_inputs)
    net = outputs['pooled_output']
    net = tf.keras.layers.Dropout(0.1)(net)
    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)
    return tf.keras.Model(text_input, net)
431/15: text_preprocessed
431/16: bert_raw_result
431/17:
classifier_model = build_classifier_model()
bert_raw_result = classifier_model(tf.constant(text_test))
print(tf.sigmoid(bert_raw_result))
431/18: bert_raw_result
431/19:
tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'
tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'

print(f'BERT model selected           : {tfhub_handle_encoder}')
print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')
431/20:
train_data = pd.read_csv('train.csv',usecols=['id','text','target'])
test_data = pd.read_csv('test.csv',usecols=['id','text'])
431/21:
bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)
bert_model = hub.KerasLayer(tfhub_handle_encoder)
431/22:
text_test = ['this is such an amazing movie!']
text_preprocessed = bert_preprocess_model(text_test)
431/23: from transformers import TFBertModel
431/24:
import os
import shutil

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
#from official.nlp import optimization  # to create AdamW optimizer
from tensorflow.keras.optimizers import Adam

import matplotlib.pyplot as plt

tf.get_logger().setLevel('ERROR')
431/25: from transformers import TFBertModel
431/26: from transformers import TFBertModel
431/27:
import os
import shutil

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
#from official.nlp import optimization  # to create AdamW optimizer
from tensorflow.keras.optimizers import Adam

import matplotlib.pyplot as plt

tf.get_logger().setLevel('ERROR')
431/28: from transformers import TFBertModel
432/1:
import os
import shutil

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
#from official.nlp import optimization  # to create AdamW optimizer
from tensorflow.keras.optimizers import Adam

import matplotlib.pyplot as plt

tf.get_logger().setLevel('ERROR')
432/2: from transformers import TFBertModel
434/1: import transformers
435/1: import transformers
435/2: import transformers
435/3: import transformers
436/1:
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from transformers import TFBertModel
from transformers import BertTokenizerFast
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
436/2:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
436/3: #bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'
436/4:
#tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'
#tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'
#
#print(f'BERT model selected           : {tfhub_handle_encoder}')
#print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')
436/5:
train_data = pd.read_csv('train.csv',usecols=['id','text','target'])
test_data = pd.read_csv('test.csv',usecols=['id','text'])
436/6:
train_data = pd.read_csv('train.csv',usecols=['id','text','target'])
test_data = pd.read_csv('test.csv',usecols=['id','text'])
436/7: train_data
436/8:
def tokenize(text):
    # Use encoding functionality from transformers lib
    example = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=MAX_LEN,
        return_attention_mask=True,
        padding="max_length",
        truncation=True,
    )

    input_ids = np.array(example["input_ids"], dtype=np.int32)
    token_type_ids = np.array(example["token_type_ids"], dtype=np.int32)
    attention_masks = np.array(example["attention_mask"], dtype=np.int32)
    return input_ids, token_type_ids, attention_masks
436/9:
def prepare_spam_dataset(df):
    # Get features and labels from spam data
    features = df["Message"].values
    labels = df["Type"].values

    input_ids_list = []
    token_type_ids_list = []
    attention_mask_list = []
    label_list = []
    for i in range(len(features)):
        feature = features[i]

        # Encode example text
        input_ids, token_type_ids, attention_masks = tokenize(feature)
        input_ids_list.append(input_ids)
        token_type_ids_list.append(token_type_ids)
        attention_mask_list.append(attention_masks)

        # Set label to 1 if example is spam and to 0 if it's ham
        label = 1 if labels[i] == "spam" else 0
        label_list.append(label)

    return np.array(input_ids_list), np.array(token_type_ids_list), np.array(attention_mask_list), np.array(label_list).reshape(-1, 1)
436/10:
# Setup training data
train_input_ids, train_token_type_ids, train_attention_mask, y_train = prepare_spam_dataset(
    train_df)

# Setup test data
test_input_ids, test_token_type_ids, test_attention_mask, y_test = prepare_spam_dataset(
    test_df)
436/11:
# Setup training data
train_input_ids, train_token_type_ids, train_attention_mask, y_train = prepare_spam_dataset(
    train_data)

# Setup test data
test_input_ids, test_token_type_ids, test_attention_mask, y_test = prepare_spam_dataset(
    test_data)
436/12: train_data
436/13:
def prepare_spam_dataset(df):
    # Get features and labels from spam data
    features = df['text'].values
    labels = df['target'].values

    input_ids_list = []
    token_type_ids_list = []
    attention_mask_list = []
    label_list = []
    for i in range(len(features)):
        feature = features[i]

        # Encode example text
        input_ids, token_type_ids, attention_masks = tokenize(feature)
        input_ids_list.append(input_ids)
        token_type_ids_list.append(token_type_ids)
        attention_mask_list.append(attention_masks)

        # Set label to 1 if example is spam and to 0 if it's ham
        label = 1 if labels[i] == 1 else 0
        label_list.append(label)

    return np.array(input_ids_list), np.array(token_type_ids_list), np.array(attention_mask_list), np.array(label_list).reshape(-1, 1)
436/14:
# Setup training data
train_input_ids, train_token_type_ids, train_attention_mask, y_train = prepare_spam_dataset(
    train_data)

# Setup test data
test_input_ids, test_token_type_ids, test_attention_mask, y_test = prepare_spam_dataset(
    test_data)
436/15:
def tokenize(text):
    # Use encoding functionality from transformers lib
    example = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=MAX_LEN,
        return_attention_mask=True,
        padding="max_length",
        truncation=True,
    )

    input_ids = np.array(example["input_ids"], dtype=np.int32)
    token_type_ids = np.array(example["token_type_ids"], dtype=np.int32)
    attention_masks = np.array(example["attention_mask"], dtype=np.int32)
    return input_ids, token_type_ids, attention_masks
436/16:
def prepare_spam_dataset(df):
    # Get features and labels from spam data
    features = df['text'].values
    labels = df['target'].values

    input_ids_list = []
    token_type_ids_list = []
    attention_mask_list = []
    label_list = []
    for i in range(len(features)):
        feature = features[i]

        # Encode example text
        input_ids, token_type_ids, attention_masks = tokenize(feature)
        input_ids_list.append(input_ids)
        token_type_ids_list.append(token_type_ids)
        attention_mask_list.append(attention_masks)

        # Set label to 1 if example is spam and to 0 if it's ham
        label = 1 if labels[i] == 1 else 0
        label_list.append(label)

    return np.array(input_ids_list), np.array(token_type_ids_list), np.array(attention_mask_list), np.array(label_list).reshape(-1, 1)
436/17:
# Setup training data
train_input_ids, train_token_type_ids, train_attention_mask, y_train = prepare_spam_dataset(
    train_data)

# Setup test data
test_input_ids, test_token_type_ids, test_attention_mask, y_test = prepare_spam_dataset(
    test_data)
436/18: tokenizer = BertTokenizerFast(vocab_file)
436/19: tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")
436/20:
def tokenize(text):
    # Use encoding functionality from transformers lib
    example = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=MAX_LEN,
        return_attention_mask=True,
        padding="max_length",
        truncation=True,
    )

    input_ids = np.array(example["input_ids"], dtype=np.int32)
    token_type_ids = np.array(example["token_type_ids"], dtype=np.int32)
    attention_masks = np.array(example["attention_mask"], dtype=np.int32)
    return input_ids, token_type_ids, attention_masks
436/21:
def prepare_spam_dataset(df):
    # Get features and labels from spam data
    features = df['text'].values
    labels = df['target'].values

    input_ids_list = []
    token_type_ids_list = []
    attention_mask_list = []
    label_list = []
    for i in range(len(features)):
        feature = features[i]

        # Encode example text
        input_ids, token_type_ids, attention_masks = tokenize(feature)
        input_ids_list.append(input_ids)
        token_type_ids_list.append(token_type_ids)
        attention_mask_list.append(attention_masks)

        # Set label to 1 if example is spam and to 0 if it's ham
        label = 1 if labels[i] == 1 else 0
        label_list.append(label)

    return np.array(input_ids_list), np.array(token_type_ids_list), np.array(attention_mask_list), np.array(label_list).reshape(-1, 1)
436/22:
# Setup training data
train_input_ids, train_token_type_ids, train_attention_mask, y_train = prepare_spam_dataset(
    train_data)

# Setup test data
test_input_ids, test_token_type_ids, test_attention_mask, y_test = prepare_spam_dataset(
    test_data)
436/23: MAX_LEN = 128
436/24:
def tokenize(text):
    # Use encoding functionality from transformers lib
    example = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=MAX_LEN,
        return_attention_mask=True,
        padding="max_length",
        truncation=True,
    )

    input_ids = np.array(example["input_ids"], dtype=np.int32)
    token_type_ids = np.array(example["token_type_ids"], dtype=np.int32)
    attention_masks = np.array(example["attention_mask"], dtype=np.int32)
    return input_ids, token_type_ids, attention_masks
436/25:
def prepare_spam_dataset(df):
    # Get features and labels from spam data
    features = df['text'].values
    labels = df['target'].values

    input_ids_list = []
    token_type_ids_list = []
    attention_mask_list = []
    label_list = []
    for i in range(len(features)):
        feature = features[i]

        # Encode example text
        input_ids, token_type_ids, attention_masks = tokenize(feature)
        input_ids_list.append(input_ids)
        token_type_ids_list.append(token_type_ids)
        attention_mask_list.append(attention_masks)

        # Set label to 1 if example is spam and to 0 if it's ham
        label = 1 if labels[i] == 1 else 0
        label_list.append(label)

    return np.array(input_ids_list), np.array(token_type_ids_list), np.array(attention_mask_list), np.array(label_list).reshape(-1, 1)
436/26:
# Setup training data
train_input_ids, train_token_type_ids, train_attention_mask, y_train = prepare_spam_dataset(
    train_data)

# Setup test data
test_input_ids, test_token_type_ids, test_attention_mask, y_test = prepare_spam_dataset(
    test_data)
436/27:
# Setup training data
train_input_ids, train_token_type_ids, train_attention_mask, y_train = prepare_spam_dataset(
    train_data)

# Setup test data
#test_input_ids, test_token_type_ids, test_attention_mask, y_test = prepare_spam_dataset(
#    test_data)
436/28: train_input_ids
436/29: train_data[:20]
436/30: train_data[:20]
436/31:
train_short = train_data[:20]
test_shore = train_data[20:30]
436/32:
train_short = train_data[:20]
test_short = train_data[20:30]
436/33: test_short
436/34:
train_short = train_data[:20]
test_short = train_data[320:330]
436/35: test_short
436/36: test_short
436/37: train_input_ids[0]
436/38: train_input_ids[0].shape
436/39: train_data.text.len()
436/40: train_data.text
436/41: len(train_data.text)
436/42: train_data.text.str.len
436/43: train_data.text.str.len()
436/44: (train_data.text.str.len())max
436/45: (train_data.text.str.len())
436/46: train_data
436/47: train_data.target
436/48: train_data.target.reshape(-1, 1)
436/49: train_data.target.to_list().reshape(-1, 1)
436/50: np.array(train_data.target).reshape(-1, 1)
436/51:
def prepare_spam_dataset(df):
# Get features and labels from spam data
    features = df['text'].values
    labels = df['target'].values

    input_ids_list = []
    token_type_ids_list = []
    attention_mask_list = []
    label_list = []
    for i in range(len(features)):
        feature = features[i]

        # Encode example text
        input_ids, token_type_ids, attention_masks = tokenize(feature)
        input_ids_list.append(input_ids)
        token_type_ids_list.append(token_type_ids)
        attention_mask_list.append(attention_masks)

        # Set label to 1 if example is spam and to 0 if it's ham
        label = 1 if labels[i] == 1 else 0
        label_list.append(label)

    return np.array(input_ids_list), np.array(token_type_ids_list), np.array(attention_mask_list), np.array(label_list).reshape(-1, 1)
436/52:
# Setup training data
train_input_ids, train_token_type_ids, train_attention_mask, y_train = prepare_spam_dataset(
    train_data)

# Setup test data
#test_input_ids, test_token_type_ids, test_attention_mask, y_test = prepare_spam_dataset(
#    test_data)
436/53: train_input_ids[0].shape
436/54: y_train
436/55: y_train.shape
436/56: y_train
436/57:
def prepare_spam_dataset(df):
# Get features and labels from spam data
    features = df['text'].values
    labels = df['target'].values

    input_ids_list = []
    token_type_ids_list = []
    attention_mask_list = []
    label_list = []
    for i in range(len(features)):
        feature = features[i]

        # Encode example text
        input_ids, token_type_ids, attention_masks = tokenize(feature)
        input_ids_list.append(input_ids)
        token_type_ids_list.append(token_type_ids)
        attention_mask_list.append(attention_masks)

        # Set label to 1 if example is spam and to 0 if it's ham
        #label = 1 if labels[i] == 1 else 0
        label_list.append(labels)
        #label_list.append(label)

    return np.array(input_ids_list), np.array(token_type_ids_list), np.array(attention_mask_list), np.array(label_list).reshape(-1, 1)
436/58:
# Setup training data
train_input_ids, train_token_type_ids, train_attention_mask, y_train = prepare_spam_dataset(
    train_data)

# Setup test data
#test_input_ids, test_token_type_ids, test_attention_mask, y_test = prepare_spam_dataset(
#    test_data)
436/59: train_input_ids[0].shape
436/60: y_train
436/61:
# Setup training data
train_input_ids, train_token_type_ids, train_attention_mask, y_train = prepare_spam_dataset(
    train_short)

# Setup test data
test_input_ids, test_token_type_ids, test_attention_mask, y_test = prepare_spam_dataset(
    test_short)
436/62: train_input_ids[0].shape
436/63: y_train
436/64: y_train.shape
436/65:
train_short = train_data[:50]
test_short = train_data[320:330]
436/66: train_short
436/67:
def tokenize(text):
# Use encoding functionality from transformers lib
    example = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=MAX_LEN,
        return_attention_mask=True,
        padding="max_length",
        truncation=True,
    )

    input_ids = np.array(example["input_ids"], dtype=np.int32)
    token_type_ids = np.array(example["token_type_ids"], dtype=np.int32)
    attention_masks = np.array(example["attention_mask"], dtype=np.int32)
    return input_ids, token_type_ids, attention_masks
436/68:
def prepare_spam_dataset(df):
# Get features and labels from spam data
    features = df['text'].values
    labels = df['target'].values

    input_ids_list = []
    token_type_ids_list = []
    attention_mask_list = []
    label_list = []
    for i in range(len(features)):
        feature = features[i]

        # Encode example text
        input_ids, token_type_ids, attention_masks = tokenize(feature)
        input_ids_list.append(input_ids)
        token_type_ids_list.append(token_type_ids)
        attention_mask_list.append(attention_masks)

        # Set label to 1 if example is spam and to 0 if it's ham
        #label = 1 if labels[i] == 1 else 0
        label_list.append(labels)
        #label_list.append(label)

    return np.array(input_ids_list), np.array(token_type_ids_list), np.array(attention_mask_list), np.array(label_list).reshape(-1, 1)
436/69:
# Setup training data
train_input_ids, train_token_type_ids, train_attention_mask, y_train = prepare_spam_dataset(
    train_short)

# Setup test data
test_input_ids, test_token_type_ids, test_attention_mask, y_test = prepare_spam_dataset(
    test_short)
436/70: train_input_ids[0].shape
436/71: y_train.shape
436/72: train_input_ids
436/73: train_input_ids.shape
436/74:
def prepare_spam_dataset(df):
# Get features and labels from spam data
    features = df['text'].values
    labels = df['target'].values

    input_ids_list = []
    token_type_ids_list = []
    attention_mask_list = []
    label_list = []
    for i in range(len(features)):
        feature = features[i]

        # Encode example text
        input_ids, token_type_ids, attention_masks = tokenize(feature)
        input_ids_list.append(input_ids)
        token_type_ids_list.append(token_type_ids)
        attention_mask_list.append(attention_masks)

        # Set label to 1 if example is spam and to 0 if it's ham
        #label = 1 if labels[i] == 1 else 0
        label_list.append(labels[i])
        #label_list.append(label)

    return np.array(input_ids_list), np.array(token_type_ids_list), np.array(attention_mask_list), np.array(label_list).reshape(-1, 1)
436/75:
# Setup training data
train_input_ids, train_token_type_ids, train_attention_mask, y_train = prepare_spam_dataset(
    train_short)

# Setup test data
test_input_ids, test_token_type_ids, test_attention_mask, y_test = prepare_spam_dataset(
    test_short)
436/76: train_input_ids[0].shape
436/77: y_train.shape
436/78: train_input_ids.shape
436/79: y_train
436/80:
def prepare_disaster_dataset(df):
# Get features and labels from spam data
    features = df['text'].values
    labels = df['target'].values

    input_ids_list = []
    token_type_ids_list = []
    attention_mask_list = []
    label_list = []
    for i in range(len(features)):
        feature = features[i]

        # Encode example text
        input_ids, token_type_ids, attention_masks = tokenize(feature)
        input_ids_list.append(input_ids)
        token_type_ids_list.append(token_type_ids)
        attention_mask_list.append(attention_masks)

        # Set label to 1 if example is spam and to 0 if it's ham
        #label = 1 if labels[i] == 1 else 0
        label_list.append(labels[i])
        #label_list.append(label)

    return np.array(input_ids_list), np.array(token_type_ids_list), np.array(attention_mask_list), np.array(label_list).reshape(-1, 1)
436/81:
# Setup training data
train_input_ids, train_token_type_ids, train_attention_mask, y_train = prepare_disaster_dataset(
    train_short)

# Setup test data
test_input_ids, test_token_type_ids, test_attention_mask, y_test = prepare_disaster_dataset(
    test_short)
436/82: train_input_ids[0].shape
436/83:
def prepare_spam_dataset(df):
    # Get features and labels from spam data
    features = df["text"].values
    labels = df["Type"].values

    input_ids_list = []
    token_type_ids_list = []
    attention_mask_list = []
    label_list = []
    for i in range(len(features)):
        feature = features[i]

        # Encode example text
        input_ids, token_type_ids, attention_masks = tokenize(feature)
        input_ids_list.append(input_ids)
        token_type_ids_list.append(token_type_ids)
        attention_mask_list.append(attention_masks)

        # Set label to 1 if example is spam and to 0 if it's ham
        label = 1 if labels[i] == "spam" else 0
        label_list.append(label)

    return np.array(input_ids_list), np.array(token_type_ids_list), np.array(attention_mask_list), np.array(label_list).reshape(-1, 1)


# Load and split dataset
spam_file = "./spam.csv"
spam_df = pd.read_csv(spam_file, sep="\t")
train_df, test_df = train_test_split(spam_df)

# Setup training data
train_input_ids, train_token_type_ids, train_attention_mask, y_train = prepare_spam_dataset(
    train_df)

# Setup test data
test_input_ids, test_token_type_ids, test_attention_mask, y_test = prepare_spam_dataset(
    test_df)
436/84:

def create_model(max_len, classifier_layer=True):
    # Load tiny BERT model
    encoder = TFBertModel.from_pretrained(
        "google/bert_uncased_L-2_H-128_A-2", from_pt=True)

    # Setup input layer
    input_ids = layers.Input(
        shape=(max_len,), dtype=tf.int32, name="input_ids")
    token_type_ids = layers.Input(
        shape=(max_len,), dtype=tf.int32, name="token_type_ids")
    attention_mask = layers.Input(
        shape=(max_len,), dtype=tf.int32, name="attention_mask")
    bert_encoder = encoder(
        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)

    # Make sure BERT weights stay the same during training
    bert.trainable = False
    
    outputs = bert_encoder(preprocessed_text)
    l = layers.Dropout(0.1, name="dropout")(outputs['pooled_output'])
    l = layers.Dense(1, activation='sigmoid', name="output")(l)

    # For python training we add a classification layer
#    if classifier_layer:
#        bert = layers.Dense(1, activation="sigmoid")(bert)
#
#    # For TFJS we just add a layer to flatten the output
#    else:
#        bert = layers.Flatten()(bert)

    # Put model together
    model = keras.Model(
        inputs=[input_ids, token_type_ids, attention_mask],
        outputs=[l],
    )
    loss = keras.losses.BinaryCrossentropy(from_logits=False)
    optimizer = keras.optimizers.Adam(lr=0.0001)
    model.compile(optimizer=optimizer, loss=[loss], metrics=["accuracy"])

    return model
436/85: model = create_model(MAX_LEN)
436/86:

def create_model(max_len, classifier_layer=True):
    # Load tiny BERT model
    encoder = TFBertModel.from_pretrained(
        "google/bert_uncased_L-2_H-128_A-2", from_pt=True)

    # Setup input layer
    input_ids = layers.Input(
        shape=(max_len,), dtype=tf.int32, name="input_ids")
    token_type_ids = layers.Input(
        shape=(max_len,), dtype=tf.int32, name="token_type_ids")
    attention_mask = layers.Input(
        shape=(max_len,), dtype=tf.int32, name="attention_mask")
    bert_encoder = encoder(
        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)

    # Make sure BERT weights stay the same during training
    bert_encoder.trainable = False
    
    outputs = bert_encoder(preprocessed_text)
    l = layers.Dropout(0.1, name="dropout")(outputs['pooled_output'])
    l = layers.Dense(1, activation='sigmoid', name="output")(l)

    # For python training we add a classification layer
#    if classifier_layer:
#        bert = layers.Dense(1, activation="sigmoid")(bert)
#
#    # For TFJS we just add a layer to flatten the output
#    else:
#        bert = layers.Flatten()(bert)

    # Put model together
    model = keras.Model(
        inputs=[input_ids, token_type_ids, attention_mask],
        outputs=[l],
    )
    loss = keras.losses.BinaryCrossentropy(from_logits=False)
    optimizer = keras.optimizers.Adam(lr=0.0001)
    model.compile(optimizer=optimizer, loss=[loss], metrics=["accuracy"])

    return model
436/87: model = create_model(MAX_LEN)
436/88:

def create_model(max_len, classifier_layer=True):
    # Load tiny BERT model
    encoder = TFBertModel.from_pretrained(
        "google/bert_uncased_L-2_H-128_A-2", from_pt=True)

    # Setup input layer
    input_ids = layers.Input(
        shape=(max_len,), dtype=tf.int32, name="input_ids")
    token_type_ids = layers.Input(
        shape=(max_len,), dtype=tf.int32, name="token_type_ids")
    attention_mask = layers.Input(
        shape=(max_len,), dtype=tf.int32, name="attention_mask")
    bert_encoder = encoder(
        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)

    # Make sure BERT weights stay the same during training
    bert_encoder.trainable = False
    
 #   outputs = bert_encoder(preprocessed_text)
    l = layers.Dropout(0.1, name="dropout")(bert_encoder['pooled_output'])
    l = layers.Dense(1, activation='sigmoid', name="output")(l)

    # For python training we add a classification layer
#    if classifier_layer:
#        bert = layers.Dense(1, activation="sigmoid")(bert)
#
#    # For TFJS we just add a layer to flatten the output
#    else:
#        bert = layers.Flatten()(bert)

    # Put model together
    model = keras.Model(
        inputs=[input_ids, token_type_ids, attention_mask],
        outputs=[l],
    )
    loss = keras.losses.BinaryCrossentropy(from_logits=False)
    optimizer = keras.optimizers.Adam(lr=0.0001)
    model.compile(optimizer=optimizer, loss=[loss], metrics=["accuracy"])

    return model
436/89: model = create_model(MAX_LEN)
436/90:

def create_model(max_len, classifier_layer=True):
    # Load tiny BERT model
    encoder = TFBertModel.from_pretrained(
        "google/bert_uncased_L-2_H-128_A-2", from_pt=True)

    # Setup input layer
    input_ids = layers.Input(
        shape=(max_len,), dtype=tf.int32, name="input_ids")
    token_type_ids = layers.Input(
        shape=(max_len,), dtype=tf.int32, name="token_type_ids")
    attention_mask = layers.Input(
        shape=(max_len,), dtype=tf.int32, name="attention_mask")
    bert_encoder = encoder(
        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)

    # Make sure BERT weights stay the same during training
    bert_encoder.trainable = False
    print(bert_encoder)
 #   outputs = bert_encoder(preprocessed_text)
    l = layers.Dropout(0.1, name="dropout")(bert_encoder['pooled_output'])
    l = layers.Dense(1, activation='sigmoid', name="output")(l)

    # For python training we add a classification layer
#    if classifier_layer:
#        bert = layers.Dense(1, activation="sigmoid")(bert)
#
#    # For TFJS we just add a layer to flatten the output
#    else:
#        bert = layers.Flatten()(bert)

    # Put model together
    model = keras.Model(
        inputs=[input_ids, token_type_ids, attention_mask],
        outputs=[l],
    )
    loss = keras.losses.BinaryCrossentropy(from_logits=False)
    optimizer = keras.optimizers.Adam(lr=0.0001)
    model.compile(optimizer=optimizer, loss=[loss], metrics=["accuracy"])

    return model
436/91: model = create_model(MAX_LEN)
436/92:

def create_model(max_len, classifier_layer=True):
    # Load tiny BERT model
    encoder = TFBertModel.from_pretrained(
        "google/bert_uncased_L-2_H-128_A-2", from_pt=True)

    # Setup input layer
    input_ids = layers.Input(
        shape=(max_len,), dtype=tf.int32, name="input_ids")
    token_type_ids = layers.Input(
        shape=(max_len,), dtype=tf.int32, name="token_type_ids")
    attention_mask = layers.Input(
        shape=(max_len,), dtype=tf.int32, name="attention_mask")
    bert_encoder = encoder(
        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)

    # Make sure BERT weights stay the same during training
    bert_encoder.trainable = False
    print(bert_encoder[0])
 #   outputs = bert_encoder(preprocessed_text)
    l = layers.Dropout(0.1, name="dropout")(bert_encoder['pooled_output'])
    l = layers.Dense(1, activation='sigmoid', name="output")(l)

    # For python training we add a classification layer
#    if classifier_layer:
#        bert = layers.Dense(1, activation="sigmoid")(bert)
#
#    # For TFJS we just add a layer to flatten the output
#    else:
#        bert = layers.Flatten()(bert)

    # Put model together
    model = keras.Model(
        inputs=[input_ids, token_type_ids, attention_mask],
        outputs=[l],
    )
    loss = keras.losses.BinaryCrossentropy(from_logits=False)
    optimizer = keras.optimizers.Adam(lr=0.0001)
    model.compile(optimizer=optimizer, loss=[loss], metrics=["accuracy"])

    return model
436/93: model = create_model(MAX_LEN)
436/94: encoder = hub.load("https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2")
436/95:
#import os
#import shutil
#
#import tensorflow as tf
import tensorflow_hub as hub
#import tensorflow_text as text
##from official.nlp import optimization  # to create AdamW optimizer
#from tensorflow.keras.optimizers import Adam
#
#import matplotlib.pyplot as plt
#
#tf.get_logger().setLevel('ERROR')
436/96: encoder = hub.load("https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2")
436/97: encoder
436/98:

def create_model(max_len, classifier_layer=True):
    # Load tiny BERT model
    encoder = TFBertModel.from_pretrained(
        "google/bert_uncased_L-2_H-128_A-2", from_pt=True)

    print(encoder)
    # Setup input layer
    input_ids = layers.Input(
        shape=(max_len,), dtype=tf.int32, name="input_ids")
    token_type_ids = layers.Input(
        shape=(max_len,), dtype=tf.int32, name="token_type_ids")
    attention_mask = layers.Input(
        shape=(max_len,), dtype=tf.int32, name="attention_mask")
    bert_encoder = encoder(
        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)

    # Make sure BERT weights stay the same during training
    bert_encoder.trainable = False
    print(bert_encoder[0])
 #   outputs = bert_encoder(preprocessed_text)
    l = layers.Dropout(0.1, name="dropout")(bert_encoder['pooled_output'])
    l = layers.Dense(1, activation='sigmoid', name="output")(l)

    # For python training we add a classification layer
#    if classifier_layer:
#        bert = layers.Dense(1, activation="sigmoid")(bert)
#
#    # For TFJS we just add a layer to flatten the output
#    else:
#        bert = layers.Flatten()(bert)

    # Put model together
    model = keras.Model(
        inputs=[input_ids, token_type_ids, attention_mask],
        outputs=[l],
    )
    loss = keras.losses.BinaryCrossentropy(from_logits=False)
    optimizer = keras.optimizers.Adam(lr=0.0001)
    model.compile(optimizer=optimizer, loss=[loss], metrics=["accuracy"])

    return model
436/99: model = create_model(MAX_LEN)
436/100:

def create_model(max_len, classifier_layer=True):
    # Load tiny BERT model
    #encoder = TFBertModel.from_pretrained(
    #    "google/bert_uncased_L-2_H-128_A-2", from_pt=True)
    
    encoder = hub.load("https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2")    
    print(encoder)
    # Setup input layer
    input_ids = layers.Input(
        shape=(max_len,), dtype=tf.int32, name="input_ids")
    token_type_ids = layers.Input(
        shape=(max_len,), dtype=tf.int32, name="token_type_ids")
    attention_mask = layers.Input(
        shape=(max_len,), dtype=tf.int32, name="attention_mask")
    bert_encoder = encoder(
        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)

    # Make sure BERT weights stay the same during training
    bert_encoder.trainable = False
    print(bert_encoder[0])
 #   outputs = bert_encoder(preprocessed_text)
    l = layers.Dropout(0.1, name="dropout")(bert_encoder['pooled_output'])
    l = layers.Dense(1, activation='sigmoid', name="output")(l)

    # For python training we add a classification layer
#    if classifier_layer:
#        bert = layers.Dense(1, activation="sigmoid")(bert)
#
#    # For TFJS we just add a layer to flatten the output
#    else:
#        bert = layers.Flatten()(bert)

    # Put model together
    model = keras.Model(
        inputs=[input_ids, token_type_ids, attention_mask],
        outputs=[l],
    )
    loss = keras.losses.BinaryCrossentropy(from_logits=False)
    optimizer = keras.optimizers.Adam(lr=0.0001)
    model.compile(optimizer=optimizer, loss=[loss], metrics=["accuracy"])

    return model
436/101: model = create_model(MAX_LEN)
436/102:

def create_model(max_len, classifier_layer=True):
    seq_length = 128
    # Load tiny BERT model
    #encoder = TFBertModel.from_pretrained(
    #    "google/bert_uncased_L-2_H-128_A-2", from_pt=True)
    
    encoder = hub.load("https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2")    
    print(encoder)
    # Setup input layer
    
    preprocessed_text = dict(
        input_word_ids=layers.Input(shape=(seq_length,), dtype=tf.int32),
        input_mask=layers.Input(shape=(seq_length,), dtype=tf.int32),
        input_type_ids=layers.Input(shape=(seq_length,), dtype=tf.int32),
    )
    
    
    #####
   # input_ids = layers.Input(
   #     shape=(max_len,), dtype=tf.int32, name="input_ids")
   # token_type_ids = layers.Input(
   #     shape=(max_len,), dtype=tf.int32, name="token_type_ids")
   # attention_mask = layers.Input(
   #     shape=(max_len,), dtype=tf.int32, name="attention_mask")
    bert_encoder = encoder(preprocessed_text)

    # Make sure BERT weights stay the same during training
    bert_encoder.trainable = False
    print(bert_encoder[0])
 #   outputs = bert_encoder(preprocessed_text)
    l = layers.Dropout(0.1, name="dropout")(bert_encoder['pooled_output'])
    l = layers.Dense(1, activation='sigmoid', name="output")(l)

    # For python training we add a classification layer
#    if classifier_layer:
#        bert = layers.Dense(1, activation="sigmoid")(bert)
#
#    # For TFJS we just add a layer to flatten the output
#    else:
#        bert = layers.Flatten()(bert)

    # Put model together
    model = keras.Model(
        inputs=[input_ids, token_type_ids, attention_mask],
        outputs=[l],
    )
    loss = keras.losses.BinaryCrossentropy(from_logits=False)
    optimizer = keras.optimizers.Adam(lr=0.0001)
    model.compile(optimizer=optimizer, loss=[loss], metrics=["accuracy"])

    return model
436/103: model = create_model(MAX_LEN)
436/104: rom tensorflow.keras import Input
436/105: from tensorflow.keras import Input
436/106:

def create_model(max_len, classifier_layer=True):
    seq_length = 128
    # Load tiny BERT model
    #encoder = TFBertModel.from_pretrained(
    #    "google/bert_uncased_L-2_H-128_A-2", from_pt=True)
    
    encoder = hub.load("https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2")    
    print(encoder)
    # Setup input layer
    
    preprocessed_text = dict(
        input_word_ids=layers.Input(shape=(seq_length,), dtype=tf.int32),
        input_mask=layers.Input(shape=(seq_length,), dtype=tf.int32),
        input_type_ids=layers.Input(shape=(seq_length,), dtype=tf.int32),
    )
    
    
    #####
   # input_ids = layers.Input(
   #     shape=(max_len,), dtype=tf.int32, name="input_ids")
   # token_type_ids = layers.Input(
   #     shape=(max_len,), dtype=tf.int32, name="token_type_ids")
   # attention_mask = layers.Input(
   #     shape=(max_len,), dtype=tf.int32, name="attention_mask")
    bert_encoder = encoder(preprocessed_text)

    # Make sure BERT weights stay the same during training
    bert_encoder.trainable = False
    print(bert_encoder[0])
 #   outputs = bert_encoder(preprocessed_text)
    l = layers.Dropout(0.1, name="dropout")(bert_encoder['pooled_output'])
    l = layers.Dense(1, activation='sigmoid', name="output")(l)

    # For python training we add a classification layer
#    if classifier_layer:
#        bert = layers.Dense(1, activation="sigmoid")(bert)
#
#    # For TFJS we just add a layer to flatten the output
#    else:
#        bert = layers.Flatten()(bert)

    # Put model together
    model = keras.Model(
        inputs=[input_ids, token_type_ids, attention_mask],
        outputs=[l],
    )
    loss = keras.losses.BinaryCrossentropy(from_logits=False)
    optimizer = keras.optimizers.Adam(lr=0.0001)
    model.compile(optimizer=optimizer, loss=[loss], metrics=["accuracy"])

    return model
436/107: model = create_model(MAX_LEN)
436/108:

def create_model(max_len, classifier_layer=True):
    seq_length = 128
    # Load tiny BERT model
    #encoder = TFBertModel.from_pretrained(
    #    "google/bert_uncased_L-2_H-128_A-2", from_pt=True)
    
    encoder = hub.load("https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2")    
    print(encoder)
    # Setup input layer
    
    preprocessed_text = dict(
        input_word_ids=f.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
        input_mask=f.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
        input_type_ids=f.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
    )
    
    
    #####
   # input_ids = layers.Input(
   #     shape=(max_len,), dtype=tf.int32, name="input_ids")
   # token_type_ids = layers.Input(
   #     shape=(max_len,), dtype=tf.int32, name="token_type_ids")
   # attention_mask = layers.Input(
   #     shape=(max_len,), dtype=tf.int32, name="attention_mask")
    bert_encoder = encoder(preprocessed_text)

    # Make sure BERT weights stay the same during training
    bert_encoder.trainable = False
    print(bert_encoder[0])
 #   outputs = bert_encoder(preprocessed_text)
    l = layers.Dropout(0.1, name="dropout")(bert_encoder['pooled_output'])
    l = layers.Dense(1, activation='sigmoid', name="output")(l)

    # For python training we add a classification layer
#    if classifier_layer:
#        bert = layers.Dense(1, activation="sigmoid")(bert)
#
#    # For TFJS we just add a layer to flatten the output
#    else:
#        bert = layers.Flatten()(bert)

    # Put model together
    model = keras.Model(
        inputs=[input_ids, token_type_ids, attention_mask],
        outputs=[l],
    )
    loss = keras.losses.BinaryCrossentropy(from_logits=False)
    optimizer = keras.optimizers.Adam(lr=0.0001)
    model.compile(optimizer=optimizer, loss=[loss], metrics=["accuracy"])

    return model
436/109:

def create_model(max_len, classifier_layer=True):
    seq_length = 128
    # Load tiny BERT model
    #encoder = TFBertModel.from_pretrained(
    #    "google/bert_uncased_L-2_H-128_A-2", from_pt=True)
    
    encoder = hub.load("https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2")    
    print(encoder)
    # Setup input layer
    
    preprocessed_text = dict(
        input_word_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
        input_mask=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
        input_type_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
    )
    
    
    #####
   # input_ids = layers.Input(
   #     shape=(max_len,), dtype=tf.int32, name="input_ids")
   # token_type_ids = layers.Input(
   #     shape=(max_len,), dtype=tf.int32, name="token_type_ids")
   # attention_mask = layers.Input(
   #     shape=(max_len,), dtype=tf.int32, name="attention_mask")
    bert_encoder = encoder(preprocessed_text)

    # Make sure BERT weights stay the same during training
    bert_encoder.trainable = False
    print(bert_encoder[0])
 #   outputs = bert_encoder(preprocessed_text)
    l = layers.Dropout(0.1, name="dropout")(bert_encoder['pooled_output'])
    l = layers.Dense(1, activation='sigmoid', name="output")(l)

    # For python training we add a classification layer
#    if classifier_layer:
#        bert = layers.Dense(1, activation="sigmoid")(bert)
#
#    # For TFJS we just add a layer to flatten the output
#    else:
#        bert = layers.Flatten()(bert)

    # Put model together
    model = keras.Model(
        inputs=[input_ids, token_type_ids, attention_mask],
        outputs=[l],
    )
    loss = keras.losses.BinaryCrossentropy(from_logits=False)
    optimizer = keras.optimizers.Adam(lr=0.0001)
    model.compile(optimizer=optimizer, loss=[loss], metrics=["accuracy"])

    return model
436/110: model = create_model(MAX_LEN)
436/111:

def create_model(max_len, classifier_layer=True):
    seq_length = 128
    # Load tiny BERT model
    #encoder = TFBertModel.from_pretrained(
    #    "google/bert_uncased_L-2_H-128_A-2", from_pt=True)
    
    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True)
    dropout = tf.keras.layers.Dropout(dropout_rate)
    dense = tf.keras.layers.Dense(num_classes)
    seq_length = 64
    
    preprocessed_text = dict(
                                input_word_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
                                input_mask=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
                                input_type_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
                            )
    encoder_outputs = encoder(preprocessed_text)
    print(bert_encoder[0])
 #   outputs = bert_encoder(preprocessed_text)
    l = layers.Dropout(0.1, name="dropout")(bert_encoder['pooled_output'])
    l = layers.Dense(1, activation='sigmoid', name="output")(l)

    # For python training we add a classification layer
#    if classifier_layer:
#        bert = layers.Dense(1, activation="sigmoid")(bert)
#
#    # For TFJS we just add a layer to flatten the output
#    else:
#        bert = layers.Flatten()(bert)

    # Put model together
    model = keras.Model(
        inputs=[input_ids, token_type_ids, attention_mask],
        outputs=[l],
    )
    loss = keras.losses.BinaryCrossentropy(from_logits=False)
    optimizer = keras.optimizers.Adam(lr=0.0001)
    model.compile(optimizer=optimizer, loss=[loss], metrics=["accuracy"])

    return model
436/112: model = create_model(MAX_LEN)
436/113: tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'
436/114:

def create_model(max_len, classifier_layer=True):
    seq_length = 128
    # Load tiny BERT model
    #encoder = TFBertModel.from_pretrained(
    #    "google/bert_uncased_L-2_H-128_A-2", from_pt=True)
    
    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True)
    dropout = tf.keras.layers.Dropout(dropout_rate)
    dense = tf.keras.layers.Dense(num_classes)
    seq_length = 64
    
    preprocessed_text = dict(
                                input_word_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
                                input_mask=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
                                input_type_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
                            )
    encoder_outputs = encoder(preprocessed_text)
    print(bert_encoder[0])
 #   outputs = bert_encoder(preprocessed_text)
    l = layers.Dropout(0.1, name="dropout")(bert_encoder['pooled_output'])
    l = layers.Dense(1, activation='sigmoid', name="output")(l)

    # For python training we add a classification layer
#    if classifier_layer:
#        bert = layers.Dense(1, activation="sigmoid")(bert)
#
#    # For TFJS we just add a layer to flatten the output
#    else:
#        bert = layers.Flatten()(bert)

    # Put model together
    model = keras.Model(
        inputs=[input_ids, token_type_ids, attention_mask],
        outputs=[l],
    )
    loss = keras.losses.BinaryCrossentropy(from_logits=False)
    optimizer = keras.optimizers.Adam(lr=0.0001)
    model.compile(optimizer=optimizer, loss=[loss], metrics=["accuracy"])

    return model
436/115: tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'
436/116: model = create_model(MAX_LEN)
436/117:

def create_model(max_len, classifier_layer=True):
    seq_length = 128
    # Load tiny BERT model
    #encoder = TFBertModel.from_pretrained(
    #    "google/bert_uncased_L-2_H-128_A-2", from_pt=True)
    
    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True)
 #   dropout = tf.keras.layers.Dropout(dropout_rate)
 #   dense = tf.keras.layers.Dense(num_classes)
    seq_length = 64
    
    preprocessed_text = dict(
                                input_word_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
                                input_mask=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
                                input_type_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
                            )
    encoder_outputs = encoder(preprocessed_text)
    print(bert_encoder[0])
 #   outputs = bert_encoder(preprocessed_text)
    l = layers.Dropout(0.1, name="dropout")(bert_encoder['pooled_output'])
    l = layers.Dense(1, activation='sigmoid', name="output")(l)

    # For python training we add a classification layer
#    if classifier_layer:
#        bert = layers.Dense(1, activation="sigmoid")(bert)
#
#    # For TFJS we just add a layer to flatten the output
#    else:
#        bert = layers.Flatten()(bert)

    # Put model together
    model = keras.Model(
        inputs=[input_ids, token_type_ids, attention_mask],
        outputs=[l],
    )
    loss = keras.losses.BinaryCrossentropy(from_logits=False)
    optimizer = keras.optimizers.Adam(lr=0.0001)
    model.compile(optimizer=optimizer, loss=[loss], metrics=["accuracy"])

    return model
436/118: model = create_model(MAX_LEN)
436/119:

def create_model(max_len, classifier_layer=True):
    seq_length = 128
    # Load tiny BERT model
    #encoder = TFBertModel.from_pretrained(
    #    "google/bert_uncased_L-2_H-128_A-2", from_pt=True)
    
    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True)
 #   dropout = tf.keras.layers.Dropout(dropout_rate)
 #   dense = tf.keras.layers.Dense(num_classes)
    seq_length = 64
    
    preprocessed_text = dict(
                                input_word_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
                                input_mask=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
                                input_type_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
                            )
    encoder_outputs = encoder(preprocessed_text)
    print(bert_encoder)
 #   outputs = bert_encoder(preprocessed_text)
    l = layers.Dropout(0.1, name="dropout")(bert_encoder['pooled_output'])
    l = layers.Dense(1, activation='sigmoid', name="output")(l)

    # For python training we add a classification layer
#    if classifier_layer:
#        bert = layers.Dense(1, activation="sigmoid")(bert)
#
#    # For TFJS we just add a layer to flatten the output
#    else:
#        bert = layers.Flatten()(bert)

    # Put model together
    model = keras.Model(
        inputs=[input_ids, token_type_ids, attention_mask],
        outputs=[l],
    )
    loss = keras.losses.BinaryCrossentropy(from_logits=False)
    optimizer = keras.optimizers.Adam(lr=0.0001)
    model.compile(optimizer=optimizer, loss=[loss], metrics=["accuracy"])

    return model
436/120: tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'
436/121: model = create_model(MAX_LEN)
436/122:

def create_model(max_len, classifier_layer=True):
    seq_length = 128
    # Load tiny BERT model
    #encoder = TFBertModel.from_pretrained(
    #    "google/bert_uncased_L-2_H-128_A-2", from_pt=True)
    
    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True)
 #   dropout = tf.keras.layers.Dropout(dropout_rate)
 #   dense = tf.keras.layers.Dense(num_classes)
    seq_length = 64
    
    preprocessed_text = dict(
                                input_word_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
                                input_mask=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
                                input_type_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
                            )
    encoder_outputs = encoder(preprocessed_text)
    print(encoder_outputs)
 #   outputs = bert_encoder(preprocessed_text)
    l = layers.Dropout(0.1, name="dropout")(encoder_outputs['pooled_output'])
    l = layers.Dense(1, activation='sigmoid', name="output")(l)

    # For python training we add a classification layer
#    if classifier_layer:
#        bert = layers.Dense(1, activation="sigmoid")(bert)
#
#    # For TFJS we just add a layer to flatten the output
#    else:
#        bert = layers.Flatten()(bert)

    # Put model together
    model = keras.Model(
        inputs=[input_ids, token_type_ids, attention_mask],
        outputs=[l],
    )
    loss = keras.losses.BinaryCrossentropy(from_logits=False)
    optimizer = keras.optimizers.Adam(lr=0.0001)
    model.compile(optimizer=optimizer, loss=[loss], metrics=["accuracy"])

    return model
436/123: tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'
436/124: model = create_model(MAX_LEN)
436/125:

def create_model(max_len, classifier_layer=True):
    seq_length = 128
    # Load tiny BERT model
    #encoder = TFBertModel.from_pretrained(
    #    "google/bert_uncased_L-2_H-128_A-2", from_pt=True)
    
    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True)
 #   dropout = tf.keras.layers.Dropout(dropout_rate)
 #   dense = tf.keras.layers.Dense(num_classes)
    seq_length = 64
    
    preprocessed_text = dict(
                                input_word_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
                                input_mask=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
                                input_type_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
                            )
    encoder_outputs = encoder(preprocessed_text)
    print(encoder_outputs)
 #   outputs = bert_encoder(preprocessed_text)
    l = layers.Dropout(0.1, name="dropout")(encoder_outputs['pooled_output'])
    l = layers.Dense(1, activation='sigmoid', name="output")(l)

    # For python training we add a classification layer
#    if classifier_layer:
#        bert = layers.Dense(1, activation="sigmoid")(bert)
#
#    # For TFJS we just add a layer to flatten the output
#    else:
#        bert = layers.Flatten()(bert)

    # Put model together
    model = keras.Model(
        inputs=preprocessed_text,
        outputs=[l],
    )
    loss = keras.losses.BinaryCrossentropy(from_logits=False)
    optimizer = keras.optimizers.Adam(lr=0.0001)
    model.compile(optimizer=optimizer, loss=[loss], metrics=["accuracy"])

    return model
436/126: tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'
436/127: model = create_model(MAX_LEN)
436/128: tf.keras.utils.plot_model(model, show_shapes=True)
436/129:

def create_model(max_len, classifier_layer=True):
    seq_length = 128
    # Load tiny BERT model
    #encoder = TFBertModel.from_pretrained(
    #    "google/bert_uncased_L-2_H-128_A-2", from_pt=True)
    
    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True)
 #   dropout = tf.keras.layers.Dropout(dropout_rate)
 #   dense = tf.keras.layers.Dense(num_classes)
    seq_length = 64
    
    preprocessed_text = dict(
                                input_word_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
                                input_mask=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
                                input_type_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
                            )
    encoder_outputs = encoder(preprocessed_text)
    print(encoder_outputs)
 #   outputs = bert_encoder(preprocessed_text)
    l = layers.Dropout(0.1, name="dropout")(encoder_outputs['pooled_output'])
    l = layers.Dense(1, activation='sigmoid', name="output")(l)

    # For python training we add a classification layer
#    if classifier_layer:
#        bert = layers.Dense(1, activation="sigmoid")(bert)
#
#    # For TFJS we just add a layer to flatten the output
#    else:
#        bert = layers.Flatten()(bert)

    # Put model together
    model = keras.Model(
        inputs=preprocessed_text,
        outputs=[l],
    )
    loss = keras.losses.BinaryCrossentropy(from_logits=False)
    optimizer = keras.optimizers.Adam(learning_rate=0.0001)
    model.compile(optimizer=optimizer, loss=[loss], metrics=["accuracy"])

    return model
436/130: tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'
436/131: model = create_model(MAX_LEN)
436/132:
#import os
#import shutil
#
#import tensorflow as tf
import tensorflow_hub as hub
#import tensorflow_text as text
##from official.nlp import optimization  # to create AdamW optimizer
#from tensorflow.keras.optimizers import Adam
#
#import matplotlib.pyplot as plt
#
#tf.get_logger().setLevel('ERROR')
436/133:
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from transformers import TFBertModel
from transformers import BertTokenizerFast
import pandas as pd
from sklearn.model_selection import train_test_split
436/134: from tensorflow.keras import Input
436/135:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
436/136: #bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'
436/137:
#tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'
#tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'
#
#print(f'BERT model selected           : {tfhub_handle_encoder}')
#print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')
436/138:
train_data = pd.read_csv('train.csv',usecols=['id','text','target'])
test_data = pd.read_csv('test.csv',usecols=['id','text'])
436/139:
#bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)
#bert_model = hub.KerasLayer(tfhub_handle_encoder)
436/140:
#text_test = ['this is such an amazing movie!']
#text_preprocessed = bert_preprocess_model(text_test)
436/141: train_data
436/142:
train_short = train_data[:50]
test_short = train_data[320:330]
436/143: tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")
436/144: MAX_LEN = 128
436/145:
def tokenize(text):
# Use encoding functionality from transformers lib
    example = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=MAX_LEN,
        return_attention_mask=True,
        padding="max_length",
        truncation=True,
    )

    input_ids = np.array(example["input_ids"], dtype=np.int32)
    token_type_ids = np.array(example["token_type_ids"], dtype=np.int32)
    attention_masks = np.array(example["attention_mask"], dtype=np.int32)
    return input_ids, token_type_ids, attention_masks
436/146:
def prepare_disaster_dataset(df):
# Get features and labels from spam data
    features = df['text'].values
    labels = df['target'].values

    input_ids_list = []
    token_type_ids_list = []
    attention_mask_list = []
    label_list = []
    for i in range(len(features)):
        feature = features[i]

        # Encode example text
        input_ids, token_type_ids, attention_masks = tokenize(feature)
        input_ids_list.append(input_ids)
        token_type_ids_list.append(token_type_ids)
        attention_mask_list.append(attention_masks)

        # Set label to 1 if example is spam and to 0 if it's ham
        #label = 1 if labels[i] == 1 else 0
        label_list.append(labels[i])
        #label_list.append(label)

    return np.array(input_ids_list), np.array(token_type_ids_list), np.array(attention_mask_list), np.array(label_list).reshape(-1, 1)
436/147:
# Setup training data
train_input_ids, train_token_type_ids, train_attention_mask, y_train = prepare_disaster_dataset(
    train_short)

# Setup test data
test_input_ids, test_token_type_ids, test_attention_mask, y_test = prepare_disaster_dataset(
    test_short)
436/148: train_input_ids[0].shape
436/149: encoder = hub.load("https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2")
436/150: encoder
436/151:

def create_model(max_len, classifier_layer=True):
    seq_length = 128
    # Load tiny BERT model
    #encoder = TFBertModel.from_pretrained(
    #    "google/bert_uncased_L-2_H-128_A-2", from_pt=True)
    
    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True)
 #   dropout = tf.keras.layers.Dropout(dropout_rate)
 #   dense = tf.keras.layers.Dense(num_classes)
    seq_length = 64
    
    preprocessed_text = dict(
                                input_word_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
                                input_mask=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
                                input_type_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
                            )
    encoder_outputs = encoder(preprocessed_text)
    print(encoder_outputs)
 #   outputs = bert_encoder(preprocessed_text)
    l = layers.Dropout(0.1, name="dropout")(encoder_outputs['pooled_output'])
    l = layers.Dense(1, activation='sigmoid', name="output")(l)

    # For python training we add a classification layer
#    if classifier_layer:
#        bert = layers.Dense(1, activation="sigmoid")(bert)
#
#    # For TFJS we just add a layer to flatten the output
#    else:
#        bert = layers.Flatten()(bert)

    # Put model together
    model = keras.Model(
        inputs=preprocessed_text,
        outputs=[l],
    )
    loss = keras.losses.BinaryCrossentropy(from_logits=False)
    optimizer = keras.optimizers.Adam(learning_rate=0.0001)
    model.compile(optimizer=optimizer, loss=[loss], metrics=["accuracy"])

    return model
436/152: tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'
436/153: model = create_model(MAX_LEN)
436/154: tf.keras.utils.plot_model(model, show_shapes=True)
436/155: !pip install pydot
436/156: tf.keras.utils.plot_model(model, show_shapes=True)
436/157: import pydot
436/158:
import pydot
import graphviz
436/159: tf.keras.utils.plot_model(model, show_shapes=True)
436/160: !conda install pydot
436/161:
import pydot
import graphviz
436/162: tf.keras.utils.plot_model(model, show_shapes=True)
436/163: !conda install graphviz
436/164:
import pydot
import graphviz
436/165: tf.keras.utils.plot_model(model, show_shapes=True)
436/166: tf.keras.utils.plot_model(model)
438/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pydot
import graphviz
from keras.utils import plot_model
438/2:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pydot
import graphviz
from tensorflow.keras.utils import plot_model
438/3: #bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'
438/4:
#tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'
#tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'
#
#print(f'BERT model selected           : {tfhub_handle_encoder}')
#print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')
438/5:
train_data = pd.read_csv('train.csv',usecols=['id','text','target'])
test_data = pd.read_csv('test.csv',usecols=['id','text'])
438/6:
#bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)
#bert_model = hub.KerasLayer(tfhub_handle_encoder)
438/7:
#text_test = ['this is such an amazing movie!']
#text_preprocessed = bert_preprocess_model(text_test)
438/8: train_data
438/9:
train_short = train_data[:50]
test_short = train_data[320:330]
438/10: tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")
438/11:
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from transformers import TFBertModel
from transformers import BertTokenizerFast
from sklearn.model_selection import train_test_split
438/12: from tensorflow.keras import Input
438/13:
#import os
#import shutil
#
#import tensorflow as tf
import tensorflow_hub as hub
#import tensorflow_text as text
##from official.nlp import optimization  # to create AdamW optimizer
#from tensorflow.keras.optimizers import Adam
#
#import matplotlib.pyplot as plt
#
#tf.get_logger().setLevel('ERROR')
438/14: tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")
438/15: tokenizer
438/16:
preprocessor = hub.KerasLayer(
    "https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3")
438/17:
preprocessor = hub.load(
    "https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3")
438/18:
#preprocessor = hub.load(
#    "https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3")
438/19: MAX_LEN = 128
438/20:
def tokenize(text):
# Use encoding functionality from transformers lib
    example = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=MAX_LEN,
        return_attention_mask=True,
        padding="max_length",
        truncation=True,
    )

    input_ids = np.array(example["input_ids"], dtype=np.int32)
    token_type_ids = np.array(example["token_type_ids"], dtype=np.int32)
    attention_masks = np.array(example["attention_mask"], dtype=np.int32)
    return input_ids, token_type_ids, attention_masks
438/21:
def prepare_disaster_dataset(df):
# Get features and labels from spam data
    features = df['text'].values
    labels = df['target'].values

    input_ids_list = []
    token_type_ids_list = []
    attention_mask_list = []
    label_list = []
    for i in range(len(features)):
        feature = features[i]

        # Encode example text
        input_ids, token_type_ids, attention_masks = tokenize(feature)
        input_ids_list.append(input_ids)
        token_type_ids_list.append(token_type_ids)
        attention_mask_list.append(attention_masks)

        # Set label to 1 if example is spam and to 0 if it's ham
        #label = 1 if labels[i] == 1 else 0
        label_list.append(labels[i])
        #label_list.append(label)

    return np.array(input_ids_list), np.array(token_type_ids_list), np.array(attention_mask_list), np.array(label_list).reshape(-1, 1)
438/22:
# Setup training data
train_input_ids, train_token_type_ids, train_attention_mask, y_train = prepare_disaster_dataset(
    train_short)

# Setup test data
test_input_ids, test_token_type_ids, test_attention_mask, y_test = prepare_disaster_dataset(
    test_short)
438/23: train_input_ids[0].shape
438/24: encoder = hub.load("https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2")
438/25: encoder = TFBertModel.from_pretrained("google/bert_uncased_L-2_H-128_A-2", from_pt=True)
438/26: encoder = hub.load("https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2")
438/27:
preprocessor = hub.load(
    "https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3")
438/28:
preprocessor = hub.load(
    "https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3")
438/29: import tensorflow_text as text
438/30: bert_preprocess = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3")
439/1:
#!pip install tensorflow-text==2.8.*
#!pip install -q tf-models-official==2.7.0
#!pip install tf-models-official
#!conda install transformers
439/2: #!conda install -c anaconda graphviz
439/3:
#import os
#import shutil
#
#import tensorflow as tf
import tensorflow_hub as hub
#import tensorflow_text as text
##from official.nlp import optimization  # to create AdamW optimizer
#from tensorflow.keras.optimizers import Adam
#
#import matplotlib.pyplot as plt
#
#tf.get_logger().setLevel('ERROR')
439/4:
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from transformers import TFBertModel
from transformers import BertTokenizerFast
from sklearn.model_selection import train_test_split
439/5: from tensorflow.keras import Input
439/6:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pydot
import graphviz
from tensorflow.keras.utils import plot_model
439/7:
train_data = pd.read_csv('train.csv',usecols=['id','text','target'])
test_data = pd.read_csv('test.csv',usecols=['id','text'])
439/8:
train_short = train_data[:50]
test_short = train_data[320:330]
439/9:
model_name = 'bert-base-uncased'
#tokenizer = BertTokenizer.from_pretrained(model_name)
439/10: tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")
439/11: tokenizer = BertTokenizerFast.from_pretrained(model_name)
439/12: bert_encoder = TFBertModel.from_pretrained(model_name)
439/13: MAX_LEN = 128
439/14:
def tokenize(text):
# Use encoding functionality from transformers lib
    example = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=MAX_LEN,
        return_attention_mask=True,
        padding="max_length",
        truncation=True,
    )

    input_ids = np.array(example["input_ids"], dtype=np.int32)
    token_type_ids = np.array(example["token_type_ids"], dtype=np.int32)
    attention_masks = np.array(example["attention_mask"], dtype=np.int32)
    return input_ids, token_type_ids, attention_masks
439/15:
def prepare_disaster_dataset(df):
# Get features and labels from spam data
    features = df['text'].values
    labels = df['target'].values

    input_ids_list = []
    token_type_ids_list = []
    attention_mask_list = []
    label_list = []
    for i in range(len(features)):
        feature = features[i]

        # Encode example text
        input_ids, token_type_ids, attention_masks = tokenize(feature)
        input_ids_list.append(input_ids)
        token_type_ids_list.append(token_type_ids)
        attention_mask_list.append(attention_masks)

        # Set label to 1 if example is spam and to 0 if it's ham
        #label = 1 if labels[i] == 1 else 0
        label_list.append(labels[i])
        #label_list.append(label)

    return np.array(input_ids_list), np.array(token_type_ids_list), np.array(attention_mask_list), np.array(label_list).reshape(-1, 1)
439/16:
def prepare_disaster_dataset(df):
# Get features and labels from spam data
    features = df['text'].values
    labels = df['target'].values

    input_ids_list = []
    token_type_ids_list = []
    attention_mask_list = []
    label_list = []
    for i in range(len(features)):
        feature = features[i]

        # Encode example text
        input_ids, token_type_ids, attention_masks = tokenize(feature)
        input_ids_list.append(input_ids)
        token_type_ids_list.append(token_type_ids)
        attention_mask_list.append(attention_masks)

        # Set label to 1 if example is spam and to 0 if it's ham
        #label = 1 if labels[i] == 1 else 0
        label_list.append(labels[i])
        #label_list.append(label)

    return np.array(input_ids_list), np.array(token_type_ids_list), np.array(attention_mask_list), np.array(label_list).reshape(-1, 1)
439/17:
# Setup training data
train_input_ids, train_token_type_ids, train_attention_mask, y_train = prepare_disaster_dataset(
    train_short)

# Setup test data
test_input_ids, test_token_type_ids, test_attention_mask, y_test = prepare_disaster_dataset(
    test_short)
439/18: #train_input_ids[0].shape
439/19: #encoder = hub.load("https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2")
439/20: #encoder = TFBertModel.from_pretrained("google/bert_uncased_L-2_H-128_A-2", from_pt=True)
439/21: encoder
439/22:
def build_model():
    bert_encoder = TFBertModel.from_pretrained(model_name)
    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_word_ids")
    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_mask")
    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_type_ids")
    
    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]
    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])
    
    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)
    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    
    return model
439/23: model = build_model()
439/24:
max_len = 50
def build_model():
    bert_encoder = TFBertModel.from_pretrained(model_name)
    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_word_ids")
    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_mask")
    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_type_ids")
    
    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]
    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])
    
    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)
    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    
    return model
439/25: model = build_model()
439/26: bert_encoder = TFBertModel.from_pretrained('bert-base-uncased')
439/27: bert_encoder = TFBertModel.from_pretrained(model_name)
440/1:
#import os
#import shutil
#
#import tensorflow as tf
import tensorflow_hub as hub
#import tensorflow_text as text
##from official.nlp import optimization  # to create AdamW optimizer
#from tensorflow.keras.optimizers import Adam
#
#import matplotlib.pyplot as plt
#
#tf.get_logger().setLevel('ERROR')
440/2:
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from transformers import TFBertModel
from transformers import BertTokenizerFast
from sklearn.model_selection import train_test_split
440/3: from tensorflow.keras import Input
440/4:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pydot
import graphviz
from tensorflow.keras.utils import plot_model
440/5:
train_data = pd.read_csv('train.csv',usecols=['id','text','target'])
test_data = pd.read_csv('test.csv',usecols=['id','text'])
440/6:
#bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)
#bert_model = hub.KerasLayer(tfhub_handle_encoder)
440/7:
#text_test = ['this is such an amazing movie!']
#text_preprocessed = bert_preprocess_model(text_test)
440/8:
train_short = train_data[:50]
test_short = train_data[320:330]
440/9:
model_name = 'bert-base-uncased'
#tokenizer = BertTokenizer.from_pretrained(model_name)
440/10: tokenizer = BertTokenizerFast.from_pretrained(model_name)
440/11: bert_encoder = TFBertModel.from_pretrained(model_name)
440/12: MAX_LEN = 128
440/13:
def tokenize(text):
# Use encoding functionality from transformers lib
    example = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=MAX_LEN,
        return_attention_mask=True,
        padding="max_length",
        truncation=True,
    )

    input_ids = np.array(example["input_ids"], dtype=np.int32)
    token_type_ids = np.array(example["token_type_ids"], dtype=np.int32)
    attention_masks = np.array(example["attention_mask"], dtype=np.int32)
    return input_ids, token_type_ids, attention_masks
440/14:
def prepare_disaster_dataset(df):
# Get features and labels from spam data
    features = df['text'].values
    labels = df['target'].values

    input_ids_list = []
    token_type_ids_list = []
    attention_mask_list = []
    label_list = []
    for i in range(len(features)):
        feature = features[i]

        # Encode example text
        input_ids, token_type_ids, attention_masks = tokenize(feature)
        input_ids_list.append(input_ids)
        token_type_ids_list.append(token_type_ids)
        attention_mask_list.append(attention_masks)

        # Set label to 1 if example is spam and to 0 if it's ham
        #label = 1 if labels[i] == 1 else 0
        label_list.append(labels[i])
        #label_list.append(label)

    return np.array(input_ids_list), np.array(token_type_ids_list), np.array(attention_mask_list), np.array(label_list).reshape(-1, 1)
440/15:
# Setup training data
train_input_ids, train_token_type_ids, train_attention_mask, y_train = prepare_disaster_dataset(
    train_short)

# Setup test data
test_input_ids, test_token_type_ids, test_attention_mask, y_test = prepare_disaster_dataset(
    test_short)
440/16: #train_input_ids[0].shape
440/17: #encoder = hub.load("https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2")
440/18:
max_len = 50
def build_model():
    bert_encoder = TFBertModel.from_pretrained('bert-base-uncased')
    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_word_ids")
    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_mask")
    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_type_ids")
    
    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]
    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])
    
    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)
    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    
    return model
440/19: model = build_model()
440/20: model
440/21: tf.keras.utils.plot_model(model, show_shapes=True)
440/22:
max_len = 50
def build_model():
    bert_encoder = TFBertModel.from_pretrained('bert-base-uncased')
    
    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_word_ids")
    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_mask")
    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_type_ids")
    
    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]
    print(embedding)
    print(embedding[:, 0, :])
    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])
    
    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)
    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    
    return model
440/23: model = build_model()
440/24: del bert_encoder
440/25:
max_len = 50
def build_model():
    bert_encoder = TFBertModel.from_pretrained('bert-base-uncased')
    
    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_word_ids")
    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_mask")
    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_type_ids")
    
    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]
    print(embedding)
    print(embedding[:, 0, :])
    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])
    
    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)
    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    
    return model
440/26: model = build_model()
441/1:
#import os
#import shutil
#
#import tensorflow as tf
import tensorflow_hub as hub
#import tensorflow_text as text
##from official.nlp import optimization  # to create AdamW optimizer
#from tensorflow.keras.optimizers import Adam
#
#import matplotlib.pyplot as plt
#
#tf.get_logger().setLevel('ERROR')
441/2:
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from transformers import TFBertModel
from transformers import BertTokenizerFast
from sklearn.model_selection import train_test_split
441/3: from tensorflow.keras import Input
441/4:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pydot
import graphviz
from tensorflow.keras.utils import plot_model
441/5:
train_data = pd.read_csv('train.csv',usecols=['id','text','target'])
test_data = pd.read_csv('test.csv',usecols=['id','text'])
441/6:
#bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)
#bert_model = hub.KerasLayer(tfhub_handle_encoder)
441/7:
#text_test = ['this is such an amazing movie!']
#text_preprocessed = bert_preprocess_model(text_test)
441/8:
train_short = train_data[:50]
test_short = train_data[320:330]
441/9:
model_name = 'bert-base-uncased'
#tokenizer = BertTokenizer.from_pretrained(model_name)
441/10: tokenizer = BertTokenizerFast.from_pretrained(model_name)
441/11: #bert_encoder = TFBertModel.from_pretrained(model_name)
441/12: MAX_LEN = 128
441/13:
def tokenize(text):
# Use encoding functionality from transformers lib
    example = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=MAX_LEN,
        return_attention_mask=True,
        padding="max_length",
        truncation=True,
    )

    input_ids = np.array(example["input_ids"], dtype=np.int32)
    token_type_ids = np.array(example["token_type_ids"], dtype=np.int32)
    attention_masks = np.array(example["attention_mask"], dtype=np.int32)
    return input_ids, token_type_ids, attention_masks
441/14:
def prepare_disaster_dataset(df):
# Get features and labels from spam data
    features = df['text'].values
    labels = df['target'].values

    input_ids_list = []
    token_type_ids_list = []
    attention_mask_list = []
    label_list = []
    for i in range(len(features)):
        feature = features[i]

        # Encode example text
        input_ids, token_type_ids, attention_masks = tokenize(feature)
        input_ids_list.append(input_ids)
        token_type_ids_list.append(token_type_ids)
        attention_mask_list.append(attention_masks)

        # Set label to 1 if example is spam and to 0 if it's ham
        #label = 1 if labels[i] == 1 else 0
        label_list.append(labels[i])
        #label_list.append(label)

    return np.array(input_ids_list), np.array(token_type_ids_list), np.array(attention_mask_list), np.array(label_list).reshape(-1, 1)
441/15:
# Setup training data
train_input_ids, train_token_type_ids, train_attention_mask, y_train = prepare_disaster_dataset(
    train_short)

# Setup test data
test_input_ids, test_token_type_ids, test_attention_mask, y_test = prepare_disaster_dataset(
    test_short)
441/16:
max_len = 50
def build_model():
    bert_encoder = TFBertModel.from_pretrained('bert-base-uncased')
    
    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_word_ids")
    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_mask")
    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_type_ids")
    
    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]
    print(embedding)
    print(embedding[:, 0, :])
    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])
    
    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)
    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    
    return model
441/17: model = build_model()
441/18:
def tokenize(text):
# Use encoding functionality from transformers lib
    example = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=MAX_LEN,
        return_attention_mask=True,
        padding="max_length",
        truncation=True,
    )

    input_ids = np.array(example["input_ids"], dtype=np.int32)
    token_type_ids = np.array(example["token_type_ids"], dtype=np.int32)
    attention_masks = np.array(example["attention_mask"], dtype=np.int32)
    return input_ids, token_type_ids, attention_masks
441/19:
def prepare_disaster_dataset(df):
# Get features and labels from spam data
    features = df['text'].values
    labels = df['target'].values

    input_ids_list = []
    token_type_ids_list = []
    attention_mask_list = []
    label_list = []
    for i in range(len(features)):
        feature = features[i]

        # Encode example text
        input_ids, token_type_ids, attention_masks = tokenize(feature)
        input_ids_list.append(input_ids)
        token_type_ids_list.append(token_type_ids)
        attention_mask_list.append(attention_masks)

        # Set label to 1 if example is spam and to 0 if it's ham
        #label = 1 if labels[i] == 1 else 0
        label_list.append(labels[i])
        #label_list.append(label)
    inputs = {
      'input_word_ids': input_word_ids.to_tensor(),
      'input_mask': input_mask,
      'input_type_ids': input_type_ids}
    return np.array(input_ids_list), np.array(token_type_ids_list), np.array(attention_mask_list), np.array(label_list).reshape(-1, 1)
441/20:
# Setup training data
train_input_ids, train_token_type_ids, train_attention_mask, y_train = prepare_disaster_dataset(
    train_short)

# Setup test data
test_input_ids, test_token_type_ids, test_attention_mask, y_test = prepare_disaster_dataset(
    test_short)
441/21:
def prepare_disaster_dataset(df):
# Get features and labels from spam data
    features = df['text'].values
    labels = df['target'].values

    input_ids_list = []
    token_type_ids_list = []
    attention_mask_list = []
    label_list = []
    for i in range(len(features)):
        feature = features[i]

        # Encode example text
        input_ids, token_type_ids, attention_masks = tokenize(feature)
        input_ids_list.append(input_ids)
        token_type_ids_list.append(token_type_ids)
        attention_mask_list.append(attention_masks)

        # Set label to 1 if example is spam and to 0 if it's ham
        #label = 1 if labels[i] == 1 else 0
        label_list.append(labels[i])
        #label_list.append(label)
    
    return np.array(input_ids_list), np.array(token_type_ids_list), np.array(attention_mask_list), np.array(label_list).reshape(-1, 1)
441/22:
# Setup training data
train_input_ids, train_token_type_ids, train_attention_mask, y_train = prepare_disaster_dataset(
    train_short)

# Setup test data
test_input_ids, test_token_type_ids, test_attention_mask, y_test = prepare_disaster_dataset(
    test_short)
441/23: train_input_ids
441/24:
model.fit({
    "input_ids": train_input_ids,
    "token_type_ids": train_token_type_ids,
    "attention_mask": train_attention_mask}, train.label.values, epochs = 2, verbose = 1, batch_size = 64, validation_split = 0.2)
441/25:
model.fit({
    "input_ids": train_input_ids,
    "token_type_ids": train_token_type_ids,
    "attention_mask": train_attention_mask}, y_train, epochs = 2, verbose = 1, batch_size = 64, validation_split = 0.2)
441/26:
model.fit({
    "input_word_ids": train_input_ids,
    "input_type_ids": train_token_type_ids,
    "input_mask": train_attention_mask}, y_train, epochs = 2, verbose = 1, batch_size = 64, validation_split = 0.2)
441/27:
max_len = 128
def build_model():
    bert_encoder = TFBertModel.from_pretrained('bert-base-uncased')
    
    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_word_ids")
    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_mask")
    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_type_ids")
    
    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]
    print(embedding)
    print(embedding[:, 0, :])
    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])
    
    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)
    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    
    return model
441/28: model = build_model()
441/29:
model.fit({
    "input_word_ids": train_input_ids,
    "input_type_ids": train_token_type_ids,
    "input_mask": train_attention_mask}, y_train, epochs = 2, verbose = 1, batch_size = 64, validation_split = 0.2)
441/30:
max_len = 128
def build_model():
    bert_encoder = TFBertModel.from_pretrained('bert-base-uncased')
    
    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_word_ids")
    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_mask")
    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_type_ids")
    
    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]
    print(embedding)
    print(embedding[:, 0, :])
    output = tf.keras.layers.Dense(1, activation='softmax')(embedding[:,0,:])
    
    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)
    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    
    return model
441/31: model = build_model()
443/1:
import tensorflow as tf
import tensorflow_hub as hub
from tensorflow import keras
from tensorflow.keras import layers, Input
from transformers import TFBertModel
from transformers import BertTokenizerFast

from sklearn.model_selection import train_test_split
443/2:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pydot
import graphviz
from tensorflow.keras.utils import plot_model
443/3:
train_data = pd.read_csv('train.csv',usecols=['id','text','target'])
test_data = pd.read_csv('test.csv',usecols=['id','text'])
443/4:
train_short = train_data[:50]
test_short = train_data[320:330]
443/5:
model_name = 'bert-base-uncased'
tokenizer = BertTokenizerFast.from_pretrained(model_name
443/6: MAX_LEN = 128
443/7:
model_name = 'bert-base-uncased'
tokenizer = BertTokenizerFast.from_pretrained(model_name)
443/8:
def tokenize(text):
# Use encoding functionality from transformers lib
    example = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=MAX_LEN,
        return_attention_mask=True,
        padding="max_length",
        truncation=True,
    )

    input_ids = np.array(example["input_ids"], dtype=np.int32)
    token_type_ids = np.array(example["token_type_ids"], dtype=np.int32)
    attention_masks = np.array(example["attention_mask"], dtype=np.int32)
    return input_ids, token_type_ids, attention_masks
443/9:
def prepare_disaster_dataset(df):
# Get features and labels from spam data
    features = df['text'].values
    labels = df['target'].values

    input_ids_list = []
    token_type_ids_list = []
    attention_mask_list = []
    label_list = []
    for i in range(len(features)):
        feature = features[i]

        # Encode example text
        input_ids, token_type_ids, attention_masks = tokenize(feature)
        input_ids_list.append(input_ids)
        token_type_ids_list.append(token_type_ids)
        attention_mask_list.append(attention_masks)

        # Set label to 1 if example is spam and to 0 if it's ham
        #label = 1 if labels[i] == 1 else 0
        label_list.append(labels[i])
        #label_list.append(label)
    
    return np.array(input_ids_list), np.array(token_type_ids_list), np.array(attention_mask_list), np.array(label_list).reshape(-1, 1)
443/10:
#def bert_encode(hypotheses, premises, tokenizer):
#    num_examples = len(hypotheses)
#    sentence1 = tf.ragged.constant([
#        encode_sentence(s)
#    for s in np.array(hypotheses)])
#  sentence2 = tf.ragged.constant([
#      encode_sentence(s)
#       for s in np.array(premises)])
#
#  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]
#  input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)
#
#  input_mask = tf.ones_like(input_word_ids).to_tensor()
#
#  type_cls = tf.zeros_like(cls)
#  type_s1 = tf.zeros_like(sentence1)
#  type_s2 = tf.ones_like(sentence2)
#  input_type_ids = tf.concat(
#      [type_cls, type_s1, type_s2], axis=-1).to_tensor()
#
#  inputs = {
#      'input_word_ids': input_word_ids.to_tensor(),
#      'input_mask': input_mask,
#      'input_type_ids': input_type_ids}
#
#  return inputs
443/11:
# Setup training data
train_input_ids, train_token_type_ids, train_attention_mask, y_train = prepare_disaster_dataset(
    train_short)

# Setup test data
test_input_ids, test_token_type_ids, test_attention_mask, y_test = prepare_disaster_dataset(
    test_short)
443/12: train_input_ids
443/13: train_input_ids.shape
443/14:
max_len = 128
def build_model():
    bert_encoder = TFBertModel.from_pretrained('bert-base-uncased')
    
    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_word_ids")
    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_mask")
    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_type_ids")
    
    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]
    output = tf.keras.layers.Dense(1, activation='softmax')(embedding[:,0,:])
    #l = layers.Dropout(0.1, name="dropout")(output)
    #l = layers.Dense(1, activation='sigmoid', name="output")(l)
    
    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)
    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    
    return model
443/15: model = build_model()
443/16:
model.fit({
    "input_word_ids": train_input_ids,
    "input_type_ids": train_token_type_ids,
    "input_mask": train_attention_mask}, 
    y_train, epochs = 2, verbose = 1, batch_size = 64, validation_split = 0.2)
443/17: model.predict(test_input)
443/18: model.predict(test_short)
443/19:
model.predict({
    "input_word_ids": test_input_ids,
    "input_type_ids": test_token_type_ids,
    "input_mask": test_attention_mask})
442/1:
#import os
#import shutil
#
#import tensorflow as tf
import tensorflow_hub as hub
#import tensorflow_text as text
##from official.nlp import optimization  # to create AdamW optimizer
#from tensorflow.keras.optimizers import Adam
#
#import matplotlib.pyplot as plt
#
#tf.get_logger().setLevel('ERROR')
442/2:
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from transformers import TFBertModel
from transformers import BertTokenizerFast
from sklearn.model_selection import train_test_split
442/3: from tensorflow.keras import Input
442/4:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pydot
import graphviz
from tensorflow.keras.utils import plot_model
442/5:
train_data = pd.read_csv('train.csv',usecols=['id','text','target'])
test_data = pd.read_csv('test.csv',usecols=['id','text'])
442/6:
#bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)
#bert_model = hub.KerasLayer(tfhub_handle_encoder)
442/7:
#text_test = ['this is such an amazing movie!']
#text_preprocessed = bert_preprocess_model(text_test)
442/8:
train_short = train_data[:50]
test_short = train_data[320:330]
442/9: tokenizer = BertTokenizerFast.from_pretrained(model_name)
442/10:
model_name = 'bert-base-uncased'
#tokenizer = BertTokenizer.from_pretrained(model_name)
442/11: tokenizer = BertTokenizerFast.from_pretrained(model_name)
442/12: #bert_encoder = TFBertModel.from_pretrained(model_name)
442/13: MAX_LEN = 128
442/14:
def tokenize(text):
# Use encoding functionality from transformers lib
    example = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=MAX_LEN,
        return_attention_mask=True,
        padding="max_length",
        truncation=True,
    )

    input_ids = np.array(example["input_ids"], dtype=np.int32)
    token_type_ids = np.array(example["token_type_ids"], dtype=np.int32)
    attention_masks = np.array(example["attention_mask"], dtype=np.int32)
    return input_ids, token_type_ids, attention_masks
442/15:
def prepare_disaster_dataset(df):
# Get features and labels from spam data
    features = df['text'].values
    labels = df['target'].values

    input_ids_list = []
    token_type_ids_list = []
    attention_mask_list = []
    label_list = []
    for i in range(len(features)):
        feature = features[i]

        # Encode example text
        input_ids, token_type_ids, attention_masks = tokenize(feature)
        input_ids_list.append(input_ids)
        token_type_ids_list.append(token_type_ids)
        attention_mask_list.append(attention_masks)

        # Set label to 1 if example is spam and to 0 if it's ham
        #label = 1 if labels[i] == 1 else 0
        label_list.append(labels[i])
        #label_list.append(label)
    
    return np.array(input_ids_list), np.array(token_type_ids_list), np.array(attention_mask_list), np.array(label_list).reshape(-1, 1)
442/16:
# Setup training data
train_input_ids, train_token_type_ids, train_attention_mask, y_train = prepare_disaster_dataset(
    train_short)

# Setup test data
test_input_ids, test_token_type_ids, test_attention_mask, y_test = prepare_disaster_dataset(
    test_short)
442/17:
max_len = 128
def build_model():
    bert_encoder = TFBertModel.from_pretrained('bert-base-uncased')
    
    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_word_ids")
    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_mask")
    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_type_ids")
    
    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]
    print(embedding)
    print(embedding.shape)
    output = tf.keras.layers.Dense(1, activation='softmax')(embedding[:,0,:])
    
    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)
    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    
    return model
442/18: model = build_model()
442/19:
max_len = 128
def build_model():
    bert_encoder = TFBertModel.from_pretrained('bert-base-uncased')
    
    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_word_ids")
    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_mask")
    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_type_ids")
    
    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]
    print(embedding)
    print(embedding.shape)
    print(embedding[:,0,:])
    output = tf.keras.layers.Dense(1, activation='softmax')(embedding[:,0,:])
    
    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)
    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    
    return model
442/20: model = build_model()
446/1:
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
446/2:
from keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences
446/3:
from keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences
import numpy as np
446/4:
maxlen = 100
train = 200
max_words = 10000
446/5:
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(texts)
446/6:
train.to_csv('embold_train.csv')
test.to_csv('embold_test.csv')
446/7:
train_data = pd.read_csv('train.csv')
test_data = pd.read_csv('test.csv')
446/8:
from keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences
import numpy as np
import pandas as pd
446/9:
maxlen = 100
train = 200
max_words = 10000
446/10:
train_data = pd.read_csv('train.csv')
test_data = pd.read_csv('test.csv')
446/11: train_data
446/12: train_data.text
446/13: train_data.text.to_list()
446/14: texts = train_data.text.to_list()
446/15:
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(texts)
446/16:
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
446/17: sequences
446/18:
word_index = tokenizer.word_index
print(f'Found {len(word_index)} unique tokens')
446/19: data = pad_sequences(sequences, maxlen=maxlen)
446/20: data
446/21: data.shape
446/22:
maxlen = 128
train = 200
max_words = 10000
446/23:
train_data = pd.read_csv('train.csv')
test_data = pd.read_csv('test.csv')
446/24: texts = train_data.text.to_list()
446/25:
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
446/26:
word_index = tokenizer.word_index
print(f'Found {len(word_index)} unique tokens')
446/27: data = pad_sequences(sequences, maxlen=maxlen)
446/28: data.shape
446/29: labels = np.array(labels)
446/30: labels = np.array(train_data.target.to_list())
446/31: labels
446/32: labels.shape
446/33: data
446/34: data[0]
446/35:
maxlen = 100
train = 200
max_words = 10000
446/36:
train_data = pd.read_csv('train.csv')
test_data = pd.read_csv('test.csv')
446/37: texts = train_data.text.to_list()
446/38:
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
446/39:
word_index = tokenizer.word_index
print(f'Found {len(word_index)} unique tokens')
446/40: data = pad_sequences(sequences, maxlen=maxlen)
446/41: data.shape
446/42: labels = np.array(train_data.target.to_list())
446/43: labels.shape
446/44: data[0]
446/45: train_data[0]
446/46: train_data.iloc[0,:]
446/47: train_data.iloc[:, 0]
446/48: train_data
446/49: train_data.iloc[0,:]
446/50: train_data.iloc[0,3]
446/51:
X_train = data[:train]
y_train = labels[:train]
X_val = data[train:]
y_val = labels[train:]
446/52: train
446/53:
X_train = data[:train]
y_train = labels[:train]
X_val = data[train:train+100]
y_val = labels[train:train+100]
446/54: X_val
446/55:
embeddings_index = {}
f = open('glove.twitter.27B/glove.twitter.27B.25d.txt')
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
446/56:
glove_dir = '/Users/Me/Desktop/GIT_pro/twitter/BERT/glove.twitter.27B'
embeddings_index = {}
f = open(glove_dir, 'glove.twitter.27B.25d.txt')
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
446/57:
glove_dir = '/Users/Me/Desktop/GIT_pro/twitter/BERT/glove.twitter.27B'
embeddings_index = {}
f = open(glove_dir, 'glove.twitter.27B.25d')
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
446/58:
glove_dir = '/Users/Me/Desktop/GIT_pro/twitter/BERT/glove.twitter.27B'
embeddings_index = {}
f = open(os.path.join(glove_dir, 'glove.twitter.27B.25d'))
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
446/59: impory os
446/60: import os
446/61:
glove_dir = '/Users/Me/Desktop/GIT_pro/twitter/BERT/glove.twitter.27B'
embeddings_index = {}
f = open(os.path.join(glove_dir, 'glove.twitter.27B.25d'))
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
446/62:
glove_dir = '/Users/Me/Desktop/GIT_pro/twitter/BERT/glove.twitter.27B'
embeddings_index = {}
f = open(os.path.join(glove_dir, 'glove.twitter.27B.25d.txt'))
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
446/63:
glove_dir = '/Users/Me/Desktop/GIT_pro/twitter/BERT/glove.twitter.27B'
embeddings_index = {}
f = open(os.path.join(glove_dir, 'glove.twitter.27B.25d.txt'), encoding="utf8")
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
446/64: embeddings_index
446/65:
embedding_dim = 100

embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i < max_words:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
446/66: word_index
446/67: embeddings_index.get('was')
446/68: embeddings_index.get('was').shape
446/69:
embedding_dim = 25

embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i < max_words:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
446/70: embedding_matrix
446/71: embedding_matrix.shape
446/72: embedding_matrix[9999]
446/73: embedding_matrix[9988:9999]
446/74:
from keras.models import Sequential
from keras.layers import Embedding, Flatten, Dense
446/75:
model = sequences()
model.add(Embedding(max_words, embedding_dim, imput_length = maxlen))
model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.summary()
446/76:
model = Sequential()
model.add(Embedding(max_words, embedding_dim, imput_length = maxlen))
model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.summary()
446/77:
model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length = maxlen))
model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.summary()
446/78:
model.layers[0].set_weights([embedding_matrix])
model.layers[0].trainable = False
446/79:
model.compile(optimizer='rmsprop',
             loss='binary_crossentropy',
             matrics=['acc'])
446/80:
model.compile(optimizer='rmsprop',
             loss='binary_crossentropy',
             matrics=['accuracy'])
446/81:
model.compile(optimizer='rmsprop',
             loss='binary_crossentropy',
             matrics=['acc'])
446/82:
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=[tf.keras.metrics.BinaryAccuracy(),
                       tf.keras.metrics.FalseNegatives()])
446/83:
model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=[tf.keras.metrics.BinaryAccuracy(),
                       tf.keras.metrics.FalseNegatives()])
446/84: model.compile(optimizer="Adam", loss="mse", metrics=["mae"])
446/85: model.compile(optimizer="Adam", loss="mse", metrics=["acc"])
446/86: model.compile(optimizer="Adam", loss="mse", metrics=['acc'])
446/87: model.compile(optimizer="Adam", loss='binary_crossentropy', metrics=['acc'])
446/88: model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
446/89: history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=[X_val, y_val])
446/90: model.save_weights('glove_model.h5')
446/91: import matplotlib.pyplot as plt
446/92:
acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
446/93: epochs = range(1, len(acc)+1)
446/94: epochs
446/95: plt.plot(epochs, acc, 'bo', label='Training acc')
446/96:
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, acc, 'rp', label='Training acc')
446/97:
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, acc, 'r-', label='Training acc')
446/98:
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'r-', label='Training val_acc')
447/1:
from keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences
import numpy as np
import pandas as pd
447/2:
maxlen = 100
train = 200
max_words = 10000
447/3:
train_data = pd.read_csv('train.csv')
test_data = pd.read_csv('test.csv')
447/4: texts = train_data.text.to_list()
447/5:
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
447/6:
word_index = tokenizer.word_index
print(f'Found {len(word_index)} unique tokens')
447/7: data = pad_sequences(sequences, maxlen=maxlen)
447/8: data.shape
447/9: labels = np.array(train_data.target.to_list())
447/10: labels.shape
447/11: data[0]
447/12: data
447/13: sum(np.all(data, axis=1))
447/14: sum(np.all(data, axis=0))
447/15: sum(np.any(data, axis=0))
447/16: sum(np.any(data, axis=1))
447/17: np.where(~data.any(axis=1))[0]
447/18: data.any(axis=1)
447/19: data[0]
447/20: data[5115]
447/21: data[7210]
447/22: data[-33]
447/23: train_data.iloc[0,3]
447/24: train_data.iloc[5115,3]
447/25: train_data.iloc[5115,:]
447/26: train_data.iloc[7210,:]
447/27:
X_train = data[:train]
y_train = labels[:train]
X_val = data[train:train+100]
y_val = labels[train:train+100]
447/28: import os
447/29:
glove_dir = '/Users/Me/Desktop/GIT_pro/twitter/BERT/glove.twitter.27B'
embeddings_index = {}
f = open(os.path.join(glove_dir, 'glove.twitter.27B.25d.txt'), encoding="utf8")
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
447/30:
embedding_dim = 25

embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i < max_words:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
447/31: embedding_matrix
447/32: embedding_matrix.shape
447/33: np.where(~embedding_matrix.any(axis=1))[0]
447/34: len(np.where(~embedding_matrix.any(axis=1))[0])
447/35: np.where(~embedding_matrix.any(axis=1))[0]
447/36: embeddings_index
447/37:
embedding_dim = 25
not_embedd = []
embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i < max_words:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
        else:
            not_embedd.append(word)
447/38: embedding_matrix.shape
447/39: np.where(~embedding_matrix.any(axis=1))[0]
447/40: not_embedd
447/41: embedding_dim
447/42: X_train
447/43: embedding_matrix
447/44: embedding_matrix.shpe
447/45: embedding_matrix.shape
448/1:
from keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences
import numpy as np
import pandas as pd
448/2:
maxlen = 100
train = 200
max_words = 10000
448/3:
train_data = pd.read_csv('train.csv')
test_data = pd.read_csv('test.csv')
448/4: texts = train_data.text.to_list()
448/5:
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
448/6:
word_index = tokenizer.word_index
print(f'Found {len(word_index)} unique tokens')
448/7: data = pad_sequences(sequences, maxlen=maxlen)
448/8: data.shape
448/9: labels = np.array(train_data.target.to_list())
448/10: labels.shape
448/11: np.where(~data.any(axis=1))[0]
448/12: train_data.iloc[7210,:]
448/13: train_data.iloc[0,3]
448/14:
X_train = data[:train]
y_train = labels[:train]
X_val = data[train:train+100]
y_val = labels[train:train+100]
448/15: import os
448/16:
glove_dir = '/Users/Me/Desktop/GIT_pro/twitter/BERT/glove.twitter.27B'
embeddings_index = {}
f = open(os.path.join(glove_dir, 'glove.twitter.27B.25d.txt'), encoding="utf8")
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
448/17:
embedding_dim = 25
not_embedd = []
embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i < max_words:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
        else:
            not_embedd.append(word)
448/18: embedding_matrix.shape
448/19: np.where(~embedding_matrix.any(axis=1))[0]
448/20: not_embedd
448/21:
from keras.models import Sequential
from keras.layers import Embedding, Flatten, Dense
448/22: X_train
448/23:
model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length = maxlen))
model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.summary()
448/24:
model.layers[0].set_weights([embedding_matrix])
model.layers[0].trainable = False
448/25: model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
448/26: history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=[X_val, y_val])
448/27: model.save_weights('glove_model.h5')
448/28: history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=[X_val, y_val])
448/29: model.save_weights('glove_model.h5')
448/30: import matplotlib.pyplot as plt
448/31:
acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
448/32: epochs = range(1, len(acc)+1)
448/33: epochs
448/34:
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'r-', label='Training val_acc')
448/35:
model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length = maxlen))
#model.add(Flatten())
#model.add(Dense(32, activation='relu'))
#model.add(Dense(1, activation='sigmoid'))
model_glove.add(Dropout(0.2))
model_glove.add(Conv1D(64, 5, activation='relu'))
model_glove.add(MaxPooling1D(pool_size=4))
model_glove.add(LSTM(300))
model_glove.add(Dense(1, activation='sigmoid'))
model.summary()
448/36:
model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length = maxlen))
#model.add(Flatten())
#model.add(Dense(32, activation='relu'))
#model.add(Dense(1, activation='sigmoid'))
model.add(Dropout(0.2))
model.add(Conv1D(64, 5, activation='relu'))
model.add(MaxPooling1D(pool_size=4))
model.add(LSTM(300))
model.add(Dense(1, activation='sigmoid'))
model.summary()
448/37: from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation
448/38:
model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length = maxlen))
#model.add(Flatten())
#model.add(Dense(32, activation='relu'))
#model.add(Dense(1, activation='sigmoid'))
model.add(Dropout(0.2))
model.add(Conv1D(64, 5, activation='relu'))
model.add(MaxPooling1D(pool_size=4))
model.add(LSTM(300))
model.add(Dense(1, activation='sigmoid'))
model.summary()
448/39:
model.layers[0].set_weights([embedding_matrix])
model.layers[0].trainable = False
448/40: model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
448/41: history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=[X_val, y_val])
448/42: history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=[X_val, y_val])
448/43: model.save_weights('glove_model.h5')
448/44: import matplotlib.pyplot as plt
448/45:
acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
448/46: epochs = range(1, len(acc)+1)
448/47: epochs
448/48:
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'r-', label='Training val_acc')
448/49:
maxlen = 100
train = 2000
max_words = 15000
448/50:
train_data = pd.read_csv('train.csv')
test_data = pd.read_csv('test.csv')
448/51: texts = train_data.text.to_list()
448/52:
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
448/53:
word_index = tokenizer.word_index
print(f'Found {len(word_index)} unique tokens')
448/54: data = pad_sequences(sequences, maxlen=maxlen)
448/55: data.shape
448/56: labels = np.array(train_data.target.to_list())
448/57: labels.shape
448/58: np.where(~data.any(axis=1))[0]
448/59: train_data.iloc[7210,:]
448/60: train_data.iloc[0,3]
448/61:
X_train = data[:train]
y_train = labels[:train]
X_val = data[train:train+1000]
y_val = labels[train:train+1000]
448/62:
glove_dir = '/Users/Me/Desktop/GIT_pro/twitter/BERT/glove.twitter.27B'
embeddings_index = {}
f = open(os.path.join(glove_dir, 'glove.twitter.27B.25d.txt'), encoding="utf8")
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
448/63:
embedding_dim = 25
not_embedd = []
embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i < max_words:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
        else:
            not_embedd.append(word)
448/64: embedding_matrix.shape
448/65: np.where(~embedding_matrix.any(axis=1))[0]
448/66: not_embedd
448/67:
from keras.models import Sequential
from keras.layers import Embedding, Flatten, Dense
448/68: X_train
448/69: X_train.shape
448/70:
model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length = maxlen))
#model.add(Flatten())
#model.add(Dense(32, activation='relu'))
#model.add(Dense(1, activation='sigmoid'))
model.add(Dropout(0.2))
model.add(Conv1D(64, 5, activation='relu'))
model.add(MaxPooling1D(pool_size=4))
model.add(LSTM(300))
model.add(Dense(1, activation='sigmoid'))
model.summary()
448/71:
model.layers[0].set_weights([embedding_matrix])
model.layers[0].trainable = False
448/72: history = model.fit(X_train, y_train, epochs=15, batch_size=32, validation_data=[X_val, y_val])
448/73:
model.layers[0].set_weights([embedding_matrix])
model.layers[0].trainable = False
448/74: model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
448/75: history = model.fit(X_train, y_train, epochs=15, batch_size=32, validation_data=[X_val, y_val])
448/76: model.save_weights('glove_model.h5')
448/77: import matplotlib.pyplot as plt
448/78:
acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
448/79: epochs = range(1, len(acc)+1)
448/80: epochs
448/81:
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'r-', label='Training val_acc')
448/82:
maxlen = 100
train = 4000
max_words = 15000
448/83:
train_data = pd.read_csv('train.csv')
test_data = pd.read_csv('test.csv')
448/84: texts = train_data.text.to_list()
448/85:
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
448/86:
word_index = tokenizer.word_index
print(f'Found {len(word_index)} unique tokens')
448/87: data = pad_sequences(sequences, maxlen=maxlen)
448/88: data.shape
448/89: labels = np.array(train_data.target.to_list())
448/90: labels.shape
448/91: np.where(~data.any(axis=1))[0]
448/92: train_data.iloc[7210,:]
448/93: train_data.iloc[0,3]
448/94:
X_train = data[:train]
y_train = labels[:train]
X_val = data[train:train+1000]
y_val = labels[train:train+1000]
448/95:
X_train = data[:train]
y_train = labels[:train]
X_val = data[train:]
y_val = labels[train:]
448/96: import os
448/97:
glove_dir = '/Users/Me/Desktop/GIT_pro/twitter/BERT/glove.twitter.27B'
embeddings_index = {}
f = open(os.path.join(glove_dir, 'glove.twitter.27B.25d.txt'), encoding="utf8")
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
448/98:
embedding_dim = 25
not_embedd = []
embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i < max_words:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
        else:
            not_embedd.append(word)
448/99: embedding_matrix.shape
448/100: np.where(~embedding_matrix.any(axis=1))[0]
448/101: not_embedd
448/102:
from keras.models import Sequential
from keras.layers import Embedding, Flatten, Dense
448/103: X_train.shape
448/104:
model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length = maxlen))
#model.add(Flatten())
#model.add(Dense(32, activation='relu'))
#model.add(Dense(1, activation='sigmoid'))
model.add(Dropout(0.2))
model.add(Conv1D(64, 5, activation='relu'))
model.add(MaxPooling1D(pool_size=4))
model.add(LSTM(300))
model.add(Dense(1, activation='sigmoid'))
model.summary()
448/105:
model.layers[0].set_weights([embedding_matrix])
model.layers[0].trainable = False
448/106: model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
448/107: history = model.fit(X_train, y_train, epochs=15, batch_size=32, validation_data=[X_val, y_val])
448/108: model.save_weights('glove_model.h5')
448/109: import matplotlib.pyplot as plt
448/110:
acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
448/111: epochs = range(1, len(acc)+1)
448/112: epochs
448/113:
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'r-', label='Training val_acc')
448/114: model.save_weights('glove_model.h5')
448/115: import matplotlib.pyplot as plt
448/116:
acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
448/117: epochs = range(1, len(acc)+1)
448/118: epochs
448/119:
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'r-', label='Training val_acc')
448/120:
data = pad_sequences(sequences, maxlen=maxlen)
test_data = pad_sequences(sequences, maxlen=maxlen)
449/1:
from keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences
import numpy as np
import pandas as pd
449/2: from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation
449/3:
maxlen = 100
train = 4000
max_words = 15000
449/4:
train_data = pd.read_csv('train.csv')
test_data = pd.read_csv('test.csv')
449/5:
texts = train_data.text.to_list()
test_texts = test_data.text.to_list()
449/6:
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
449/7:
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
test_sequences = tokenizer.texts_to_sequences(test_texts)
449/8:
word_index = tokenizer.word_index
print(f'Found {len(word_index)} unique tokens')
449/9:
data = pad_sequences(sequences, maxlen=maxlen)
test_data = pad_sequences(test_sequences, maxlen=maxlen)
449/10:
print(data.shape)
print(test_data.shape)
449/11: labels = np.array(train_data.target.to_list())
449/12: labels.shape
449/13: np.where(~data.any(axis=1))[0]
449/14: train_data.iloc[7210,:]
449/15: train_data.iloc[0,3]
449/16:
X_train = data[:train]
y_train = labels[:train]
X_val = data[train:]
y_val = labels[train:]
449/17: import os
449/18:
glove_dir = '/Users/Me/Desktop/GIT_pro/twitter/BERT/glove.twitter.27B'
embeddings_index = {}
f = open(os.path.join(glove_dir, 'glove.twitter.27B.25d.txt'), encoding="utf8")
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
449/19:
embedding_dim = 25
not_embedd = []
embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i < max_words:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
        else:
            not_embedd.append(word)
449/20: embedding_matrix.shape
449/21: np.where(~embedding_matrix.any(axis=1))[0]
449/22: not_embedd
449/23:
from keras.models import Sequential
from keras.layers import Embedding, Flatten, Dense
449/24: X_train.shape
449/25:
model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length = maxlen))
#model.add(Flatten())
#model.add(Dense(32, activation='relu'))
#model.add(Dense(1, activation='sigmoid'))
model.add(Dropout(0.2))
model.add(Conv1D(64, 5, activation='relu'))
model.add(MaxPooling1D(pool_size=4))
model.add(LSTM(300))
model.add(Dense(1, activation='sigmoid'))
model.summary()
449/26:
model.layers[0].set_weights([embedding_matrix])
model.layers[0].trainable = False
449/27: model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
449/28: history = model.fit(X_train, y_train, epochs=8, batch_size=32, validation_data=[X_val, y_val])
449/29: model.save_weights('glove_model.h5')
449/30: import matplotlib.pyplot as plt
449/31:
acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
449/32: epochs = range(1, len(acc)+1)
449/33: epochs
449/34:
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'r-', label='Training val_acc')
449/35: df_t = df_test[['id']]
449/36: df_t = test_data[['id']]
449/37: test_data
449/38:
train_data = pd.read_csv('train.csv')
test_data_df = pd.read_csv('test.csv')
449/39: df_t = test_data_df[['id']]
449/40: df_t
449/41: model.predict(test_sequences)
449/42: X_train
449/43: test_data
449/44: X_train.shape
449/45: test_data.shape
449/46: test_sequences = np.asarray(test_sequences).astype(np.float32)
449/47: np.asarray(test_sequences).astype(np.float32)
449/48: np.asarray(test_sequences)
449/49: np.asarray(test_sequences).astype(np.float32)
449/50: test_sequences
449/51: X_train
449/52: data
449/53: test_data
449/54: model.predict(test_data)
449/55: model.predict(test_data).to_list()
449/56: y_pred = model.predict(test_data)
449/57: y_pred
449/58: y_pred.shape
449/59: y_pred
449/60: argmax(y_pred)
449/61: y_pred > .5
449/62: (y_pred > .5).astype(int)
449/63: df_t['target'] = (y_pred >= 0.5).astype(int)
449/64: df_t
449/65: y_pred
449/66: df_t.to_csv("LSTM.csv", index=False)
449/67: %load_ext tensorboard
451/1:
from keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences
import numpy as np
import pandas as pd
import string
451/2: from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation
451/3: %load_ext tensorboard
451/4:
##### PREPROC
#remove not ascii
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    return text

# Change an abbreviation by its true meaning
#def word_abbrev(word):
#    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word
#
# Replace all abbreviations
#def replace_abbrev(text):
#    string = ""
#    for word in text.split():
#        string += word_abbrev(word) + " "        
#    return string

# Remove @ and mention, replace by USER
def replace_mention(text):
    pat=re.compile(r'@\S+')
    return pat.sub(r'USER',text)

# Remove numbers, replace it by NUMBER
def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r' NUMBER ', text)

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()

#contractions and remowing stopwords and very short words
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 3:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
451/5:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    #df[column_name] = df[column_name].apply(replace_abbrev)
    #df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(string_contractions)
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(clean_text)
    return df
451/6:
df_train = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')
451/7: df_train_clean = basic_cleaning(df_train, 'text')
451/8:
from keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences
import numpy as np
import pandas as pd
import string
import copy
451/9: df_train_clean = basic_cleaning(df_train, 'text')
451/10:
from keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences
import numpy as np
import pandas as pd
import string
import copy
import re
451/11: df_train_clean = basic_cleaning(df_train, 'text')
451/12: from nltk.corpus import stopwords
451/13: STOPWORDS = set(stopwords.words('english'))
451/14: df_train_clean = basic_cleaning(df_train, 'text')
451/15:
from keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences
import numpy as np
import pandas as pd
import string
import copy
import re
import contractions
451/16: df_train_clean = basic_cleaning(df_train, 'text')
451/17: df_train_clean
451/18:
df_train_clean = basic_cleaning(df_train, 'text')
df_test_clean = basic_cleaning(df_test, 'text')
451/19:
maxlen = 150
train = 4000
max_words = 10000
451/20:
train_texts = df_train_clean.text.to_list()
test_texts = df_test_clean.text.to_list()
451/21:
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(train_texts)

train_sequences = tokenizer.texts_to_sequences(train_texts)
test_sequences = tokenizer.texts_to_sequences(test_texts)
451/22:
word_index = tokenizer.word_index
print(f'Found {len(word_index)} unique tokens')
451/23:
train_data = pad_sequences(train_sequences, maxlen=maxlen)
test_data = pad_sequences(test_sequences, maxlen=maxlen)
451/24:
print(train_data.shape)
print(test_data.shape)
451/25: labels = np.array(train_data.target.to_list())
451/26: labels.shape
451/27: labels = np.array(df_train.target.to_list())
451/28: labels.shape
451/29: labels.shape
451/30: np.where(~data.any(axis=1))[0]
451/31: np.where(~train_data.any(axis=1))[0]
451/32: train_data.iloc[7210,:]
451/33: train_data
451/34:
X_train = data[:train]
y_train = labels[:train]
X_val = data[train:]
y_val = labels[train:]
451/35:
X_train = train_data[:train]
y_train = labels[:train]
X_val = train_data[train:]
y_val = labels[train:]
451/36: import os
451/37:
glove_dir = '/Users/Me/Desktop/GIT_pro/twitter/BERT/glove.twitter.27B'
embeddings_index = {}
f = open(os.path.join(glove_dir, 'glove.twitter.27B.25d.txt'), encoding="utf8")
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
451/38:
embedding_dim = 25
not_embedd = []
embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i < max_words:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
        else:
            not_embedd.append(word)
451/39: embedding_matrix.shape
451/40: np.where(~embedding_matrix.any(axis=1))[0]
451/41: not_embedd
451/42: len(not_embedd)
451/43: not_embedd
451/44: len(not_embedd)
451/45:
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
wnl = WordNetLemmatizer()
451/46:
w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
def lemmatize_text(text):
    return [wnl.lemmatize(w) for w in w_tokenizer.tokenize(text)]

df_train_clean['text_lemmatized'] = df_train_clean.text.apply(lemmatize_text)
df_train_clean['all_text'] = df_train_clean['text_lemmatized'].apply(lambda x: " ".join(x))

#df_test_text_clean_f['text_lemmatized'] = df_test_text_clean_f.clean_text.apply(lemmatize_text)
#df_test_text_clean_f['all_text'] = df_test_text_clean_f['text_lemmatized'].apply(lambda x: " ".join(x))
451/47: from nltk.tokenize import WhitespaceTokenizer
451/48:
w_tokenizer = WhitespaceTokenizer()
def lemmatize_text(text):
    return [wnl.lemmatize(w) for w in w_tokenizer.tokenize(text)]

df_train_clean['text_lemmatized'] = df_train_clean.text.apply(lemmatize_text)
df_train_clean['all_text'] = df_train_clean['text_lemmatized'].apply(lambda x: " ".join(x))

#df_test_text_clean_f['text_lemmatized'] = df_test_text_clean_f.clean_text.apply(lemmatize_text)
#df_test_text_clean_f['all_text'] = df_test_text_clean_f['text_lemmatized'].apply(lambda x: " ".join(x))
451/49: df_train_clean
451/50:
train_texts = df_train_clean.all_text.to_list()
test_texts = df_test_clean.text.to_list()
451/51:
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(train_texts)

train_sequences = tokenizer.texts_to_sequences(train_texts)
test_sequences = tokenizer.texts_to_sequences(test_texts)
451/52:
word_index = tokenizer.word_index
print(f'Found {len(word_index)} unique tokens')
451/53:
train_data = pad_sequences(train_sequences, maxlen=maxlen)
test_data = pad_sequences(test_sequences, maxlen=maxlen)
451/54:
print(train_data.shape)
print(test_data.shape)
451/55: labels = np.array(df_train.target.to_list())
451/56: labels.shape
451/57: np.where(~train_data.any(axis=1))[0]
451/58:
X_train = train_data[:train]
y_train = labels[:train]
X_val = train_data[train:]
y_val = labels[train:]
451/59: import os
451/60:
glove_dir = '/Users/Me/Desktop/GIT_pro/twitter/BERT/glove.twitter.27B'
embeddings_index = {}
f = open(os.path.join(glove_dir, 'glove.twitter.27B.25d.txt'), encoding="utf8")
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
451/61:
embedding_dim = 25
not_embedd = []
embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i < max_words:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
        else:
            not_embedd.append(word)
451/62: embedding_matrix.shape
451/63: np.where(~embedding_matrix.any(axis=1))[0]
451/64: len(not_embedd)
451/65: not_embedd
451/66: df_text.text.str.contains('disea')
451/67: df_train.text.str.contains('disea')
451/68: df_train[df_train.text.str.contains('disea')]
451/69: df_train_clean[df_train_clean.text.str.contains('disea')]
451/70:
from keras.models import Sequential
from keras.layers import Embedding, Flatten, Dense
451/71: X_train.shape
451/72:
model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length = maxlen))
#model.add(Flatten())
#model.add(Dense(32, activation='relu'))
#model.add(Dense(1, activation='sigmoid'))
model.add(Dropout(0.2))
model.add(Conv1D(64, 5, activation='relu'))
model.add(MaxPooling1D(pool_size=4))
model.add(LSTM(300))
model.add(Dense(1, activation='sigmoid'))
model.summary()
451/73:
model.layers[0].set_weights([embedding_matrix])
model.layers[0].trainable = False
451/74: model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['f1'])
451/75: history = model.fit(X_train, y_train, epochs=8, batch_size=32, validation_data=[X_val, y_val])
451/76: model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
451/77: history = model.fit(X_train, y_train, epochs=8, batch_size=32, validation_data=[X_val, y_val])
451/78: history = model.fit(X_train, y_train, epochs=15, batch_size=32, validation_data=[X_val, y_val])
451/79: model.save_weights('glove_model.h5')
451/80: import matplotlib.pyplot as plt
451/81:
acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
451/82: epochs = range(1, len(acc)+1)
451/83: epochs
451/84:
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'r-', label='Training val_acc')
451/85: test_sequences
451/86: np.asarray(test_sequences).astype(np.float32)
451/87: y_pred = model.predict(test_data)
452/1:
from keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences
import numpy as np
import pandas as pd
import string
import copy
import re
import contractions
452/2:
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import WhitespaceTokenizer
wnl = WordNetLemmatizer()
452/3: STOPWORDS = set(stopwords.words('english'))
452/4: from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation
452/5: %load_ext tensorboard
452/6:
##### PREPROC
#remove not ascii
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    return text

# Change an abbreviation by its true meaning
#def word_abbrev(word):
#    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word
#
# Replace all abbreviations
#def replace_abbrev(text):
#    string = ""
#    for word in text.split():
#        string += word_abbrev(word) + " "        
#    return string

# Remove @ and mention, replace by USER
def replace_mention(text):
    pat=re.compile(r'@\S+')
    return pat.sub(r' ',text)

# Remove numbers, replace it by NUMBER
def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r' ', text)

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()

#contractions and remowing stopwords and very short words
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 3:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
452/7:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(remove_urls)
    #df[column_name] = df[column_name].apply(replace_abbrev)
    #df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(string_contractions)
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(clean_text)
    return df
452/8:
df_train = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')
452/9:
df_train_clean = basic_cleaning(df_train, 'text')
df_test_clean = basic_cleaning(df_test, 'text')
452/10:
w_tokenizer = WhitespaceTokenizer()
def lemmatize_text(text):
    return [wnl.lemmatize(w) for w in w_tokenizer.tokenize(text)]

df_train_clean['text_lemmatized'] = df_train_clean.text.apply(lemmatize_text)
df_train_clean['all_text'] = df_train_clean['text_lemmatized'].apply(lambda x: " ".join(x))

df_test_clean['text_lemmatized'] = df_test_clean.text.apply(lemmatize_text)
df_test_clean['all_text'] = df_test_clean['text_lemmatized'].apply(lambda x: " ".join(x))
452/11:
maxlen = 150
train = 4000
max_words = 10000
452/12:
train_texts = df_train_clean.all_text.to_list()
test_texts = df_test_clean.all_text.to_list()
452/13:
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(train_texts)

train_sequences = tokenizer.texts_to_sequences(train_texts)
test_sequences = tokenizer.texts_to_sequences(test_texts)
452/14:
word_index = tokenizer.word_index
print(f'Found {len(word_index)} unique tokens')
452/15: word_index
452/16: train_texts
452/17: #train_texts
452/18:
train_data = pad_sequences(train_sequences, maxlen=maxlen)
test_data = pad_sequences(test_sequences, maxlen=maxlen)
452/19:
print(train_data.shape)
print(test_data.shape)
452/20: labels = np.array(df_train.target.to_list())
452/21: labels.shape
452/22: np.where(~train_data.any(axis=1))[0]
452/23:
from keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences
import numpy as np
import pandas as pd
import string
import copy
import re
import contractions
import os
452/24:
X_train = train_data[:train]
y_train = labels[:train]
X_val = train_data[train:]
y_val = labels[train:]
452/25:
glove_dir = '/Users/Me/Desktop/GIT_pro/twitter/BERT/glove.twitter.27B'
embeddings_index = {}
f = open(os.path.join(glove_dir, 'glove.twitter.27B.25d.txt'), encoding="utf8")
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
452/26:
embedding_dim = 25
not_embedd = []
embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i < max_words:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
        else:
            not_embedd.append(word)
452/27: embedding_matrix.shape
452/28: np.where(~embedding_matrix.any(axis=1))[0]
452/29: len(not_embedd)
452/30: not_embedd
452/31: df_train_clean[df_train_clean.text.str.contains('disea')]
452/32: #df_train_clean[df_train_clean.text.str.contains('disea')]
452/33:
from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Embedding
from keras.models import Sequential
452/34:
model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length = maxlen))
#model.add(Flatten())
#model.add(Dense(32, activation='relu'))
#model.add(Dense(1, activation='sigmoid'))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))
model.summary()
452/35:
from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Embedding, SpatialDropout1D
from keras.models import Sequential
452/36:
model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length = maxlen))
#model.add(Flatten())
#model.add(Dense(32, activation='relu'))
#model.add(Dense(1, activation='sigmoid'))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))
model.summary()
452/37:
model.layers[0].set_weights([embedding_matrix])
model.layers[0].trainable = False
452/38: model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
452/39: history = model.fit(X_train, y_train, epochs=15, batch_size=64, validation_data=[X_val, y_val])
452/40: history = model.fit(X_train, y_train, epochs=15, batch_size=32, validation_data=[X_val, y_val])
452/41: history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=[X_val, y_val])
452/42: model.save_weights('glove_model.h5')
452/43: import matplotlib.pyplot as plt
452/44:
acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
452/45: epochs = range(1, len(acc)+1)
452/46: epochs
452/47:
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'r-', label='Training val_acc')
452/48: test_sequences
452/49: np.asarray(test_sequences).astype(np.float32)
452/50: y_pred = model.predict(test_data)
452/51: y_pred.shape
452/52: (y_pred > .5).astype(int)
452/53: df_t['target'] = (y_pred >= 0.5).astype(int)
452/54: df_t = df_test[['id']]
452/55: df_t['target'] = (y_pred >= 0.5).astype(int)
452/56: df_t.to_csv("LSTM_2.csv", index=False)
452/57: y_train
452/58: sum(y_train)/len(y_train)
452/59: sum(y_val)/len(y_val)
452/60: X_train, X_test, y_train, y_test = train_test_split(train_data, labels, test_size=0.3, stratify=y, shuffle=True)
452/61: from sklearn.model_selection import train_test_split
452/62: X_train, X_test, y_train, y_test = train_test_split(train_data, labels, test_size=0.3, stratify=y, shuffle=True)
452/63: X_train, X_test, y_train, y_test = train_test_split(train_data, labels, test_size=0.3, stratify=labels, shuffle=True)
452/64:
#X_train = train_data[:train]
#y_train = labels[:train]
#X_val = train_data[train:]
#y_val = labels[train:]
452/65:
glove_dir = '/Users/Me/Desktop/GIT_pro/twitter/BERT/glove.twitter.27B'
embeddings_index = {}
f = open(os.path.join(glove_dir, 'glove.twitter.27B.25d.txt'), encoding="utf8")
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
452/66:
embedding_dim = 25
not_embedd = []
embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i < max_words:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
        else:
            not_embedd.append(word)
452/67: embedding_matrix.shape
453/1:
import numpy as np
import pandas as pd
import string
import copy
import re
import contractions
import os

from sklearn.model_selection import train_test_split
453/2:
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import WhitespaceTokenizer
wnl = WordNetLemmatizer()
STOPWORDS = set(stopwords.words('english'))
453/3: STOPWORDS = set(stopwords.words('english'))
453/4:
from keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences
from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Embedding, SpatialDropout1D
from keras.models import Sequential
453/5: %load_ext tensorboard
453/6:
##### PREPROC
#remove not ascii
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    return text

# Change an abbreviation by its true meaning
#def word_abbrev(word):
#    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word
#
# Replace all abbreviations
#def replace_abbrev(text):
#    string = ""
#    for word in text.split():
#        string += word_abbrev(word) + " "        
#    return string

# Remove @ and mention, replace by USER
def replace_mention(text):
    pat=re.compile(r'@\S+')
    return pat.sub(r' ',text)

# Remove numbers, replace it by NUMBER
def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r' ', text)

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()

#contractions and remowing stopwords and very short words
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 3:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
453/7:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(remove_urls)
    #df[column_name] = df[column_name].apply(replace_abbrev)
    #df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(string_contractions)
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(clean_text)
    return df
453/8:
df_train = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')
453/9:
df_train_clean = basic_cleaning(df_train, 'text')
df_test_clean = basic_cleaning(df_test, 'text')
453/10:
w_tokenizer = WhitespaceTokenizer()
def lemmatize_text(text):
    return [wnl.lemmatize(w) for w in w_tokenizer.tokenize(text)]

df_train_clean['text_lemmatized'] = df_train_clean.text.apply(lemmatize_text)
df_train_clean['all_text'] = df_train_clean['text_lemmatized'].apply(lambda x: " ".join(x))

df_test_clean['text_lemmatized'] = df_test_clean.text.apply(lemmatize_text)
df_test_clean['all_text'] = df_test_clean['text_lemmatized'].apply(lambda x: " ".join(x))
453/11:
maxlen = 150
#train = 4000
max_words = 20000
453/12:
train_texts = df_train_clean.all_text.to_list()
test_texts = df_test_clean.all_text.to_list()
453/13:
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(train_texts)

train_sequences = tokenizer.texts_to_sequences(train_texts)
test_sequences = tokenizer.texts_to_sequences(test_texts)
453/14:
word_index = tokenizer.word_index
print(f'Found {len(word_index)} unique tokens')
453/15:
train_data = pad_sequences(train_sequences, maxlen=maxlen)
test_data = pad_sequences(test_sequences, maxlen=maxlen)
453/16:
print(train_data.shape)
print(test_data.shape)
453/17:
train_data = pad_sequences(train_sequences, maxlen=maxlen)
test_data = pad_sequences(test_sequences, maxlen=maxlen)
labels = np.array(df_train.target.to_list())
453/18: np.where(~train_data.any(axis=1))[0]
453/19: X_train, X_test, y_train, y_test = train_test_split(train_data, labels, test_size=0.25, stratify=labels, shuffle=True)
453/20:
glove_dir = '/Users/Me/Desktop/GIT_pro/twitter/BERT/glove.twitter.27B'
embeddings_index = {}
f = open(os.path.join(glove_dir, 'glove.twitter.27B.50d.txt'), encoding="utf8")
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
453/21:
embedding_dim = 50
not_embedd = []
embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i < max_words:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
        else:
            not_embedd.append(word)
453/22: embedding_matrix.shape
453/23: np.where(~embedding_matrix.any(axis=1))[0]
453/24: len(not_embedd)
453/25: not_embedd
453/26: not_embedd[-20:]
453/27: np.where(~embedding_matrix.any(axis=1))
453/28: np.where(~embedding_matrix.any(axis=1))[0]
453/29: embedding_matrix.shape
453/30: max_words
453/31: X_train.shape
453/32: X_train.shape[1]
453/33: X_train.shape
453/34: embedding_dim
453/35: maxlen
453/36:
model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length = maxlen))
#model.add(Flatten())
#model.add(Dense(32, activation='relu'))
#model.add(Dense(1, activation='sigmoid'))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))
model.summary()
453/37:
model.layers[0].set_weights([embedding_matrix])
model.layers[0].trainable = False
453/38: model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
453/39: history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=[X_val, y_val])
453/40: X_train, X_val, y_train, y_val = train_test_split(train_data, labels, test_size=0.25, stratify=labels, shuffle=True)
453/41:
#X_train = train_data[:train]
#y_train = labels[:train]
#X_val = train_data[train:]
#y_val = labels[train:]
453/42:
glove_dir = '/Users/Me/Desktop/GIT_pro/twitter/BERT/glove.twitter.27B'
embeddings_index = {}
f = open(os.path.join(glove_dir, 'glove.twitter.27B.50d.txt'), encoding="utf8")
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
453/43:
embedding_dim = 50
not_embedd = []
embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i < max_words:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
        else:
            not_embedd.append(word)
453/44: embedding_matrix.shape
453/45: np.where(~embedding_matrix.any(axis=1))[0]
453/46: not_embedd[-20:]
453/47: #df_train_clean[df_train_clean.text.str.contains('disea')]
453/48: #df_train[df_train.text.str.contains('disea')]
453/49: embedding_matrix.shape
453/50: model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
453/51:
#model_glove = Sequential()
#model_glove.add(Embedding(20000, 100, input_length=150, weights=[embedding_matrix], trainable=False))
#model_glove.add(Dropout(0.2))
#model_glove.add(Conv1D(64, 5, activation='relu'))
#model_glove.add(MaxPooling1D(pool_size=4))
#model_glove.add(LSTM(300))
#model_glove.add(Dense(1, activation='sigmoid'))
#model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
453/52:
model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length = maxlen))
#model.add(Flatten())
#model.add(Dense(32, activation='relu'))
#model.add(Dense(1, activation='sigmoid'))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))
model.summary()
453/53:
model.layers[0].set_weights([embedding_matrix])
model.layers[0].trainable = False
453/54: model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
453/55: history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=[X_val, y_val])
453/56:
import numpy as np
import pandas as pd
import string
import copy
import re
import contractions
import os

from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
453/57: model.save_weights('glove_model.h5')
453/58: %tensorboard --logdir logs/fit
453/59: #%tensorboard --logdir logs/fit
453/60:
acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
453/61: epochs = range(1, len(acc)+1)
453/62: epochs
453/63:
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'r-', label='Training val_acc')
453/64: y_pred = model.predict(test_data)
453/65: df_t = df_test[['id']]
453/66: df_t['target'] = (y_pred >= 0.5).astype(int)
453/67: df_t.to_csv("LSTM_3.csv", index=False)
453/68: df_t.describe()
453/69:
tokenizer = Tokenizer(num_words=max_words, oov_token=oov_tok)
tokenizer.fit_on_texts(train_texts)

train_sequences = tokenizer.texts_to_sequences(train_texts)
test_sequences = tokenizer.texts_to_sequences(test_texts)
453/70: oov_tok = '<OOV>'
453/71:
tokenizer = Tokenizer(num_words=max_words, oov_token=oov_tok)
tokenizer.fit_on_texts(train_texts)

train_sequences = tokenizer.texts_to_sequences(train_texts)
test_sequences = tokenizer.texts_to_sequences(test_texts)
453/72:
word_index = tokenizer.word_index
dict(list(word_index.items())[0:10])
453/73: train_texts
453/74: df_train_clean.all_text.str.contains(' s ')
453/75: df_train_clean[df_train_clean.all_text.str.contains(' s ')]
453/76: df_test.iloc[246,:]
453/77: df_test.iloc[246,3]
453/78: df_train.iloc[250, 3]
453/79: df_train.iloc[7580, 3]
453/80:
abbreviations = {
    "$" : " dollar ",
    "‚Ç¨" : " euro ",
    "4ao" : "for adults only",
    "a.m" : "before midday",
    "a3" : "anytime anywhere anyplace",
    "aamof" : "as a matter of fact",
    "acct" : "account",
    "adih" : "another day in hell",
    "afaic" : "as far as i am concerned",
    "afaict" : "as far as i can tell",
    "afaik" : "as far as i know",
    "afair" : "as far as i remember",
    "afk" : "away from keyboard",
    "app" : "application",
    "approx" : "approximately",
    "apps" : "applications",
    "asap" : "as soon as possible",
    "asl" : "age, sex, location",
    "atk" : "at the keyboard",
    "ave." : "avenue",
    "aymm" : "are you my mother",
    "ayor" : "at your own risk", 
    "b&b" : "bed and breakfast",
    "b+b" : "bed and breakfast",
    "b.c" : "before christ",
    "b2b" : "business to business",
    "b2c" : "business to customer",
    "b4" : "before",
    "b4n" : "bye for now",
    "b@u" : "back at you",
    "bae" : "before anyone else",
    "bak" : "back at keyboard",
    "bbbg" : "bye bye be good",
    "bbc" : "british broadcasting corporation",
    "bbias" : "be back in a second",
    "bbl" : "be back later",
    "bbs" : "be back soon",
    "be4" : "before",
    "bfn" : "bye for now",
    "blvd" : "boulevard",
    "bout" : "about",
    "brb" : "be right back",
    "bros" : "brothers",
    "brt" : "be right there",
    "bsaaw" : "big smile and a wink",
    "btw" : "by the way",
    "bwl" : "bursting with laughter",
    "c/o" : "care of",
    "cet" : "central european time",
    "cf" : "compare",
    "cia" : "central intelligence agency",
    "csl" : "can not stop laughing",
    "cu" : "see you",
    "cul8r" : "see you later",
    "cv" : "curriculum vitae",
    "cwot" : "complete waste of time",
    "cya" : "see you",
    "cyt" : "see you tomorrow",
    "dae" : "does anyone else",
    "dbmib" : "do not bother me i am busy",
    "diy" : "do it yourself",
    "dm" : "direct message",
    "dwh" : "during work hours",
    "e123" : "easy as one two three",
    "eet" : "eastern european time",
    "eg" : "example",
    "embm" : "early morning business meeting",
    "encl" : "enclosed",
    "encl." : "enclosed",
    "etc" : "and so on",
    "faq" : "frequently asked questions",
    "fawc" : "for anyone who cares",
    "fb" : "facebook",
    "fc" : "fingers crossed",
    "fig" : "figure",
    "fimh" : "forever in my heart", 
    "ft." : "feet",
    "ft" : "featuring",
    "ftl" : "for the loss",
    "ftw" : "for the win",
    "fwiw" : "for what it is worth",
    "fyi" : "for your information",
    "g9" : "genius",
    "gahoy" : "get a hold of yourself",
    "gal" : "get a life",
    "gcse" : "general certificate of secondary education",
    "gfn" : "gone for now",
    "gg" : "good game",
    "gl" : "good luck",
    "glhf" : "good luck have fun",
    "gmt" : "greenwich mean time",
    "gmta" : "great minds think alike",
    "gn" : "good night",
    "g.o.a.t" : "greatest of all time",
    "goat" : "greatest of all time",
    "goi" : "get over it",
    "gps" : "global positioning system",
    "gr8" : "great",
    "gratz" : "congratulations",
    "gyal" : "girl",
    "h&c" : "hot and cold",
    "hp" : "horsepower",
    "hr" : "hour",
    "hrh" : "his royal highness",
    "ht" : "height",
    "ibrb" : "i will be right back",
    "ic" : "i see",
    "icq" : "i seek you",
    "icymi" : "in case you missed it",
    "idc" : "i do not care",
    "idgadf" : "i do not give a damn fuck",
    "idgaf" : "i do not give a fuck",
    "idk" : "i do not know",
    "ie" : "that is",
    "i.e" : "that is",
    "ifyp" : "i feel your pain",
    "IG" : "instagram",
    "iirc" : "if i remember correctly",
    "ilu" : "i love you",
    "ily" : "i love you",
    "imho" : "in my humble opinion",
    "imo" : "in my opinion",
    "imu" : "i miss you",
    "iow" : "in other words",
    "irl" : "in real life",
    "j4f" : "just for fun",
    "jic" : "just in case",
    "jk" : "just kidding",
    "jsyk" : "just so you know",
    "l8r" : "later",
    "lb" : "pound",
    "lbs" : "pounds",
    "ldr" : "long distance relationship",
    "lmao" : "laugh my ass off",
    "lmfao" : "laugh my fucking ass off",
    "lol" : "laughing out loud",
    "ltd" : "limited",
    "ltns" : "long time no see",
    "m8" : "mate",
    "mf" : "motherfucker",
    "mfs" : "motherfuckers",
    "mfw" : "my face when",
    "mofo" : "motherfucker",
    "mph" : "miles per hour",
    "mr" : "mister",
    "mrw" : "my reaction when",
    "ms" : "miss",
    "mte" : "my thoughts exactly",
    "nagi" : "not a good idea",
    "nbc" : "national broadcasting company",
    "nbd" : "not big deal",
    "nfs" : "not for sale",
    "ngl" : "not going to lie",
    "nhs" : "national health service",
    "nrn" : "no reply necessary",
    "nsfl" : "not safe for life",
    "nsfw" : "not safe for work",
    "nth" : "nice to have",
    "nvr" : "never",
    "nyc" : "new york city",
    "oc" : "original content",
    "og" : "original",
    "ohp" : "overhead projector",
    "oic" : "oh i see",
    "omdb" : "over my dead body",
    "omg" : "oh my god",
    "omw" : "on my way",
    "p.a" : "per annum",
    "p.m" : "after midday",
    "pm" : "prime minister",
    "poc" : "people of color",
    "pov" : "point of view",
    "pp" : "pages",
    "ppl" : "people",
    "prw" : "parents are watching",
    "ps" : "postscript",
    "pt" : "point",
    "ptb" : "please text back",
    "pto" : "please turn over",
    "qpsa" : "what happens", #"que pasa",
    "ratchet" : "rude",
    "rbtl" : "read between the lines",
    "rlrt" : "real life retweet", 
    "rofl" : "rolling on the floor laughing",
    "roflol" : "rolling on the floor laughing out loud",
    "rotflmao" : "rolling on the floor laughing my ass off",
    "rt" : "retweet",
    "ruok" : "are you ok",
    "sfw" : "safe for work",
    "sk8" : "skate",
    "smh" : "shake my head",
    "sq" : "square",
    "srsly" : "seriously", 
    "ssdd" : "same stuff different day",
    "tbh" : "to be honest",
    "tbs" : "tablespooful",
    "tbsp" : "tablespooful",
    "tfw" : "that feeling when",
    "thks" : "thank you",
    "tho" : "though",
    "thx" : "thank you",
    "tia" : "thanks in advance",
    "til" : "today i learned",
    "tl;dr" : "too long i did not read",
    "tldr" : "too long i did not read",
    "tmb" : "tweet me back",
    "tntl" : "trying not to laugh",
    "ttyl" : "talk to you later",
    "u" : "you",
    "u2" : "you too",
    "u4e" : "yours for ever",
    "utc" : "coordinated universal time",
    "w/" : "with",
    "w/o" : "without",
    "w8" : "wait",
    "wassup" : "what is up",
    "wb" : "welcome back",
    "wtf" : "what the fuck",
    "wtg" : "way to go",
    "wtpa" : "where the party at",
    "wuf" : "where are you from",
    "wuzup" : "what is up",
    "wywh" : "wish you were here",
    "yd" : "yard",
    "ygtr" : "you got that right",
    "ynk" : "you never know",
    "zzz" : "sleeping bored and tired",
    
    "u.s " : "united states",
    "u.s." : "united states"
}
453/81:
##### PREPROC
#remove not ascii
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    return text

# Change an abbreviation by its true meaning
def word_abbrev(word):
    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word

# Replace all abbreviations
def replace_abbrev(text):
    string = ""
    for word in text.split():
        string += word_abbrev(word) + " "        
    return string

# Remove @ and mention, replace by USER
def replace_mention(text):
    pat=re.compile(r'@\S+')
    return pat.sub(r' ',text)

# Remove numbers, replace it by NUMBER
def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r' ', text)

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()

#contractions and remowing stopwords and very short words
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 3:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
453/82:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(string_contractions)
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(clean_text)
    return df
453/83:
df_train = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')
453/84:
df_train_clean = basic_cleaning(df_train, 'text')
df_test_clean = basic_cleaning(df_test, 'text')
453/85:
w_tokenizer = WhitespaceTokenizer()
def lemmatize_text(text):
    return [wnl.lemmatize(w) for w in w_tokenizer.tokenize(text)]

df_train_clean['text_lemmatized'] = df_train_clean.text.apply(lemmatize_text)
df_train_clean['all_text'] = df_train_clean['text_lemmatized'].apply(lambda x: " ".join(x))

df_test_clean['text_lemmatized'] = df_test_clean.text.apply(lemmatize_text)
df_test_clean['all_text'] = df_test_clean['text_lemmatized'].apply(lambda x: " ".join(x))
453/86:
maxlen = 150
#train = 4000
max_words = 20000
453/87:
train_texts = df_train_clean.all_text.to_list()
test_texts = df_test_clean.all_text.to_list()
453/88: df_train.iloc[7580, 3]
453/89: df_train_clean[df_train_clean.all_text.str.contains(' s ')]
453/90: oov_tok = '<OOV>'
453/91:
tokenizer = Tokenizer(num_words=max_words, oov_token=oov_tok)
tokenizer.fit_on_texts(train_texts)

train_sequences = tokenizer.texts_to_sequences(train_texts)
test_sequences = tokenizer.texts_to_sequences(test_texts)
453/92:
word_index = tokenizer.word_index
dict(list(word_index.items())[0:10])
453/93:
word_index = tokenizer.word_index
print(f'Found {len(word_index)} unique tokens')
453/94: df_train.iloc[349, 3]
453/95: df_train.iloc[246, 3]
453/96: re.sub(r'[^\x00-\x7f]', '', text)
453/97: re.sub(r'[^\x00-\x7f]', '', df_train.text)
453/98: re.findall(r'[^\x00-\x7f]', '', df_train.text)
453/99:
pat = re.sub(r'[^\x00-\x7f]', '', df_train.text)
df_train['text'].apply(pat)
453/100:
##### PREPROC
#remove not ascii
def replace_us(text):
    pat=re.compile(r'U.S')
    return pat.sub(r'united states',text)
    
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    return text

# Change an abbreviation by its true meaning
def word_abbrev(word):
    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word

# Replace all abbreviations
def replace_abbrev(text):
    string = ""
    for word in text.split():
        string += word_abbrev(word) + " "        
    return string

# Remove @ and mention, replace by USER
def replace_mention(text):
    pat=re.compile(r'@\S+')
    return pat.sub(r' ',text)

# Remove numbers, replace it by NUMBER
def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r' ', text)

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()

#contractions and remowing stopwords and very short words
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 3:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
453/101:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(replace_us)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(string_contractions)
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(clean_text)
    return df
453/102:
df_train = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')
453/103:
df_train_clean = basic_cleaning(df_train, 'text')
df_test_clean = basic_cleaning(df_test, 'text')
453/104: df_train.iloc[246, 3]
453/105: df_train.iloc[246, :]
453/106: df_train_clean.iloc[246, :]
453/107:
w_tokenizer = WhitespaceTokenizer()
def lemmatize_text(text):
    return [wnl.lemmatize(w) for w in w_tokenizer.tokenize(text)]

df_train_clean['text_lemmatized'] = df_train_clean.text.apply(lemmatize_text)
df_train_clean['all_text'] = df_train_clean['text_lemmatized'].apply(lambda x: " ".join(x))

df_test_clean['text_lemmatized'] = df_test_clean.text.apply(lemmatize_text)
df_test_clean['all_text'] = df_test_clean['text_lemmatized'].apply(lambda x: " ".join(x))
453/108:
maxlen = 150
#train = 4000
max_words = 20000
453/109:
train_texts = df_train_clean.all_text.to_list()
test_texts = df_test_clean.all_text.to_list()
453/110: df_train_clean.iloc[246, :]
453/111: df_train_clean[df_train_clean.all_text.str.contains(' s ')]
453/112: oov_tok = '<OOV>'
453/113:
tokenizer = Tokenizer(num_words=max_words, oov_token=oov_tok)
tokenizer.fit_on_texts(train_texts)

train_sequences = tokenizer.texts_to_sequences(train_texts)
test_sequences = tokenizer.texts_to_sequences(test_texts)
453/114:
word_index = tokenizer.word_index
dict(list(word_index.items())[0:10])
453/115:
word_index = tokenizer.word_index
print(f'Found {len(word_index)} unique tokens')
453/116: df_train_clean.iloc[326, :]
453/117: df_train.iloc[326, :]
453/118: df_train.iloc[148, :]
453/119: df_train.iloc[326, :]
453/120: df_train.iloc[7570, :]
453/121:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(replace_us)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(string_contractions)
    df[column_name] = df[column_name].apply(clean_text)
    return df
453/122:
df_train = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')
453/123:
df_train_clean = basic_cleaning(df_train, 'text')
df_test_clean = basic_cleaning(df_test, 'text')
453/124:
w_tokenizer = WhitespaceTokenizer()
def lemmatize_text(text):
    return [wnl.lemmatize(w) for w in w_tokenizer.tokenize(text)]

df_train_clean['text_lemmatized'] = df_train_clean.text.apply(lemmatize_text)
df_train_clean['all_text'] = df_train_clean['text_lemmatized'].apply(lambda x: " ".join(x))

df_test_clean['text_lemmatized'] = df_test_clean.text.apply(lemmatize_text)
df_test_clean['all_text'] = df_test_clean['text_lemmatized'].apply(lambda x: " ".join(x))
453/125:
maxlen = 150
#train = 4000
max_words = 20000
453/126:
train_texts = df_train_clean.all_text.to_list()
test_texts = df_test_clean.all_text.to_list()
453/127: df_train.iloc[7570, :]
453/128: df_train_clean[df_train_clean.all_text.str.contains(' s ')]
453/129: oov_tok = '<OOV>'
453/130:
tokenizer = Tokenizer(num_words=max_words, oov_token=oov_tok)
tokenizer.fit_on_texts(train_texts)

train_sequences = tokenizer.texts_to_sequences(train_texts)
test_sequences = tokenizer.texts_to_sequences(test_texts)
453/131:
word_index = tokenizer.word_index
dict(list(word_index.items())[0:10])
453/132:
word_index = tokenizer.word_index
print(f'Found {len(word_index)} unique tokens')
453/133: #train_texts
453/134:
train_data = pad_sequences(train_sequences, maxlen=maxlen)
test_data = pad_sequences(test_sequences, maxlen=maxlen)
labels = np.array(df_train.target.to_list())
453/135:
print(train_data.shape)
print(test_data.shape)
453/136: X_train, X_val, y_train, y_val = train_test_split(train_data, labels, test_size=0.25, stratify=labels, shuffle=True)
453/137:
#X_train = train_data[:train]
#y_train = labels[:train]
#X_val = train_data[train:]
#y_val = labels[train:]
453/138:
glove_dir = '/Users/Me/Desktop/GIT_pro/twitter/BERT/glove.twitter.27B'
embeddings_index = {}
f = open(os.path.join(glove_dir, 'glove.twitter.27B.50d.txt'), encoding="utf8")
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
453/139:
embedding_dim = 50
not_embedd = []
embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i < max_words:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
        else:
            not_embedd.append(word)
453/140: embedding_matrix.shape
453/141: np.where(~embedding_matrix.any(axis=1))[0]
453/142: len(not_embedd)
453/143: not_embedd[-20:]
453/144: df_train_clean
453/145:
##### PREPROC
#remove not ascii
def replace_us(text):
    pat=re.compile(r'U.S')
    return pat.sub(r'united states',text)
    
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    return text

# Change an abbreviation by its true meaning
def word_abbrev(word):
    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word

# Replace all abbreviations
def replace_abbrev(text):
    string = ""
    for word in text.split():
        string += word_abbrev(word) + " "  
    print(string)
    return string

# Remove @ and mention, replace by USER
def replace_mention(text):
    pat=re.compile(r'@\S+')
    return pat.sub(r' ',text)

# Remove numbers, replace it by NUMBER
def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r' ', text)

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

def convert_lower_case(text):
    return text.lower()

#contractions and remowing stopwords and very short words
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 3:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
453/146:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(replace_us)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(string_contractions)
    df[column_name] = df[column_name].apply(clean_text)
    return df
453/147:
df_train = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')
453/148:
df_train_clean = basic_cleaning(df_train, 'text')
df_test_clean = basic_cleaning(df_test, 'text')
453/149:
##### PREPROC
#remove not ascii
def convert_lower_case(text):
    return text.lower()

def replace_us(text):
    pat=re.compile(r'u.s')
    return pat.sub(r'united states',text)
    
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    return text

# Change an abbreviation by its true meaning
def word_abbrev(word):
    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word

# Replace all abbreviations
def replace_abbrev(text):
    string = ""
    for word in text.split():
        string += word_abbrev(word) + " "  
    print(string)
    return string

# Remove @ and mention, replace by USER
def replace_mention(text):
    pat=re.compile(r'@\S+')
    return pat.sub(r' ',text)

# Remove numbers, replace it by NUMBER
def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r' ', text)

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

#contractions and remowing stopwords and very short words
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 3:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
453/150:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(replace_us)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(string_contractions)
    df[column_name] = df[column_name].apply(clean_text)
    return df
453/151:
df_train = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')
453/152:
df_train_clean = basic_cleaning(df_train, 'text')
df_test_clean = basic_cleaning(df_test, 'text')
453/153:
##### PREPROC
#remove not ascii
def convert_lower_case(text):
    return text.lower()

def replace_us(text):
    pat=re.compile(r'u.s')
    return pat.sub(r'united states',text)
    
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    return text

# Change an abbreviation by its true meaning
def word_abbrev(word):
    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word

# Replace all abbreviations
def replace_abbrev(text):
    string = ""
    for word in text.split():
        string += word_abbrev(word) + " "  
    return string

# Remove @ and mention, replace by USER
def replace_mention(text):
    pat=re.compile(r'@\S+')
    return pat.sub(r'USER',text)

# Remove numbers, replace it by NUMBER
def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r'NUMBER', text)

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

#contractions and remowing stopwords and very short words
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 3:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
453/154:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(replace_us)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(string_contractions)
    df[column_name] = df[column_name].apply(clean_text)
    return df
453/155:
df_train = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')
453/156:
df_train_clean = basic_cleaning(df_train, 'text')
df_test_clean = basic_cleaning(df_test, 'text')
453/157:
w_tokenizer = WhitespaceTokenizer()
def lemmatize_text(text):
    return [wnl.lemmatize(w) for w in w_tokenizer.tokenize(text)]

df_train_clean['text_lemmatized'] = df_train_clean.text.apply(lemmatize_text)
df_train_clean['all_text'] = df_train_clean['text_lemmatized'].apply(lambda x: " ".join(x))

df_test_clean['text_lemmatized'] = df_test_clean.text.apply(lemmatize_text)
df_test_clean['all_text'] = df_test_clean['text_lemmatized'].apply(lambda x: " ".join(x))
453/158: df_train_clean
453/159:
maxlen = 150
#train = 4000
max_words = 20000
453/160:
train_texts = df_train_clean.all_text.to_list()
test_texts = df_test_clean.all_text.to_list()
453/161: df_train.iloc[7570, :]
453/162: df_train_clean[df_train_clean.all_text.str.contains(' s ')]
453/163: oov_tok = '<OOV>'
453/164:
tokenizer = Tokenizer(num_words=max_words, oov_token=oov_tok)
tokenizer.fit_on_texts(train_texts)

train_sequences = tokenizer.texts_to_sequences(train_texts)
test_sequences = tokenizer.texts_to_sequences(test_texts)
453/165:
word_index = tokenizer.word_index
dict(list(word_index.items())[0:10])
453/166: df_train[df_train.text.str.contains('utc')]
453/167: df_train_clean[df_train_clean.text.str.contains('utc')]
453/168: df_train[df_train.text.str.contains('utc ')]
453/169: df_train[df_train.text.str.contains(' utc')]
453/170: df_train[df_train.text.str.contains('utc')]
453/171: df_train.iloc[3053, :]
453/172:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(replace_us)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(string_contractions)
    df[column_name] = df[column_name].apply(clean_text)
    return df
453/173:
df_train = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')
453/174:
df_train_clean = basic_cleaning(df_train, 'text')
df_test_clean = basic_cleaning(df_test, 'text')
453/175:
w_tokenizer = WhitespaceTokenizer()
def lemmatize_text(text):
    return [wnl.lemmatize(w) for w in w_tokenizer.tokenize(text)]

df_train_clean['text_lemmatized'] = df_train_clean.text.apply(lemmatize_text)
df_train_clean['all_text'] = df_train_clean['text_lemmatized'].apply(lambda x: " ".join(x))

df_test_clean['text_lemmatized'] = df_test_clean.text.apply(lemmatize_text)
df_test_clean['all_text'] = df_test_clean['text_lemmatized'].apply(lambda x: " ".join(x))
453/176: df_train_clean
453/177:
maxlen = 150
#train = 4000
max_words = 20000
453/178:
train_texts = df_train_clean.all_text.to_list()
test_texts = df_test_clean.all_text.to_list()
453/179: df_train.iloc[7570, :]
453/180: df_train_clean[df_train_clean.all_text.str.contains(' s ')]
453/181: oov_tok = '<OOV>'
453/182:
tokenizer = Tokenizer(num_words=max_words, oov_token=oov_tok)
tokenizer.fit_on_texts(train_texts)

train_sequences = tokenizer.texts_to_sequences(train_texts)
test_sequences = tokenizer.texts_to_sequences(test_texts)
453/183:
word_index = tokenizer.word_index
dict(list(word_index.items())[0:10])
453/184: df_train_clean
453/185: df_train.iloc[7610, :]
453/186: df_train_clean.iloc[7610, :]
453/187:
##### PREPROC
#remove not ascii
def convert_lower_case(text):
    return text.lower()

def replace_us(text):
    pat=re.compile(r'u.s')
    return pat.sub(r'united states',text)
    
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    return text

# Change an abbreviation by its true meaning
def word_abbrev(word):
    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word

# Replace all abbreviations
def replace_abbrev(text):
    string = ""
    for word in text.split():
        string += word_abbrev(word) + " "  
    return string

# Remove @ and mention, replace by USER
def replace_mention(text):
    pat=re.compile(r'@\S+')
    return pat.sub(r' USER ',text)

# Remove numbers, replace it by NUMBER
def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r' NUMBER ', text)

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

#contractions and remowing stopwords and very short words
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 2:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
453/188:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(replace_us)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(string_contractions)
    df[column_name] = df[column_name].apply(clean_text)
    return df
453/189:
df_train = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')
453/190:
df_train_clean = basic_cleaning(df_train, 'text')
df_test_clean = basic_cleaning(df_test, 'text')
453/191:
w_tokenizer = WhitespaceTokenizer()
def lemmatize_text(text):
    return [wnl.lemmatize(w) for w in w_tokenizer.tokenize(text)]

df_train_clean['text_lemmatized'] = df_train_clean.text.apply(lemmatize_text)
df_train_clean['all_text'] = df_train_clean['text_lemmatized'].apply(lambda x: " ".join(x))

df_test_clean['text_lemmatized'] = df_test_clean.text.apply(lemmatize_text)
df_test_clean['all_text'] = df_test_clean['text_lemmatized'].apply(lambda x: " ".join(x))
453/192: df_train_clean.iloc[7610, :]
453/193: df_train_clean
453/194:
maxlen = 150
#train = 4000
max_words = 20000
453/195:
train_texts = df_train_clean.all_text.to_list()
test_texts = df_test_clean.all_text.to_list()
453/196: df_train.iloc[7570, :]
453/197: df_train_clean[df_train_clean.all_text.str.contains(' s ')]
453/198: oov_tok = '<OOV>'
453/199:
tokenizer = Tokenizer(num_words=max_words, oov_token=oov_tok)
tokenizer.fit_on_texts(train_texts)

train_sequences = tokenizer.texts_to_sequences(train_texts)
test_sequences = tokenizer.texts_to_sequences(test_texts)
453/200:
word_index = tokenizer.word_index
dict(list(word_index.items())[0:10])
453/201:
word_index = tokenizer.word_index
dict(list(word_index.items()))
453/202:
##### PREPROC
#remove not ascii
def convert_lower_case(text):
    return text.lower()

def replace_us(text):
    pat=re.compile(r'u.s')
    return pat.sub(r'united states',text)
    
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    return text

# Change an abbreviation by its true meaning
def word_abbrev(word):
    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word

# Replace all abbreviations
def replace_abbrev(text):
    string = ""
    for word in text.split():
        string += word_abbrev(word) + " "  
    return string

# Remove @ and mention, replace by USER
def replace_mention(text):
    pat=re.compile(r'@\S+')
    return pat.sub(r' USER ',text)

# Remove numbers, replace it by NUMBER
def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r' NUMBER ', text)

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

#contractions and remowing stopwords and very short words
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 2:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
453/203:
##### PREPROC
#remove not ascii
def convert_lower_case(text):
    return text.lower()

def replace_us(text):
    pat=re.compile(r'u.s')
    return pat.sub(r'united states',text)
    
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    return text

# Change an abbreviation by its true meaning
def word_abbrev(word):
    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word

# Replace all abbreviations
def replace_abbrev(text):
    string = ""
    for word in text.split():
        string += word_abbrev(word) + " "  
    return string

# Remove @ and mention, replace by USER
def replace_mention(text):
    pat=re.compile(r'@\S+')
    return pat.sub(r' USER ',text)

# Remove numbers, replace it by NUMBER
def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r' NUMBER ', text)

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

#contractions and remowing stopwords and very short words
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 2:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
453/204:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(replace_us)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(string_contractions)
    return df
453/205:
df_train = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')
453/206:
df_train_clean = basic_cleaning(df_train, 'text')
df_test_clean = basic_cleaning(df_test, 'text')
453/207:
w_tokenizer = WhitespaceTokenizer()
def lemmatize_text(text):
    return [wnl.lemmatize(w) for w in w_tokenizer.tokenize(text)]

df_train_clean['text_lemmatized'] = df_train_clean.text.apply(lemmatize_text)
df_train_clean['all_text'] = df_train_clean['text_lemmatized'].apply(lambda x: " ".join(x))

df_test_clean['text_lemmatized'] = df_test_clean.text.apply(lemmatize_text)
df_test_clean['all_text'] = df_test_clean['text_lemmatized'].apply(lambda x: " ".join(x))
453/208: df_train_clean.iloc[7610, :]
453/209: df_train_clean
453/210:
maxlen = 150
#train = 4000
max_words = 20000
453/211:
train_texts = df_train_clean.all_text.to_list()
test_texts = df_test_clean.all_text.to_list()
453/212: df_train.iloc[7570, :]
453/213: df_train_clean[df_train_clean.all_text.str.contains(' s ')]
453/214: oov_tok = '<OOV>'
453/215:
tokenizer = Tokenizer(num_words=max_words, oov_token=oov_tok)
tokenizer.fit_on_texts(train_texts)

train_sequences = tokenizer.texts_to_sequences(train_texts)
test_sequences = tokenizer.texts_to_sequences(test_texts)
453/216:
word_index = tokenizer.word_index
dict(list(word_index.items()))
453/217: df_train_clean[df_train_clean.all_text.str.contains(' u ')]
453/218:
##### PREPROC
#remove not ascii
def convert_lower_case(text):
    return text.lower()

def replace_us(text):
    pat=re.compile(r'u.s ')
    return pat.sub(r'united states',text)
    
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    return text

# Change an abbreviation by its true meaning
def word_abbrev(word):
    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word

# Replace all abbreviations
def replace_abbrev(text):
    string = ""
    for word in text.split():
        string += word_abbrev(word) + " "  
    return string

# Remove @ and mention, replace by USER
def replace_mention(text):
    pat=re.compile(r'@\S+')
    return pat.sub(r' USER ',text)

# Remove numbers, replace it by NUMBER
def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r' NUMBER ', text)

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

#contractions and remowing stopwords and very short words
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 2:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
453/219:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(replace_us)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(string_contractions)
    return df
453/220:
df_train = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')
453/221:
df_train_clean = basic_cleaning(df_train, 'text')
df_test_clean = basic_cleaning(df_test, 'text')
453/222:
w_tokenizer = WhitespaceTokenizer()
def lemmatize_text(text):
    return [wnl.lemmatize(w) for w in w_tokenizer.tokenize(text)]

df_train_clean['text_lemmatized'] = df_train_clean.text.apply(lemmatize_text)
df_train_clean['all_text'] = df_train_clean['text_lemmatized'].apply(lambda x: " ".join(x))

df_test_clean['text_lemmatized'] = df_test_clean.text.apply(lemmatize_text)
df_test_clean['all_text'] = df_test_clean['text_lemmatized'].apply(lambda x: " ".join(x))
453/223: df_train_clean.iloc[7610, :]
453/224: df_train_clean[df_train_clean.all_text.str.contains(' u ')]
453/225: contractions.fix('us')
453/226:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(replace_us)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(clean_text)
 #   df[column_name] = df[column_name].apply(string_contractions)
    return df
453/227:
df_train = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')
453/228:
df_train_clean = basic_cleaning(df_train, 'text')
df_test_clean = basic_cleaning(df_test, 'text')
453/229:
w_tokenizer = WhitespaceTokenizer()
def lemmatize_text(text):
    return [wnl.lemmatize(w) for w in w_tokenizer.tokenize(text)]

df_train_clean['text_lemmatized'] = df_train_clean.text.apply(lemmatize_text)
df_train_clean['all_text'] = df_train_clean['text_lemmatized'].apply(lambda x: " ".join(x))

df_test_clean['text_lemmatized'] = df_test_clean.text.apply(lemmatize_text)
df_test_clean['all_text'] = df_test_clean['text_lemmatized'].apply(lambda x: " ".join(x))
453/230: df_train_clean.iloc[7610, :]
453/231: df_train_clean[df_train_clean.all_text.str.contains(' u ')]
453/232:
maxlen = 150
#train = 4000
max_words = 20000
453/233:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(convert_lower_case)
  #  df[column_name] = df[column_name].apply(replace_us)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(remove_punct)
   # df[column_name] = df[column_name].apply(replace_abbrev)
   # df[column_name] = df[column_name].apply(replace_mention) 
   # df[column_name] = df[column_name].apply(delete_not_ascii)
   # df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(string_contractions)
    return df
453/234:
df_train = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')
453/235:
df_train_clean = basic_cleaning(df_train, 'text')
df_test_clean = basic_cleaning(df_test, 'text')
453/236:
w_tokenizer = WhitespaceTokenizer()
def lemmatize_text(text):
    return [wnl.lemmatize(w) for w in w_tokenizer.tokenize(text)]

df_train_clean['text_lemmatized'] = df_train_clean.text.apply(lemmatize_text)
df_train_clean['all_text'] = df_train_clean['text_lemmatized'].apply(lambda x: " ".join(x))

df_test_clean['text_lemmatized'] = df_test_clean.text.apply(lemmatize_text)
df_test_clean['all_text'] = df_test_clean['text_lemmatized'].apply(lambda x: " ".join(x))
453/237: df_train_clean.iloc[7610, :]
453/238: df_train_clean[df_train_clean.all_text.str.contains(' u ')]
453/239:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(replace_us)
    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(string_contractions)
    return df
453/240:
df_train = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')
453/241:
df_train_clean = basic_cleaning(df_train, 'text')
df_test_clean = basic_cleaning(df_test, 'text')
453/242:
w_tokenizer = WhitespaceTokenizer()
def lemmatize_text(text):
    return [wnl.lemmatize(w) for w in w_tokenizer.tokenize(text)]

df_train_clean['text_lemmatized'] = df_train_clean.text.apply(lemmatize_text)
df_train_clean['all_text'] = df_train_clean['text_lemmatized'].apply(lambda x: " ".join(x))

df_test_clean['text_lemmatized'] = df_test_clean.text.apply(lemmatize_text)
df_test_clean['all_text'] = df_test_clean['text_lemmatized'].apply(lambda x: " ".join(x))
453/243: df_train_clean.iloc[7610, :]
453/244:
def word_tokenize_pos(text):
    for word, tag in pos_tag(word_tokenize(sent)):
        wntag = tag[0].lower()
        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
        if not wntag:
            lemma = word
        else:
            lemma = wnl.lemmatize(word, wntag)
    print lemma
453/245:
def word_tokenize_pos(text):
    for word, tag in pos_tag(word_tokenize(sent)):
        wntag = tag[0].lower()
        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
        if not wntag:
            lemma = word
        else:
            lemma = wnl.lemmatize(word, wntag)
    print (lemma)
453/246: df_train_clean.text.apply(word_tokenize_pos)
453/247: from nltk import pos_tag, word_tokenize
453/248:
def word_tokenize_pos(text):
    for word, tag in pos_tag(word_tokenize(sent)):
        wntag = tag[0].lower()
        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
        if not wntag:
            lemma = word
        else:
            lemma = wnl.lemmatize(word, wntag)
    print (lemma)
453/249: df_train_clean.text.apply(word_tokenize_pos)
453/250:
def word_tokenize_pos(text):
    for word, tag in pos_tag(word_tokenize(text)):
        wntag = tag[0].lower()
        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
        if not wntag:
            lemma = word
        else:
            lemma = wnl.lemmatize(word, wntag)
    print (lemma)
453/251: df_train_clean.text.apply(word_tokenize_pos)
453/252: df_train_clean[0].text.apply(word_tokenize_pos)
453/253: df_train_clean[0]
453/254: df_train_clean[[0]]
453/255: df_train_clean[1:2]
453/256: df_train_clean[1:2].text.apply(word_tokenize_pos)
453/257: df_train_clean[1:2].text
453/258:
def word_tokenize_pos(text):
    for word, tag in pos_tag(word_tokenize(text)):
        print(tag)
        print(word)
        wntag = tag[0].lower()
        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
        if not wntag:
            lemma = word
        else:
            lemma = wnl.lemmatize(word, wntag)
    print (lemma)
453/259: df_train_clean[1:2].text
453/260: df_train_clean[1:2].text.apply(word_tokenize_pos)
453/261:
def word_tokenize_pos(text):
    for word, tag in pos_tag(word_tokenize(text)):
        print(tag[0])
        print(word)
        wntag = tag[0].lower()
        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
        if not wntag:
            lemma = word
        else:
            lemma = wnl.lemmatize(word, wntag)
    print (lemma)
453/262: df_train_clean[1:2].text
453/263: df_train_clean[1:2].text.apply(word_tokenize_pos)
453/264:
def word_tokenize_pos(text):
    for word, tag in pos_tag(word_tokenize(text)):
        print(tag[0].lower())
        print(word)
        wntag = tag[0].lower()
        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
        if not wntag:
            lemma = word
        else:
            lemma = wnl.lemmatize(word, wntag)
    print (lemma)
453/265: df_train_clean[1:2].text
453/266: df_train_clean[1:2].text.apply(word_tokenize_pos)
453/267:
def word_tokenize_pos(text):
    for word, tag in pos_tag(word_tokenize(text)):
        print(tag.lower())
        print(word)
        wntag = tag[0].lower()
        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
        if not wntag:
            lemma = word
        else:
            lemma = wnl.lemmatize(word, wntag)
    print (lemma)
453/268: df_train_clean[1:2].text
453/269: df_train_clean[1:2].text.apply(word_tokenize_pos)
453/270:
def word_tokenize_pos(text):
    for word, tag in pos_tag(word_tokenize(text)):
        str_lem = ''
        print(tag.lower())
        print(word)
        wntag = tag[0].lower()
        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
        if not wntag:
            lemma = str_lem.join(word)
        else:
            lemma = str_lem.join(wnl.lemmatize(word, wntag))
    print (lemma)
453/271: df_train_clean[1:2].text
453/272: df_train_clean[1:2].text.apply(word_tokenize_pos)
453/273:
def word_tokenize_pos(text):
    for word, tag in pos_tag(word_tokenize(text)):
        str_lem = ''

        wntag = tag[0].lower()
        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
        if not wntag:
            lemma = str_lem.join(word)
        else:
            lemma = str_lem.join(wnl.lemmatize(word, wntag))
    print (lemma)
453/274: df_train_clean[1:2].text
453/275: df_train_clean[1:2].text.apply(word_tokenize_pos)
453/276:
def word_tokenize_pos(text):
    str_lem = ''
    for word, tag in pos_tag(word_tokenize(text)):

        wntag = tag[0].lower()
        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
        if not wntag:
            lemma = str_lem.join(word)
        else:
            lemma = str_lem.join(wnl.lemmatize(word, wntag))
    print (lemma)
453/277: df_train_clean[1:2].text
453/278: df_train_clean[1:2].text.apply(word_tokenize_pos)
453/279:
def word_tokenize_pos(text):
    str_lem = ''
    for word, tag in pos_tag(word_tokenize(text)):
        wntag = tag[0].lower()
        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
        if not wntag:
            lemma = str_lem.join(word)
        else:
            lemma = str_lem.join(wnl.lemmatize(word, wntag))
    print(lemma)
453/280: df_train_clean[1:2].text
453/281: df_train_clean[1:2].text.apply(word_tokenize_pos)
453/282:
def word_tokenize_pos(text):
    str_lem = ''
    for word, tag in pos_tag(word_tokenize(text)):
        wntag = tag[0].lower()
        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
        if not wntag:
            lemma = str_lem.join(word)
        else:
            lemma = str_lem.join(wnl.lemmatize(word, wntag))
    print(lemma)
    return lemma
453/283: df_train_clean[1:2].text
453/284: df_train_clean[1:2].text.apply(word_tokenize_pos)
453/285:
def word_tokenize_pos(text):
    str_lem = ''
    for word, tag in pos_tag(word_tokenize(text)):
        wntag = tag[0].lower()
        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
        if not wntag:
            str_lem = str_lem.join(word)
        else:
            str_lem = str_lem.join(wnl.lemmatize(word, wntag))
    print(lemma)
    return lemma
453/286: df_train_clean[1:2].text
453/287: df_train_clean[1:2].text.apply(word_tokenize_pos)
453/288:
def word_tokenize_pos(text):
    str_lem = ''
    for word, tag in pos_tag(word_tokenize(text)):
        wntag = tag[0].lower()
        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
        if not wntag:
            str_lem = str_lem.join(word)
        else:
            str_lem = str_lem.join(wnl.lemmatize(word, wntag))
    print(str_lem)
    return str_lem
453/289: df_train_clean[1:2].text
453/290: df_train_clean[1:2].text.apply(word_tokenize_pos)
453/291:
def word_tokenize_pos(text):
    str_lem = ''
    for word, tag in pos_tag(word_tokenize(text)):
        wntag = tag[0].lower()
        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
        if not wntag:
            str_lem = str_lem.join(word)
        else:
            str_lem = str_lem.join(wnl.lemmatize(word, wntag))
  #  print(str_lem)
    return str_lem
453/292: df_train_clean[1:2].text
453/293: df_train_clean[1:2].text.apply(word_tokenize_pos)
453/294:
def word_tokenize_pos(text):
    str_lem = ''
    for word, tag in pos_tag(word_tokenize(text)):
        print(word)
        wntag = tag[0].lower()
        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
        if not wntag:
            str_lem = str_lem.join(word)
        else:
            str_lem = str_lem.join(wnl.lemmatize(word, wntag))
  #  print(str_lem)
    return str_lem
453/295: df_train_clean[1:2].text
453/296: df_train_clean[1:2].text.apply(word_tokenize_pos)
453/297:
def word_tokenize_pos(text):
    list_lem = []
    for word, tag in pos_tag(word_tokenize(text)):
        print(word)
        wntag = tag[0].lower()
        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
        if not wntag:
            list_lem.append(word)
        else:
            list_lem.append(word)
  #  print(str_lem)
    return list_lem
453/298: df_train_clean[1:2].text
453/299: df_train_clean[1:2].text.apply(word_tokenize_pos)
453/300: df_train_clean[7232:7233].text.apply(word_tokenize_pos)
453/301:
def word_tokenize_pos(text):
    list_lem = []
    for word, tag in pos_tag(word_tokenize(text)):
        print(word)
        wntag = tag[0].lower()
        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
        if not wntag:
            list_lem.append(word)
        else:
            list_lem.append(word)
  #  print(str_lem)
    return list_lem
453/302: df_train_clean[1:2].text
453/303: df_train_clean[7232:7233].text.apply(word_tokenize_pos)
453/304:
w_tokenizer = WhitespaceTokenizer()
def lemmatize_text(text):
    return [wnl.lemmatize(w) for w in w_tokenizer.tokenize(text)]

df_train_clean['text_lemmatized'] = df_train_clean.text.apply(word_tokenize_pos)
df_train_clean['all_text'] = df_train_clean['text_lemmatized'].apply(lambda x: " ".join(x))

df_test_clean['text_lemmatized'] = df_test_clean.text.apply(word_tokenize_pos)
df_test_clean['all_text'] = df_test_clean['text_lemmatized'].apply(lambda x: " ".join(x))
453/305:
def word_tokenize_pos(text):
    list_lem = []
    for word, tag in pos_tag(word_tokenize(text)):
        wntag = tag[0].lower()
        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
        if not wntag:
            list_lem.append(word)
        else:
            list_lem.append(word)
  #  print(str_lem)
    return list_lem
453/306: df_train_clean[1:2].text
453/307: df_train_clean[7232:7233].text.apply(word_tokenize_pos)
453/308:
w_tokenizer = WhitespaceTokenizer()
def lemmatize_text(text):
    return [wnl.lemmatize(w) for w in w_tokenizer.tokenize(text)]

df_train_clean['text_lemmatized'] = df_train_clean.text.apply(word_tokenize_pos)
df_train_clean['all_text'] = df_train_clean['text_lemmatized'].apply(lambda x: " ".join(x))

df_test_clean['text_lemmatized'] = df_test_clean.text.apply(word_tokenize_pos)
df_test_clean['all_text'] = df_test_clean['text_lemmatized'].apply(lambda x: " ".join(x))
453/309: df_train_clean.iloc[7610, :]
453/310: df_train_clean[df_train_clean.all_text.str.contains(' u ')]
453/311: df_train_clean
453/312:
maxlen = 150
#train = 4000
max_words = 20000
453/313:
train_texts = df_train_clean.all_text.to_list()
test_texts = df_test_clean.all_text.to_list()
453/314: df_train.iloc[7570, :]
453/315: df_train_clean[df_train_clean.all_text.str.contains(' s ')]
453/316: oov_tok = '<OOV>'
453/317:
tokenizer = Tokenizer(num_words=max_words, oov_token=oov_tok)
tokenizer.fit_on_texts(train_texts)

train_sequences = tokenizer.texts_to_sequences(train_texts)
test_sequences = tokenizer.texts_to_sequences(test_texts)
453/318:
word_index = tokenizer.word_index
dict(list(word_index.items()))
453/319: df_train_clean[df_train_clean.all_text.str.contains(' gt ')]
453/320: df_train.iloc[178, :]
453/321: df_train.iloc[344, :]
453/322:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(replace_us)

    df[column_name] = df[column_name].apply(replace_gt)
    df[column_name] = df[column_name].apply(replace_lt)

    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(string_contractions)
    return df
453/323:
df_train = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')
453/324:
df_train_clean = basic_cleaning(df_train, 'text')
df_test_clean = basic_cleaning(df_test, 'text')
453/325: from nltk import pos_tag, word_tokenize
453/326:
##### PREPROC
#remove not ascii
def convert_lower_case(text):
    return text.lower()

def replace_us(text):
    pat=re.compile(r'u.s ')
    return pat.sub(r'united states',text)

def replace_gt(text):
    pat=re.compile(r'&gt')
    return pat.sub(r' ',text)

def replace_lt(text):
    pat=re.compile(r'&lt')
    return pat.sub(r' ',text)

def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    return text

# Change an abbreviation by its true meaning
def word_abbrev(word):
    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word

# Replace all abbreviations
def replace_abbrev(text):
    string = ""
    for word in text.split():
        string += word_abbrev(word) + " "  
    return string

# Remove @ and mention, replace by USER
def replace_mention(text):
    pat=re.compile(r'@\S+')
    return pat.sub(r' USER ',text)

# Remove numbers, replace it by NUMBER
def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r' NUMBER ', text)

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

#contractions and remowing stopwords and very short words
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 2:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
453/327: contractions.fix('us')
453/328:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(replace_us)

    df[column_name] = df[column_name].apply(replace_gt)
    df[column_name] = df[column_name].apply(replace_lt)

    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(string_contractions)
    return df
453/329:
df_train = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')
453/330:
df_train_clean = basic_cleaning(df_train, 'text')
df_test_clean = basic_cleaning(df_test, 'text')
453/331: from nltk import pos_tag, word_tokenize
453/332:
def word_tokenize_pos(text):
    list_lem = []
    for word, tag in pos_tag(word_tokenize(text)):
        wntag = tag[0].lower()
        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
        if not wntag:
            list_lem.append(word)
        else:
            list_lem.append(word)
  #  print(str_lem)
    return list_lem
453/333: df_train_clean[1:2].text
453/334: df_train_clean[7232:7233].text.apply(word_tokenize_pos)
453/335:
w_tokenizer = WhitespaceTokenizer()
def lemmatize_text(text):
    return [wnl.lemmatize(w) for w in w_tokenizer.tokenize(text)]

df_train_clean['text_lemmatized'] = df_train_clean.text.apply(word_tokenize_pos)
df_train_clean['all_text'] = df_train_clean['text_lemmatized'].apply(lambda x: " ".join(x))

df_test_clean['text_lemmatized'] = df_test_clean.text.apply(word_tokenize_pos)
df_test_clean['all_text'] = df_test_clean['text_lemmatized'].apply(lambda x: " ".join(x))
453/336: df_train_clean.iloc[7610, :]
453/337: df_train_clean[df_train_clean.all_text.str.contains(' gt ')]
453/338:
maxlen = 150
#train = 4000
max_words = 20000
453/339:
train_texts = df_train_clean.all_text.to_list()
test_texts = df_test_clean.all_text.to_list()
453/340: df_train.iloc[344, :]
453/341: df_train_clean[df_train_clean.all_text.str.contains(' gt ')]
453/342: oov_tok = '<OOV>'
453/343:
tokenizer = Tokenizer(num_words=max_words, oov_token=oov_tok)
tokenizer.fit_on_texts(train_texts)

train_sequences = tokenizer.texts_to_sequences(train_texts)
test_sequences = tokenizer.texts_to_sequences(test_texts)
453/344:
word_index = tokenizer.word_index
dict(list(word_index.items()))
453/345: df_train.iloc[1974, :]
453/346: df_train_clean[df_train_clean.all_text.str.contains(' gt ')]
453/347: df_train.iloc[1971, :]
453/348: df_train.iloc[1974, 3]
453/349: df_train.iloc[1971, 3]
453/350: df_train_clean[df_train_clean.all_text.str.contains(' mh ')]
454/1:
import numpy as np
import pandas as pd
import string
import copy
import re
import contractions
import os

from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
454/2:
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import WhitespaceTokenizer
wnl = WordNetLemmatizer()
STOPWORDS = set(stopwords.words('english'))
454/3:
from keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences
from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Embedding, SpatialDropout1D
from keras.models import Sequential
454/4: %load_ext tensorboard
454/5:
abbreviations = {
    "$" : " dollar ",
    "‚Ç¨" : " euro ",
    "4ao" : "for adults only",
    "a.m" : "before midday",
    "a3" : "anytime anywhere anyplace",
    "aamof" : "as a matter of fact",
    "acct" : "account",
    "adih" : "another day in hell",
    "afaic" : "as far as i am concerned",
    "afaict" : "as far as i can tell",
    "afaik" : "as far as i know",
    "afair" : "as far as i remember",
    "afk" : "away from keyboard",
    "app" : "application",
    "approx" : "approximately",
    "apps" : "applications",
    "asap" : "as soon as possible",
    "asl" : "age, sex, location",
    "atk" : "at the keyboard",
    "ave." : "avenue",
    "aymm" : "are you my mother",
    "ayor" : "at your own risk", 
    "b&b" : "bed and breakfast",
    "b+b" : "bed and breakfast",
    "b.c" : "before christ",
    "b2b" : "business to business",
    "b2c" : "business to customer",
    "b4" : "before",
    "b4n" : "bye for now",
    "b@u" : "back at you",
    "bae" : "before anyone else",
    "bak" : "back at keyboard",
    "bbbg" : "bye bye be good",
    "bbc" : "british broadcasting corporation",
    "bbias" : "be back in a second",
    "bbl" : "be back later",
    "bbs" : "be back soon",
    "be4" : "before",
    "bfn" : "bye for now",
    "blvd" : "boulevard",
    "bout" : "about",
    "brb" : "be right back",
    "bros" : "brothers",
    "brt" : "be right there",
    "bsaaw" : "big smile and a wink",
    "btw" : "by the way",
    "bwl" : "bursting with laughter",
    "c/o" : "care of",
    "cet" : "central european time",
    "cf" : "compare",
    "cia" : "central intelligence agency",
    "csl" : "can not stop laughing",
    "cu" : "see you",
    "cul8r" : "see you later",
    "cv" : "curriculum vitae",
    "cwot" : "complete waste of time",
    "cya" : "see you",
    "cyt" : "see you tomorrow",
    "dae" : "does anyone else",
    "dbmib" : "do not bother me i am busy",
    "diy" : "do it yourself",
    "dm" : "direct message",
    "dwh" : "during work hours",
    "e123" : "easy as one two three",
    "eet" : "eastern european time",
    "eg" : "example",
    "embm" : "early morning business meeting",
    "encl" : "enclosed",
    "encl." : "enclosed",
    "etc" : "and so on",
    "faq" : "frequently asked questions",
    "fawc" : "for anyone who cares",
    "fb" : "facebook",
    "fc" : "fingers crossed",
    "fig" : "figure",
    "fimh" : "forever in my heart", 
    "ft." : "feet",
    "ft" : "featuring",
    "ftl" : "for the loss",
    "ftw" : "for the win",
    "fwiw" : "for what it is worth",
    "fyi" : "for your information",
    "g9" : "genius",
    "gahoy" : "get a hold of yourself",
    "gal" : "get a life",
    "gcse" : "general certificate of secondary education",
    "gfn" : "gone for now",
    "gg" : "good game",
    "gl" : "good luck",
    "glhf" : "good luck have fun",
    "gmt" : "greenwich mean time",
    "gmta" : "great minds think alike",
    "gn" : "good night",
    "g.o.a.t" : "greatest of all time",
    "goat" : "greatest of all time",
    "goi" : "get over it",
    "gps" : "global positioning system",
    "gr8" : "great",
    "gratz" : "congratulations",
    "gyal" : "girl",
    "h&c" : "hot and cold",
    "hp" : "horsepower",
    "hr" : "hour",
    "hrh" : "his royal highness",
    "ht" : "height",
    "ibrb" : "i will be right back",
    "ic" : "i see",
    "icq" : "i seek you",
    "icymi" : "in case you missed it",
    "idc" : "i do not care",
    "idgadf" : "i do not give a damn fuck",
    "idgaf" : "i do not give a fuck",
    "idk" : "i do not know",
    "ie" : "that is",
    "i.e" : "that is",
    "ifyp" : "i feel your pain",
    "IG" : "instagram",
    "iirc" : "if i remember correctly",
    "ilu" : "i love you",
    "ily" : "i love you",
    "imho" : "in my humble opinion",
    "imo" : "in my opinion",
    "imu" : "i miss you",
    "iow" : "in other words",
    "irl" : "in real life",
    "j4f" : "just for fun",
    "jic" : "just in case",
    "jk" : "just kidding",
    "jsyk" : "just so you know",
    "l8r" : "later",
    "lb" : "pound",
    "lbs" : "pounds",
    "ldr" : "long distance relationship",
    "lmao" : "laugh my ass off",
    "lmfao" : "laugh my fucking ass off",
    "lol" : "laughing out loud",
    "ltd" : "limited",
    "ltns" : "long time no see",
    "m8" : "mate",
    "mf" : "motherfucker",
    "mfs" : "motherfuckers",
    "mfw" : "my face when",
    "mofo" : "motherfucker",
    "mph" : "miles per hour",
    "mr" : "mister",
    "mrw" : "my reaction when",
    "ms" : "miss",
    "mte" : "my thoughts exactly",
    "nagi" : "not a good idea",
    "nbc" : "national broadcasting company",
    "nbd" : "not big deal",
    "nfs" : "not for sale",
    "ngl" : "not going to lie",
    "nhs" : "national health service",
    "nrn" : "no reply necessary",
    "nsfl" : "not safe for life",
    "nsfw" : "not safe for work",
    "nth" : "nice to have",
    "nvr" : "never",
    "nyc" : "new york city",
    "oc" : "original content",
    "og" : "original",
    "ohp" : "overhead projector",
    "oic" : "oh i see",
    "omdb" : "over my dead body",
    "omg" : "oh my god",
    "omw" : "on my way",
    "p.a" : "per annum",
    "p.m" : "after midday",
    "pm" : "prime minister",
    "poc" : "people of color",
    "pov" : "point of view",
    "pp" : "pages",
    "ppl" : "people",
    "prw" : "parents are watching",
    "ps" : "postscript",
    "pt" : "point",
    "ptb" : "please text back",
    "pto" : "please turn over",
    "qpsa" : "what happens", #"que pasa",
    "ratchet" : "rude",
    "rbtl" : "read between the lines",
    "rlrt" : "real life retweet", 
    "rofl" : "rolling on the floor laughing",
    "roflol" : "rolling on the floor laughing out loud",
    "rotflmao" : "rolling on the floor laughing my ass off",
    "rt" : "retweet",
    "ruok" : "are you ok",
    "sfw" : "safe for work",
    "sk8" : "skate",
    "smh" : "shake my head",
    "sq" : "square",
    "srsly" : "seriously", 
    "ssdd" : "same stuff different day",
    "tbh" : "to be honest",
    "tbs" : "tablespooful",
    "tbsp" : "tablespooful",
    "tfw" : "that feeling when",
    "thks" : "thank you",
    "tho" : "though",
    "thx" : "thank you",
    "tia" : "thanks in advance",
    "til" : "today i learned",
    "tl;dr" : "too long i did not read",
    "tldr" : "too long i did not read",
    "tmb" : "tweet me back",
    "tntl" : "trying not to laugh",
    "ttyl" : "talk to you later",
    "u" : "you",
    "u2" : "you too",
    "u4e" : "yours for ever",
    "utc" : "coordinated universal time",
    "w/" : "with",
    "w/o" : "without",
    "w8" : "wait",
    "wassup" : "what is up",
    "wb" : "welcome back",
    "wtf" : "what the fuck",
    "wtg" : "way to go",
    "wtpa" : "where the party at",
    "wuf" : "where are you from",
    "wuzup" : "what is up",
    "wywh" : "wish you were here",
    "yd" : "yard",
    "ygtr" : "you got that right",
    "ynk" : "you never know",
    "zzz" : "sleeping bored and tired",
}
454/6:
##### PREPROC
#remove not ascii
def convert_lower_case(text):
    return text.lower()

def replace_us(text):
    pat=re.compile(r'u.s ')
    return pat.sub(r'united states',text)

def replace_gt(text):
    pat=re.compile(r'&gt')
    return pat.sub(r' ',text)

def replace_lt(text):
    pat=re.compile(r'&lt')
    return pat.sub(r' ',text)

def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    return text

# Change an abbreviation by its true meaning
def word_abbrev(word):
    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word

# Replace all abbreviations
def replace_abbrev(text):
    string = ""
    for word in text.split():
        string += word_abbrev(word) + " "  
    return string

# Remove @ and mention, replace by USER
def replace_mention(text):
    pat=re.compile(r'@\S+')
    return pat.sub(r' USER ',text)

# Remove numbers, replace it by NUMBER
def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r' NUMBER ', text)

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

#contractions and remowing stopwords and very short words
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 2:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
454/7: contractions.fix('us')
454/8:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(replace_us)

    df[column_name] = df[column_name].apply(replace_gt)
    df[column_name] = df[column_name].apply(replace_lt)

    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(string_contractions)
    return df
454/9:
df_train = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')
454/10:
df_train_clean = basic_cleaning(df_train, 'text')
df_test_clean = basic_cleaning(df_test, 'text')
454/11: from nltk import pos_tag, word_tokenize
454/12:
def word_tokenize_pos(text):
    list_lem = []
    for word, tag in pos_tag(word_tokenize(text)):
        wntag = tag[0].lower()
        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
        if not wntag:
            list_lem.append(word)
        else:
            list_lem.append(word)
  #  print(str_lem)
    return list_lem
454/13:
df_train_clean['text_lemmatized'] = df_train_clean.text.apply(word_tokenize_pos)
df_train_clean['all_text'] = df_train_clean['text_lemmatized'].apply(lambda x: " ".join(x))

df_test_clean['text_lemmatized'] = df_test_clean.text.apply(word_tokenize_pos)
df_test_clean['all_text'] = df_test_clean['text_lemmatized'].apply(lambda x: " ".join(x))
454/14: df_train_clean
454/15:
maxlen = 150
#train = 4000
max_words = 20000
454/16:
train_texts = df_train_clean.all_text.to_list()
test_texts = df_test_clean.all_text.to_list()
454/17: oov_tok = '<OOV>'
454/18:
tokenizer = Tokenizer(num_words=max_words, oov_token=oov_tok)
tokenizer.fit_on_texts(train_texts)

train_sequences = tokenizer.texts_to_sequences(train_texts)
test_sequences = tokenizer.texts_to_sequences(test_texts)
454/19:
word_index = tokenizer.word_index
dict(list(word_index.items()))
454/20: df_train_clean[df_train_clean.all_text.str.contains(' amp ')]
454/21: df_train.iloc[67, 3]
454/22: df_train.iloc[67, :]
454/23:
##### PREPROC
#remove not ascii
def convert_lower_case(text):
    return text.lower()

def replace_us(text):
    pat=re.compile(r'u.s ')
    return pat.sub(r'united states',text)

def replace_amp(text):
    pat=re.compile(r'&amp;')
    return pat.sub(r'and',text)

def replace_gt(text):
    pat=re.compile(r'&gt')
    return pat.sub(r' ',text)


def replace_lt(text):
    pat=re.compile(r'&lt')
    return pat.sub(r' ',text)

def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    return text

# Change an abbreviation by its true meaning
def word_abbrev(word):
    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word

# Replace all abbreviations
def replace_abbrev(text):
    string = ""
    for word in text.split():
        string += word_abbrev(word) + " "  
    return string

# Remove @ and mention, replace by USER
def replace_mention(text):
    pat=re.compile(r'@\S+')
    return pat.sub(r' USER ',text)

# Remove numbers, replace it by NUMBER
def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r' NUMBER ', text)

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

#contractions and remowing stopwords and very short words
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 2:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
454/24:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(replace_us)

    df[column_name] = df[column_name].apply(replace_amp)
    df[column_name] = df[column_name].apply(replace_gt)
    df[column_name] = df[column_name].apply(replace_lt)

    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(string_contractions)
    return df
454/25:
df_train = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')
454/26:
df_train_clean = basic_cleaning(df_train, 'text')
df_test_clean = basic_cleaning(df_test, 'text')
454/27: from nltk import pos_tag, word_tokenize
454/28:
def word_tokenize_pos(text):
    list_lem = []
    for word, tag in pos_tag(word_tokenize(text)):
        wntag = tag[0].lower()
        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
        if not wntag:
            list_lem.append(word)
        else:
            list_lem.append(word)
  #  print(str_lem)
    return list_lem
454/29:
df_train_clean['text_lemmatized'] = df_train_clean.text.apply(word_tokenize_pos)
df_train_clean['all_text'] = df_train_clean['text_lemmatized'].apply(lambda x: " ".join(x))

df_test_clean['text_lemmatized'] = df_test_clean.text.apply(word_tokenize_pos)
df_test_clean['all_text'] = df_test_clean['text_lemmatized'].apply(lambda x: " ".join(x))
454/30: df_train_clean
454/31:
maxlen = 150
#train = 4000
max_words = 20000
454/32:
train_texts = df_train_clean.all_text.to_list()
test_texts = df_test_clean.all_text.to_list()
454/33: df_train_clean[df_train_clean.all_text.str.contains(' amp ')]
454/34: oov_tok = '<OOV>'
454/35:
tokenizer = Tokenizer(num_words=max_words, oov_token=oov_tok)
tokenizer.fit_on_texts(train_texts)

train_sequences = tokenizer.texts_to_sequences(train_texts)
test_sequences = tokenizer.texts_to_sequences(test_texts)
454/36:
word_index = tokenizer.word_index
dict(list(word_index.items()))
454/37: df_train
454/38: df_train[df_train.text.str.contains('&')]
454/39: df_train_clean.iloc[7374, :]
454/40:
word_index = tokenizer.word_index
print(f'Found {len(word_index)} unique tokens')
454/41:
train_data = pad_sequences(train_sequences, maxlen=maxlen)
test_data = pad_sequences(test_sequences, maxlen=maxlen)
labels = np.array(df_train.target.to_list())
454/42:
print(train_data.shape)
print(test_data.shape)
454/43: X_train, X_val, y_train, y_val = train_test_split(train_data, labels, test_size=0.25, stratify=labels, shuffle=True)
454/44:
glove_dir = '/Users/Me/Desktop/GIT_pro/twitter/BERT/glove.twitter.27B'
embeddings_index = {}
f = open(os.path.join(glove_dir, 'glove.twitter.27B.50d.txt'), encoding="utf8")
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
454/45:
embedding_dim = 50
not_embedd = []
embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i < max_words:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
        else:
            not_embedd.append(word)
454/46: embedding_matrix.shape
454/47: np.where(~embedding_matrix.any(axis=1))[0]
454/48: len(not_embedd)
454/49: not_embedd[]
454/50: not_embedd
454/51:
glove_dir = '/Users/Me/Desktop/GIT_pro/twitter/BERT/glove.twitter.27B'
embeddings_index = {}
f = open(os.path.join(glove_dir, 'glove.twitter.27B.200d.txt'), encoding="utf8")
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
454/52:
glove_dir = '/Users/Me/Desktop/GIT_pro/twitter/BERT/glove.twitter.27B'
embeddings_index = {}
f = open(os.path.join(glove_dir, 'glove.twitter.27B.100d.txt'), encoding="utf8")
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
454/53:
embedding_dim = 100
not_embedd = []
embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i < max_words:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
        else:
            not_embedd.append(word)
454/54: embedding_matrix.shape
454/55: np.where(~embedding_matrix.any(axis=1))[0]
454/56: len(not_embedd)
454/57: not_embedd
454/58:
glove_dir = '/Users/Me/Desktop/GIT_pro/twitter/BERT/glove.twitter.27B'
embeddings_index = {}
f = open(os.path.join(glove_dir, 'glove.twitter.27B.50d.txt'), encoding="utf8")
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
454/59:
embedding_dim = 50
not_embedd = []
embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i < max_words:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
        else:
            not_embedd.append(word)
454/60: embedding_matrix.shape
454/61: np.where(~embedding_matrix.any(axis=1))[0]
454/62: len(not_embedd)
454/63: embedding_matrix.shape
454/64:
model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length = maxlen))
#model.add(Flatten())
#model.add(Dense(32, activation='relu'))
#model.add(Dense(1, activation='sigmoid'))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))
model.summary()
454/65:
model.layers[0].set_weights([embedding_matrix])
model.layers[0].trainable = False
454/66: model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
454/67: history = model.fit(X_train, y_train, epochs=3, batch_size=32, validation_data=[X_val, y_val])
454/68:
glove_dir = '/Users/Me/Desktop/GIT_pro/twitter/BERT/glove.twitter.27B'
embeddings_index = {}
f = open(os.path.join(glove_dir, 'glove.6B.50d.txt'), encoding="utf8")
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
454/69:
embedding_dim = 50
not_embedd = []
embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i < max_words:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
        else:
            not_embedd.append(word)
454/70: embedding_matrix.shape
454/71: np.where(~embedding_matrix.any(axis=1))[0]
454/72: len(not_embedd)
454/73:
glove_dir = '/Users/Me/Desktop/GIT_pro/twitter/BERT/glove.twitter.27B'
embeddings_index = {}
f = open(os.path.join(glove_dir, 'glove.twitter.27B.50d.txt'), encoding="utf8")
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
454/74:
embedding_dim = 50
not_embedd = []
embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i < max_words:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
        else:
            not_embedd.append(word)
454/75: embedding_matrix.shape
454/76: np.where(~embedding_matrix.any(axis=1))[0]
454/77: len(not_embedd)
454/78: not_embedd
454/79: #df_train_clean[df_train_clean.text.str.contains('disea')]
454/80: #df_train[df_train.text.str.contains('disea')]
454/81: embedding_matrix.shape
454/82: model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
454/83:
model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length = maxlen))
#model.add(Flatten())
#model.add(Dense(32, activation='relu'))
#model.add(Dense(1, activation='sigmoid'))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))
model.summary()
454/84:
model.layers[0].set_weights([embedding_matrix])
model.layers[0].trainable = False
454/85: model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
454/86: history = model.fit(X_train, y_train, epochs=15, batch_size=32, validation_data=[X_val, y_val])
454/87: model.save_weights('glove_model.h5')
454/88: #%tensorboard --logdir logs/fit
454/89:
acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
454/90: epochs = range(1, len(acc)+1)
454/91: epochs
454/92:
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'r-', label='Training val_acc')
454/93: y_pred = model.predict(test_data)
454/94: df_t = df_test[['id']]
454/95: df_t['target'] = (y_pred >= 0.5).astype(int)
454/96: df_t.to_csv("LSTM_4.csv", index=False)
454/97: df_t.describe()
454/98: model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
454/99: history = model.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=[X_val, y_val])
454/100: model.save_weights('glove_model.h5')
454/101: #%tensorboard --logdir logs/fit
454/102:
acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
454/103: epochs = range(1, len(acc)+1)
454/104: epochs
454/105:
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'r-', label='Training val_acc')
454/106:
model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length = maxlen))
#model.add(Flatten())
#model.add(Dense(32, activation='relu'))
#model.add(Dense(1, activation='sigmoid'))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))
model.summary()
454/107:
model.layers[0].set_weights([embedding_matrix])
model.layers[0].trainable = False
454/108: model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
454/109: history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=[X_val, y_val])
456/1:
import pandas as pd
import numpy as np
import catboost
import optuna
import pickle
import os
from scipy import sparse
456/2:
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score
456/3:
from catboost import CatBoostClassifier, Pool
from optuna.integration import CatBoostPruningCallback
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
456/4:
import os
import matplotlib.pyplot as plt#visualization
%matplotlib inline
import seaborn as sns#visualization
import plotly.offline as py #visualization
py.init_notebook_mode(connected=True)#visualization
import plotly.graph_objs as go#visualization
import plotly.tools as tls#visualization
import plotly.figure_factory as ff
from plotly.offline import init_notebook_mode, iplot
457/1:
import pandas as pd
import numpy as np
import re
import copy
from collections import Counter
import string
import sys
from scipy.sparse import hstack
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline
457/2: pd.set_option('max_colwidth', None)
457/3:
import matplotlib.pyplot as plt
import seaborn as sns
457/4:
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.tokenize.treebank import TreebankWordDetokenizer
from nltk.corpus import stopwords
import contractions

from nltk import word_tokenize, TweetTokenizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer

import emoji as emj
import spacy 
## It's important to use _lg for OOV in future
nlp = spacy.load('en_core_web_lg')
457/5:
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report, roc_curve
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
from sklearn.metrics import f1_score, accuracy_score
457/6:
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
457/7:
import pickle
from joblib import dump
457/8:
df_train = pd.read_csv("../data/train.csv")
df_test = pd.read_csv("../data/test.csv")
457/9:
nltk.download()
STOPWORDS = set(stopwords.words('english'))
457/10: wnl = WordNetLemmatizer()
457/11:
##### PREPROC

def convert_lower_case(text):
    return text.lower()

def replace_us(text):
    pat=re.compile(r'u.s ')
    return pat.sub(r'united states',text)

def replace_amp(text):
    pat=re.compile(r'&amp;')
    return pat.sub(r'and',text)

def replace_gt(text):
    pat=re.compile(r'&gt')
    return pat.sub(r' ',text)


def replace_lt(text):
    pat=re.compile(r'&lt')
    return pat.sub(r' ',text)

#remove not ascii
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    return text

# Change an abbreviation by its true meaning
def word_abbrev(word):
    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word

# Replace all abbreviations
def replace_abbrev(text):
    string = ""
    for word in text.split():
        string += word_abbrev(word) + " "  
    return string

# Remove @ and mention, replace by USER
def replace_mention(text):
    pat=re.compile(r'@\S+')
    return pat.sub(r' USER ',text)

# Remove numbers, replace it by NUMBER
def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r' NUMBER ', text)

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

#contractions and remowing stopwords and very short words
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 2:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
457/12:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(replace_us)

    df[column_name] = df[column_name].apply(replace_amp)
    df[column_name] = df[column_name].apply(replace_gt)
    df[column_name] = df[column_name].apply(replace_lt)

    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(string_contractions)
    return df
457/13: df_text = df_train.loc[:, ['id', 'target', 'keyword', 'text']]
457/14:
df_train_clean = basic_cleaning(df_train, 'text')
df_test_clean = basic_cleaning(df_test, 'text')
457/15:
abbreviations = {
    "$" : " dollar ",
    "‚Ç¨" : " euro ",
    "4ao" : "for adults only",
    "a.m" : "before midday",
    "a3" : "anytime anywhere anyplace",
    "aamof" : "as a matter of fact",
    "acct" : "account",
    "adih" : "another day in hell",
    "afaic" : "as far as i am concerned",
    "afaict" : "as far as i can tell",
    "afaik" : "as far as i know",
    "afair" : "as far as i remember",
    "afk" : "away from keyboard",
    "app" : "application",
    "approx" : "approximately",
    "apps" : "applications",
    "asap" : "as soon as possible",
    "asl" : "age, sex, location",
    "atk" : "at the keyboard",
    "ave." : "avenue",
    "aymm" : "are you my mother",
    "ayor" : "at your own risk", 
    "b&b" : "bed and breakfast",
    "b+b" : "bed and breakfast",
    "b.c" : "before christ",
    "b2b" : "business to business",
    "b2c" : "business to customer",
    "b4" : "before",
    "b4n" : "bye for now",
    "b@u" : "back at you",
    "bae" : "before anyone else",
    "bak" : "back at keyboard",
    "bbbg" : "bye bye be good",
    "bbc" : "british broadcasting corporation",
    "bbias" : "be back in a second",
    "bbl" : "be back later",
    "bbs" : "be back soon",
    "be4" : "before",
    "bfn" : "bye for now",
    "blvd" : "boulevard",
    "bout" : "about",
    "brb" : "be right back",
    "bros" : "brothers",
    "brt" : "be right there",
    "bsaaw" : "big smile and a wink",
    "btw" : "by the way",
    "bwl" : "bursting with laughter",
    "c/o" : "care of",
    "cet" : "central european time",
    "cf" : "compare",
    "cia" : "central intelligence agency",
    "csl" : "can not stop laughing",
    "cu" : "see you",
    "cul8r" : "see you later",
    "cv" : "curriculum vitae",
    "cwot" : "complete waste of time",
    "cya" : "see you",
    "cyt" : "see you tomorrow",
    "dae" : "does anyone else",
    "dbmib" : "do not bother me i am busy",
    "diy" : "do it yourself",
    "dm" : "direct message",
    "dwh" : "during work hours",
    "e123" : "easy as one two three",
    "eet" : "eastern european time",
    "eg" : "example",
    "embm" : "early morning business meeting",
    "encl" : "enclosed",
    "encl." : "enclosed",
    "etc" : "and so on",
    "faq" : "frequently asked questions",
    "fawc" : "for anyone who cares",
    "fb" : "facebook",
    "fc" : "fingers crossed",
    "fig" : "figure",
    "fimh" : "forever in my heart", 
    "ft." : "feet",
    "ft" : "featuring",
    "ftl" : "for the loss",
    "ftw" : "for the win",
    "fwiw" : "for what it is worth",
    "fyi" : "for your information",
    "g9" : "genius",
    "gahoy" : "get a hold of yourself",
    "gal" : "get a life",
    "gcse" : "general certificate of secondary education",
    "gfn" : "gone for now",
    "gg" : "good game",
    "gl" : "good luck",
    "glhf" : "good luck have fun",
    "gmt" : "greenwich mean time",
    "gmta" : "great minds think alike",
    "gn" : "good night",
    "g.o.a.t" : "greatest of all time",
    "goat" : "greatest of all time",
    "goi" : "get over it",
    "gps" : "global positioning system",
    "gr8" : "great",
    "gratz" : "congratulations",
    "gyal" : "girl",
    "h&c" : "hot and cold",
    "hp" : "horsepower",
    "hr" : "hour",
    "hrh" : "his royal highness",
    "ht" : "height",
    "ibrb" : "i will be right back",
    "ic" : "i see",
    "icq" : "i seek you",
    "icymi" : "in case you missed it",
    "idc" : "i do not care",
    "idgadf" : "i do not give a damn fuck",
    "idgaf" : "i do not give a fuck",
    "idk" : "i do not know",
    "ie" : "that is",
    "i.e" : "that is",
    "ifyp" : "i feel your pain",
    "IG" : "instagram",
    "iirc" : "if i remember correctly",
    "ilu" : "i love you",
    "ily" : "i love you",
    "imho" : "in my humble opinion",
    "imo" : "in my opinion",
    "imu" : "i miss you",
    "iow" : "in other words",
    "irl" : "in real life",
    "j4f" : "just for fun",
    "jic" : "just in case",
    "jk" : "just kidding",
    "jsyk" : "just so you know",
    "l8r" : "later",
    "lb" : "pound",
    "lbs" : "pounds",
    "ldr" : "long distance relationship",
    "lmao" : "laugh my ass off",
    "lmfao" : "laugh my fucking ass off",
    "lol" : "laughing out loud",
    "ltd" : "limited",
    "ltns" : "long time no see",
    "m8" : "mate",
    "mf" : "motherfucker",
    "mfs" : "motherfuckers",
    "mfw" : "my face when",
    "mofo" : "motherfucker",
    "mph" : "miles per hour",
    "mr" : "mister",
    "mrw" : "my reaction when",
    "ms" : "miss",
    "mte" : "my thoughts exactly",
    "nagi" : "not a good idea",
    "nbc" : "national broadcasting company",
    "nbd" : "not big deal",
    "nfs" : "not for sale",
    "ngl" : "not going to lie",
    "nhs" : "national health service",
    "nrn" : "no reply necessary",
    "nsfl" : "not safe for life",
    "nsfw" : "not safe for work",
    "nth" : "nice to have",
    "nvr" : "never",
    "nyc" : "new york city",
    "oc" : "original content",
    "og" : "original",
    "ohp" : "overhead projector",
    "oic" : "oh i see",
    "omdb" : "over my dead body",
    "omg" : "oh my god",
    "omw" : "on my way",
    "p.a" : "per annum",
    "p.m" : "after midday",
    "pm" : "prime minister",
    "poc" : "people of color",
    "pov" : "point of view",
    "pp" : "pages",
    "ppl" : "people",
    "prw" : "parents are watching",
    "ps" : "postscript",
    "pt" : "point",
    "ptb" : "please text back",
    "pto" : "please turn over",
    "qpsa" : "what happens", #"que pasa",
    "ratchet" : "rude",
    "rbtl" : "read between the lines",
    "rlrt" : "real life retweet", 
    "rofl" : "rolling on the floor laughing",
    "roflol" : "rolling on the floor laughing out loud",
    "rotflmao" : "rolling on the floor laughing my ass off",
    "rt" : "retweet",
    "ruok" : "are you ok",
    "sfw" : "safe for work",
    "sk8" : "skate",
    "smh" : "shake my head",
    "sq" : "square",
    "srsly" : "seriously", 
    "ssdd" : "same stuff different day",
    "tbh" : "to be honest",
    "tbs" : "tablespooful",
    "tbsp" : "tablespooful",
    "tfw" : "that feeling when",
    "thks" : "thank you",
    "tho" : "though",
    "thx" : "thank you",
    "tia" : "thanks in advance",
    "til" : "today i learned",
    "tl;dr" : "too long i did not read",
    "tldr" : "too long i did not read",
    "tmb" : "tweet me back",
    "tntl" : "trying not to laugh",
    "ttyl" : "talk to you later",
    "u" : "you",
    "u2" : "you too",
    "u4e" : "yours for ever",
    "utc" : "coordinated universal time",
    "w/" : "with",
    "w/o" : "without",
    "w8" : "wait",
    "wassup" : "what is up",
    "wb" : "welcome back",
    "wtf" : "what the fuck",
    "wtg" : "way to go",
    "wtpa" : "where the party at",
    "wuf" : "where are you from",
    "wuzup" : "what is up",
    "wywh" : "wish you were here",
    "yd" : "yard",
    "ygtr" : "you got that right",
    "ynk" : "you never know",
    "zzz" : "sleeping bored and tired"
}
457/16:
##### PREPROC

def convert_lower_case(text):
    return text.lower()

def replace_us(text):
    pat=re.compile(r'u.s ')
    return pat.sub(r'united states',text)

def replace_amp(text):
    pat=re.compile(r'&amp;')
    return pat.sub(r'and',text)

def replace_gt(text):
    pat=re.compile(r'&gt')
    return pat.sub(r' ',text)


def replace_lt(text):
    pat=re.compile(r'&lt')
    return pat.sub(r' ',text)

#remove not ascii
def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    return text

# Change an abbreviation by its true meaning
def word_abbrev(word):
    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word

# Replace all abbreviations
def replace_abbrev(text):
    string = ""
    for word in text.split():
        string += word_abbrev(word) + " "  
    return string

# Remove @ and mention, replace by USER
def replace_mention(text):
    pat=re.compile(r'@\S+')
    return pat.sub(r' USER ',text)

# Remove numbers, replace it by NUMBER
def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r' NUMBER ', text)

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

#contractions and remowing stopwords and very short words
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 2:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
457/17:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(replace_us)

    df[column_name] = df[column_name].apply(replace_amp)
    df[column_name] = df[column_name].apply(replace_gt)
    df[column_name] = df[column_name].apply(replace_lt)

    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(string_contractions)
    return df
457/18: df_text = df_train.loc[:, ['id', 'target', 'keyword', 'text']]
457/19:
df_train_clean = basic_cleaning(df_train, 'text')
df_test_clean = basic_cleaning(df_test, 'text')
457/20: df_train_clean
457/21: df_train_clean
457/22:
def word_tokenize_pos(text):
    list_lem = []
    for word, tag in pos_tag(word_tokenize(text)):
        wntag = tag[0].lower()
        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
        if not wntag:
            list_lem.append(word)
        else:
            list_lem.append(word)
  #  print(str_lem)
    return list_lem
457/23:
df_train_clean['text_lemmatized'] = df_train_clean.text.apply(word_tokenize_pos)
df_train_clean['all_text'] = df_train_clean['text_lemmatized'].apply(lambda x: " ".join(x))

df_test_clean['text_lemmatized'] = df_test_clean.text.apply(word_tokenize_pos)
df_test_clean['all_text'] = df_test_clean['text_lemmatized'].apply(lambda x: " ".join(x))
457/24: from nltk import pos_tag, word_tokenize
457/25:
df_train_clean['text_lemmatized'] = df_train_clean.text.apply(word_tokenize_pos)
df_train_clean['all_text'] = df_train_clean['text_lemmatized'].apply(lambda x: " ".join(x))

df_test_clean['text_lemmatized'] = df_test_clean.text.apply(word_tokenize_pos)
df_test_clean['all_text'] = df_test_clean['text_lemmatized'].apply(lambda x: " ".join(x))
457/26: df_train_clean
457/27: ## CREATE A MODEL BASED ON TEXT
457/28: ### with eng features
457/29: df_copy = copy.deepcopy(df_train_clean)
457/30: text_column = 'all_text'
457/31: df_copy
457/32:
## split to test and train
y = df_copy['target']
X = df_copy.drop('target', axis=1)
# This dataset is unbalanced so add stratify parameter for splitting
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, shuffle=True)
print('X_train shape', X_train.shape)
print('X_test shape', X_test.shape)
457/33:
max_features=15000

# CountVectorizer
count_vectorizer = CountVectorizer(max_features=max_features)

def count_vector(data):
    count_vectorizer = CountVectorizer()
    vect = count_vectorizer.fit_transform(data)
    return vect, count_vectorizer

def tfidf_vector(data):
    tfidf_vectorizer = TfidfVectorizer()
    vect = tfidf_vectorizer.fit_transform(data)
    return vect, tfidf_vectorizer

def min_max(data):
    min_max_scaler = MinMaxScaler()
    vect = min_max_scaler.fit_transform(data)
    return vect, min_max_scaler

# TRAIN
X_train_count, count_vectorizer = count_vector(X_train[text_column])
X_train_tfidf, tfidf_vectorizer = tfidf_vector(X_train[text_column])

#### TEST
X_test_count = count_vectorizer.transform(X_test [text_column])                                                     
X_test_tfidf = tfidf_vectorizer.transform(X_test [text_column])

### VALIDATION DATASET
X_test_count_f = count_vectorizer.transform(df_test_full[text_column])                                                     
X_test_tfidf_f = tfidf_vectorizer.transform(df_test_full[text_column])
457/34:
max_features=15000

# CountVectorizer
count_vectorizer = CountVectorizer(max_features=max_features)

def count_vector(data):
    count_vectorizer = CountVectorizer()
    vect = count_vectorizer.fit_transform(data)
    return vect, count_vectorizer

def tfidf_vector(data):
    tfidf_vectorizer = TfidfVectorizer()
    vect = tfidf_vectorizer.fit_transform(data)
    return vect, tfidf_vectorizer

def min_max(data):
    min_max_scaler = MinMaxScaler()
    vect = min_max_scaler.fit_transform(data)
    return vect, min_max_scaler

# TRAIN
X_train_count, count_vectorizer = count_vector(X_train[text_column])
X_train_tfidf, tfidf_vectorizer = tfidf_vector(X_train[text_column])

#### TEST
X_test_count = count_vectorizer.transform(X_test[text_column])                                                     
X_test_tfidf = tfidf_vectorizer.transform(X_test[text_column])

### VALIDATION DATASET
X_test_count_f = count_vectorizer.transform(df_test_full[text_column])                                                     
X_test_tfidf_f = tfidf_vectorizer.transform(df_test_full[text_column])
457/35:
max_features=15000

# CountVectorizer
count_vectorizer = CountVectorizer(max_features=max_features)

def count_vector(data):
    count_vectorizer = CountVectorizer()
    vect = count_vectorizer.fit_transform(data)
    return vect, count_vectorizer

def tfidf_vector(data):
    tfidf_vectorizer = TfidfVectorizer()
    vect = tfidf_vectorizer.fit_transform(data)
    return vect, tfidf_vectorizer

def min_max(data):
    min_max_scaler = MinMaxScaler()
    vect = min_max_scaler.fit_transform(data)
    return vect, min_max_scaler

# TRAIN
X_train_count, count_vectorizer = count_vector(X_train[text_column])
X_train_tfidf, tfidf_vectorizer = tfidf_vector(X_train[text_column])

#### TEST
X_test_count = count_vectorizer.transform(X_test[text_column])                                                     
X_test_tfidf = tfidf_vectorizer.transform(X_test[text_column])

### VALIDATION DATASET
X_test_count_f = count_vectorizer.transform(df_test_clean[text_column])                                                     
X_test_tfidf_f = tfidf_vectorizer.transform(df_test_clean[text_column])
457/36:
#from scipy import sparse
#### SAVE ALL DF
#sparse.save_npz("../data/X_train_count.npz", X_train_count)
#sparse.save_npz("../data/X_train_tfidf.npz", X_train_tfidf)
#
#sparse.save_npz("../data/X_test_count.npz", X_test_count)
#sparse.save_npz("../data/X_test_tfidf.npz", X_test_tfidf)
#
#sparse.save_npz("../data/X_train_tfidf_minmax.npz", X_train_tfidf_minmax)
#sparse.save_npz("../data/X_train_count_eng.npz", X_train_count_eng)
#
#sparse.save_npz("../data/X_test_tfidf_minmax.npz", X_test_tfidf_minmax)
#sparse.save_npz("../data/X_test_count_eng.npz", X_test_count_eng)
#
##########
#sparse.save_npz("../data/X_test_count_f.npz", X_test_count_f)
#sparse.save_npz("../data/X_test_tfidf_f.npz", X_test_tfidf_f)
#sparse.save_npz("../data/X_test_count_eng_f.npz", X_test_count_eng_f)
#sparse.save_npz("../data/X_test_tfidf_minmax_f.npz", X_test_tfidf_minmax_f)
457/37:
#np.save('../data/y_train.npy', y_train)
#np.save('../data/y_test.npy', y_test)
457/38:
#
#with open('count_vectorizer.pickle', 'wb') as handle:
#    pickle.dump(count_vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
#with open('tfidf_vectorizer.pickle', 'wb') as handle:
#    pickle.dump(tfidf_vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
#with open('minmax_scaler.pickle', 'wb') as handle:
#    pickle.dump(minmax_scaler, handle, protocol=pickle.HIGHEST_PROTOCOL)
457/39:
#functions for searching best threshold
def to_labels(pos_probs, threshold):
    return (pos_probs >= threshold).astype('int')

def best_tresholds(y_pred_proba, y_test):
    # define thresholds
    thresholds = np.arange(0, 1, 0.001)
    # evaluate each threshold
    scores = [f1_score(y_test, to_labels(y_pred_proba, t)) for t in thresholds]
    # get best threshold
    ix = np.argmax(scores)
    #print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))
    return thresholds[ix], scores[ix]
457/40:
#create a df for saving results of each model
metrics = pd.DataFrame(columns=['model' ,'vectoriser', 'f1 score', 'best_f1', 'threshold', 
                                'train accuracy','test accuracy'])
457/41:
def fit_and_predict(model, x_train ,x_test, y_train, y_test, vectoriser):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)

    y_pred = classifier.predict(x_test)
    cmatrix = confusion_matrix(y_test,y_pred)
    
    y_hat = classifier.predict_proba(x_test)[:,1]
    threshold, best_score = best_tresholds(y_hat, y_test)


    f1score = f1_score(y_test,y_pred)
    train_accuracy = round(classifier.score(x_train,y_train)*100)
    test_accuracy =  round(accuracy_score(y_test,y_pred)*100)

    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'f1 score': f1score, 
                              'best_f1': best_score,
                              'threshold': threshold,
                              'train accuracy': train_accuracy, 
                              'test accuracy': test_accuracy, 
                              'vectoriser': str(vectoriser),
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using "+ str(vectoriser))
    print('f1 score for test: {}'.format(f1score))
    print(classification_report(y_test,y_pred))    
    print('Threshold={:.5f}, F-Score={:.5f}'.format(threshold, best_score))
    print('Accuracy of classifier on training set:{}%'.format(train_accuracy))
    print('Accuracy of classifier on test set:{}%' .format(test_accuracy))
457/42:
models=[
        LogisticRegression(random_state=30),
        SVC(random_state=30, probability=True),
        MultinomialNB(),
        DecisionTreeClassifier(random_state = 30),
        KNeighborsClassifier(),
        RandomForestClassifier(random_state=30, max_depth=4),
       ]
457/43:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train_count, X_test_count, y_train ,y_test, 'Count vector')
    
    fit_and_predict(model, X_train_tfidf, X_test_tfidf, y_train, y_test, 'Tfidf vector')
457/44: metrics.sort_values('best_f1')
457/45: metrics.to_csv('classification_models_score_new_preproc.csv')
457/46:
param_grid = {'C': [1, 5, 8, 10, 12, 14], 
              'gamma': [0.1, 0.07, 0.05, 0.01],
              'kernel': ['rbf']}
457/47:
from sklearn.model_selection import StratifiedKFold
StratifiedKFold(n_splits=3, random_state=None, shuffle=False)
457/48:
### CREATE A FILE WITH THE RESULTS OF EACH MODEL IN GRIDSEARCHCV
#In .log files are results of Gridsearchcv. I tried different hyperparameters and datasets 
#and this score is the best one.

#old_stdout = sys.stdout
#log_file = open("SVC_tfidf_vect_3.log","w")
#sys.stdout = log_file
457/49:
grid = GridSearchCV(SVC(probability=True),param_grid,cv=StratifiedKFold(),verbose=0,scoring='f1')
grid.fit(X_train_tfidf_minmax,y_train)
457/50:
grid = GridSearchCV(SVC(probability=True),param_grid,cv=StratifiedKFold(),verbose=0,scoring='f1')
grid.fit(X_train_count, y_train)
457/51:
grid = GridSearchCV(SVC(probability=True),param_grid,cv=StratifiedKFold(),verbose=1,scoring='f1')
grid.fit(X_train_count, y_train)
454/110: model.save_weights('glove_model.h5')
454/111: #%tensorboard --logdir logs/fit
454/112:
acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
454/113: epochs = range(1, len(acc)+1)
454/114: epochs
454/115:
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'r-', label='Training val_acc')
454/116: y_pred = model.predict(test_data)
454/117: df_t = df_test[['id']]
454/118: df_t['target'] = (y_pred >= 0.5).astype(int)
454/119: df_t.to_csv("LSTM_6.csv", index=False)
454/120: df_t.describe()
454/121:
from keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences
from keras.layers import Dense, Flatten, LSTM, Conv1D, Bidirectional, Dropout, Activation, Embedding, SpatialDropout1D
from keras.models import Sequential
454/122:
model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length = maxlen))
#model.add(Flatten())
#model.add(Dense(32, activation='relu'))
#model.add(Dense(1, activation='sigmoid'))
model.add(SpatialDropout1D(0.2))
model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))
model.add(Dense(1, activation='sigmoid'))
model.summary()
454/123:
model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length = maxlen))
#model.add(Flatten())
#model.add(Dense(32, activation='relu'))
#model.add(Dense(1, activation='sigmoid'))
model.add(SpatialDropout1D(0.2))
model.add(Bidirectional(LSTM(100, dropout=0.2, activation='tanh', recurrent_dropout=0.2)))
model.add(Dense(1, activation='sigmoid'))
model.summary()
454/124:
model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length = maxlen))
#model.add(Flatten())
#model.add(Dense(32, activation='relu'))
#model.add(Dense(1, activation='sigmoid'))
model.add(SpatialDropout1D(0.2))
model.add(Bidirectional(LSTM(100, dropout=0.2, activation='tanh', recurrent_dropout=0.2, unroll=True)))
model.add(Dense(1, activation='sigmoid'))
model.summary()
454/125:
model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length = maxlen))
#model.add(Flatten())
#model.add(Dense(32, activation='relu'))
#model.add(Dense(1, activation='sigmoid'))
model.add(SpatialDropout1D(0.2))
model.add(Bidirectional(LSTM(100, dropout=0.2, activation='tanh', recurrent_dropout=0.2)))
model.add(Dense(1, activation='sigmoid'))
model.summary()
454/126:
model.layers[0].set_weights([embedding_matrix])
model.layers[0].trainable = False
454/127: model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
454/128: history = model.fit(X_train, y_train, epochs=3, batch_size=32, validation_data=[X_val, y_val])
457/52:
sys.stdout = old_stdout
log_file.close()
457/53:
print('Best hyperparameters: ', grid.best_params_)
print('Best score: ', grid.best_score_)
457/54:
svc_best = grid.best_estimator_
### test df
y_predict_proba_svc = svc_best.predict_proba(X_test_tfidf_minmax)
y_pred_svc = svc_best.predict(X_test_tfidf_minmax)
print(f1_score(y_test, y_pred_svc))

### validation df
y_predict_proba_svc_f = svc_best.predict_proba(X_test_tfidf_minmax_f)[:,1]
457/55:
svc_best = grid.best_estimator_
### test df
y_predict_proba_svc = svc_best.predict_proba(X_test_tfidf)
y_pred_svc = svc_best.predict(X_test_tfidf_minmax)
print(f1_score(y_test, y_pred_svc))

### validation df
y_predict_proba_svc_f = svc_best.predict_proba(X_test_tfidf_f)[:,1]
457/56:
svc_best = grid.best_estimator_
### test df
y_predict_proba_svc = svc_best.predict_proba(X_test_tfidf)
y_pred_svc = svc_best.predict(X_test_tfidf)
print(f1_score(y_test, y_pred_svc))

### validation df
y_predict_proba_svc_f = svc_best.predict_proba(X_test_tfidf_f)[:,1]
457/57:
svc_best = grid.best_estimator_
### test df
y_predict_proba_svc = svc_best.predict_proba(X_test_tfidf)
y_pred_svc = svc_best.predict(X_test_tfidf)
print(accuracy_score(y_test, y_pred_svc))

### validation df
y_predict_proba_svc_f = svc_best.predict_proba(X_test_tfidf_f)[:,1]
457/58:
svc_best = grid.best_estimator_
### test df
y_predict_proba_svc = svc_best.predict_proba(X_test_count)
y_pred_svc = svc_best.predict(X_test_count)
print(accuracy_score(y_test, y_pred_svc))

### validation df
y_predict_proba_svc_f = svc_best.predict_proba(X_test_tfidf_f)[:,1]
457/59:
param_grid = {'C': [7, 8, 9, 10], 
              'gamma': [0.02, 0.01, 0.008],
              'kernel': ['rbf']}
457/60:
from sklearn.model_selection import StratifiedKFold
StratifiedKFold(n_splits=3, random_state=None, shuffle=False)
457/61:
### CREATE A FILE WITH THE RESULTS OF EACH MODEL IN GRIDSEARCHCV
#In .log files are results of Gridsearchcv. I tried different hyperparameters and datasets 
#and this score is the best one.

#old_stdout = sys.stdout
#log_file = open("SVC_tfidf_vect_3.log","w")
#sys.stdout = log_file
457/62:
grid = GridSearchCV(SVC(probability=True),param_grid,cv=StratifiedKFold(),verbose=10,scoring='f1')
grid.fit(X_train_count, y_train)
454/129: history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=[X_val, y_val])
457/63: grid_svd.best_params_
457/64: grid_svd.best_params_
457/65: param_grid = {'C': [11, 12, 13], 'gamma': [0.07, 0.06, 0.08],'kernel': ['rbf']}
457/66: grid.best_estimator_
457/67:
print('Best hyperparameters: ', grid.best_params_)
print('Best score: ', grid.best_score_)
457/68:
svc_best = grid.best_estimator_
### test df
y_predict_proba_svc = svc_best.predict_proba(X_test_count)
y_pred_svc = svc_best.predict(X_test_count)
print(accuracy_score(y_test, y_pred_svc))

### validation df
y_predict_proba_svc_f = svc_best.predict_proba(X_test_count_f)[:,1]
457/69:
#sys.stdout = old_stdout
#log_file.close()
457/70:
print('Best hyperparameters: ', grid.best_params_)
print('Best score: ', grid.best_score_)
457/71:
svc_best = grid.best_estimator_
### test df
y_predict_proba_svc = svc_best.predict_proba(X_test_count)
y_pred_svc = svc_best.predict(X_test_count)
print(accuracy_score(y_test, y_pred_svc))

### validation df
y_predict_proba_svc_f = svc_best.predict_proba(X_test_count_f)[:,1]
457/72:
#So, on Kaggle the better result was shown with treshold = 0.5 --> Score: 0.80018
df_t = df_test[['id']]
df_t['target'] = (y_predict_proba_svc_f >= 0.5).astype(int)
df_t.to_csv("submission_svm_05.csv", index=False)
457/73: df_t.describe()
457/74:
#So, on Kaggle the better result was shown with treshold = 0.5 --> Score: 0.80018
df_t = df_test[['id']]
df_t['target'] = (y_predict_proba_svc_f >= 0.5).astype(int)
df_t.to_csv("submission_svm_preproc_new.csv", index=False)
457/75: df_t.describe()
457/76: best_tresholds(svc_best.predict_proba(X_train_count), y_train)
457/77: best_tresholds(svc_best.predict_proba(X_train_count)[:,1], y_train)
457/78:
#So, on Kaggle the better result was shown with treshold = 0.5 --> Score: 0.80018
df_t = df_test[['id']]
df_t['target'] = (y_predict_proba_svc_f >= 0.3).astype(int)
df_t.to_csv("submission_svm_preproc_new.csv", index=False)
457/79: df_t.describe()
454/130: history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=[X_val, y_val])
454/131: model.save_weights('glove_model.h5')
454/132: #%tensorboard --logdir logs/fit
454/133:
acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
454/134: epochs = range(1, len(acc)+1)
454/135: epochs
454/136:
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'r-', label='Training val_acc')
454/137: y_pred = model.predict(test_data)
454/138: df_t = df_test[['id']]
454/139: df_t['target'] = (y_pred >= 0.5).astype(int)
454/140: df_t.to_csv("LSTM_7.csv", index=False)
454/141: df_t.describe()
457/80:
#So, on Kaggle the better result was shown with treshold = 0.5 --> Score: 0.80018
df_t = df_test[['id']]
df_t['target'] = (y_predict_proba_svc_f >= 0.3).astype(int)
df_t.to_csv("submission_svm_preproc_new.csv", index=False)
457/81: df_t.describe()
457/82:
#So, on Kaggle the better result was shown with treshold = 0.5 --> Score: 0.80018
df_t = df_test[['id']]
df_t['target'] = (y_predict_proba_svc_f >= 0.308).astype(int)
df_t.to_csv("submission_svm_preproc_new.csv", index=False)
457/83: df_t.describe()
457/84:
param_grid = {'C': [1, 5, 10, 14], 
              'gamma': [0.1, 0.07, 0.05, 0.01],
              'kernel': ['rbf']}
457/85:
from sklearn.model_selection import StratifiedKFold
StratifiedKFold(n_splits=3, random_state=None, shuffle=False)
457/86:
### CREATE A FILE WITH THE RESULTS OF EACH MODEL IN GRIDSEARCHCV
#In .log files are results of Gridsearchcv. I tried different hyperparameters and datasets 
#and this score is the best one.

#old_stdout = sys.stdout
#log_file = open("SVC_tfidf_vect_3.log","w")
#sys.stdout = log_file
457/87:
grid = GridSearchCV(SVC(probability=True),param_grid,cv=StratifiedKFold(),verbose=10,scoring='f1')
grid.fit(X_train_tfidf, y_train)
457/88:
print('Best hyperparameters: ', grid.best_params_)
print('Best score: ', grid.best_score_)
460/1:
import numpy as np
import pandas as pd
import string
import copy
import re
import contractions
import os

from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
460/2:
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import WhitespaceTokenizer
wnl = WordNetLemmatizer()
STOPWORDS = set(stopwords.words('english'))
460/3:
from keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences
from keras.layers import Dense, Flatten, LSTM, Conv1D, Bidirectional, Dropout, Activation, Embedding, SpatialDropout1D
from keras.models import Sequential
460/4: %load_ext tensorboard
460/5:
abbreviations = {
    "$" : " dollar ",
    "‚Ç¨" : " euro ",
    "4ao" : "for adults only",
    "a.m" : "before midday",
    "a3" : "anytime anywhere anyplace",
    "aamof" : "as a matter of fact",
    "acct" : "account",
    "adih" : "another day in hell",
    "afaic" : "as far as i am concerned",
    "afaict" : "as far as i can tell",
    "afaik" : "as far as i know",
    "afair" : "as far as i remember",
    "afk" : "away from keyboard",
    "app" : "application",
    "approx" : "approximately",
    "apps" : "applications",
    "asap" : "as soon as possible",
    "asl" : "age, sex, location",
    "atk" : "at the keyboard",
    "ave." : "avenue",
    "aymm" : "are you my mother",
    "ayor" : "at your own risk", 
    "b&b" : "bed and breakfast",
    "b+b" : "bed and breakfast",
    "b.c" : "before christ",
    "b2b" : "business to business",
    "b2c" : "business to customer",
    "b4" : "before",
    "b4n" : "bye for now",
    "b@u" : "back at you",
    "bae" : "before anyone else",
    "bak" : "back at keyboard",
    "bbbg" : "bye bye be good",
    "bbc" : "british broadcasting corporation",
    "bbias" : "be back in a second",
    "bbl" : "be back later",
    "bbs" : "be back soon",
    "be4" : "before",
    "bfn" : "bye for now",
    "blvd" : "boulevard",
    "bout" : "about",
    "brb" : "be right back",
    "bros" : "brothers",
    "brt" : "be right there",
    "bsaaw" : "big smile and a wink",
    "btw" : "by the way",
    "bwl" : "bursting with laughter",
    "c/o" : "care of",
    "cet" : "central european time",
    "cf" : "compare",
    "cia" : "central intelligence agency",
    "csl" : "can not stop laughing",
    "cu" : "see you",
    "cul8r" : "see you later",
    "cv" : "curriculum vitae",
    "cwot" : "complete waste of time",
    "cya" : "see you",
    "cyt" : "see you tomorrow",
    "dae" : "does anyone else",
    "dbmib" : "do not bother me i am busy",
    "diy" : "do it yourself",
    "dm" : "direct message",
    "dwh" : "during work hours",
    "e123" : "easy as one two three",
    "eet" : "eastern european time",
    "eg" : "example",
    "embm" : "early morning business meeting",
    "encl" : "enclosed",
    "encl." : "enclosed",
    "etc" : "and so on",
    "faq" : "frequently asked questions",
    "fawc" : "for anyone who cares",
    "fb" : "facebook",
    "fc" : "fingers crossed",
    "fig" : "figure",
    "fimh" : "forever in my heart", 
    "ft." : "feet",
    "ft" : "featuring",
    "ftl" : "for the loss",
    "ftw" : "for the win",
    "fwiw" : "for what it is worth",
    "fyi" : "for your information",
    "g9" : "genius",
    "gahoy" : "get a hold of yourself",
    "gal" : "get a life",
    "gcse" : "general certificate of secondary education",
    "gfn" : "gone for now",
    "gg" : "good game",
    "gl" : "good luck",
    "glhf" : "good luck have fun",
    "gmt" : "greenwich mean time",
    "gmta" : "great minds think alike",
    "gn" : "good night",
    "g.o.a.t" : "greatest of all time",
    "goat" : "greatest of all time",
    "goi" : "get over it",
    "gps" : "global positioning system",
    "gr8" : "great",
    "gratz" : "congratulations",
    "gyal" : "girl",
    "h&c" : "hot and cold",
    "hp" : "horsepower",
    "hr" : "hour",
    "hrh" : "his royal highness",
    "ht" : "height",
    "ibrb" : "i will be right back",
    "ic" : "i see",
    "icq" : "i seek you",
    "icymi" : "in case you missed it",
    "idc" : "i do not care",
    "idgadf" : "i do not give a damn fuck",
    "idgaf" : "i do not give a fuck",
    "idk" : "i do not know",
    "ie" : "that is",
    "i.e" : "that is",
    "ifyp" : "i feel your pain",
    "IG" : "instagram",
    "iirc" : "if i remember correctly",
    "ilu" : "i love you",
    "ily" : "i love you",
    "imho" : "in my humble opinion",
    "imo" : "in my opinion",
    "imu" : "i miss you",
    "iow" : "in other words",
    "irl" : "in real life",
    "j4f" : "just for fun",
    "jic" : "just in case",
    "jk" : "just kidding",
    "jsyk" : "just so you know",
    "l8r" : "later",
    "lb" : "pound",
    "lbs" : "pounds",
    "ldr" : "long distance relationship",
    "lmao" : "laugh my ass off",
    "lmfao" : "laugh my fucking ass off",
    "lol" : "laughing out loud",
    "ltd" : "limited",
    "ltns" : "long time no see",
    "m8" : "mate",
    "mf" : "motherfucker",
    "mfs" : "motherfuckers",
    "mfw" : "my face when",
    "mofo" : "motherfucker",
    "mph" : "miles per hour",
    "mr" : "mister",
    "mrw" : "my reaction when",
    "ms" : "miss",
    "mte" : "my thoughts exactly",
    "nagi" : "not a good idea",
    "nbc" : "national broadcasting company",
    "nbd" : "not big deal",
    "nfs" : "not for sale",
    "ngl" : "not going to lie",
    "nhs" : "national health service",
    "nrn" : "no reply necessary",
    "nsfl" : "not safe for life",
    "nsfw" : "not safe for work",
    "nth" : "nice to have",
    "nvr" : "never",
    "nyc" : "new york city",
    "oc" : "original content",
    "og" : "original",
    "ohp" : "overhead projector",
    "oic" : "oh i see",
    "omdb" : "over my dead body",
    "omg" : "oh my god",
    "omw" : "on my way",
    "p.a" : "per annum",
    "p.m" : "after midday",
    "pm" : "prime minister",
    "poc" : "people of color",
    "pov" : "point of view",
    "pp" : "pages",
    "ppl" : "people",
    "prw" : "parents are watching",
    "ps" : "postscript",
    "pt" : "point",
    "ptb" : "please text back",
    "pto" : "please turn over",
    "qpsa" : "what happens", #"que pasa",
    "ratchet" : "rude",
    "rbtl" : "read between the lines",
    "rlrt" : "real life retweet", 
    "rofl" : "rolling on the floor laughing",
    "roflol" : "rolling on the floor laughing out loud",
    "rotflmao" : "rolling on the floor laughing my ass off",
    "rt" : "retweet",
    "ruok" : "are you ok",
    "sfw" : "safe for work",
    "sk8" : "skate",
    "smh" : "shake my head",
    "sq" : "square",
    "srsly" : "seriously", 
    "ssdd" : "same stuff different day",
    "tbh" : "to be honest",
    "tbs" : "tablespooful",
    "tbsp" : "tablespooful",
    "tfw" : "that feeling when",
    "thks" : "thank you",
    "tho" : "though",
    "thx" : "thank you",
    "tia" : "thanks in advance",
    "til" : "today i learned",
    "tl;dr" : "too long i did not read",
    "tldr" : "too long i did not read",
    "tmb" : "tweet me back",
    "tntl" : "trying not to laugh",
    "ttyl" : "talk to you later",
    "u" : "you",
    "u2" : "you too",
    "u4e" : "yours for ever",
    "utc" : "coordinated universal time",
    "w/" : "with",
    "w/o" : "without",
    "w8" : "wait",
    "wassup" : "what is up",
    "wb" : "welcome back",
    "wtf" : "what the fuck",
    "wtg" : "way to go",
    "wtpa" : "where the party at",
    "wuf" : "where are you from",
    "wuzup" : "what is up",
    "wywh" : "wish you were here",
    "yd" : "yard",
    "ygtr" : "you got that right",
    "ynk" : "you never know",
    "zzz" : "sleeping bored and tired",
}
460/6: df_train.iloc[3053, :]
460/7:
##### PREPROC
#remove not ascii
def convert_lower_case(text):
    return text.lower()

def replace_us(text):
    pat=re.compile(r'u.s ')
    return pat.sub(r'united states',text)

def replace_amp(text):
    pat=re.compile(r'&amp;')
    return pat.sub(r'and',text)

def replace_gt(text):
    pat=re.compile(r'&gt')
    return pat.sub(r' ',text)


def replace_lt(text):
    pat=re.compile(r'&lt')
    return pat.sub(r' ',text)

def delete_not_ascii(text):
    text = re.sub(r'[^\x00-\x7f]', '', text)
    return text 

#remove urls
def remove_urls(text):
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    return text

# Change an abbreviation by its true meaning
def word_abbrev(word):
    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word

# Replace all abbreviations
def replace_abbrev(text):
    string = ""
    for word in text.split():
        string += word_abbrev(word) + " "  
    return string

# Remove @ and mention, replace by USER
def replace_mention(text):
    pat=re.compile(r'@\S+')
    return pat.sub(r' USER ',text)

# Remove numbers, replace it by NUMBER
def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r' NUMBER ', text)

#remove punctuation
def remove_punct(text):
    text = re.sub('[' + string.punctuation +']', ' ', text)
    return text     

# remove line breaks and extra spaces
def clean_text(text):
    text = re.sub(r'\n',' ', text) # Remove line breaks
    text = re.sub('\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces
    return text

#contractions and remowing stopwords and very short words
def string_contractions(text):
    string_wothout_contractions = ''
    expanded_words = []  
    for each_word in text.split():
        if each_word not in (STOPWORDS):
            if len(each_word) >= 2:
                expanded_words.append(contractions.fix(each_word))
    string_wothout_contractions = ' '.join(expanded_words)
    return string_wothout_contractions
460/8:
def basic_cleaning(df_in, column_name):
    df = copy.deepcopy(df_in)
    df[column_name] = df[column_name].apply(convert_lower_case)
    df[column_name] = df[column_name].apply(replace_us)

    df[column_name] = df[column_name].apply(replace_amp)
    df[column_name] = df[column_name].apply(replace_gt)
    df[column_name] = df[column_name].apply(replace_lt)

    df[column_name] = df[column_name].apply(remove_urls)
    df[column_name] = df[column_name].apply(remove_number)
    df[column_name] = df[column_name].apply(remove_punct)
    df[column_name] = df[column_name].apply(replace_abbrev)
    df[column_name] = df[column_name].apply(replace_mention) 
    df[column_name] = df[column_name].apply(delete_not_ascii)
    df[column_name] = df[column_name].apply(clean_text)
    df[column_name] = df[column_name].apply(string_contractions)
    return df
460/9:
df_train = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')
460/10:
df_train_clean = basic_cleaning(df_train, 'text')
df_test_clean = basic_cleaning(df_test, 'text')
460/11: from nltk import pos_tag, word_tokenize
460/12:
def word_tokenize_pos(text):
    list_lem = []
    for word, tag in pos_tag(word_tokenize(text)):
        wntag = tag[0].lower()
        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
        if not wntag:
            list_lem.append(word)
        else:
            list_lem.append(word)
  #  print(str_lem)
    return list_lem
460/13: df_train_clean[1:2].text
460/14: df_train_clean[7232:7233].text.apply(word_tokenize_pos)
460/15:
df_train_clean['text_lemmatized'] = df_train_clean.text.apply(word_tokenize_pos)
df_train_clean['all_text'] = df_train_clean['text_lemmatized'].apply(lambda x: " ".join(x))

df_test_clean['text_lemmatized'] = df_test_clean.text.apply(word_tokenize_pos)
df_test_clean['all_text'] = df_test_clean['text_lemmatized'].apply(lambda x: " ".join(x))
460/16: df_train_clean.iloc[7610, :]
460/17: df_train_clean[df_train_clean.all_text.str.contains(' gt ')]
460/18: df_train_clean
460/19:
maxlen = 250
#train = 4000
max_words = 20000
460/20:
train_texts = df_train_clean.all_text.to_list()
test_texts = df_test_clean.all_text.to_list()
460/21: df_train.iloc[1974, 3]
460/22: df_train_clean.iloc[7374, :]
460/23: df_train[df_train.text.str.contains('&')]
460/24: df_train_clean[df_train_clean.all_text.str.contains(' amp ')]
460/25: oov_tok = '<OOV>'
460/26:
tokenizer = Tokenizer(num_words=max_words, oov_token=oov_tok)
tokenizer.fit_on_texts(train_texts)

train_sequences = tokenizer.texts_to_sequences(train_texts)
test_sequences = tokenizer.texts_to_sequences(test_texts)
460/27:
word_index = tokenizer.word_index
dict(list(word_index.items()))
460/28:
word_index = tokenizer.word_index
print(f'Found {len(word_index)} unique tokens')
460/29: #train_texts
460/30:
train_data = pad_sequences(train_sequences, maxlen=maxlen)
test_data = pad_sequences(test_sequences, maxlen=maxlen)
labels = np.array(df_train.target.to_list())
460/31: X_train, X_val, y_train, y_val = train_test_split(train_data, labels, test_size=0.25, stratify=labels, shuffle=True)
460/32:
#X_train = train_data[:train]
#y_train = labels[:train]
#X_val = train_data[train:]
#y_val = labels[train:]
460/33:
glove_dir = '/Users/Me/Desktop/GIT_pro/twitter/BERT/glove.twitter.27B'
embeddings_index = {}
f = open(os.path.join(glove_dir, 'glove.twitter.27B.50d.txt'), encoding="utf8")
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
460/34:
embedding_dim = 50
not_embedd = []
embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i < max_words:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
        else:
            not_embedd.append(word)
460/35: embedding_matrix.shape
460/36: np.where(~embedding_matrix.any(axis=1))[0]
460/37: len(not_embedd)
460/38: not_embedd
460/39: #df_train_clean[df_train_clean.text.str.contains('disea')]
460/40: #df_train[df_train.text.str.contains('disea')]
460/41:
model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length = maxlen))
#model.add(Flatten())
#model.add(Dense(32, activation='relu'))
#model.add(Dense(1, activation='sigmoid'))
model.add(SpatialDropout1D(0.2))
model.add(Bidirectional(LSTM(100, dropout=0.2, activation='tanh', recurrent_dropout=0.2)))
model.add(Dense(1, activation='sigmoid'))
model.summary()
460/42:
model.layers[0].set_weights([embedding_matrix])
model.layers[0].trainable = False
460/43: model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
460/44: history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=[X_val, y_val])
462/1:
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

from sklearn.feature_selection import VarianceThreshold
462/2:
data = pd.read_csv('../data/dataset_1.csv')
data.shape
462/3:
data = pd.read_csv('../data/train.csv')
data.shape
462/4:
data = pd.read_csv('data/train.csv')
data.shape
462/5:
df_train = pd.read_csv('data/train.csv')
df_test = pd.read_csv('data/test.csv')
print(df_train.shape)
print(df_test.shape)
462/6: df_train.head(3)
462/7: df_train.describe()
462/8:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')
print(df_train.shape)
print(df_val.shape)
462/9: df_train.head(3)
462/10: df_train.describe()
462/11:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
462/12:
sel = VarianceThreshold(threshold=0)

sel.fit(X_train)  # fit finds the features with zero variance
462/13: sel.get_support()
462/14:
sel.get_support()
constant = X_train.columns[~sel.get_support()]
len(constant)
462/15:
const_f = [feat for feat in X_train.columns if X_train[feat].std() == 0]
len(const_f)
462/16:
constant_features = [feat for feat in X_train.columns if X_train[feat].nunique() == 1]
len(constant_features)
462/17:
sel = VarianceThreshold(threshold=0.01)
sel.fit(X_train)  # fit finds the features with zero variance
462/18:
sel.get_support()
constant = X_train.columns[~sel.get_support()]
len(constant)
462/19:
sel = VarianceThreshold(threshold=0.1)
sel.fit(X_train)  # fit finds the features with zero variance
462/20:
sel.get_support()
constant = X_train.columns[~sel.get_support()]
len(constant)
462/21:
sel = VarianceThreshold(threshold=0.05)
sel.fit(X_train)  # fit finds the features with zero variance
462/22:
sel.get_support()
constant = X_train.columns[~sel.get_support()]
len(constant)
462/23:
sel = VarianceThreshold(threshold=0.02)
sel.fit(X_train)  # fit finds the features with zero variance
462/24:
sel.get_support()
constant = X_train.columns[~sel.get_support()]
len(constant)
462/25:
sel = VarianceThreshold(threshold=0.025)
sel.fit(X_train)  # fit finds the features with zero variance
462/26:
sel.get_support()
constant = X_train.columns[~sel.get_support()]
len(constant)
462/27:
sel = VarianceThreshold(threshold=0.03)
sel.fit(X_train)  # fit finds the features with zero variance
462/28:
sel.get_support()
constant = X_train.columns[~sel.get_support()]
len(constant)
462/29:
sel = VarianceThreshold(threshold=0.04)
sel.fit(X_train)  # fit finds the features with zero variance
462/30:
sel.get_support()
constant = X_train.columns[~sel.get_support()]
len(constant)
462/31:
sel = VarianceThreshold(threshold=0.0)
sel.fit(X_train)  # fit finds the features with zero variance
462/32:
sel.get_support()
constant = X_train.columns[~sel.get_support()]
len(constant)
462/33:
### only works with numerical
const_f = [feat for feat in X_train.columns if X_train[feat].std() == 0]
len(const_f)
462/34: ### works also with categorical variables
462/35: ## quasi-constant features
462/36:
sel = VarianceThreshold(threshold=0.03)  
sel.fit(X_train)  # fit finds the features with low variance
462/37:
sel = VarianceThreshold(threshold=0.03)  
sel.fit(X_train)  # fit finds the features with low variance
sum(sel.get_support())
462/38:
sel = VarianceThreshold(threshold=0.02)  
sel.fit(X_train)  # fit finds the features with low variance
sum(sel.get_support())
462/39:
sel = VarianceThreshold(threshold=0.01)  
sel.fit(X_train)  # fit finds the features with low variance
sum(sel.get_support())
462/40:
sel = VarianceThreshold(threshold=0.01)  
sel.fit(X_train)  # fit finds the features with low variance
sum(sel.get_support())
462/41: sel.get_support()
462/42:
sel = VarianceThreshold(threshold=0.01)  
sel.fit(X_train)  # fit finds the features with low variance
quasi_constant = X_train.columns[~sel.get_support()]

len(quasi_constant)
462/43:
sel = VarianceThreshold(threshold=0.03)  
sel.fit(X_train)  # fit finds the features with low variance
quasi_constant = X_train.columns[~sel.get_support()]

len(quasi_constant)
462/44:
sel = VarianceThreshold(threshold=0.04)  
sel.fit(X_train)  # fit finds the features with low variance
quasi_constant = X_train.columns[~sel.get_support()]

len(quasi_constant)
462/45: X_train
462/46: quasi_constant
462/47:
# percentage of observations showing each of the different values
# of the variable

X_train['cont7'].value_counts() / np.float(len(X_train))
462/48:
# percentage of observations showing each of the different values
# of the variable

X_train['cont7'].value_counts() / float(len(X_train))
462/49:
# percentage of observations showing each of the different values
# of the variable

X_train['cont7'].value_counts() / np.float(len(X_train))
462/50:
# percentage of observations showing each of the different values
# of the variable

X_train['cont7'].value_counts() / len(X_train)
462/51:
# percentage of observations showing each of the different values
# of the variable

(X_train['cont7'].value_counts() / len(X_train)).sort_values(ascending=False).values[0]
462/52:
sel = VarianceThreshold(threshold=0.01)  
sel.fit(X_train)  # fit finds the features with low variance
quasi_constant = X_train.columns[~sel.get_support()]

len(quasi_constant)
462/53: df_train.sum()
462/54: df_train.sum().to_dictd()
462/55: df_train.sum()
462/56: df_train.sum().to_list().to_dictd()
462/57: df_train.sum().to_list().to_dict()
462/58: df_train.sum().to_dict()
462/59: dic_sum = df_train.sum().to_dict()
462/60: dic_sum
462/61:
car = {
  "brand": "Ford",
  "model": "Mustang",
  "year": 1964
}

x = car.setdefault("color", "white")

print(x)
462/62: x
462/63:
car = {
  "brand": "Ford",
  "model": "Mustang",
  "year": 1964
}

x = car.setdefault("model", "Bronco")

print(x)
462/64: x
462/65: car
462/66:
for key, val in dic_sum:
    print(key)
462/67:
for key, val in dic_sum.items():
    print(key)
462/68:
k_v_exchanged = {}
for key, val in dic_sum.items():
    print(key)
    if val not in k_v_exchanged:
        k_v_exchanged[value] = [key]
    else:
        k_v_exchanged[value].append(key)
462/69:
k_v_exchanged = {}
for key, val in dic_sum.items():
    print(key)
    if val not in k_v_exchanged:
        k_v_exchanged[val] = [key]
    else:
        k_v_exchanged[val].append(key)
462/70:
k_v_exchanged = {}
for key, val in dic_sum.items():
    if val not in k_v_exchanged:
        k_v_exchanged[val] = [key]
    else:
        k_v_exchanged[val].append(key)
462/71: k_v_exchanged
462/72: k_v_exchanged.values
462/73: k_v_exchanged.values.len()
462/74: k_v_exchanged.values
462/75: len(k_v_exchanged.values)
462/76: [len(v) for v in k_v_exchanged.values()]
462/77: max([len(v) for v in k_v_exchanged.values()])
462/78: ## correlation
462/79:
import numpy as np
import pandas as pd
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold
462/80:
plt.figure(figsize=(14,10))
sns.heatmap(df_train.corr(),annot=True)
462/81:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold
462/82:
plt.figure(figsize=(14,10))
sns.heatmap(df_train.corr(),annot=True)
462/83:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
462/84:
corr_features = correlation(X_train, 0.75)
print('correlated features: ', len(set(corr_features)) )
462/85: corr_features
462/86:
corr_features = correlation(X_train, 0.75)
print('correlated features: ', len(set(corr_features)) )
462/87: corr_features
462/88:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
462/89: #The mutual information measures the reduction in uncertainty in variable A when variable B is known.
462/90: from sklearn.feature_selection import mutual_info_regression
462/91:
# determine the mutual information
mutual_info_regression

# and make a bar  plot
mi = pd.Series(mi)
mi.index = X_train.columns
mi.sort_values(ascending=False).plot.bar(figsize=(20,6))
plt.ylabel('Mutual Information')
462/92:
# determine the mutual information
mi = mutual_info_regression(X_train, y_train)

# and make a bar  plot
mi = pd.Series(mi)
mi.index = X_train.columns
mi.sort_values(ascending=False).plot.bar(figsize=(20,6))
plt.ylabel('Mutual Information')
462/93: mi
462/94: mi.dtype
462/95: sort(mi)
462/96: np.sort(mi)
462/97: mi.to_dcit()
462/98: mi.to_sict()
462/99: mi.to_dict()
462/100: dict(sorted(mi.to_dict().items(), key=lambda item: item[1]))
462/101: mi
462/102: mi_dict = dict(sorted(mi.to_dict().items(), key=lambda item: item[1]))
462/103: mi_dict[0]
462/104: mi_dict.items(1)
462/105: list(data.keys())[0]
462/106: list(data.keys())[0:2]
462/107: del_col = list(data.keys())[0]
462/108: unmutal_cols = list(data.keys())[0]
462/109:
X_train.drop(labels=unmutal_cols, axis=1, inplace=True)
X_test.drop(labels=unmutal_cols, axis=1, inplace=True)

X_train.shape, X_test.shape
462/110:
## ANOVA
from sklearn.feature_selection import f_regression
462/111:
# univariate anova
univariate = f_regression(X_train.fillna(0), y_train)

# plot values
univariate = pd.Series(univariate[1])
univariate.index = X_train.columns
univariate.sort_values(ascending=False).plot.bar(figsize=(20,6))
462/112: univariate
462/113: univariate_dict = dict(sorted(univariate.to_dict().items(), key=lambda item: item[1]))
462/114: univariate_dict
462/115: unmutal_cols = list(mi_dict.keys())[0]
462/116: unmutal_cols
462/117: univariate_cols = list(univariate_dict.keys())[0:2]
462/118: univariate_cols
462/119: X_train
462/120:
X_train.drop(labels=univariate_cols, axis=1, inplace=True)
X_test.drop(labels=univariate_cols, axis=1, inplace=True)

X_train.shape, X_test.shape
462/121: X_train
462/122: from mlxtend.feature_selection import SequentialFeatureSelector as SFS
462/123: !conda install mlxtend
477/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectFromModel
from sklearn.metrics import roc_auc_score
477/2:
# load the Santander customer satisfaction dataset from Kaggle

data = pd.read_csv('../dataset_1.csv')
data.shape
477/3:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),
    data['target'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
477/4:
# I keep a copy of the dataset with all the variables
# to compare the performance of machine learning models
# at the end of the notebook

X_train_original = X_train.copy()
X_test_original = X_test.copy()
477/5:
constant_features = [
    feat for feat in X_train.columns if X_train[feat].std() == 0
]

X_train.drop(labels=constant_features, axis=1, inplace=True)
X_test.drop(labels=constant_features, axis=1, inplace=True)

X_train.shape, X_test.shape
477/6:
# find features with low variance
sel = VarianceThreshold(threshold=0.01)
sel.fit(X_train)  

# how many not quasi-constant?
sum(sel.get_support())
477/7: features_to_keep = X_train.columns[sel.get_support()]
477/8:
# remove the features
X_train = sel.transform(X_train)
X_test = sel.transform(X_test)

X_train.shape, X_test.shape
477/9:
# sklearn transformations lead to numpy arrays
# here we transform the arrays back to dataframes

X_train= pd.DataFrame(X_train)
X_train.columns = features_to_keep

X_test= pd.DataFrame(X_test)
X_test.columns = features_to_keep
477/10:
duplicated_feat = []
for i in range(0, len(X_train.columns)):
    if i % 10 == 0:  # this helps me understand how the loop is going
        print(i)

    col_1 = X_train.columns[i]

    for col_2 in X_train.columns[i + 1:]:
        if X_train[col_1].equals(X_train[col_2]):
            duplicated_feat.append(col_2)
            
len(duplicated_feat)
477/11:
# remove duplicated features
X_train.drop(labels=duplicated_feat, axis=1, inplace=True)
X_test.drop(labels=duplicated_feat, axis=1, inplace=True)

X_train.shape, X_test.shape
477/12:
# I keep a copy of the dataset except constant, quasi-constant and duplicated variables

X_train_basic_filter = X_train.copy()
X_test_basic_filter = X_test.copy()
477/13:
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr

corr_features = correlation(X_train, 0.8)
print('correlated features: ', len(set(corr_features)) )
477/14:
# remove correlated features
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
477/15:
# keep a copy of the dataset without correlated features
X_train_corr = X_train.copy()
X_test_corr = X_test.copy()
477/16:
scaler = StandardScaler()
scaler.fit(X_train)
477/17:
# we use regularisation by setting a low value of C

sel_ = SelectFromModel(
    LogisticRegression(C=0.0001, random_state=10, max_iter=1000, penalty='l2'))

sel_.fit(scaler.transform(X_train), y_train)

# select features where coefficient is above the mean
# coefficient value and parse again as dataframe
# (remember that the output of sklearn is a
# numpy array)

X_train_coef = pd.DataFrame(sel_.transform(X_train))
X_test_coef = pd.DataFrame(sel_.transform(X_test))

# add the columns name
X_train_coef.columns = X_train.columns[(sel_.get_support())]
X_test_coef.columns = X_train.columns[(sel_.get_support())]
477/18: X_train_coef.shape, X_test_coef.shape
477/19:
# create a function to train a logistic regression 
# and compare its performance in the train and test sets

def run_logistic(X_train, X_test, y_train, y_test):
    
    scaler = StandardScaler().fit(X_train)
    
    logit = LogisticRegression(C=0.0005, random_state=10, max_iter=10000, penalty='l2')
    logit.fit(scaler.transform(X_train), y_train)
    
    print('Train set')
    pred = logit.predict_proba(scaler.transform(X_train))
    print('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))
    
    print('Test set')
    pred = logit.predict_proba(scaler.transform(X_test))
    print('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))
477/20:
# original dataset - all variables
run_logistic(X_train_original,
             X_test_original,
             y_train,
             y_test)
477/21:
# filter methods - basic
run_logistic(X_train_basic_filter,
             X_test_basic_filter,
             y_train,
             y_test)
477/22:
# filter methods - correlation
run_logistic(X_train_corr,
             X_test_corr,
             y_train,
             y_test)
477/23:
# embedded methods - Logistic regression coefficients
run_logistic(X_train_coef,
             X_test_coef,
             y_train,
             y_test)
477/24:
# create a function to train a logistic regression 
# and compare its performance in the train and test sets

def run_logistic(X_train, X_test, y_train, y_test):
    
    scaler = StandardScaler().fit(X_train)
    
    logit = LogisticRegression(C=100, random_state=10, max_iter=10000, penalty='l2')
    logit.fit(scaler.transform(X_train), y_train)
    
    print('Train set')
    pred = logit.predict_proba(scaler.transform(X_train))
    print('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))
    
    print('Test set')
    pred = logit.predict_proba(scaler.transform(X_test))
    print('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))
477/25:
# original dataset - all variables
run_logistic(X_train_original,
             X_test_original,
             y_train,
             y_test)
477/26:
# filter methods - basic
run_logistic(X_train_basic_filter,
             X_test_basic_filter,
             y_train,
             y_test)
477/27:
# filter methods - correlation
run_logistic(X_train_corr,
             X_test_corr,
             y_train,
             y_test)
477/28:
# create a function to train a logistic regression 
# and compare its performance in the train and test sets

def run_logistic(X_train, X_test, y_train, y_test):
    
    scaler = StandardScaler().fit(X_train)
    
    logit = LogisticRegression(C=1000, random_state=10, max_iter=10000, penalty='l2')
    logit.fit(scaler.transform(X_train), y_train)
    
    print('Train set')
    pred = logit.predict_proba(scaler.transform(X_train))
    print('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))
    
    print('Test set')
    pred = logit.predict_proba(scaler.transform(X_test))
    print('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))
477/29:
# original dataset - all variables
run_logistic(X_train_original,
             X_test_original,
             y_train,
             y_test)
462/124: import mlxtend
462/125: from mlxtend.feature_selection import SequentialFeatureSelector as SFS
482/1:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold

from sklearn.linear_model import LinearRegression
482/2:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

df_train_original = pd.read_csv('data/train.csv')
df_val_original = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
482/3: df_train.head(3)
482/4: df_train.describe()
482/5: df_val_original
482/6:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

df_train_original = pd.read_csv('data/train.csv')

print(df_train.shape)
print(df_val.shape)
482/7: df_train.head(3)
482/8: df_train.describe()
482/9:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
482/10: lr = LinearRegression()
482/11: df_train.head(3)
482/12: df_train.describe()
482/13:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
482/14:
### Baseline model
lr.fit(X_train, y_train)
482/15:
### Baseline model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_text)
rmse = mean_squared_error(y_train, y_predict, squared=False)
482/16:
### Baseline model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_train, y_predict, squared=False)
482/17:
### Baseline model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
482/18:
### Baseline model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(rmse)
482/19:
### Baseline model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
482/20:
## PREPROC
# 1. VarianceThreshold from Scikit-learn
482/21:
sel = VarianceThreshold(threshold=0.0)
sel.fit(X_train)  # fit finds the features with zero variance
482/22:
sel.get_support()
constant = X_train.columns[~sel.get_support()]
len(constant)
482/23:
### only works with numerical
const_f = [feat for feat in X_train.columns if X_train[feat].std() == 0]
len(const_f)
482/24: ### works also with categorical variables
482/25:
constant_features = [feat for feat in X_train.columns if X_train[feat].nunique() == 1]
len(constant_features)
482/26: ## quasi-constant features
482/27:
sel = VarianceThreshold(threshold=0.01)  
sel.fit(X_train)  # fit finds the features with low variance
quasi_constant = X_train.columns[~sel.get_support()]

len(quasi_constant)
482/28: ### duplicated features
482/29: dic_sum = df_train.sum().to_dict()
482/30:
k_v_exchanged = {}
for key, val in dic_sum.items():
    if val not in k_v_exchanged:
        k_v_exchanged[val] = [key]
    else:
        k_v_exchanged[val].append(key)
482/31: max([len(v) for v in k_v_exchanged.values()])
482/32: ## correlation
482/33:
plt.figure(figsize=(14,10))
sns.heatmap(df_train.corr(),annot=True)
482/34:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
482/35:
corr_features = correlation(X_train, 0.75)
print('correlated features: ', len(set(corr_features)) )
482/36: corr_features
482/37:
plt.figure(figsize=(7,5))
sns.heatmap(df_train.corr(),annot=True)
482/38:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True)
482/39:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True)
sns.set(font_scale=1.4)
482/40:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True)
sns.set(font_scale=.3)
482/41:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True)
sns.set(font_scale=2.3)
482/42:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True)
sns.set(font_scale=1.6)
482/43:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True)
sns.set(font_scale=2.1)
482/44:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True)
sns.set(font_scale=0.1)
482/45:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True)
sns.set(font_scale=3)
482/46:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True)
sns.set(font_scale=2)
482/47:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True, annot_kws={"size":8})
482/48:
plt.figure(figsize=(8,6))
sns.heatmap(df_train.corr(),annot=True, annot_kws={"size":8})
482/49:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
482/50:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

df_train_original = pd.read_csv('data/train.csv')

print(df_train.shape)
print(df_val.shape)
482/51:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True, annot_kws={"size":8})
482/52:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True, annot_kws={"size":8})
sns.set(font_scale = 2)
482/53:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True, annot_kws={"size":8})
sns.set(font_scale = .5)
482/54:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True, annot_kws={"size":18})
sns.set(font_scale = .5)
482/55:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True, annot_kws={"size":10})
sns.set(font_scale = .5)
482/56:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True, annot_kws={"size":6})
sns.set(font_scale = .5)
482/57:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True, annot_kws={"size":8})
sns.set(font_scale = .7)
482/58:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
482/59:
corr_features = correlation(X_train, 0.75)
print('correlated features: ', len(set(corr_features)) )
482/60: corr_features
482/61:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
482/62:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
482/63:
## PREPROC
# 1. VarianceThreshold from Scikit-learn
482/64:
sel = VarianceThreshold(threshold=0.0)
sel.fit(X_train)  # fit finds the features with zero variance
482/65:
sel.get_support()
constant = X_train.columns[~sel.get_support()]
len(constant)
482/66:
### only works with numerical
const_f = [feat for feat in X_train.columns if X_train[feat].std() == 0]
len(const_f)
482/67: ### works also with categorical variables
482/68:
constant_features = [feat for feat in X_train.columns if X_train[feat].nunique() == 1]
len(constant_features)
482/69: ## quasi-constant features
482/70:
sel = VarianceThreshold(threshold=0.01)  
sel.fit(X_train)  # fit finds the features with low variance
quasi_constant = X_train.columns[~sel.get_support()]

len(quasi_constant)
482/71: ### duplicated features
482/72: dic_sum = df_train.sum().to_dict()
482/73:
k_v_exchanged = {}
for key, val in dic_sum.items():
    if val not in k_v_exchanged:
        k_v_exchanged[val] = [key]
    else:
        k_v_exchanged[val].append(key)
482/74: max([len(v) for v in k_v_exchanged.values()])
482/75: ## correlation
482/76:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True, annot_kws={"size":8})
sns.set(font_scale = .7)
482/77:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
482/78:
corr_features = correlation(X_train, 0.75)
print('correlated features: ', len(set(corr_features)) )
482/79: corr_features
482/80:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
482/81:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
482/82: #The mutual information measures the reduction in uncertainty in variable A when variable B is known.
482/83: from sklearn.feature_selection import mutual_info_regression
482/84:
# determine the mutual information
mi = mutual_info_regression(X_train, y_train)

# and make a bar  plot
mi = pd.Series(mi)
mi.index = X_train.columns
mi.sort_values(ascending=False).plot.bar(figsize=(20,6))
plt.ylabel('Mutual Information')
482/85: mi_dict = dict(sorted(mi.to_dict().items(), key=lambda item: item[1]))
482/86: unmutal_cols = list(mi_dict.keys())[0]
482/87:
X_train.drop(labels=unmutal_cols, axis=1, inplace=True)
X_test.drop(labels=unmutal_cols, axis=1, inplace=True)

X_train.shape, X_test.shape
482/88:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
482/89:
## ANOVA
from sklearn.feature_selection import f_regression
482/90:
# univariate anova
univariate = f_regression(X_train.fillna(0), y_train)

# plot values
univariate = pd.Series(univariate[1])
univariate.index = X_train.columns
univariate.sort_values(ascending=False).plot.bar(figsize=(20,6))
482/91: univariate_dict = dict(sorted(univariate.to_dict().items(), key=lambda item: item[1]))
482/92:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
482/93:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
482/94:
corr_features = correlation(X_train, 0.75)
print('correlated features: ', len(set(corr_features)) )
482/95: corr_features
482/96:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
482/97:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
482/98: #The mutual information measures the reduction in uncertainty in variable A when variable B is known.
482/99: from sklearn.feature_selection import mutual_info_regression
482/100:
# determine the mutual information
mi = mutual_info_regression(X_train, y_train)

# and make a bar  plot
mi = pd.Series(mi)
mi.index = X_train.columns
mi.sort_values(ascending=False).plot.bar(figsize=(20,6))
plt.ylabel('Mutual Information')
482/101: mi_dict = dict(sorted(mi.to_dict().items(), key=lambda item: item[1]))
482/102: unmutal_cols = list(mi_dict.keys())[0]
482/103:
X_train.drop(labels=unmutal_cols, axis=1, inplace=True)
X_test.drop(labels=unmutal_cols, axis=1, inplace=True)

X_train.shape, X_test.shape
482/104:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
482/105:
## ANOVA
from sklearn.feature_selection import f_regression
482/106:
# univariate anova
univariate = f_regression(X_train.fillna(0), y_train)

# plot values
univariate = pd.Series(univariate[1])
univariate.index = X_train.columns
univariate.sort_values(ascending=False).plot.bar(figsize=(20,6))
482/107: univariate_dict = dict(sorted(univariate.to_dict().items(), key=lambda item: item[1]))
482/108: univariate_cols = list(univariate_dict.keys())[0:2]
482/109:
X_train.drop(labels=univariate_cols, axis=1, inplace=True)
X_test.drop(labels=univariate_cols, axis=1, inplace=True)

X_train.shape, X_test.shape
482/110:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
482/111: sfs = SFS(LinearRegression(), forward=True, verbose=2, scoring='mean_squared_error')
482/112: from mlxtend.feature_selection import SequentialFeatureSelector as SFS
482/113: sfs = SFS(LinearRegression(), forward=True, verbose=2, scoring='mean_squared_error')
482/114: sfs = SFS(LinearRegression(), forward=True, verbose=2, scoring='neg_root_mean_squared_error')
482/115: sfs = sfs.fit(np.array(X_train), y_train)
482/116: sfs = SFS(LinearRegression(), forward=True, verbose=10, scoring='neg_root_mean_squared_error')
482/117: sfs = sfs.fit(np.array(X_train), y_train)
482/118: sfs = SFS(LinearRegression(), forward=True, verbose=2, scoring='neg_mean_squared_error',cv=5)
482/119: sfs = sfs.fit(np.array(X_train), y_train)
482/120: sfs.k_feature_idx_
482/121:
sfs = SFS(LinearRegression(), k_features=5, forward=True, verbose=2, scoring='neg_mean_squared_error',cv=5)
sfs = sfs.fit(np.array(X_train), y_train)
482/122: sfs.k_feature_idx_
482/123: X_train
482/124:
neg_mean_squared_error_scorer = make_scorer(mean_squared_error,
                                        greater_is_better=False)
482/125:
from sklearn.metrics import make_scorer
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
482/126: rmse_scorer = make_scorer(mean_squared_error, greater_is_better=False)
482/127:
sfs = SFS(LinearRegression(), k_features=5, forward=True, verbose=2, scoring='rmse_scorer',cv=5)
sfs = sfs.fit(np.array(X_train), y_train)
482/128: rmse_scorer = make_scorer(mean_squared_error, greater_is_better=False)
482/129:
sfs = SFS(LinearRegression(), k_features=5, forward=True, verbose=2, scoring='rmse_scorer',cv=5)
sfs = sfs.fit(np.array(X_train), y_train)
482/130:
from sklearn.metrics import make_scorer
from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS
482/131:
efs = EFS(LinearRegression(), k_features=5, forward=True, verbose=2, scoring='rmse_scorer',cv=5)
efs = efs.fit(np.array(X_train), y_train)
482/132:
efs = EFS(LinearRegression(), min_features=1, max_features=5, verbose=2, scoring='rmse_scorer', print_progress=True,)
efs = efs.fit(np.array(X_train), y_train)
482/133:
efs = EFS(LinearRegression(), min_features=1, max_features=5, scoring='rmse_scorer', print_progress=True,)
efs = efs.fit(np.array(X_train), y_train)
482/134:
efs = EFS(LinearRegression(), min_features=1, max_features=5, scoring='neg_mean_squared_error', print_progress=True,)
efs = efs.fit(np.array(X_train), y_train)
482/135: efs.best_idx_
482/136:
selected_feat = X_train.columns[list(efs.best_idx_)]
selected_feat
482/137: list(efs.best_idx_)
482/138:
lr.fit(X_train[selected_feat], y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test[selected_feat], y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
482/139:
lr.fit(X_train[selected_feat], y_train)
y_predict = lr.predict(X_test[selected_feat])
rmse = mean_squared_error(y_test[selected_feat], y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
482/140:
lr.fit(X_train[selected_feat], y_train)
y_predict = lr.predict(X_test[selected_feat])
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
482/141:
efs = EFS(LinearRegression(), min_features=1, max_features=4, scoring='neg_mean_squared_error', print_progress=True,)
efs = efs.fit(np.array(X_train), y_train)
482/142: efs.best_idx_
482/143:
selected_feat = X_train.columns[list(efs.best_idx_)]
selected_feat
482/144:
lr.fit(X_train[selected_feat], y_train)
y_predict = lr.predict(X_test[selected_feat])
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
482/145:
efs = EFS(LinearRegression(), min_features=1, max_features=3, scoring='neg_mean_squared_error', print_progress=True,)
efs = efs.fit(np.array(X_train), y_train)
482/146: efs.best_idx_
482/147:
selected_feat = X_train.columns[list(efs.best_idx_)]
selected_feat
482/148:
lr.fit(X_train[selected_feat], y_train)
y_predict = lr.predict(X_test[selected_feat])
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
482/149:
# exhaustive search
efs = EFS(LinearRegression(), min_features=1, max_features=3, scoring='neg_mean_squared_error', print_progress=True,)
efs = efs.fit(np.array(X_train), y_train)
482/150: efs.best_idx_
482/151:
selected_feat = X_train.columns[list(efs.best_idx_)]
selected_feat
482/152:
lr.fit(X_train[selected_feat], y_train)
y_predict = lr.predict(X_test[selected_feat])
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
482/153:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
482/154:
sfs = SFS(LinearRegression(),
          k_features=3, # the lower the features we want, the longer this will take
          forward=False,
          verbose=2,
          scoring='neg_mean_squared_error')

sfs = sfs.fit(np.array(X_train), y_train)
482/155: sfs.k_feature_idx_
482/156:
models=[
        LinearRegression(),
        Lasso(alpha=1.0),
        ElasticNet(alpha=1.0, l1_ratio=.5),
        Ridge(alpha=1.0),
        SVR(kernel = 'rbf')
       ]
482/157:
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import Ridge
from sklearn.svm import SVR
482/158:
models=[
        LinearRegression(),
        Lasso(alpha=1.0),
        ElasticNet(alpha=1.0, l1_ratio=.5),
        Ridge(alpha=1.0),
        SVR(kernel = 'rbf')
       ]
482/159:
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import Ridge
from sklearn.svm import SVR

from sklearn.compose import TransformedTargetRegressor
482/160:
def fit_and_predict(model, x_train ,x_test, y_train, y_test):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    #####################################################
    ttr = TransformedTargetRegressor(regressor=classifier, func=np.log, inverse_func=np.exp)
    ttr.fit(X_train, y_train)
    y_pred_ttr = ttr.predict(X_test)
    rmse_ttr = mean_squared_error(y_test, y_pred_ttr, squared=False)
    
    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'test rmse': rmse, 
                              'test ttr rmse': rmse_ttr
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using "+ str(vectoriser))
    print('RMSE: {}'.format(rmse))
    print('RMSE TARGET TRANSFORM: {}'.format(rmse_ttr))
482/161:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train_count, X_test_count, y_train ,y_test)
    
    fit_and_predict(model, X_train_tfidf, X_test_tfidf, y_train, y_test)
482/162: metrics = pd.DataFrame(columns=['model' ,'test rmse', 'test ttr rmse'])
482/163:
def fit_and_predict(model, x_train ,x_test, y_train, y_test):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    #####################################################
    ttr = TransformedTargetRegressor(regressor=classifier, func=np.log, inverse_func=np.exp)
    ttr.fit(X_train, y_train)
    y_pred_ttr = ttr.predict(X_test)
    rmse_ttr = mean_squared_error(y_test, y_pred_ttr, squared=False)
    
    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'test rmse': rmse, 
                              'test ttr rmse': rmse_ttr
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using "+ str(vectoriser))
    print('RMSE: {}'.format(rmse))
    print('RMSE TARGET TRANSFORM: {}'.format(rmse_ttr))
482/164:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train, X_test, y_train ,y_test)
    
    fit_and_predict(model, X_train, X_test, y_train, y_test)
482/165: X_train.isnull().mean()
482/166: y_train
482/167: TransformedTargetRegressor(regressor=LinearRegression(), func=np.log, inverse_func=np.exp)
482/168: ttr = TransformedTargetRegressor(regressor=LinearRegression(), func=np.log, inverse_func=np.exp)
482/169: ttr.fit(X_train, y_train)
482/170: y_train.hist()
482/171: y_train.describe()
482/172: y_train.isna()
482/173: sum(y_train.isna())
482/174: y_train.describe()
482/175: np.log(y_train)
482/176: np.—É—á–∑(y_train)
482/177: np.exp(y_train)
482/178: np.log(y_train)
482/179: from sklearn.preprocessing import MinMaxScaler
482/180:
def fit_and_predict(model, x_train ,x_test, y_train, y_test):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    #####################################################
    target_scaler = MinMaxScaler()
    target_scaler.fit(y_train)

    y_train = target_scaler.transform(y_train)
    y_test = target_scaler.transform(y_test)

    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    
    y_pred = target_scaler.inverse_transform(y_pred)
    rmse_scale = mean_squared_error(y_test, y_pred, squared=False)
    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'test rmse': rmse, 
                              'test ttr rmse': rmse_scale
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using "+ str(vectoriser))
    print('RMSE: {}'.format(rmse))
    print('RMSE TARGET TRANSFORM: {}'.format(rmse_scale))
482/181:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train, X_test, y_train ,y_test)
    
    fit_and_predict(model, X_train, X_test, y_train, y_test)
482/182:
target_scaler = MinMaxScaler()
target_scaler.fit(y_train)
482/183:
target_scaler = MinMaxScaler()
target_scaler.fit(y_train.reshape(-1, 1))
482/184:
target_scaler = MinMaxScaler()
target_scaler.fit(y_train.values.reshape(-1, 1))
482/185: lr.predict(X_test)
482/186: lr.predict(X_test[selected_feat])
482/187: target_scaler.transform(y_test)
482/188: target_scaler.transform(y_test.values.reshape(-1, 1))
482/189: q = target_scaler.transform(y_test.values.reshape(-1, 1))
482/190: target_scaler.inverse_transform(q)
482/191: q
482/192: y_test
482/193:
def fit_and_predict(model, x_train ,x_test, y_train, y_test):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    #####################################################
    target_scaler = MinMaxScaler()
    target_scaler.fit(y_train.values.reshape(-1, 1))

    y_train = target_scaler.transform(y_train.values.reshape(-1, 1))
    y_test = target_scaler.transform(y_test.values.reshape(-1, 1))

    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    
    y_pred = target_scaler.inverse_transform(y_pred)
    rmse_scale = mean_squared_error(y_test, y_pred, squared=False)
    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'test rmse': rmse, 
                              'test ttr rmse': rmse_scale
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using "+ str(vectoriser))
    print('RMSE: {}'.format(rmse))
    print('RMSE TARGET TRANSFORM: {}'.format(rmse_scale))
482/194:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train, X_test, y_train ,y_test)
    
    fit_and_predict(model, X_train, X_test, y_train, y_test)
482/195:
def fit_and_predict(model, x_train ,x_test, y_train, y_test):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    #####################################################
    target_scaler = MinMaxScaler()
    target_scaler.fit(y_train.values.reshape(-1, 1))

    y_train = target_scaler.transform(y_train.values.reshape(-1, 1))
    y_test = target_scaler.transform(y_test.values.reshape(-1, 1))

    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    
    y_pred = target_scaler.inverse_transform(y_pred)
    rmse_scale = mean_squared_error(y_test, y_pred, squared=False)
    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'test rmse': rmse, 
                              'test ttr rmse': rmse_scale
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using ")
    print('RMSE: {}'.format(rmse))
    print('RMSE TARGET TRANSFORM: {}'.format(rmse_scale))
482/196:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train, X_test, y_train ,y_test)
    
    fit_and_predict(model, X_train, X_test, y_train, y_test)
482/197:
def fit_and_predict(model, x_train ,x_test, y_train, y_test):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    #####################################################
    target_scaler = MinMaxScaler()
    target_scaler.fit(y_train.values.reshape(-1, 1))

    y_train = target_scaler.transform(y_train.values.reshape(-1, 1))
    y_test = target_scaler.transform(y_test.values.reshape(-1, 1))

    classifier.fit(x_train,y_train)
    y_pred_scale = classifier.predict(x_test)
    
    y_pred_scale = target_scaler.inverse_transform(y_pred_scale)
    rmse_scale = mean_squared_error(y_test, y_pred_scale, squared=False)
    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'test rmse': rmse, 
                              'test ttr rmse': rmse_scale
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using ")
    print('RMSE: {}'.format(rmse))
    print('RMSE TARGET TRANSFORM: {}'.format(rmse_scale))
482/198:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train, X_test, y_train ,y_test)
    
    fit_and_predict(model, X_train, X_test, y_train, y_test)
482/199:
def fit_and_predict(model, x_train ,x_test, y_train, y_test):
    print(model)
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    #####################################################
    target_scaler = MinMaxScaler()
    target_scaler.fit(y_train.values.reshape(-1, 1))

    y_train = target_scaler.transform(y_train.values.reshape(-1, 1))
    y_test = target_scaler.transform(y_test.values.reshape(-1, 1))

    classifier.fit(x_train,y_train)
    y_pred_scale = classifier.predict(x_test)
    
    y_pred_scale = target_scaler.inverse_transform(y_pred_scale)
    rmse_scale = mean_squared_error(y_test, y_pred_scale, squared=False)
    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'test rmse': rmse, 
                              'test ttr rmse': rmse_scale
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using ")
    print('RMSE: {}'.format(rmse))
    print('RMSE TARGET TRANSFORM: {}'.format(rmse_scale))
482/200:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train, X_test, y_train ,y_test)
    
    fit_and_predict(model, X_train, X_test, y_train, y_test)
482/201:
models=[
        LinearRegression(),
        Lasso(alpha=1.0),
        ElasticNet(alpha=1.0, l1_ratio=.5),
        Ridge(alpha=1.0),
        SVR(kernel = 'rbf')
       ]
482/202:
def fit_and_predict(model, x_train ,x_test, y_train, y_test):
    print(model)
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    #####################################################
    target_scaler = MinMaxScaler()
    target_scaler.fit(y_train.values.reshape(-1, 1))

    y_train = target_scaler.transform(y_train.values.reshape(-1, 1))
   # y_test = target_scaler.transform(y_test.values.reshape(-1, 1))

    classifier.fit(x_train,y_train)
    y_pred_scale = classifier.predict(x_test)
    
    y_pred_scale = target_scaler.inverse_transform(y_pred_scale)
    rmse_scale = mean_squared_error(y_test, y_pred_scale, squared=False)
    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'test rmse': rmse, 
                              'test ttr rmse': rmse_scale
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using ")
    print('RMSE: {}'.format(rmse))
    print('RMSE TARGET TRANSFORM: {}'.format(rmse_scale))
482/203:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train, X_test, y_train ,y_test)
    
    fit_and_predict(model, X_train, X_test, y_train, y_test)
482/204:
lasso = Lasso(alpha=1)
lasso.fit(X_train, y_train)
lasso.predict(X_test)
482/205:
lasso = Lasso(alpha=1)
lasso.fit(X_train, y_train)
q = lasso.predict(X_test)
q.shape
482/206:
def fit_and_predict(model, x_train ,x_test, y_train, y_test):
    print(model)
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    #####################################################
    target_scaler = MinMaxScaler()
    target_scaler.fit(y_train.values.reshape(-1, 1))

    y_train = target_scaler.transform(y_train.values.reshape(-1, 1))
   # y_test = target_scaler.transform(y_test.values.reshape(-1, 1))

    classifier.fit(x_train,y_train)
    y_pred_scale = classifier.predict(x_test)
    
    y_pred_scale = target_scaler.inverse_transform(y_pred_scale.values.reshape(-1, 1))
    rmse_scale = mean_squared_error(y_test, y_pred_scale, squared=False)
    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'test rmse': rmse, 
                              'test ttr rmse': rmse_scale
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using ")
    print('RMSE: {}'.format(rmse))
    print('RMSE TARGET TRANSFORM: {}'.format(rmse_scale))
482/207:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train, X_test, y_train ,y_test)
    
    fit_and_predict(model, X_train, X_test, y_train, y_test)
482/208:
lasso = Lasso(alpha=1)
lasso.fit(X_train, y_train)
q = lasso.predict(X_test)
q.stype
482/209: y_test.dtype
482/210:
lasso = Lasso(alpha=1)
lasso.fit(X_train, y_train)
q = lasso.predict(X_test)
q.dtype
482/211: y_test.stype
482/212:
lasso = Lasso(alpha=1)
lasso.fit(X_train, y_train)
q = lasso.predict(X_test)
q.stype
482/213:
def fit_and_predict(model, x_train ,x_test, y_train, y_test):
    print(model)
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    #####################################################
    target_scaler = MinMaxScaler()
    target_scaler.fit(y_train.values.reshape(-1, 1))

    y_train = target_scaler.transform(y_train.values.reshape(-1, 1))
   # y_test = target_scaler.transform(y_test.values.reshape(-1, 1))

    classifier.fit(x_train,y_train)
    y_pred_scale = classifier.predict(x_test)
    
    y_pred_scale = target_scaler.inverse_transform(y_pred_scale)
    rmse_scale = mean_squared_error(y_test, y_pred_scale, squared=False)
    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'test rmse': rmse, 
                              'test ttr rmse': rmse_scale
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using ")
    print('RMSE: {}'.format(rmse))
    print('RMSE TARGET TRANSFORM: {}'.format(rmse_scale))
482/214:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train, X_test, y_train ,y_test)
    
    fit_and_predict(model, X_train, X_test, y_train, y_test)
482/215:
def fit_and_predict(model, x_train ,x_test, y_train, y_test):
    print(model)
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    #####################################################
    target_scaler = MinMaxScaler()
    target_scaler.fit(y_train.values.reshape(-1, 1))

    y_train = target_scaler.transform(y_train.values.reshape(-1, 1))
   # y_test = target_scaler.transform(y_test.values.reshape(-1, 1))

    classifier.fit(x_train,y_train)
    y_pred_scale = classifier.predict(x_test)
    
    y_pred_scale = target_scaler.inverse_transform(y_pred_scale.reshape(-1, 1))
    rmse_scale = mean_squared_error(y_test, y_pred_scale, squared=False)
    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'test rmse': rmse, 
                              'test ttr rmse': rmse_scale
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using ")
    print('RMSE: {}'.format(rmse))
    print('RMSE TARGET TRANSFORM: {}'.format(rmse_scale))
482/216:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train, X_test, y_train ,y_test)
    
    fit_and_predict(model, X_train, X_test, y_train, y_test)
484/1:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
484/2:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

df_train_original = pd.read_csv('data/train.csv')

print(df_train.shape)
print(df_val.shape)
484/3: lr = LinearRegression()
484/4: df_train.head(3)
484/5: df_train.describe()
484/6:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
484/7:
### Baseline model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
484/8:
## PREPROC
# 1. VarianceThreshold from Scikit-learn
484/9:
sel = VarianceThreshold(threshold=0.0)
sel.fit(X_train)  # fit finds the features with zero variance
484/10:
sel.get_support()
constant = X_train.columns[~sel.get_support()]
len(constant)
484/11:
### only works with numerical
const_f = [feat for feat in X_train.columns if X_train[feat].std() == 0]
len(const_f)
484/12: ### works also with categorical variables
484/13:
constant_features = [feat for feat in X_train.columns if X_train[feat].nunique() == 1]
len(constant_features)
484/14: ## quasi-constant features
484/15:
sel = VarianceThreshold(threshold=0.01)  
sel.fit(X_train)  # fit finds the features with low variance
quasi_constant = X_train.columns[~sel.get_support()]

len(quasi_constant)
484/16: ### duplicated features
484/17: dic_sum = df_train.sum().to_dict()
484/18:
k_v_exchanged = {}
for key, val in dic_sum.items():
    if val not in k_v_exchanged:
        k_v_exchanged[val] = [key]
    else:
        k_v_exchanged[val].append(key)
484/19: max([len(v) for v in k_v_exchanged.values()])
484/20: ## correlation
484/21:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
484/22:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
484/23:
corr_features = correlation(X_train, 0.75)
print('correlated features: ', len(set(corr_features)) )
484/24: corr_features
484/25:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
484/26:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
484/27: #The mutual information measures the reduction in uncertainty in variable A when variable B is known.
484/28: from sklearn.feature_selection import mutual_info_regression
484/29:
# determine the mutual information
mi = mutual_info_regression(X_train, y_train)

# and make a bar  plot
mi = pd.Series(mi)
mi.index = X_train.columns
mi.sort_values(ascending=False).plot.bar(figsize=(20,6))
plt.ylabel('Mutual Information')
484/30: mi_dict = dict(sorted(mi.to_dict().items(), key=lambda item: item[1]))
484/31: unmutal_cols = list(mi_dict.keys())[0]
484/32:
X_train.drop(labels=unmutal_cols, axis=1, inplace=True)
X_test.drop(labels=unmutal_cols, axis=1, inplace=True)

X_train.shape, X_test.shape
484/33:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
484/34:
## ANOVA
from sklearn.feature_selection import f_regression
484/35:
# univariate anova
univariate = f_regression(X_train.fillna(0), y_train)

# plot values
univariate = pd.Series(univariate[1])
univariate.index = X_train.columns
univariate.sort_values(ascending=False).plot.bar(figsize=(20,6))
484/36: univariate_dict = dict(sorted(univariate.to_dict().items(), key=lambda item: item[1]))
484/37: univariate_cols = list(univariate_dict.keys())[0:2]
484/38:
X_train.drop(labels=univariate_cols, axis=1, inplace=True)
X_test.drop(labels=univariate_cols, axis=1, inplace=True)

X_train.shape, X_test.shape
484/39:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
484/40: ## pi
484/41: import mlxtend
484/42:
from sklearn.metrics import make_scorer
from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS
484/43:
# exhaustive search
efs = EFS(LinearRegression(), min_features=1, max_features=3, scoring='neg_mean_squared_error', print_progress=True,)
efs = efs.fit(np.array(X_train), y_train)
484/44: efs.best_idx_
484/45:
selected_feat = X_train.columns[list(efs.best_idx_)]
selected_feat
484/46:
lr.fit(X_train[selected_feat], y_train)
y_predict = lr.predict(X_test[selected_feat])
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
484/47:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
484/48:
sfs = SFS(LinearRegression(),
          k_features=3, # the lower the features we want, the longer this will take
          forward=False,
          verbose=2,
          scoring='neg_mean_squared_error')

sfs = sfs.fit(np.array(X_train), y_train)
484/49:
sfs.k_feature_idx_
## The same result, but faster :)
484/50:
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import Ridge
from sklearn.svm import SVR

from sklearn.compose import TransformedTargetRegressor
484/51:
LinearRegression()
Lasso(alpha=1.0)
ElasticNet(alpha=1.0, l1_ratio=.5)
Ridge(alpha=1.0)
SVR(kernel = 'rbf')
484/52:
models=[
        LinearRegression(),
        Lasso(alpha=1.0),
        ElasticNet(alpha=1.0, l1_ratio=.5),
        Ridge(alpha=1.0),
        SVR(kernel = 'rbf')
       ]
484/53: from sklearn.preprocessing import MinMaxScaler
484/54: metrics = pd.DataFrame(columns=['model' ,'test rmse', 'test ttr rmse'])
484/55:
def fit_and_predict(model, x_train ,x_test, y_train, y_test):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    #####################################################
    target_scaler = MinMaxScaler()
    target_scaler.fit(y_train.values.reshape(-1, 1))

    y_train = target_scaler.transform(y_train.values.reshape(-1, 1))
   # y_test = target_scaler.transform(y_test.values.reshape(-1, 1))

    classifier.fit(x_train,y_train)
    y_pred_scale = classifier.predict(x_test)
    
    y_pred_scale = target_scaler.inverse_transform(y_pred_scale.reshape(-1, 1))
    rmse_scale = mean_squared_error(y_test, y_pred_scale, squared=False)
    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'test rmse': rmse, 
                              'test ttr rmse': rmse_scale
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using ")
    print('RMSE: {}'.format(rmse))
    print('RMSE TARGET TRANSFORM: {}'.format(rmse_scale))
484/56:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train, X_test, y_train ,y_test)
    
    fit_and_predict(model, X_train, X_test, y_train, y_test)
485/1:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
485/2:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

df_train_original = pd.read_csv('data/train.csv')

print(df_train.shape)
print(df_val.shape)
485/3: lr = LinearRegression()
485/4: df_train.head(3)
485/5: df_train.describe()
485/6:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
485/7:
### Baseline model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
485/8:
## PREPROC
# 1. VarianceThreshold from Scikit-learn
485/9:
sel = VarianceThreshold(threshold=0.0)
sel.fit(X_train)  # fit finds the features with zero variance
485/10:
sel.get_support()
constant = X_train.columns[~sel.get_support()]
len(constant)
485/11:
### only works with numerical
const_f = [feat for feat in X_train.columns if X_train[feat].std() == 0]
len(const_f)
485/12: ### works also with categorical variables
485/13:
constant_features = [feat for feat in X_train.columns if X_train[feat].nunique() == 1]
len(constant_features)
485/14: ## quasi-constant features
485/15:
sel = VarianceThreshold(threshold=0.01)  
sel.fit(X_train)  # fit finds the features with low variance
quasi_constant = X_train.columns[~sel.get_support()]

len(quasi_constant)
485/16: ### duplicated features
485/17: dic_sum = df_train.sum().to_dict()
485/18:
k_v_exchanged = {}
for key, val in dic_sum.items():
    if val not in k_v_exchanged:
        k_v_exchanged[val] = [key]
    else:
        k_v_exchanged[val].append(key)
485/19: max([len(v) for v in k_v_exchanged.values()])
485/20: ## correlation
485/21:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
485/22:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
485/23:
corr_features = correlation(X_train, 0.75)
print('correlated features: ', len(set(corr_features)) )
485/24: corr_features
485/25:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
485/26:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
485/27: #The mutual information measures the reduction in uncertainty in variable A when variable B is known.
485/28: from sklearn.feature_selection import mutual_info_regression
485/29:
# determine the mutual information
mi = mutual_info_regression(X_train, y_train)

# and make a bar  plot
mi = pd.Series(mi)
mi.index = X_train.columns
mi.sort_values(ascending=False).plot.bar(figsize=(20,6))
plt.ylabel('Mutual Information')
485/30: mi_dict = dict(sorted(mi.to_dict().items(), key=lambda item: item[1]))
485/31: unmutal_cols = list(mi_dict.keys())[0]
485/32:
X_train.drop(labels=unmutal_cols, axis=1, inplace=True)
X_test.drop(labels=unmutal_cols, axis=1, inplace=True)

X_train.shape, X_test.shape
485/33:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
485/34:
## ANOVA
from sklearn.feature_selection import f_regression
485/35:
# univariate anova
univariate = f_regression(X_train.fillna(0), y_train)

# plot values
univariate = pd.Series(univariate[1])
univariate.index = X_train.columns
univariate.sort_values(ascending=False).plot.bar(figsize=(20,6))
485/36: univariate_dict = dict(sorted(univariate.to_dict().items(), key=lambda item: item[1]))
485/37: univariate_cols = list(univariate_dict.keys())[0:2]
485/38:
X_train.drop(labels=univariate_cols, axis=1, inplace=True)
X_test.drop(labels=univariate_cols, axis=1, inplace=True)

X_train.shape, X_test.shape
485/39:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
485/40: ## pi
485/41: import mlxtend
485/42:
from sklearn.metrics import make_scorer
from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS
485/43:
# exhaustive search
efs = EFS(LinearRegression(), min_features=1, max_features=3, scoring='neg_mean_squared_error', print_progress=True,)
efs = efs.fit(np.array(X_train), y_train)
485/44: efs.best_idx_
485/45:
selected_feat = X_train.columns[list(efs.best_idx_)]
selected_feat
485/46:
lr.fit(X_train[selected_feat], y_train)
y_predict = lr.predict(X_test[selected_feat])
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
485/47:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
485/48:
sfs = SFS(LinearRegression(),
          k_features=3, # the lower the features we want, the longer this will take
          forward=False,
          verbose=2,
          scoring='neg_mean_squared_error')

sfs = sfs.fit(np.array(X_train), y_train)
485/49:
sfs.k_feature_idx_
## The same result, but faster :)
485/50:
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import Ridge
from sklearn.svm import SVR

from sklearn.compose import TransformedTargetRegressor
485/51:
LinearRegression()
Lasso(alpha=1.0)
ElasticNet(alpha=1.0, l1_ratio=.5)
Ridge(alpha=1.0)
SVR(kernel = 'rbf')
485/52:
models=[
        LinearRegression(),
        Lasso(alpha=1.0),
        ElasticNet(alpha=1.0, l1_ratio=.5),
        Ridge(alpha=1.0),
        SVR(kernel = 'rbf')
       ]
485/53: from sklearn.preprocessing import MinMaxScaler
485/54: metrics = pd.DataFrame(columns=['model' ,'test rmse', 'test ttr rmse'])
485/55:
def fit_and_predict(model, x_train ,x_test, y_train, y_test):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    #####################################################
    target_scaler = MinMaxScaler()
    target_scaler.fit(y_train.values.reshape(-1, 1))

    y_train = target_scaler.transform(y_train.values.reshape(-1, 1))
   # y_test = target_scaler.transform(y_test.values.reshape(-1, 1))

    classifier.fit(x_train,y_train)
    y_pred_scale = classifier.predict(x_test)
    
    y_pred_scale = target_scaler.inverse_transform(y_pred_scale.reshape(-1, 1))
    rmse_scale = mean_squared_error(y_test, y_pred_scale, squared=False)
    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'test rmse': rmse, 
                              'test ttr rmse': rmse_scale
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using ")
    print('RMSE: {}'.format(rmse))
    print('RMSE TARGET TRANSFORM: {}'.format(rmse_scale))
485/56:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train, X_test, y_train ,y_test)
    
    fit_and_predict(model, X_train[selected_feat], X_test[selected_feat], y_train, y_test)
486/1:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
486/2:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

df_train_original = pd.read_csv('data/train.csv')

print(df_train.shape)
print(df_val.shape)
486/3: lr = LinearRegression()
486/4: df_train.head(3)
486/5: df_train.describe()
486/6:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
486/7:
### Baseline model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
486/8:
## PREPROC
# 1. VarianceThreshold from Scikit-learn
486/9:
sel = VarianceThreshold(threshold=0.0)
sel.fit(X_train)  # fit finds the features with zero variance
486/10:
sel.get_support()
constant = X_train.columns[~sel.get_support()]
len(constant)
486/11:
### only works with numerical
const_f = [feat for feat in X_train.columns if X_train[feat].std() == 0]
len(const_f)
486/12: ### works also with categorical variables
486/13:
constant_features = [feat for feat in X_train.columns if X_train[feat].nunique() == 1]
len(constant_features)
486/14: ## quasi-constant features
486/15:
sel = VarianceThreshold(threshold=0.01)  
sel.fit(X_train)  # fit finds the features with low variance
quasi_constant = X_train.columns[~sel.get_support()]

len(quasi_constant)
486/16: ### duplicated features
486/17: dic_sum = df_train.sum().to_dict()
486/18:
k_v_exchanged = {}
for key, val in dic_sum.items():
    if val not in k_v_exchanged:
        k_v_exchanged[val] = [key]
    else:
        k_v_exchanged[val].append(key)
486/19: max([len(v) for v in k_v_exchanged.values()])
486/20: ## correlation
486/21:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
486/22:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
486/23:
corr_features = correlation(X_train, 0.75)
print('correlated features: ', len(set(corr_features)) )
486/24: corr_features
486/25:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
486/26:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
486/27: #The mutual information measures the reduction in uncertainty in variable A when variable B is known.
486/28: from sklearn.feature_selection import mutual_info_regression
486/29:
# determine the mutual information
mi = mutual_info_regression(X_train, y_train)

# and make a bar  plot
mi = pd.Series(mi)
mi.index = X_train.columns
mi.sort_values(ascending=False).plot.bar(figsize=(20,6))
plt.ylabel('Mutual Information')
486/30: mi_dict = dict(sorted(mi.to_dict().items(), key=lambda item: item[1]))
486/31: unmutal_cols = list(mi_dict.keys())[0]
486/32:
X_train.drop(labels=unmutal_cols, axis=1, inplace=True)
X_test.drop(labels=unmutal_cols, axis=1, inplace=True)

X_train.shape, X_test.shape
486/33:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
486/34:
## ANOVA
from sklearn.feature_selection import f_regression
486/35:
# univariate anova
univariate = f_regression(X_train.fillna(0), y_train)

# plot values
univariate = pd.Series(univariate[1])
univariate.index = X_train.columns
univariate.sort_values(ascending=False).plot.bar(figsize=(20,6))
486/36: univariate_dict = dict(sorted(univariate.to_dict().items(), key=lambda item: item[1]))
486/37: univariate_cols = list(univariate_dict.keys())[0:2]
486/38:
X_train.drop(labels=univariate_cols, axis=1, inplace=True)
X_test.drop(labels=univariate_cols, axis=1, inplace=True)

X_train.shape, X_test.shape
486/39:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
486/40: ## pi
486/41: import mlxtend
486/42:
from sklearn.metrics import make_scorer
from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS
486/43:
# exhaustive search
efs = EFS(LinearRegression(), min_features=1, max_features=3, scoring='neg_mean_squared_error', print_progress=True,)
efs = efs.fit(np.array(X_train), y_train)
486/44: efs.best_idx_
486/45:
selected_feat = X_train.columns[list(efs.best_idx_)]
selected_feat
486/46:
lr.fit(X_train[selected_feat], y_train)
y_predict = lr.predict(X_test[selected_feat])
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
486/47:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
486/48:
sfs = SFS(LinearRegression(),
          k_features=3, # the lower the features we want, the longer this will take
          forward=False,
          verbose=2,
          scoring='neg_mean_squared_error')

sfs = sfs.fit(np.array(X_train), y_train)
486/49:
sfs.k_feature_idx_
## The same result, but faster :)
486/50:
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import Ridge
from sklearn.svm import SVR

from sklearn.compose import TransformedTargetRegressor
486/51:
LinearRegression()
Lasso(alpha=1.0)
ElasticNet(alpha=1.0, l1_ratio=.5)
Ridge(alpha=1.0)
SVR(kernel = 'rbf')
486/52:
svr = SVR(kernel = 'rbf')
svr.fit(X_train, y_train)
487/1:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
487/2:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

df_train_original = pd.read_csv('data/train.csv')

print(df_train.shape)
print(df_val.shape)
487/3: lr = LinearRegression()
487/4: df_train.head(3)
487/5: df_train.describe()
487/6:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
487/7:
### Baseline model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
487/8:
## PREPROC
# 1. VarianceThreshold from Scikit-learn
487/9:
sel = VarianceThreshold(threshold=0.0)
sel.fit(X_train)  # fit finds the features with zero variance
487/10:
sel.get_support()
constant = X_train.columns[~sel.get_support()]
len(constant)
487/11:
### only works with numerical
const_f = [feat for feat in X_train.columns if X_train[feat].std() == 0]
len(const_f)
487/12: ### works also with categorical variables
487/13:
constant_features = [feat for feat in X_train.columns if X_train[feat].nunique() == 1]
len(constant_features)
487/14: ## quasi-constant features
487/15:
sel = VarianceThreshold(threshold=0.01)  
sel.fit(X_train)  # fit finds the features with low variance
quasi_constant = X_train.columns[~sel.get_support()]

len(quasi_constant)
487/16: ### duplicated features
487/17: dic_sum = df_train.sum().to_dict()
487/18:
k_v_exchanged = {}
for key, val in dic_sum.items():
    if val not in k_v_exchanged:
        k_v_exchanged[val] = [key]
    else:
        k_v_exchanged[val].append(key)
487/19: max([len(v) for v in k_v_exchanged.values()])
487/20: ## correlation
487/21:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
487/22:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
487/23:
corr_features = correlation(X_train, 0.75)
print('correlated features: ', len(set(corr_features)) )
487/24: corr_features
487/25:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
487/26:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
487/27: #The mutual information measures the reduction in uncertainty in variable A when variable B is known.
487/28: from sklearn.feature_selection import mutual_info_regression
487/29:
# determine the mutual information
mi = mutual_info_regression(X_train, y_train)

# and make a bar  plot
mi = pd.Series(mi)
mi.index = X_train.columns
mi.sort_values(ascending=False).plot.bar(figsize=(20,6))
plt.ylabel('Mutual Information')
487/30: mi_dict = dict(sorted(mi.to_dict().items(), key=lambda item: item[1]))
487/31: unmutal_cols = list(mi_dict.keys())[0]
487/32:
X_train.drop(labels=unmutal_cols, axis=1, inplace=True)
X_test.drop(labels=unmutal_cols, axis=1, inplace=True)

X_train.shape, X_test.shape
487/33:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
487/34:
## ANOVA
from sklearn.feature_selection import f_regression
487/35:
# univariate anova
univariate = f_regression(X_train.fillna(0), y_train)

# plot values
univariate = pd.Series(univariate[1])
univariate.index = X_train.columns
univariate.sort_values(ascending=False).plot.bar(figsize=(20,6))
487/36: univariate_dict = dict(sorted(univariate.to_dict().items(), key=lambda item: item[1]))
487/37: univariate_cols = list(univariate_dict.keys())[0:2]
487/38:
X_train.drop(labels=univariate_cols, axis=1, inplace=True)
X_test.drop(labels=univariate_cols, axis=1, inplace=True)

X_train.shape, X_test.shape
487/39:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
487/40: ## pi
487/41: import mlxtend
487/42:
from sklearn.metrics import make_scorer
from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS
487/43:
# exhaustive search
efs = EFS(LinearRegression(), min_features=1, max_features=3, scoring='neg_mean_squared_error', print_progress=True,)
efs = efs.fit(np.array(X_train), y_train)
487/44: efs.best_idx_
487/45:
selected_feat = X_train.columns[list(efs.best_idx_)]
selected_feat
487/46:
lr.fit(X_train[selected_feat], y_train)
y_predict = lr.predict(X_test[selected_feat])
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
487/47:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
487/48:
sfs = SFS(LinearRegression(),
          k_features=3, # the lower the features we want, the longer this will take
          forward=False,
          verbose=2,
          scoring='neg_mean_squared_error')

sfs = sfs.fit(np.array(X_train), y_train)
487/49:
sfs.k_feature_idx_
## The same result, but faster :)
487/50:
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import Ridge
from sklearn.svm import SVR

from sklearn.compose import TransformedTargetRegressor
487/51:
LinearRegression()
Lasso(alpha=1.0)
ElasticNet(alpha=1.0, l1_ratio=.5)
Ridge(alpha=1.0)
SVR(kernel = 'rbf')
487/52:
svr = SVR(C=1.0, epsilon=0.2)
svr.fit(X_train[:200], y_train[:200])
487/53:
svr = SVR(C=1.0, epsilon=0.2, kernel='rbf')
svr.fit(X_train[:200], y_train[:200])
487/54: svr.predict(X_test)
487/55:
svr = SVR(C=1.0, epsilon=0.2, kernel='rbf')
svr.fit(X_train[:2000], y_train[:2000])
487/56: svr.predict(X_test)
487/57:
svr = SVR(C=1.0, epsilon=0.2, kernel='linear')
svr.fit(X_train[:2000], y_train[:2000])
487/58: svr.predict(X_test)
487/59:
svr = SVR(C=1.0, epsilon=0.1, kernel='linear')
svr.fit(X_train[:2000], y_train[:2000])
487/60: svr.predict(X_test)
487/61: svr.predict(X_test)
487/62: y_test
487/63:
svr = SVR(C=1.0, epsilon=0.1, kernel='rbf')
svr.fit(X_train[:2000], y_train[:2000])
487/64: svr.predict(X_test)
487/65:
LinearRegression()
Lasso(alpha=1.0)
ElasticNet(alpha=1.0, l1_ratio=.5)
Ridge(alpha=1.0)
SVR(C=1.0, epsilon=0.1, kernel='linear')
487/66:
models=[
        LinearRegression(),
        Lasso(alpha=1.0),
        ElasticNet(alpha=1.0, l1_ratio=.5),
        Ridge(alpha=1.0),
        SVR(C=1.0, epsilon=0.1, kernel='linear')
       ]
487/67: from sklearn.preprocessing import MinMaxScaler
487/68: metrics = pd.DataFrame(columns=['model' ,'test rmse', 'test ttr rmse'])
487/69:
def fit_and_predict(model, x_train ,x_test, y_train, y_test):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    #####################################################
    target_scaler = MinMaxScaler()
    target_scaler.fit(y_train.values.reshape(-1, 1))

    y_train = target_scaler.transform(y_train.values.reshape(-1, 1))
   # y_test = target_scaler.transform(y_test.values.reshape(-1, 1))

    classifier.fit(x_train,y_train)
    y_pred_scale = classifier.predict(x_test)
    
    y_pred_scale = target_scaler.inverse_transform(y_pred_scale.reshape(-1, 1))
    rmse_scale = mean_squared_error(y_test, y_pred_scale, squared=False)
    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'test rmse': rmse, 
                              'test ttr rmse': rmse_scale
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using ")
    print('RMSE: {}'.format(rmse))
    print('RMSE TARGET TRANSFORM: {}'.format(rmse_scale))
487/70:
for model in models:
    ## only text Count vector
    fit_and_predict(model, X_train, X_test, y_train ,y_test)
    
    fit_and_predict(model, X_train[selected_feat], X_test[selected_feat], y_train, y_test)
488/1:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
488/2:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

df_train_original = pd.read_csv('data/train.csv')

print(df_train.shape)
print(df_val.shape)
488/3: lr = LinearRegression()
488/4: df_train.head(3)
488/5: df_train.describe()
488/6:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
488/7:
### Baseline model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
488/8:
## PREPROC
# 1. VarianceThreshold from Scikit-learn
488/9:
sel = VarianceThreshold(threshold=0.0)
sel.fit(X_train)  # fit finds the features with zero variance
488/10:
sel.get_support()
constant = X_train.columns[~sel.get_support()]
len(constant)
488/11:
### only works with numerical
const_f = [feat for feat in X_train.columns if X_train[feat].std() == 0]
len(const_f)
488/12: ### works also with categorical variables
488/13:
constant_features = [feat for feat in X_train.columns if X_train[feat].nunique() == 1]
len(constant_features)
488/14: ## quasi-constant features
488/15:
sel = VarianceThreshold(threshold=0.01)  
sel.fit(X_train)  # fit finds the features with low variance
quasi_constant = X_train.columns[~sel.get_support()]

len(quasi_constant)
488/16: ### duplicated features
488/17: dic_sum = df_train.sum().to_dict()
488/18:
k_v_exchanged = {}
for key, val in dic_sum.items():
    if val not in k_v_exchanged:
        k_v_exchanged[val] = [key]
    else:
        k_v_exchanged[val].append(key)
488/19: max([len(v) for v in k_v_exchanged.values()])
488/20: ## correlation
488/21:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
488/22:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
488/23:
corr_features = correlation(X_train, 0.75)
print('correlated features: ', len(set(corr_features)) )
488/24: corr_features
488/25:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
488/26:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
488/27: #The mutual information measures the reduction in uncertainty in variable A when variable B is known.
488/28: from sklearn.feature_selection import mutual_info_regression
488/29:
# determine the mutual information
mi = mutual_info_regression(X_train, y_train)

# and make a bar  plot
mi = pd.Series(mi)
mi.index = X_train.columns
mi.sort_values(ascending=False).plot.bar(figsize=(20,6))
plt.ylabel('Mutual Information')
488/30: mi_dict = dict(sorted(mi.to_dict().items(), key=lambda item: item[1]))
488/31: unmutal_cols = list(mi_dict.keys())[0]
488/32:
X_train.drop(labels=unmutal_cols, axis=1, inplace=True)
X_test.drop(labels=unmutal_cols, axis=1, inplace=True)

X_train.shape, X_test.shape
488/33:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
488/34:
## ANOVA
from sklearn.feature_selection import f_regression
488/35:
# univariate anova
univariate = f_regression(X_train.fillna(0), y_train)

# plot values
univariate = pd.Series(univariate[1])
univariate.index = X_train.columns
univariate.sort_values(ascending=False).plot.bar(figsize=(20,6))
488/36: univariate_dict = dict(sorted(univariate.to_dict().items(), key=lambda item: item[1]))
488/37: univariate_cols = list(univariate_dict.keys())[0:2]
488/38:
X_train.drop(labels=univariate_cols, axis=1, inplace=True)
X_test.drop(labels=univariate_cols, axis=1, inplace=True)

X_train.shape, X_test.shape
488/39:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
488/40: ## pi
488/41: import mlxtend
488/42:
from sklearn.metrics import make_scorer
from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS
488/43:
# exhaustive search
efs = EFS(LinearRegression(), min_features=1, max_features=3, scoring='neg_mean_squared_error', print_progress=True,)
efs = efs.fit(np.array(X_train), y_train)
488/44: efs.best_idx_
488/45:
selected_feat = X_train.columns[list(efs.best_idx_)]
selected_feat
488/46:
lr.fit(X_train[selected_feat], y_train)
y_predict = lr.predict(X_test[selected_feat])
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
488/47:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
488/48:
sfs = SFS(LinearRegression(),
          k_features=3, # the lower the features we want, the longer this will take
          forward=False,
          verbose=2,
          scoring='neg_mean_squared_error')

sfs = sfs.fit(np.array(X_train), y_train)
488/49:
sfs.k_feature_idx_
## The same result, but faster :)
488/50:
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import Ridge
from sklearn.svm import SVR

from sklearn.compose import TransformedTargetRegressor
488/51:
LinearRegression()
Lasso(alpha=1.0)
ElasticNet(alpha=1.0, l1_ratio=.5)
Ridge(alpha=1.0)
SVR(C=1.0, epsilon=0.1, kernel='linear')
488/52:
svr = SVR(C=1.0, epsilon=0.1, kernel='linear')
svr.fit(X_train[:2000], y_train[:2000])
488/53: svr.predict(X_test)
488/54: svr.predict(X_test)
488/55: y_test
488/56:
models=[
        LinearRegression(),
        Lasso(alpha=1.0),
        ElasticNet(alpha=1.0, l1_ratio=.5),
        Ridge(alpha=1.0),
        SVR(C=1.0, epsilon=0.1, kernel='linear')
       ]
488/57: from sklearn.preprocessing import MinMaxScaler
488/58:
models=[
        LinearRegression(),
        Lasso(alpha=1.0),
        ElasticNet(alpha=1.0, l1_ratio=.5),
        Ridge(alpha=1.0),
   #     SVR(C=1.0, epsilon=0.1, kernel='linear')
       ]
488/59: from sklearn.preprocessing import MinMaxScaler
488/60: metrics = pd.DataFrame(columns=['model' ,'test rmse', 'test ttr rmse'])
488/61:
def fit_and_predict(model, x_train ,x_test, y_train, y_test):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    #####################################################
    target_scaler = MinMaxScaler()
    target_scaler.fit(y_train.values.reshape(-1, 1))

    y_train = target_scaler.transform(y_train.values.reshape(-1, 1))
   # y_test = target_scaler.transform(y_test.values.reshape(-1, 1))

    classifier.fit(x_train,y_train)
    y_pred_scale = classifier.predict(x_test)
    
    y_pred_scale = target_scaler.inverse_transform(y_pred_scale.reshape(-1, 1))
    rmse_scale = mean_squared_error(y_test, y_pred_scale, squared=False)
    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'test rmse': rmse, 
                              'test ttr rmse': rmse_scale
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using ")
    print('RMSE: {}'.format(rmse))
    print('RMSE TARGET TRANSFORM: {}'.format(rmse_scale))
488/62:
for model in models:
    ## all columns
    fit_and_predict(model, X_train, X_test, y_train ,y_test)
    ## selected columns
    fit_and_predict(model, X_train[selected_feat], X_test[selected_feat], y_train, y_test)
488/63: metrics
488/64:
for model in models:
    ## all columns
    fit_and_predict(model, X_train, X_test, y_train ,y_test)
    ## selected columns
    print('\n')
    fit_and_predict(model, X_train[selected_feat], X_test[selected_feat], y_train, y_test)
488/65:
for model in models:
    ## all columns
    fit_and_predict(model, X_train, X_test, y_train ,y_test)
    ## selected columns
    fit_and_predict(model, X_train[selected_feat], X_test[selected_feat], y_train, y_test)
    print('\n')
488/66: metrics
488/67: metrics = pd.DataFrame(columns=['model' ,'test rmse', 'test ttr rmse'])
488/68:
def fit_and_predict(model, x_train ,x_test, y_train, y_test):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    #####################################################
    target_scaler = MinMaxScaler()
    target_scaler.fit(y_train.values.reshape(-1, 1))

    y_train = target_scaler.transform(y_train.values.reshape(-1, 1))
   # y_test = target_scaler.transform(y_test.values.reshape(-1, 1))

    classifier.fit(x_train,y_train)
    y_pred_scale = classifier.predict(x_test)
    
    y_pred_scale = target_scaler.inverse_transform(y_pred_scale.reshape(-1, 1))
    rmse_scale = mean_squared_error(y_test, y_pred_scale, squared=False)
    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'test rmse': rmse, 
                              'test ttr rmse': rmse_scale
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using ")
    print(f'RMSE: {rmse:.>{10}}')
    print(f'RMSE TARGET TRANSFORM: {rmse_scale:.>{10}}')
488/69:
for model in models:
    ## all columns
    fit_and_predict(model, X_train, X_test, y_train ,y_test)
    ## selected columns
    fit_and_predict(model, X_train[selected_feat], X_test[selected_feat], y_train, y_test)
    print('\n')
488/70:
def fit_and_predict(model, x_train ,x_test, y_train, y_test):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    #####################################################
    target_scaler = MinMaxScaler()
    target_scaler.fit(y_train.values.reshape(-1, 1))

    y_train = target_scaler.transform(y_train.values.reshape(-1, 1))
   # y_test = target_scaler.transform(y_test.values.reshape(-1, 1))

    classifier.fit(x_train,y_train)
    y_pred_scale = classifier.predict(x_test)
    
    y_pred_scale = target_scaler.inverse_transform(y_pred_scale.reshape(-1, 1))
    rmse_scale = mean_squared_error(y_test, y_pred_scale, squared=False)
    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'test rmse': rmse, 
                              'test ttr rmse': rmse_scale
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using ")
    print(f'RMSE: {rmse:.>{10}.4f}')
    print(f'RMSE TARGET TRANSFORM: {rmse_scale:.>{10}.4f}')
488/71:
for model in models:
    ## all columns
    fit_and_predict(model, X_train, X_test, y_train ,y_test)
    ## selected columns
    fit_and_predict(model, X_train[selected_feat], X_test[selected_feat], y_train, y_test)
    print('\n')
488/72:
def fit_and_predict(model, x_train ,x_test, y_train, y_test):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    #####################################################
    target_scaler = MinMaxScaler()
    target_scaler.fit(y_train.values.reshape(-1, 1))

    y_train = target_scaler.transform(y_train.values.reshape(-1, 1))
   # y_test = target_scaler.transform(y_test.values.reshape(-1, 1))

    classifier.fit(x_train,y_train)
    y_pred_scale = classifier.predict(x_test)
    
    y_pred_scale = target_scaler.inverse_transform(y_pred_scale.reshape(-1, 1))
    rmse_scale = mean_squared_error(y_test, y_pred_scale, squared=False)
    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'test rmse': rmse, 
                              'test ttr rmse': rmse_scale
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using ")
    print(f'RMSE: {rmse:.>{30}.4f}')
    print(f'RMSE TARGET TRANSFORM: {rmse_scale:.>{10}.4f}')
488/73:
for model in models:
    ## all columns
    fit_and_predict(model, X_train, X_test, y_train ,y_test)
    ## selected columns
    fit_and_predict(model, X_train[selected_feat], X_test[selected_feat], y_train, y_test)
    print('\n')
488/74:
def fit_and_predict(model, x_train ,x_test, y_train, y_test):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    #####################################################
    target_scaler = MinMaxScaler()
    target_scaler.fit(y_train.values.reshape(-1, 1))

    y_train = target_scaler.transform(y_train.values.reshape(-1, 1))
   # y_test = target_scaler.transform(y_test.values.reshape(-1, 1))

    classifier.fit(x_train,y_train)
    y_pred_scale = classifier.predict(x_test)
    
    y_pred_scale = target_scaler.inverse_transform(y_pred_scale.reshape(-1, 1))
    rmse_scale = mean_squared_error(y_test, y_pred_scale, squared=False)
    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'test rmse': rmse, 
                              'test ttr rmse': rmse_scale
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using ")
    print(f'RMSE: {rmse:.>{30}.4f}')
    print(f'RMSE TARGET TRANSFORM: {rmse_scale:.>{14}.4f}')
488/75:
for model in models:
    ## all columns
    fit_and_predict(model, X_train, X_test, y_train ,y_test)
    ## selected columns
    fit_and_predict(model, X_train[selected_feat], X_test[selected_feat], y_train, y_test)
    print('\n')
488/76:
def fit_and_predict(model, x_train ,x_test, y_train, y_test):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    #####################################################
    target_scaler = MinMaxScaler()
    target_scaler.fit(y_train.values.reshape(-1, 1))

    y_train = target_scaler.transform(y_train.values.reshape(-1, 1))
   # y_test = target_scaler.transform(y_test.values.reshape(-1, 1))

    classifier.fit(x_train,y_train)
    y_pred_scale = classifier.predict(x_test)
    
    y_pred_scale = target_scaler.inverse_transform(y_pred_scale.reshape(-1, 1))
    rmse_scale = mean_squared_error(y_test, y_pred_scale, squared=False)
    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'test rmse': rmse, 
                              'test ttr rmse': rmse_scale
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using ")
    print(f'RMSE: {rmse:.>{30}.4f}')
    print(f'RMSE TARGET TRANSFORM: {rmse_scale:.>{13}.4f}')
488/77:
for model in models:
    ## all columns
    fit_and_predict(model, X_train, X_test, y_train ,y_test)
    ## selected columns
    fit_and_predict(model, X_train[selected_feat], X_test[selected_feat], y_train, y_test)
    print('\n')
488/78: y.hist()
488/79: y_train.hist()
488/80:
target_scaler = MinMaxScaler()
target_scaler.fit(y_train.values.reshape(-1, 1))

y_train = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test = target_scaler.transform(y_test.values.reshape(-1, 1))

classifier.fit(x_train,y_train)
y_pred_scale = classifier.predict(x_test)

y_pred_scale = target_scaler.inverse_transform(y_pred_scale.reshape(-1, 1))
rmse_scale = mean_squared_error(y_test, y_pred_scale, squared=False)
488/81:
target_scaler = MinMaxScaler()
target_scaler.fit(y_train.values.reshape(-1, 1))

y_train = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test = target_scaler.transform(y_test.values.reshape(-1, 1))

lr.fit(x_train,y_train)
y_pred_scale = classifier.predict(x_test)

y_pred_scale = target_scaler.inverse_transform(y_pred_scale.reshape(-1, 1))
rmse_scale = mean_squared_error(y_test, y_pred_scale, squared=False)
488/82: y_train
488/83:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
488/84: y_train
488/85:
target_scaler = MinMaxScaler()
target_scaler.fit(y_train.values.reshape(-1, 1))

y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
#y_test = target_scaler.transform(y_test.values.reshape(-1, 1))

lr.fit(X_train,y_train_scale)
y_pred_scale = classifier.predict(X_test)

y_pred_scale = target_scaler.inverse_transform(y_pred_scale)
rmse_scale = mean_squared_error(y_test, y_pred_scale, squared=False)
488/86:
target_scaler = MinMaxScaler()
target_scaler.fit(y_train.values.reshape(-1, 1))

y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
#y_test = target_scaler.transform(y_test.values.reshape(-1, 1))

lr.fit(X_train,y_train_scale)
y_pred_scale = lr.predict(X_test)

y_pred_scale = target_scaler.inverse_transform(y_pred_scale)
rmse_scale = mean_squared_error(y_test, y_pred_scale, squared=False)
488/87: rmse_scale
488/88:
target_scaler = MinMaxScaler()
target_scaler.fit(y_train.values.reshape(-1, 1))

y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
#y_test = target_scaler.transform(y_test.values.reshape(-1, 1))

lr.fit(X_train,y_train_scale)
y_pred_scale = lr.predict(X_test)

y_pred_scale = target_scaler.inverse_transform(y_pred_scale)
rmse_scale = mean_squared_error(y_test, y_pred_scale, squared=False)
488/89: rmse_scale
488/90:
lr.fit(X_train,y_train)
y_pred = lr.predict(X_test)
488/91: mean_squared_error(y_test, y_pred, squared=False)
488/92:
target_scaler = MinMaxScaler()
target_scaler.fit(y_train.values.reshape(-1, 1))

y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
#y_test = target_scaler.transform(y_test.values.reshape(-1, 1))

lr.fit(X_train,y_train_scale)
y_pred_scale = lr.predict(X_test)

y_pred_scale_a = target_scaler.inverse_transform(y_pred_scale)
rmse_scale = mean_squared_error(y_test, y_pred_scale_a, squared=False)
488/93: rmse_scale
488/94: y_pred_scale_a
488/95: y_pred_scale
488/96: y_train_scale
488/97: y_train_scale.hist()
488/98: y_train_scale.to_list().hist()
488/99: list(y_train_scale).hist()
488/100: pd.Series(y_train_scale)
488/101: pd.Series(y_train_scale)
488/102: pd.Series(y_train_scale.reshape(-1, 1))
488/103: pd.Series(y_train_scale)
488/104: pd.Series(y_train_scale,index=labels)
488/105: y_train_scale
488/106: list(y_train_scale)
488/107: y_train_scale.to_list()
488/108: y_train_scale
488/109: list(y_train_scale)
488/110: list(list(y_train_scale))
488/111: list(y_train_scale)
488/112: y_pred_scale
488/113: y_pred_scale[0]
488/114: y_pred_scale[:,0]
488/115: y_pred_scale
488/116: list(y_pred_scale[:,0])
488/117: list(y_pred_scale[:,0]).hist)
488/118: list(y_pred_scale[:,0]).hist()
488/119: (y_pred_scale[:,0]).hist()
488/120: (y_pred_scale[:,0])
488/121: pd.Series(y_pred_scale[:,0])
488/122: pd.Series(y_pred_scale[:,0]).hist()
488/123: rmse_scale
488/124:
lr.fit(X_train,y_train)
y_pred = lr.predict(X_test)
488/125:
lr.fit(X_train,y_train)
y_pred = lr.predict(X_test)
rmse_scale = mean_squared_error(y_test, y_pred, squared=False)
print(rmse_scale)
488/126:
models=[
        LinearRegression(),
        Lasso(alpha=1.0),
        ElasticNet(alpha=1.0, l1_ratio=.5),
        Ridge(alpha=1.0),
   #     SVR(C=1.0, epsilon=0.1, kernel='linear')
       ]
488/127: from sklearn.preprocessing import MinMaxScaler
488/128: #from sklearn.preprocessing import MinMaxScaler
488/129: #metrics = pd.DataFrame(columns=['model' ,'test rmse', 'test ttr rmse'])
488/130: #y_train.hist()
488/131:
#target_scaler = MinMaxScaler()
#target_scaler.fit(y_train.values.reshape(-1, 1))
#
#y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
##y_test = target_scaler.transform(y_test.values.reshape(-1, 1))
#
#lr.fit(X_train,y_train_scale)
#y_pred_scale = lr.predict(X_test)
#
#y_pred_scale_a = target_scaler.inverse_transform(y_pred_scale)
#rmse_scale = mean_squared_error(y_test, y_pred_scale_a, squared=False)
488/132: #pd.Series(y_pred_scale[:,0]).hist()
488/133: metrics = pd.DataFrame(columns=['model' ,'test rmse', 'test ttr rmse'])
488/134:
def fit_and_predict(model, x_train ,x_test, y_train, y_test):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    #####################################################
#    target_scaler = MinMaxScaler()
#    target_scaler.fit(y_train.values.reshape(-1, 1))
#
#    y_train = target_scaler.transform(y_train.values.reshape(-1, 1))
#   # y_test = target_scaler.transform(y_test.values.reshape(-1, 1))
#
#    classifier.fit(x_train,y_train)
#    y_pred_scale = classifier.predict(x_test)
#    
#    y_pred_scale = target_scaler.inverse_transform(y_pred_scale.reshape(-1, 1))
#    rmse_scale = mean_squared_error(y_test, y_pred_scale, squared=False)
#    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'test rmse': rmse, 
                              'test ttr rmse': rmse_scale
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using ")
    print(f'RMSE: {rmse:.>{30}.4f}')
    print(f'RMSE TARGET TRANSFORM: {rmse_scale:.>{13}.4f}')
488/135:
for model in models:
    ## all columns
    fit_and_predict(model, X_train, X_test, y_train ,y_test)
    ## selected columns
    fit_and_predict(model, X_train[selected_feat], X_test[selected_feat], y_train, y_test)
    print('\n')
488/136:
def fit_and_predict(model, x_train ,x_test, y_train, y_test):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    #####################################################
#    target_scaler = MinMaxScaler()
#    target_scaler.fit(y_train.values.reshape(-1, 1))
#
#    y_train = target_scaler.transform(y_train.values.reshape(-1, 1))
#   # y_test = target_scaler.transform(y_test.values.reshape(-1, 1))
#
#    classifier.fit(x_train,y_train)
#    y_pred_scale = classifier.predict(x_test)
#    
#    y_pred_scale = target_scaler.inverse_transform(y_pred_scale.reshape(-1, 1))
#    rmse_scale = mean_squared_error(y_test, y_pred_scale, squared=False)
    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'test rmse': rmse, 
                              'test ttr rmse': rmse_scale
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using ")
    print(f'RMSE: {rmse:.>{30}.4f}')
    print(f'RMSE TARGET TRANSFORM: {rmse_scale:.>{13}.4f}')
488/137:
for model in models:
    ## all columns
    fit_and_predict(model, X_train, X_test, y_train ,y_test)
    ## selected columns
    fit_and_predict(model, X_train[selected_feat], X_test[selected_feat], y_train, y_test)
    print('\n')
488/138: metrics
488/139: metrics.describe()
488/140: metrics.sort_values('test rmse')
488/141:
X_train, X_test, y_train, y_test = train_test_split(
    df_train_original.drop(labels=['target'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
488/142: metrics = pd.DataFrame(columns=['model' ,'test rmse', 'test ttr rmse'])
488/143:
def fit_and_predict(model, x_train ,x_test, y_train, y_test):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    #####################################################
#    target_scaler = MinMaxScaler()
#    target_scaler.fit(y_train.values.reshape(-1, 1))
#
#    y_train = target_scaler.transform(y_train.values.reshape(-1, 1))
#   # y_test = target_scaler.transform(y_test.values.reshape(-1, 1))
#
#    classifier.fit(x_train,y_train)
#    y_pred_scale = classifier.predict(x_test)
#    
#    y_pred_scale = target_scaler.inverse_transform(y_pred_scale.reshape(-1, 1))
#    rmse_scale = mean_squared_error(y_test, y_pred_scale, squared=False)
    global metrics
    metrics = metrics.append({
                              'model': classifier_name,
                              'test rmse': rmse, 
                              'test ttr rmse': rmse_scale
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using ")
    print(f'RMSE: {rmse:.>{30}.4f}')
    print(f'RMSE TARGET TRANSFORM: {rmse_scale:.>{13}.4f}')
488/144:
for model in models:
    ## all columns
    fit_and_predict(model, X_train, X_test, y_train ,y_test)
    ## selected columns
    fit_and_predict(model, X_train[selected_feat], X_test[selected_feat], y_train, y_test)
    print('\n')
488/145: metrics.sort_values('test rmse')
488/146:
def fit_and_predict(model, x_train ,x_test, y_train, y_test, flag=''):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    #####################################################
#    target_scaler = MinMaxScaler()
#    target_scaler.fit(y_train.values.reshape(-1, 1))
#
#    y_train = target_scaler.transform(y_train.values.reshape(-1, 1))
#   # y_test = target_scaler.transform(y_test.values.reshape(-1, 1))
#
#    classifier.fit(x_train,y_train)
#    y_pred_scale = classifier.predict(x_test)
#    
#    y_pred_scale = target_scaler.inverse_transform(y_pred_scale.reshape(-1, 1))
#    rmse_scale = mean_squared_error(y_test, y_pred_scale, squared=False)
    global metrics
    metrics = metrics.append({
                              'model'+flag: classifier_name,
                              'test rmse': rmse, 
                              'test ttr rmse': rmse_scale
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using ")
    print(f'RMSE: {rmse:.>{30}.4f}')
    print(f'RMSE TARGET TRANSFORM: {rmse_scale:.>{13}.4f}')
488/147:
for model in models:
    ## all columns
    fit_and_predict(model, X_train, X_test, y_train ,y_test)
    ## selected columns
    fit_and_predict(model, X_train[selected_feat], X_test[selected_feat], y_train, y_test)
    print('\n')
488/148:
for model in models:
    ## all columns
    fit_and_predict(model, X_train, X_test, y_train ,y_test)
    ## selected columns
    fit_and_predict(model, X_train[selected_feat], X_test[selected_feat], y_train, y_test, 'selected')
    print('\n')
488/149: metrics.sort_values('test rmse')
488/150:
models=[
        LinearRegression(),
        Lasso(alpha=1.0),
        ElasticNet(alpha=1.0, l1_ratio=.5),
        Ridge(alpha=1.0),
   #     SVR(C=1.0, epsilon=0.1, kernel='linear')
       ]
488/151: #from sklearn.preprocessing import MinMaxScaler
488/152: metrics = pd.DataFrame(columns=['model' ,'test rmse', 'test ttr rmse'])
488/153: #y_train.hist()
488/154:
#target_scaler = MinMaxScaler()
#target_scaler.fit(y_train.values.reshape(-1, 1))
#
#y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
##y_test = target_scaler.transform(y_test.values.reshape(-1, 1))
#
#lr.fit(X_train,y_train_scale)
#y_pred_scale = lr.predict(X_test)
#
#y_pred_scale_a = target_scaler.inverse_transform(y_pred_scale)
#rmse_scale = mean_squared_error(y_test, y_pred_scale_a, squared=False)
488/155: #pd.Series(y_pred_scale[:,0]).hist()
488/156:
X_train, X_test, y_train, y_test = train_test_split(
    df_train_original.drop(labels=['target'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
488/157:
def fit_and_predict(model, x_train ,x_test, y_train, y_test, flag=''):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    #####################################################
#    target_scaler = MinMaxScaler()
#    target_scaler.fit(y_train.values.reshape(-1, 1))
#
#    y_train = target_scaler.transform(y_train.values.reshape(-1, 1))
#   # y_test = target_scaler.transform(y_test.values.reshape(-1, 1))
#
#    classifier.fit(x_train,y_train)
#    y_pred_scale = classifier.predict(x_test)
#    
#    y_pred_scale = target_scaler.inverse_transform(y_pred_scale.reshape(-1, 1))
#    rmse_scale = mean_squared_error(y_test, y_pred_scale, squared=False)
    global metrics
    metrics = metrics.append({
                              'model': classifier_name+flag,
                              'test rmse': rmse, 
                              'test ttr rmse': rmse_scale
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using ")
    print(f'RMSE: {rmse:.>{30}.4f}')
    print(f'RMSE TARGET TRANSFORM: {rmse_scale:.>{13}.4f}')
488/158:
for model in models:
    ## all columns
    fit_and_predict(model, X_train, X_test, y_train ,y_test)
    ## selected columns
    fit_and_predict(model, X_train[selected_feat], X_test[selected_feat], y_train, y_test, 'selected')
    print('\n')
488/159: metrics.sort_values('test rmse')
488/160: metrics = pd.DataFrame(columns=['model' ,'test rmse', 'test ttr rmse'])
488/161: #y_train.hist()
488/162:
#target_scaler = MinMaxScaler()
#target_scaler.fit(y_train.values.reshape(-1, 1))
#
#y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
##y_test = target_scaler.transform(y_test.values.reshape(-1, 1))
#
#lr.fit(X_train,y_train_scale)
#y_pred_scale = lr.predict(X_test)
#
#y_pred_scale_a = target_scaler.inverse_transform(y_pred_scale)
#rmse_scale = mean_squared_error(y_test, y_pred_scale_a, squared=False)
488/163: #pd.Series(y_pred_scale[:,0]).hist()
488/164:
X_train, X_test, y_train, y_test = train_test_split(
    df_train_original.drop(labels=['target'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
488/165:
def fit_and_predict(model, x_train ,x_test, y_train, y_test, flag=''):
    classifier = model
    classifier_name = str(classifier.__class__.__name__)


    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    #####################################################
#    target_scaler = MinMaxScaler()
#    target_scaler.fit(y_train.values.reshape(-1, 1))
#
#    y_train = target_scaler.transform(y_train.values.reshape(-1, 1))
#   # y_test = target_scaler.transform(y_test.values.reshape(-1, 1))
#
#    classifier.fit(x_train,y_train)
#    y_pred_scale = classifier.predict(x_test)
#    
#    y_pred_scale = target_scaler.inverse_transform(y_pred_scale.reshape(-1, 1))
#    rmse_scale = mean_squared_error(y_test, y_pred_scale, squared=False)
    global metrics
    metrics = metrics.append({
                              'model': classifier_name+' '+flag,
                              'test rmse': rmse, 
                              'test ttr rmse': rmse_scale
                             },
                               ignore_index=True
                            )

    print(str(classifier.__class__.__name__) +" using ")
    print(f'RMSE: {rmse:.>{30}.4f}')
    print(f'RMSE TARGET TRANSFORM: {rmse_scale:.>{13}.4f}')
488/166:
for model in models:
    ## all columns
    fit_and_predict(model, X_train, X_test, y_train ,y_test)
    ## selected columns
    fit_and_predict(model, X_train[selected_feat], X_test[selected_feat], y_train, y_test, 'selected')
    print('\n')
488/167: metrics.sort_values('test rmse')
489/1:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
489/2:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

df_train_original = pd.read_csv('data/train.csv')

print(df_train.shape)
print(df_val.shape)
489/3: df_train.head(3)
489/4: df_train.describe()
489/5:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
489/6: ### baseline model
489/7: ### baseline model
489/8:
### Baseline model
lr = LinearRegression()
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
489/9:
sel = VarianceThreshold(threshold=0.0)
sel.fit(X_train)  # fit finds the features with zero variance
sel.get_support()
constant = X_train.columns[~sel.get_support()]
len(constant)
489/10:
### only works with numerical
const_f = [feat for feat in X_train.columns if X_train[feat].std() == 0]
len(const_f)
489/11: ### works also with categorical variables
489/12:
constant_features = [feat for feat in X_train.columns if X_train[feat].nunique() == 1]
len(constant_features)
489/13:
constant_features = [feat for feat in X_train.columns if X_train[feat].nunique() == 2]
len(constant_features)
489/14:
constant_features = [feat for feat in X_train.columns if X_train[feat].nunique() == 20]
len(constant_features)
489/15: ## quasi-constant features
489/16:
constant_features = [feat for feat in X_train.columns if X_train[feat].nunique() == 200]
len(constant_features)
489/17:
constant_features = [feat for feat in X_train.columns if X_train[feat].nunique() == 2000]
len(constant_features)
489/18:
constant_features = [feat for feat in X_train.columns if X_train[feat].nunique() >99]
len(constant_features)
489/19:
constant_features = [feat for feat in X_train.columns if X_train[feat].nunique() == 1]
len(constant_features)
489/20:
sel = VarianceThreshold(threshold=0.01)  
sel.fit(X_train)  # fit finds the features with low variance
quasi_constant = X_train.columns[~sel.get_support()]

len(quasi_constant)
489/21:
sel = VarianceThreshold(threshold=0.02)  
sel.fit(X_train)  # fit finds the features with low variance
quasi_constant = X_train.columns[~sel.get_support()]

len(quasi_constant)
489/22:
dic_sum = df_train.sum().to_dict()
k_v_exchanged = {}
for key, val in dic_sum.items():
    if val not in k_v_exchanged:
        k_v_exchanged[val] = [key]
    else:
        k_v_exchanged[val].append(key)
489/23: max([len(v) for v in k_v_exchanged.values()])
489/24:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
489/25:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
489/26:
corr_features = correlation(X_train, 0.75)
print('correlated features: ', len(set(corr_features)) )
489/27: corr_features
489/28:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
489/29:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
489/30: from sklearn.feature_selection import mutual_info_regression
489/31:
# determine the mutual information
mi = mutual_info_regression(X_train, y_train)

# and make a bar  plot
mi = pd.Series(mi)
mi.index = X_train.columns
mi.sort_values(ascending=False).plot.bar(figsize=(20,6))
plt.ylabel('Mutual Information')
489/32: mi_dict = dict(sorted(mi.to_dict().items(), key=lambda item: item[1]))
489/33: unmutal_cols = list(mi_dict.keys())[0]
489/34:
X_train.drop(labels=unmutal_cols, axis=1, inplace=True)
X_test.drop(labels=unmutal_cols, axis=1, inplace=True)

X_train.shape, X_test.shape
489/35:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
489/36:
## ANOVA
from sklearn.feature_selection import f_regression
489/37:
# univariate anova
univariate = f_regression(X_train.fillna(0), y_train)

# plot values
univariate = pd.Series(univariate[1])
univariate.index = X_train.columns
univariate.sort_values(ascending=False).plot.bar(figsize=(20,6))
489/38: univariate_dict = dict(sorted(univariate.to_dict().items(), key=lambda item: item[1]))
489/39: univariate_cols = list(univariate_dict.keys())[0:2]
489/40: univariate_cols
489/41: univariate_cols = list(univariate_dict.keys())[0:7]
489/42: univariate_cols
489/43: univariate_cols = list(univariate_dict.keys())[-1:]
489/44: univariate_cols
489/45: univariate_cols = list(univariate_dict.keys())[:-1]
489/46: univariate_cols
489/47: univariate_dict = dict(sorted(univariate.to_dict().items(), key=lambda item: item[1]))
489/48: univariate_cols = list(univariate_dict.keys())[:-1]
489/49: univariate_cols
489/50:
X_train.drop(labels=univariate_cols, axis=1, inplace=True)
X_test.drop(labels=univariate_cols, axis=1, inplace=True)

X_train.shape, X_test.shape
489/51:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
489/52:
##### 7.1 Exhaustive Feature Selection
Exhaustive Feature Selection finds the best subset of features of all possible feature subsets, according to a determined performance metric for a certain machine learning algorithm.
489/53: import mlxtend
489/54:
from sklearn.metrics import make_scorer
from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS
489/55:
# exhaustive search
efs = EFS(LinearRegression(), min_features=1, max_features=3, scoring='neg_mean_squared_error', print_progress=True,)
efs = efs.fit(np.array(X_train), y_train)
489/56: X_train
489/57:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
489/58:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

df_train_original = pd.read_csv('data/train.csv')

print(df_train.shape)
print(df_val.shape)
489/59: #### The next part is an example of features selection.
489/60: df_train.head(3)
489/61: df_train.describe()
489/62:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
489/63:
### Baseline model
lr = LinearRegression()
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
489/64:
sel = VarianceThreshold(threshold=0.0)
sel.fit(X_train)  # fit finds the features with zero variance
sel.get_support()
constant = X_train.columns[~sel.get_support()]
len(constant)
489/65:
### only works with numerical
const_f = [feat for feat in X_train.columns if X_train[feat].std() == 0]
len(const_f)
489/66: ### works also with categorical variables
489/67:
constant_features = [feat for feat in X_train.columns if X_train[feat].nunique() == 1]
len(constant_features)
489/68:
sel = VarianceThreshold(threshold=0.02)  
sel.fit(X_train)  # fit finds the features with low variance
quasi_constant = X_train.columns[~sel.get_support()]

len(quasi_constant)
489/69:
dic_sum = df_train.sum().to_dict()
k_v_exchanged = {}
for key, val in dic_sum.items():
    if val not in k_v_exchanged:
        k_v_exchanged[val] = [key]
    else:
        k_v_exchanged[val].append(key)
489/70: max([len(v) for v in k_v_exchanged.values()])
489/71:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
489/72:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
489/73:
corr_features = correlation(X_train, 0.75)
print('correlated features: ', len(set(corr_features)) )
489/74: corr_features
489/75:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
489/76:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
### So, with selected features RMSE a little bit increase
489/77: from sklearn.feature_selection import mutual_info_regression
489/78:
# determine the mutual information
mi = mutual_info_regression(X_train, y_train)

# and make a bar  plot
mi = pd.Series(mi)
mi.index = X_train.columns
mi.sort_values(ascending=False).plot.bar(figsize=(20,6))
plt.ylabel('Mutual Information')
489/79: mi_dict = dict(sorted(mi.to_dict().items(), key=lambda item: item[1]))
489/80: unmutal_cols = list(mi_dict.keys())[0]
489/81:
X_train.drop(labels=unmutal_cols, axis=1, inplace=True)
X_test.drop(labels=unmutal_cols, axis=1, inplace=True)

X_train.shape, X_test.shape
489/82:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
### So, without the 'Id' feature RMSE doesn't change.
489/83:
## ANOVA
from sklearn.feature_selection import f_regression
489/84:
# univariate anova
univariate = f_regression(X_train.fillna(0), y_train)

# plot values
univariate = pd.Series(univariate[1])
univariate.index = X_train.columns
univariate.sort_values(ascending=False).plot.bar(figsize=(20,6))
489/85: univariate_dict = dict(sorted(univariate.to_dict().items(), key=lambda item: item[1]))
489/86: univariate_cols = list(univariate_dict.keys())[-1:]
489/87: univariate_cols
489/88:
X_train.drop(labels=univariate_cols, axis=1, inplace=True)
X_test.drop(labels=univariate_cols, axis=1, inplace=True)

X_train.shape, X_test.shape
489/89:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
#### not a big success, but I'll save this part as an example
489/90: import mlxtend
489/91:
from sklearn.metrics import make_scorer
from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS
489/92:
# exhaustive search
efs = EFS(LinearRegression(), min_features=1, max_features=3, scoring='neg_mean_squared_error', print_progress=True,)
efs = efs.fit(np.array(X_train), y_train)
489/93: efs.best_idx_
489/94:
selected_feat = X_train.columns[list(efs.best_idx_)]
selected_feat
489/95:
lr.fit(X_train[selected_feat], y_train)
y_predict = lr.predict(X_test[selected_feat])
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
489/96:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
489/97:
sfs = SFS(LinearRegression(),
          k_features=3, # the lower the features we want, the longer this will take
          forward=False,
          verbose=2,
          scoring='neg_mean_squared_error')

sfs = sfs.fit(np.array(X_train), y_train)
489/98:
sfs.k_feature_idx_
## The same result, but faster :)
489/99: #### 8.Regression models
489/100:
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import Ridge

from sklearn.compose import TransformedTargetRegressor
489/101:
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV

from sklearn.compose import TransformedTargetRegressor
489/102:
def test(models, data, iterations = 100):
    results = {}
    for i in models:
        r2_train = []
        r2_test = []
        for j in range(iterations):
            X_train, X_test, y_train, y_test = train_test_split(data[X], 
                                                                data[Y], 
                                                                test_size= 0.2)
            r2_test.append(metrics.r2_score(y_test,
                                            models[i].fit(X_train, 
                                                         y_train).predict(X_test)))
            r2_train.append(metrics.r2_score(y_train, 
                                             models[i].fit(X_train, 
                                                          y_train).predict(X_train)))
        results[i] = [np.mean(r2_train), np.mean(r2_test)]
    return pd.DataFrame(results)
489/103:
lasso_params = {'alpha':[0.02, 0.024, 0.025, 0.026, 0.03]}
ridge_params = {'alpha':[200, 230, 250,265, 270, 275, 290, 300, 500]}

models2 = {'OLS': linear_model.LinearRegression(),
           'Lasso': GridSearchCV(linear_model.Lasso(), 
                               param_grid=lasso_params).fit(df[X], df[Y]).best_estimator_,
           'Ridge': GridSearchCV(linear_model.Ridge(), 
                               param_grid=ridge_params).fit(df[X], df[Y]).best_estimator_,}
489/104:
lasso_params = {'alpha':[0.02, 0.024, 0.025, 0.026, 0.03]}
ridge_params = {'alpha':[200, 230, 250,265, 270, 275, 290, 300, 500]}

models2 = {'OLS': LinearRegression(),
           'Lasso': GridSearchCV(Lasso(), 
                               param_grid=lasso_params).fit(df[X], df[Y]).best_estimator_,
           'Ridge': GridSearchCV(Ridge(), 
                               param_grid=ridge_params).fit(df[X], df[Y]).best_estimator_,}
489/105:
lasso_params = {'alpha':[0.02, 0.024, 0.025, 0.026, 0.03]}
ridge_params = {'alpha':[200, 230, 250,265, 270, 275, 290, 300, 500]}

models2 = {'OLS': LinearRegression(),
           'Lasso': GridSearchCV(Lasso(), param_grid=lasso_params),
           'Ridge': GridSearchCV(Ridge(), param_grid=ridge_params)
          }
489/106:
for i in models2:
    print(i)
489/107:
for i in models2:
    print(i.values)
489/108: models2
489/109:
for i in models2:
    print(models2[i])
489/110:
for i in models2:
    print(models2[i])
    clf = models2[i]
    clf.fit(X_train, y_train)
489/111:
for i in models2:
    print(models2[i])
    clf = models2[i]
    clf.fit(X_train, y_train)
    y_pred = clf.best_estimator_.predict(X_test)
    rmse = mean_squared_error(y_test, y_predict, squared=False)
    print(f'RMSE: {rmse:.5f}')
489/112:
lasso_params = {'alpha':[0.02, 0.024, 0.025, 0.026, 0.03]}
ridge_params = {'alpha':[200, 230, 250,265, 270, 275, 290, 300, 500]}

models2 = {'OLS': GridSearchCV(LinearRegression(),param_grid={})
           'Lasso': GridSearchCV(Lasso(), param_grid=lasso_params),
           'Ridge': GridSearchCV(Ridge(), param_grid=ridge_params)
          }
489/113: models2
489/114:
lasso_params = {'alpha':[0.02, 0.024, 0.025, 0.026, 0.03]}
ridge_params = {'alpha':[200, 230, 250,265, 270, 275, 290, 300, 500]}

models2 = {'OLS': GridSearchCV(LinearRegression(),param_grid={}),
           'Lasso': GridSearchCV(Lasso(), param_grid=lasso_params),
           'Ridge': GridSearchCV(Ridge(), param_grid=ridge_params)
          }
489/115: models2
489/116:
for i in models2:
    print(models2[i])
    clf = models2[i]
    clf.fit(X_train, y_train)
    y_pred = clf.best_estimator_.predict(X_test)
    rmse = mean_squared_error(y_test, y_predict, squared=False)
    print(f'RMSE: {rmse:.5f}')
489/117:
lasso_params = {'alpha':[0.01, 0.1, 0.5, 1, 5, 10]}

ridge_params = {'alpha': [0.1, 1, 10, 100] , 
                "fit_intercept": [True, False], 
                "solver": ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']
               }

elastic_params = {"max_iter": [1, 5, 10],
                  "alpha": [0.01, 0.1, 1, 10, 100],
                  "l1_ratio": np.arange(0.0, 1.0, 0.1)
                 }

models2 = {'OLS': GridSearchCV(LinearRegression(),param_grid={}),
           'Lasso': GridSearchCV(Lasso(), param_grid=lasso_params),
           'Ridge': GridSearchCV(Ridge(), param_grid=ridge_params),
           'Elastic': GridSearchCV(ElasticNet(), param_grid=elastic_params)
          }
489/118:
lasso_params = {'alpha':[0.01, 0.1, 0.5, 1, 5, 10]}

ridge_params = {'alpha': [0.1, 1, 10, 100] , 
                "fit_intercept": [True, False], 
                "solver": ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']
               }

elastic_params = {"max_iter": [1, 5, 10],
                  "alpha": [0.01, 0.1, 1, 10, 100],
                  "l1_ratio": np.arange(0.0, 1.0, 0.1)
                 }

models2 = {'OLS': GridSearchCV(LinearRegression(),param_grid={}),
           'Lasso': GridSearchCV(Lasso(), param_grid=lasso_params, scoring='neg_mean_squared_error', cv=5),
           'Ridge': GridSearchCV(Ridge(), param_grid=ridge_params, scoring='neg_mean_squared_error', cv=5),
           'Elastic': GridSearchCV(ElasticNet(), param_grid=elastic_params, scoring='neg_mean_squared_error', cv=5)
          }
489/119:
for i in models2:
    clf = models2[i]
    clf.fit(X_train, y_train)
    y_pred = clf.best_estimator_.predict(X_test)
    rmse = mean_squared_error(y_test, y_predict, squared=False)
    print(f'RMSE: {rmse:.5f}')
489/120: metrics = pd.DataFrame(columns=['model' , 'best_params', 'best score', 'test rmse',])
489/121:
def tune(models, X_train, X_test, y_train, y_test):
    global metrics
    for i in models2:
        clf = models2[i]
        clf.fit(X_train, y_train)
        y_pred = clf.best_estimator_.predict(X_test)
        rmse = mean_squared_error(y_test, y_predict, squared=False)
        print(models)
        print(f'RMSE: {rmse:.5f}')  
        metrics = metrics.append({
                                'model': models,
                                'best_params': clf.best_params_,
                                'best score': clf.best_score_,
                                'test rmse': rmse
                                 },
                               ignore_index=True)
489/122: tune(models2, X_train, X_test, y_train, y_test)
489/123:
def tune(models, X_train, X_test, y_train, y_test):
    global metrics
    for i in models2:
        clf = models2[i]
        clf.fit(X_train, y_train)
        y_pred = clf.best_estimator_.predict(X_test)
        rmse = mean_squared_error(y_test, y_predict, squared=False)
        print(i)
        print(f'RMSE: {rmse:.5f}')  
        metrics = metrics.append({
                                'model': models,
                                'best_params': clf.best_params_,
                                'best score': clf.best_score_,
                                'test rmse': rmse
                                 },
                               ignore_index=True)
489/124: tune(models2, X_train, X_test, y_train, y_test)
489/125: metrics
489/126: metrics = pd.DataFrame(columns=['model' , 'best_params', 'best score', 'test rmse',])
489/127: metrics
489/128:
def tune(models, X_train, X_test, y_train, y_test):
    global metrics
    for i in models2:
        clf = models2[i]
        clf.fit(X_train, y_train)
        y_pred = clf.best_estimator_.predict(X_test)
        rmse = mean_squared_error(y_test, y_pred, squared=False)
        print(i)
        print(f'RMSE: {rmse:.5f}')  
        metrics = metrics.append({
                                'model': models,
                                'best_params': clf.best_params_,
                                'best score': clf.best_score_,
                                'test rmse': rmse
                                 },
                               ignore_index=True)
489/129: tune(models2, X_train, X_test, y_train, y_test)
489/130: metrics
489/131: metrics = pd.DataFrame(columns=['model' , 'best_params', 'best score', 'test rmse',])
489/132: metrics
489/133:
def tune(models, X_train, X_test, y_train, y_test):
    global metrics
    for i in models2:
        clf = models2[i]
        clf.fit(X_train, y_train)
        y_pred = clf.best_estimator_.predict(X_test)
        rmse = mean_squared_error(y_test, y_pred, squared=False)
        print(i)
        print(f'RMSE: {rmse:.5f}')  
        metrics = metrics.append({
                                'model': i,
                                'best_params': clf.best_params_,
                                'best score': clf.best_score_,
                                'test rmse': rmse
                                 },
                               ignore_index=True)
489/134: tune(models2, X_train, X_test, y_train, y_test)
489/135: metrics
489/136: metrics.iloc[2,1]
489/137:
lasso_params = {'alpha':[0.01, 0.1, 0.5, 1, 5, 10]}

ridge_params = {'alpha': [1, 5, 8, 10, 12, 15] , 
                "fit_intercept": [True, False], 
                "solver": ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']
               }

elastic_params = {"max_iter": [1, 5, 10],
                  "alpha": [0.01, 0.1, 1, 10, 100],
                  "l1_ratio": np.arange(0.0, 1.0, 0.1)
                 }

models2 = {'OLS': GridSearchCV(LinearRegression(),param_grid={}),
           'Lasso': GridSearchCV(Lasso(), param_grid=lasso_params, scoring='neg_mean_squared_error', cv=5),
           'Ridge': GridSearchCV(Ridge(), param_grid=ridge_params, scoring='neg_mean_squared_error', cv=5),
           'Elastic': GridSearchCV(ElasticNet(), param_grid=elastic_params, scoring='neg_mean_squared_error', cv=5)
          }
489/138: metrics = pd.DataFrame(columns=['model' , 'best_params', 'best score', 'test rmse',])
489/139: metrics
489/140:
def tune(models, X_train, X_test, y_train, y_test):
    global metrics
    for i in models2:
        clf = models2[i]
        clf.fit(X_train, y_train)
        y_pred = clf.best_estimator_.predict(X_test)
        rmse = mean_squared_error(y_test, y_pred, squared=False)
        print(i)
        print(f'RMSE: {rmse:.5f}')  
        metrics = metrics.append({
                                'model': i,
                                'best_params': clf.best_params_,
                                'best score': clf.best_score_,
                                'test rmse': rmse
                                 },
                               ignore_index=True)
489/141: X_train
489/142:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
489/143: X_train
489/144:
def tune(models, X_train, X_test, y_train, y_test):
    global metrics
    for i in models2:
        clf = models2[i]
        clf.fit(X_train, y_train)
        y_pred = clf.best_estimator_.predict(X_test)
        rmse = mean_squared_error(y_test, y_pred, squared=False)
        print(i)
        print(f'RMSE: {rmse:.5f}')  
        metrics = metrics.append({
                                'model': i,
                                'best_params': clf.best_params_,
                                'best score': clf.best_score_,
                                'test rmse': rmse
                                 },
                               ignore_index=True)
489/145: tune(models2, X_train, X_test, y_train, y_test)
493/1:
import pandas as pd
import numpy as np
import catboost
import optuna
493/2:
from catboost import CatBoostClassifier, Pool
from optuna.integration import CatBoostPruningCallback
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
493/3:
import os
import matplotlib.pyplot as plt#visualization
%matplotlib inline
import seaborn as sns#visualization
import plotly.offline as py #visualization
py.init_notebook_mode(connected=True)#visualization
import plotly.graph_objs as go#visualization
import plotly.tools as tls#visualization
import plotly.figure_factory as ff
from plotly.offline import init_notebook_mode, iplot
493/4:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

df_train_original = pd.read_csv('data/train.csv')

print(df_train.shape)
print(df_val.shape)
493/5:
from catboost import CatBoostRegressor, Pool
from optuna.integration import CatBoostPruningCallback
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
489/146: metrics
489/147: metrics
489/148: metrics.sort_values('test rmse')
489/149: metrics.iloc[1,1]
489/150: metrics.loc['Ridge', :]
489/151: metrics
489/152: metrics.iloc[2,1]
489/153: X_train
489/154: X_train
489/155: X_train.drop('id', axis=1, inplace=True)
489/156: X_test.drop('id', axis=1, inplace=True)
489/157: X_test
489/158:
models=[
        LinearRegression(),
        Lasso(alpha=1.0),
        ElasticNet(alpha=1.0, l1_ratio=.5),
        Ridge(alpha=1.0),
       ]
489/159:
lasso_params = {'alpha':[0.01, 0.1, 0.5, 1, 5, 10]}

ridge_params = {'alpha': [1, 5, 10, 15] , 
                "fit_intercept": [True, False], 
                "solver": ['svd', 'sag']
               }

elastic_params = {"max_iter": [1, 5, 10],
                  "alpha": [0.01, 0.1, 1, 10, 100],
                  "l1_ratio": np.arange(0.0, 1.0, 0.1)
                 }

models2 = {'OLS': GridSearchCV(LinearRegression(),param_grid={}),
           'Lasso': GridSearchCV(Lasso(), param_grid=lasso_params, scoring='neg_mean_squared_error', cv=5),
           'Ridge': GridSearchCV(Ridge(), param_grid=ridge_params, scoring='neg_mean_squared_error', cv=5),
           'Elastic': GridSearchCV(ElasticNet(), param_grid=elastic_params, scoring='neg_mean_squared_error', cv=5)
          }
489/160: metrics = pd.DataFrame(columns=['model' , 'best_params', 'best score', 'test rmse',])
489/161: metrics
489/162:
def tune(models, X_train, X_test, y_train, y_test):
    global metrics
    for i in models2:
        clf = models2[i]
        clf.fit(X_train, y_train)
        y_pred = clf.best_estimator_.predict(X_test)
        rmse = mean_squared_error(y_test, y_pred, squared=False)
        print(i)
        print(f'RMSE: {rmse:.5f}')  
        metrics = metrics.append({
                                'model': i,
                                'best_params': clf.best_params_,
                                'best score': clf.best_score_,
                                'test rmse': rmse
                                 },
                               ignore_index=True)
489/163: X_train
489/164: tune(models2, X_train, X_test, y_train, y_test)
489/165: metrics.sort_values('test rmse')
489/166: metrics.iloc[2,1]
489/167:
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV

from sklearn.compose import TransformedTargetRegressor
489/168:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
489/169:
X_train.drop('id', axis=1, inplace=True)
X_test.drop('id', axis=1, inplace=True)
489/170:
models=[
        LinearRegression(),
        Lasso(alpha=1.0),
        ElasticNet(alpha=1.0, l1_ratio=.5),
        Ridge(alpha=1.0),
       ]
489/171:
lasso_params = {'alpha':[0.01, 0.1, 0.5, 1, 5, 10]}

ridge_params = {'alpha': [1, 5, 10, 15] , 
                "fit_intercept": [True, False], 
                "solver": ['svd', 'sag']
               }

elastic_params = {"max_iter": [1, 5, 10],
                  "alpha": [0.01, 0.1, 1, 10, 100],
                  "l1_ratio": np.arange(0.0, 1.0, 0.1)
                 }

models2 = {'OLS': GridSearchCV(LinearRegression(),param_grid={}),
           'Lasso': GridSearchCV(Lasso(), param_grid=lasso_params, scoring='neg_mean_squared_error', cv=5),
           'Ridge': GridSearchCV(Ridge(), param_grid=ridge_params, scoring='neg_mean_squared_error', cv=5),
           'Elastic': GridSearchCV(ElasticNet(), param_grid=elastic_params, scoring='neg_mean_squared_error', cv=5)
          }
489/172: metrics = pd.DataFrame(columns=['model' , 'best_params', 'best score', 'test rmse',])
489/173: metrics
489/174:
def tune(models, X_train, X_test, y_train, y_test, flag_selected=''):
    global metrics
    for i in models2:
        clf = models2[i]
        clf.fit(X_train, y_train)
        y_pred = clf.best_estimator_.predict(X_test)
        rmse = mean_squared_error(y_test, y_pred, squared=False)
        print(i)
        print(f'RMSE: {rmse:.5f}')  
        metrics = metrics.append({
                                'model': i + ' ' + flag_selected,
                                'best_params': clf.best_params_,
                                'best score': clf.best_score_,
                                'test rmse': rmse
                                 },
                               ignore_index=True)
489/175:
### from part below
selected_cols = ['cont1', 'cont2', 'cont7']
489/176:
tune(models2, X_train, X_test, y_train, y_test)
### Also let's check a data set with selected solumns
tune(models2, X_train[selected_cols], X_test[selected_cols], y_train, y_test)
489/177: metrics.sort_values('test rmse')
489/178:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
489/179:
X_train.drop('id', axis=1, inplace=True)
X_test.drop('id', axis=1, inplace=True)
489/180:
models=[
        LinearRegression(),
        Lasso(alpha=1.0),
        ElasticNet(alpha=1.0, l1_ratio=.5),
        Ridge(alpha=1.0),
       ]
489/181:
lasso_params = {'alpha':[0.01, 0.1, 0.5, 1, 5, 10]}

ridge_params = {'alpha': [1, 5, 10, 15] , 
                "fit_intercept": [True, False], 
                "solver": ['svd', 'sag']
               }

elastic_params = {"max_iter": [1, 5, 10],
                  "alpha": [0.01, 0.1, 1, 10, 100],
                  "l1_ratio": np.arange(0.0, 1.0, 0.1)
                 }

models2 = {'OLS': GridSearchCV(LinearRegression(),param_grid={}),
           'Lasso': GridSearchCV(Lasso(), param_grid=lasso_params, scoring='neg_mean_squared_error', cv=5),
           'Ridge': GridSearchCV(Ridge(), param_grid=ridge_params, scoring='neg_mean_squared_error', cv=5),
           'Elastic': GridSearchCV(ElasticNet(), param_grid=elastic_params, scoring='neg_mean_squared_error', cv=5)
          }
489/182: metrics = pd.DataFrame(columns=['model' , 'best_params', 'best score', 'test rmse',])
489/183: metrics
489/184:
def tune(models, X_train, X_test, y_train, y_test, flag_selected=''):
    global metrics
    for i in models2:
        clf = models2[i]
        clf.fit(X_train, y_train)
        y_pred = clf.best_estimator_.predict(X_test)
        rmse = mean_squared_error(y_test, y_pred, squared=False)
        print(i)
        print(f'RMSE: {rmse:.5f}')  
        metrics = metrics.append({
                                'model': i + ' ' + flag_selected,
                                'best_params': clf.best_params_,
                                'best score': clf.best_score_,
                                'test rmse': rmse
                                 },
                               ignore_index=True)
489/185:
### from part below
selected_cols = ['cont1', 'cont2', 'cont7']
489/186:
tune(models2, X_train, X_test, y_train, y_test)
### Also let's check a data set with selected solumns
tune(models2, X_train[selected_cols], X_test[selected_cols], y_train, y_test, 'selected')
493/6:
def objective(trial, X_train, X_test, y_train, y_test):
    global cbc
    X_train_cbc = X_train
    X_test_cbc = X_test
    y_train_cbc = y_train
    y_test_cbc = y_test
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 100, 2500, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 3, step=1),
        'subsample': trial.suggest_float('subsample', 0.5, 0.99),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.95),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 50.0)
        'eval_metric': 'RMSE',
    }
    
    cbc = CatBoostClassifier(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbc.fit(
        X_train_cbc,
        y_train_cbc,
        eval_set=[(X_test_cbc, y_test_cbc],
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbc)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbc.predict(X_test_cbc)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    return rmse
493/7:
def objective(trial, X_train, X_test, y_train, y_test):
    global cbc
    X_train_cbc = X_train
    X_test_cbc = X_test
    y_train_cbc = y_train
    y_test_cbc = y_test
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 100, 2500, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 3, step=1),
        'subsample': trial.suggest_float('subsample', 0.5, 0.99),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.95),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 50.0),
        'eval_metric': 'RMSE',
    }
    
    cbc = CatBoostClassifier(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbc.fit(
        X_train_cbc,
        y_train_cbc,
        eval_set=[(X_test_cbc, y_test_cbc],
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbc)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbc.predict(X_test_cbc)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    return rmse
493/8:
def objective(trial, X_train, X_test, y_train, y_test):
    global cbc
    X_train_cbc = X_train
    X_test_cbc = X_test
    y_train_cbc = y_train
    y_test_cbc = y_test
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 100, 2500, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 3, step=1),
        'subsample': trial.suggest_float('subsample', 0.5, 0.99),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.95),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 50.0),
        'eval_metric': 'RMSE',
    }
    
    cbc = CatBoostClassifier(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbc.fit(
        X_train_cbc,
        y_train_cbc,
        eval_set=[(X_test_cbc, y_test_cbc)],
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbc)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbc.predict(X_test_cbc)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    return rmse
493/9:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
493/10:
def objective(trial):
    global cbc
    X_train_cbc = X_train
    X_test_cbc = X_test
    y_train_cbc = y_train
    y_test_cbc = y_test
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 100, 2500, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 3, step=1),
        'subsample': trial.suggest_float('subsample', 0.5, 0.99),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.95),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 50.0),
        'eval_metric': 'RMSE',
    }
    
    cbc = CatBoostClassifier(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbc.fit(
        X_train_cbc,
        y_train_cbc,
        eval_set=[(X_test_cbc, y_test_cbc)],
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbc)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbc.predict(X_test_cbc)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    return rmse
493/11:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=150, timeout=600, callbacks=[callback])
493/12:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target', 'id'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
493/13:
from catboost import CatBoostRegressor, Pool
from optuna.integration import CatBoostPruningCallback
from sklearn.metrics import mean_squared_error

from sklearn.model_selection import train_test_split
493/14:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target', 'id'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
493/15:
def objective(trial):
    global cbc
    X_train_cbc = X_train
    X_test_cbc = X_test
    y_train_cbc = y_train
    y_test_cbc = y_test
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 100, 2500, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 3, step=1),
        'subsample': trial.suggest_float('subsample', 0.5, 0.99),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.95),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 50.0),
        'eval_metric': 'RMSE',
    }
    
    cbc = CatBoostClassifier(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbc.fit(
        X_train_cbc,
        y_train_cbc,
        eval_set=[(X_test_cbc, y_test_cbc)],
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbc)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbc.predict(X_test_cbc)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    return rmse
493/16:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
493/17:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=150, timeout=600, callbacks=[callback])
493/18:
def objective(trial):
    global cbc
    X_train_cbc = X_train
    X_test_cbc = X_test
    y_train_cbc = y_train
    y_test_cbc = y_test
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
   #     'n_estimators': trial.suggest_int('n_estimators', 100, 2500, step=1),
   #     'max_depth': trial.suggest_int('max_depth', 1, 3, step=1),
   #     'subsample': trial.suggest_float('subsample', 0.5, 0.99),
   #     'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.95),
   #     'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 50.0),
        'eval_metric': 'RMSE',
    }
    
    cbc = CatBoostClassifier(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbc.fit(
        X_train_cbc,
        y_train_cbc,
        eval_set=[(X_test_cbc, y_test_cbc)],
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbc)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbc.predict(X_test_cbc)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    return rmse
493/19:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
493/20:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=150, timeout=600, callbacks=[callback])
493/21:
def objective(trial):
    global cbr
    X_train_cbr = X_train
    X_test_cbr = X_test
    y_train_cbr = y_train
    y_test_cbr = y_test
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
   #     'n_estimators': trial.suggest_int('n_estimators', 100, 2500, step=1),
   #     'max_depth': trial.suggest_int('max_depth', 1, 3, step=1),
   #     'subsample': trial.suggest_float('subsample', 0.5, 0.99),
   #     'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.95),
   #     'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 50.0),
        'eval_metric': 'RMSE',
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        X_train_cbr,
        y_train_cbr,
        eval_set=[(X_test_cbr, y_test_cbr)],
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test_cbr)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    return rmse
493/22:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
493/23:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=150, timeout=600, callbacks=[callback])
493/24:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
493/25:
def objective(trial):
    global cbr
    X_train_cbr = X_train
    X_test_cbr = X_test
    y_train_cbr = y_train
    y_test_cbr = y_test
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 100, 2500, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 3, step=1),
        'subsample': trial.suggest_float('subsample', 0.5, 0.99),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.95),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 50.0),
        'eval_metric': 'RMSE',
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        X_train_cbr,
        y_train_cbr,
        eval_set=[(X_test_cbr, y_test_cbr)],
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test_cbr)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    return rmse
493/26:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
493/27:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
489/187: metrics.sort_values('test rmse')
489/188: metrics
489/189: metrics.iloc[2,1]
493/28:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
493/29:
rmse = mean_squared_error(y_test, best_model.predict(X_test), squared=False)
print('RMSE: ', rmse)
493/30:
df_features_importance = pd.DataFrame(
    {'feature_names': best_model.feature_names_,
     'feature_importances': best_model.feature_importances_
    })
493/31: df_features_importance
493/32: df_features_importance.sort_values('feature_importances', ascending=False)
493/33: df_val
493/34: X_train
493/35: df_val.drop('id', axis=1, inplace=True)
493/36: df_val
493/37: df_val
493/38:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

df_train_original = pd.read_csv('data/train.csv')

print(df_train.shape)
print(df_val.shape)
493/39: df_val
493/40: df_t = df_val[['id']]
493/41: df_t
493/42: df_val
493/43: df_val
493/44: df_val.drop('id', axis=1, inplace=True)
493/45: best_model.predict(df_val)
493/46: df_t['target'] = best_model.predict(df_val)
493/47: df_t
493/48: df_t.describe()
493/49: df_t
493/50: df_t.to_csv("catboost_reg_1.csv", index=False)
493/51:
def objective(trial):
    global cbr
    X_train_cbr = X_train
    X_test_cbr = X_test
    y_train_cbr = y_train
    y_test_cbr = y_test
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 100, 2500, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.5, 0.99),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.95),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 50.0),
        'eval_metric': 'RMSE',
        'rsm': 0.5
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        X_train_cbr,
        y_train_cbr,
        eval_set=[(X_test_cbr, y_test_cbr)],
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test_cbr)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    return rmse
493/52:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
493/53:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
493/54:
def objective(trial):
    global cbr
    X_train_cbr = X_train
    X_test_cbr = X_test
    y_train_cbr = y_train
    y_test_cbr = y_test
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 100, 2500, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.5, 0.99),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.95),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 50.0),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        X_train_cbr,
        y_train_cbr,
        eval_set=[(X_test_cbr, y_test_cbr)],
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test_cbr)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    return rmse
493/55:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
493/56:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
493/57:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
493/58:
rmse = mean_squared_error(y_test, best_model.predict(X_test), squared=False)
print('RMSE: ', rmse)
493/59:
df_features_importance = pd.DataFrame(
    {'feature_names': best_model.feature_names_,
     'feature_importances': best_model.feature_importances_
    })
493/60: df_features_importance.sort_values('feature_importances', ascending=False)
489/190: metrics.iloc[2,1]
489/191:
parameters = {'alpha':[5, 15],
              'fit_intercept': [True],
              'solver': ['sag']}
model = Ridge()

# define the grid search
Ridge_reg= GridSearchCV(model, parameters, scoring='neg_mean_squared_error',cv=5)
489/192:
parameters = {'alpha':[5, 15],
              'fit_intercept': [True],
              'solver': ['sag']}
model = Ridge()

# define the grid search
ridge_reg= GridSearchCV(model, parameters, scoring='neg_mean_squared_error',cv=5)
489/193: ridge_reg.fit(X_train, y_train)
489/194: ridge_reg.best_estimator_
489/195:
parameters = {'alpha': np.arange(5, 15, 1),
              'fit_intercept': [True],
              'solver': ['sag']}
model = Ridge()

# define the grid search
ridge_reg= GridSearchCV(model, parameters, scoring='neg_mean_squared_error',cv=5)
489/196: ridge_reg.fit(X_train, y_train)
489/197: ridge_reg.best_estimator_
493/61: df_features_importance.sort_values('feature_importances', ascending=False)
493/62:
def objective(trial):
    global cbr
    X_train_cbr = X_train
    X_test_cbr = X_test
    y_train_cbr = y_train
    y_test_cbr = y_test
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.95),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        X_train_cbr,
        y_train_cbr,
        eval_set=[(X_test_cbr, y_test_cbr)],
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test_cbr)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    return rmse
493/63:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
493/64:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
489/198: ridge_reg.best_estimator_.predict(X_test)
489/199: mean_squared_error(y_test, ridge_reg.best_estimator_.predict(X_test), squared=False)
489/200:
parameters = {'alpha': np.arange(1, 20, 1),
              'fit_intercept': [True],
              'solver': ['sag']}
model = Ridge()

# define the grid search
ridge_reg= GridSearchCV(model, parameters, scoring='neg_mean_squared_error',cv=5)
489/201: ridge_reg.fit(X_train, y_train)
489/202: ridge_reg.best_estimator_
489/203: mean_squared_error(y_test, ridge_reg.best_estimator_.predict(X_test), squared=False)
489/204: df_val
489/205: df_val
489/206:
df_t = df_val[['id']]
df_val.drop('id', axis=1, inplace=True)
df_t['target'] = ridge_reg.best_estimator_.predict(df_val)
df_t.to_csv("ridge_reg_1.csv", index=False)
489/207: df_t.describe()
489/208: from sklearn.ensemble import RandomForestRegressor
489/209:
 tuned_parameters = {'n_estimators': np.arange(500, 1500, 100), 
                     'max_depth': [None, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 
                     'min_samples_split': [1, 2, 3],
                     'min_samples_leaf': [1, 5, 10, 15, 20, 50]
                    }
489/210: rfr = RandomForestRegressor()
489/211:
clf = GridSearchCV(rfr, tuned_parameters, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)
clf.fit(X_train, y_train)
493/65:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
493/66:
rmse = mean_squared_error(y_test, best_model.predict(X_test), squared=False)
print('RMSE: ', rmse)
493/67:
df_features_importance = pd.DataFrame(
    {'feature_names': best_model.feature_names_,
     'feature_importances': best_model.feature_importances_
    })
493/68: df_features_importance.sort_values('feature_importances', ascending=False)
494/1:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
494/2:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

df_train_original = pd.read_csv('data/train.csv')

print(df_train.shape)
print(df_val.shape)
494/3: #### The next part is an example of features selection.
494/4: df_train.head(3)
494/5: df_train.describe()
494/6:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
494/7:
### Baseline model
lr = LinearRegression()
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
494/8:
sel = VarianceThreshold(threshold=0.0)
sel.fit(X_train)  # fit finds the features with zero variance
sel.get_support()
constant = X_train.columns[~sel.get_support()]
len(constant)
494/9:
### only works with numerical
const_f = [feat for feat in X_train.columns if X_train[feat].std() == 0]
len(const_f)
494/10: ### works also with categorical variables
494/11:
constant_features = [feat for feat in X_train.columns if X_train[feat].nunique() == 1]
len(constant_features)
494/12:
sel = VarianceThreshold(threshold=0.02)  
sel.fit(X_train)  # fit finds the features with low variance
quasi_constant = X_train.columns[~sel.get_support()]

len(quasi_constant)
494/13:
dic_sum = df_train.sum().to_dict()
k_v_exchanged = {}
for key, val in dic_sum.items():
    if val not in k_v_exchanged:
        k_v_exchanged[val] = [key]
    else:
        k_v_exchanged[val].append(key)
494/14: max([len(v) for v in k_v_exchanged.values()])
494/15:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
494/16:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
494/17:
corr_features = correlation(X_train, 0.75)
print('correlated features: ', len(set(corr_features)) )
494/18: corr_features
494/19:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
494/20:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
### So, with selected features RMSE a little bit increase
494/21: from sklearn.feature_selection import mutual_info_regression
494/22:
# determine the mutual information
mi = mutual_info_regression(X_train, y_train)

# and make a bar  plot
mi = pd.Series(mi)
mi.index = X_train.columns
mi.sort_values(ascending=False).plot.bar(figsize=(20,6))
plt.ylabel('Mutual Information')
494/23: mi_dict = dict(sorted(mi.to_dict().items(), key=lambda item: item[1]))
494/24: unmutal_cols = list(mi_dict.keys())[0]
494/25:
X_train.drop(labels=unmutal_cols, axis=1, inplace=True)
X_test.drop(labels=unmutal_cols, axis=1, inplace=True)

X_train.shape, X_test.shape
494/26:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
### So, without the 'Id' feature RMSE doesn't change.
494/27:
## ANOVA
from sklearn.feature_selection import f_regression
494/28:
# univariate anova
univariate = f_regression(X_train.fillna(0), y_train)

# plot values
univariate = pd.Series(univariate[1])
univariate.index = X_train.columns
univariate.sort_values(ascending=False).plot.bar(figsize=(20,6))
494/29: univariate_dict = dict(sorted(univariate.to_dict().items(), key=lambda item: item[1]))
494/30: univariate_cols = list(univariate_dict.keys())[-1:]
494/31: univariate_cols
494/32:
X_train.drop(labels=univariate_cols, axis=1, inplace=True)
X_test.drop(labels=univariate_cols, axis=1, inplace=True)

X_train.shape, X_test.shape
494/33:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
#### not a big success, but I'll save this part as an example
494/34:
import mlxtend
from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS
494/35:
# exhaustive search
efs = EFS(LinearRegression(), min_features=1, max_features=3, scoring='neg_mean_squared_error', print_progress=True,)
efs = efs.fit(np.array(X_train), y_train)
494/36: efs.best_idx_
494/37:
selected_feat = X_train.columns[list(efs.best_idx_)]
selected_feat
494/38:
lr.fit(X_train[selected_feat], y_train)
y_predict = lr.predict(X_test[selected_feat])
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
494/39:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
494/40:
sfs = SFS(LinearRegression(),
          k_features=3, # the lower the features we want, the longer this will take
          forward=False,
          verbose=2,
          scoring='neg_mean_squared_error')

sfs = sfs.fit(np.array(X_train), y_train)
494/41:
sfs.k_feature_idx_
## The same result as before, but faster :)
494/42:
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV

from sklearn.compose import TransformedTargetRegressor
494/43:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
494/44:
X_train.drop('id', axis=1, inplace=True)
X_test.drop('id', axis=1, inplace=True)
494/45:
models=[
        LinearRegression(),
        Lasso(alpha=1.0),
        ElasticNet(alpha=1.0, l1_ratio=.5),
        Ridge(alpha=1.0),
       ]
494/46:
lasso_params = {'alpha':[0.01, 0.1, 0.5, 1, 5, 10]}

ridge_params = {'alpha': [1, 5, 10, 15] , 
                "fit_intercept": [True, False], 
                "solver": ['svd', 'sag']
               }

elastic_params = {"max_iter": [1, 5, 10],
                  "alpha": [0.01, 0.1, 1, 10, 100],
                  "l1_ratio": np.arange(0.0, 1.0, 0.1)
                 }

models2 = {'OLS': GridSearchCV(LinearRegression(),param_grid={}),
           'Lasso': GridSearchCV(Lasso(), param_grid=lasso_params, scoring='neg_mean_squared_error', cv=5),
           'Ridge': GridSearchCV(Ridge(), param_grid=ridge_params, scoring='neg_mean_squared_error', cv=5),
           'Elastic': GridSearchCV(ElasticNet(), param_grid=elastic_params, scoring='neg_mean_squared_error', cv=5)
          }
494/47: metrics = pd.DataFrame(columns=['model' , 'best_params', 'best score', 'test rmse',])
494/48: metrics
494/49:
def tune(models, X_train, X_test, y_train, y_test, flag_selected=''):
    global metrics
    for i in models2:
        clf = models2[i]
        clf.fit(X_train, y_train)
        y_pred = clf.best_estimator_.predict(X_test)
        rmse = mean_squared_error(y_test, y_pred, squared=False)
        print(i)
        print(f'RMSE: {rmse:.5f}')  
        metrics = metrics.append({
                                'model': i + ' ' + flag_selected,
                                'best_params': clf.best_params_,
                                'best score': clf.best_score_,
                                'test rmse': rmse
                                 },
                               ignore_index=True)
494/50:
### from part below
selected_cols = ['cont1', 'cont2', 'cont7']
494/51:
tune(models2, X_train, X_test, y_train, y_test)
### Also let's check a data set with selected solumns
tune(models2, X_train[selected_cols], X_test[selected_cols], y_train, y_test, 'selected')
495/1:
import pandas as pd
import numpy as np
import catboost
import optuna
495/2:
from catboost import CatBoostRegressor, Pool
from optuna.integration import CatBoostPruningCallback
from sklearn.metrics import mean_squared_error

from sklearn.model_selection import train_test_split
495/3:
import os
import matplotlib.pyplot as plt#visualization
%matplotlib inline
import seaborn as sns#visualization
import plotly.offline as py #visualization
py.init_notebook_mode(connected=True)#visualization
import plotly.graph_objs as go#visualization
import plotly.tools as tls#visualization
import plotly.figure_factory as ff
from plotly.offline import init_notebook_mode, iplot
495/4:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

df_train_original = pd.read_csv('data/train.csv')

print(df_train.shape)
print(df_val.shape)
495/5:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target', 'id'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
495/6:

    optuna_params = {"subsample": trial.suggest_float("subsample", 0.5, 0.99),
                     'od_wait': trial.suggest_int('od_wait', 10, 50, step=1),
                     "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.5, 0.99),
                     "random_strength": trial.suggest_int("random_strength", 1, 10, step=1),
                     "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1.0, 50.0),
                     "max_depth": trial.suggest_int("max_depth", 4, 10, step=1),
                     "n_estimators": trial.suggest_int("n_estimators", 100, 2500, step=1),
                     'learning_rate': trial.suggest_loguniform("learning_rate", 0.005, 0.1)}
495/7:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
495/8:
def objective(trial):
    global cbr
    X_train_cbr = X_train
    X_test_cbr = X_test
    y_train_cbr = y_train
    y_test_cbr = y_test
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.95),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        X_train_cbr,
        y_train_cbr,
        eval_set=[(X_test_cbr, y_test_cbr)],
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test_cbr)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    return rmse
495/9:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
495/10:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
495/11:
def objective(trial):
    global cbr
    X_train_cbr = X_train
    X_test_cbr = X_test
    y_train_cbr = y_train
    y_test_cbr = y_test
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.95),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'eval_metric': 'RMSE',
        'rsm': [0.5]
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        X_train_cbr,
        y_train_cbr,
        eval_set=[(X_test_cbr, y_test_cbr)],
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test_cbr)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    return rmse
495/12:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
495/13:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
494/52: metrics.sort_values('test rmse')
494/53: metrics.iloc[2,1]
494/54: ### I'll try to tune hyperparameters for ridge regression
494/55:
parameters = {'alpha': np.arange(1, 20, 1),
              'fit_intercept': [True],
              'solver': ['sag']}
model = Ridge()

# define the grid search
ridge_reg= GridSearchCV(model, parameters, scoring='neg_mean_squared_error',cv=5)
494/56: ridge_reg.fit(X_train, y_train)
495/14:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
495/15:
def objective(trial):
    global cbr
    X_train_cbr = X_train
    X_test_cbr = X_test
    y_train_cbr = y_train
    y_test_cbr = y_test
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.95),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        X_train_cbr,
        y_train_cbr,
        eval_set=[(X_test_cbr, y_test_cbr)],
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test_cbr)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    return rmse
495/16:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
495/17:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
494/57: ridge_reg.best_estimator_
494/58: mean_squared_error(y_test, ridge_reg.best_estimator_.predict(X_test), squared=False)
494/59:
df_t = df_val[['id']]
df_val.drop('id', axis=1, inplace=True)
df_t['target'] = ridge_reg.best_estimator_.predict(df_val)
df_t.to_csv("ridge_reg_1.csv", index=False)
494/60: from sklearn.ensemble import RandomForestRegressor
494/61:
 tuned_parameters = {'n_estimators': np.arange(500, 1500, 250), 
                     'max_depth': [None, 3, 7], 
                     'min_samples_split': [1, 2, 3],
                     'min_samples_leaf': [1, 5, 10]
                    }
494/62:
 tuned_parameters = {'n_estimators': np.arange(500, 1000, 250), 
                     'max_depth': [None, 3, 7], 
                     'min_samples_split': [1, 2, 3],
                     'min_samples_leaf': [1, 5, 10]
                    }
494/63: rfr = RandomForestRegressor()
494/64:
clf = GridSearchCV(rfr, tuned_parameters, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)
clf.fit(X_train, y_train)
495/18:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
495/19:
rmse = mean_squared_error(y_test, best_model.predict(X_test), squared=False)
print('RMSE: ', rmse)
495/20:
df_features_importance = pd.DataFrame(
    {'feature_names': best_model.feature_names_,
     'feature_importances': best_model.feature_importances_
    })
495/21: df_features_importance.sort_values('feature_importances', ascending=False)
495/22:
df_t = df_val[['id']]
df_val.drop('id', axis=1, inplace=True)
df_t['target'] = best_model.predict(df_val)
df_t.to_csv("catboost_reg_2.csv", index=False)
495/23:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
495/24:
### Let's check rsm.
### RSM - Random subspace method. The percentage of features to use at each split selection, when features are selected over again at random
495/25: best_params = best_model.get_params()
495/26: best_params
495/27: best_params.dtype
495/28: best_params['rsm'] = 0.5
495/29: best_params.fit(X_train, y_train)
495/30: cb_set_params = CatBoostRegressor(best_params)
495/31: cb_set_params.fit(X_train, y_train)
495/32: cb_set_params
495/33: best_params
495/34:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
495/35:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
498/1:
import pandas as pd
import numpy as np
import catboost
import optuna
498/2:
from catboost import CatBoostRegressor, Pool
from optuna.integration import CatBoostPruningCallback
from sklearn.metrics import mean_squared_error

from sklearn.model_selection import train_test_split
498/3:
import os
import matplotlib.pyplot as plt#visualization
%matplotlib inline
import seaborn as sns#visualization
import plotly.offline as py #visualization
py.init_notebook_mode(connected=True)#visualization
import plotly.graph_objs as go#visualization
import plotly.tools as tls#visualization
import plotly.figure_factory as ff
from plotly.offline import init_notebook_mode, iplot
498/4:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

df_train_original = pd.read_csv('data/train.csv')

print(df_train.shape)
print(df_val.shape)
498/5:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target', 'id'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
498/6:
def objective(trial):
    global cbr
    X_train_cbr = X_train
    X_test_cbr = X_test
    y_train_cbr = y_train
    y_test_cbr = y_test
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.95),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'rsm': trial.suggest_float('rsm', 0.5)
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        X_train_cbr,
        y_train_cbr,
        eval_set=[(X_test_cbr, y_test_cbr)],
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test_cbr)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    return rmse
498/7:
def objective(trial):
    global cbr
    X_train_cbr = X_train
    X_test_cbr = X_test
    y_train_cbr = y_train
    y_test_cbr = y_test
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.95),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'rsm': trial.suggest_float('rsm', 0.5),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        X_train_cbr,
        y_train_cbr,
        eval_set=[(X_test_cbr, y_test_cbr)],
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test_cbr)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    return rmse
498/8:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
498/9:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
499/1:
import pandas as pd
import numpy as np
import catboost
import optuna
499/2:
from catboost import CatBoostRegressor, Pool
from optuna.integration import CatBoostPruningCallback
from sklearn.metrics import mean_squared_error

from sklearn.model_selection import train_test_split
499/3:
import os
import matplotlib.pyplot as plt#visualization
%matplotlib inline
import seaborn as sns#visualization
import plotly.offline as py #visualization
py.init_notebook_mode(connected=True)#visualization
import plotly.graph_objs as go#visualization
import plotly.tools as tls#visualization
import plotly.figure_factory as ff
from plotly.offline import init_notebook_mode, iplot
499/4:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

df_train_original = pd.read_csv('data/train.csv')

print(df_train.shape)
print(df_val.shape)
499/5:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target', 'id'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
499/6:
def objective(trial):
    global cbr
    X_train_cbr = X_train
    X_test_cbr = X_test
    y_train_cbr = y_train
    y_test_cbr = y_test
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.95),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'rsm': trial.suggest_float('rsm', 0.5, 0.5),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        X_train_cbr,
        y_train_cbr,
        eval_set=[(X_test_cbr, y_test_cbr)],
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test_cbr)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    return rmse
499/7:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
499/8:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
499/9:
def objective(trial):
    global cbr
    X_train_cbr = X_train
    X_test_cbr = X_test
    y_train_cbr = y_train
    y_test_cbr = y_test
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.95),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        X_train_cbr,
        y_train_cbr,
        eval_set=[(X_test_cbr, y_test_cbr)],
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test_cbr)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    return rmse
499/10:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
499/11:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
497/1:
 tuned_parameters = {'n_estimators': np.arange(1000), 
                     'max_depth': [3, 7], 
                     'min_samples_split': [2, 3],
                     'min_samples_leaf': [1, 5]
                    }
497/2:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
497/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

df_train_original = pd.read_csv('data/train.csv')

print(df_train.shape)
print(df_val.shape)
497/4: #### The next part is an example of features selection.
497/5: df_train.head(3)
497/6: df_train.describe()
497/7:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
497/8:
### Baseline model
lr = LinearRegression()
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
497/9:
sel = VarianceThreshold(threshold=0.0)
sel.fit(X_train)  # fit finds the features with zero variance
sel.get_support()
constant = X_train.columns[~sel.get_support()]
len(constant)
497/10:
### only works with numerical
const_f = [feat for feat in X_train.columns if X_train[feat].std() == 0]
len(const_f)
497/11: ### works also with categorical variables
497/12:
constant_features = [feat for feat in X_train.columns if X_train[feat].nunique() == 1]
len(constant_features)
497/13:
sel = VarianceThreshold(threshold=0.02)  
sel.fit(X_train)  # fit finds the features with low variance
quasi_constant = X_train.columns[~sel.get_support()]

len(quasi_constant)
497/14:
dic_sum = df_train.sum().to_dict()
k_v_exchanged = {}
for key, val in dic_sum.items():
    if val not in k_v_exchanged:
        k_v_exchanged[val] = [key]
    else:
        k_v_exchanged[val].append(key)
497/15: max([len(v) for v in k_v_exchanged.values()])
497/16:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
497/17:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
497/18:
corr_features = correlation(X_train, 0.75)
print('correlated features: ', len(set(corr_features)) )
497/19: corr_features
497/20:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
497/21:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
### So, with selected features RMSE a little bit increase
497/22: from sklearn.feature_selection import mutual_info_regression
497/23:
# determine the mutual information
mi = mutual_info_regression(X_train, y_train)

# and make a bar  plot
mi = pd.Series(mi)
mi.index = X_train.columns
mi.sort_values(ascending=False).plot.bar(figsize=(20,6))
plt.ylabel('Mutual Information')
497/24: mi_dict = dict(sorted(mi.to_dict().items(), key=lambda item: item[1]))
497/25: unmutal_cols = list(mi_dict.keys())[0]
497/26:
X_train.drop(labels=unmutal_cols, axis=1, inplace=True)
X_test.drop(labels=unmutal_cols, axis=1, inplace=True)

X_train.shape, X_test.shape
497/27:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
### So, without the 'Id' feature RMSE doesn't change.
497/28:
## ANOVA
from sklearn.feature_selection import f_regression
497/29:
# univariate anova
univariate = f_regression(X_train.fillna(0), y_train)

# plot values
univariate = pd.Series(univariate[1])
univariate.index = X_train.columns
univariate.sort_values(ascending=False).plot.bar(figsize=(20,6))
497/30: univariate_dict = dict(sorted(univariate.to_dict().items(), key=lambda item: item[1]))
497/31: univariate_cols = list(univariate_dict.keys())[-1:]
497/32: univariate_cols
497/33:
X_train.drop(labels=univariate_cols, axis=1, inplace=True)
X_test.drop(labels=univariate_cols, axis=1, inplace=True)

X_train.shape, X_test.shape
497/34:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
#### not a big success, but I'll save this part as an example
497/35:
import mlxtend
from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS
497/36:
# exhaustive search
efs = EFS(LinearRegression(), min_features=1, max_features=3, scoring='neg_mean_squared_error', print_progress=True,)
efs = efs.fit(np.array(X_train), y_train)
497/37: efs.best_idx_
497/38:
selected_feat = X_train.columns[list(efs.best_idx_)]
selected_feat
497/39:
lr.fit(X_train[selected_feat], y_train)
y_predict = lr.predict(X_test[selected_feat])
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
497/40:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
497/41:
sfs = SFS(LinearRegression(),
          k_features=3, # the lower the features we want, the longer this will take
          forward=False,
          verbose=2,
          scoring='neg_mean_squared_error')

sfs = sfs.fit(np.array(X_train), y_train)
497/42:
sfs.k_feature_idx_
## The same result as before, but faster :)
497/43:
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV

from sklearn.compose import TransformedTargetRegressor
497/44:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
497/45:
X_train.drop('id', axis=1, inplace=True)
X_test.drop('id', axis=1, inplace=True)
497/46:
models=[
        LinearRegression(),
        Lasso(alpha=1.0),
        ElasticNet(alpha=1.0, l1_ratio=.5),
        Ridge(alpha=1.0),
       ]
497/47:
lasso_params = {'alpha':[0.01, 0.1, 0.5, 1, 5, 10]}

ridge_params = {'alpha': [1, 5, 10, 15] , 
                "fit_intercept": [True, False], 
                "solver": ['svd', 'sag']
               }

elastic_params = {"max_iter": [1, 5, 10],
                  "alpha": [0.01, 0.1, 1, 10, 100],
                  "l1_ratio": np.arange(0.0, 1.0, 0.1)
                 }

models2 = {'OLS': GridSearchCV(LinearRegression(),param_grid={}),
           'Lasso': GridSearchCV(Lasso(), param_grid=lasso_params, scoring='neg_mean_squared_error', cv=5),
           'Ridge': GridSearchCV(Ridge(), param_grid=ridge_params, scoring='neg_mean_squared_error', cv=5),
           'Elastic': GridSearchCV(ElasticNet(), param_grid=elastic_params, scoring='neg_mean_squared_error', cv=5)
          }
497/48: metrics = pd.DataFrame(columns=['model' , 'best_params', 'best score', 'test rmse',])
497/49: metrics
497/50:
def tune(models, X_train, X_test, y_train, y_test, flag_selected=''):
    global metrics
    for i in models2:
        clf = models2[i]
        clf.fit(X_train, y_train)
        y_pred = clf.best_estimator_.predict(X_test)
        rmse = mean_squared_error(y_test, y_pred, squared=False)
        print(i)
        print(f'RMSE: {rmse:.5f}')  
        metrics = metrics.append({
                                'model': i + ' ' + flag_selected,
                                'best_params': clf.best_params_,
                                'best score': clf.best_score_,
                                'test rmse': rmse
                                 },
                               ignore_index=True)
497/51:
### from part below
selected_cols = ['cont1', 'cont2', 'cont7']
497/52:
tune(models2, X_train, X_test, y_train, y_test)
### Also let's check a data set with selected solumns
tune(models2, X_train[selected_cols], X_test[selected_cols], y_train, y_test, 'selected')
497/53: metrics.sort_values('test rmse')
497/54: metrics.iloc[2,1]
497/55: ### I'll try to tune hyperparameters for ridge regression
497/56:
parameters = {'alpha': np.arange(1, 20, 1),
              'fit_intercept': [True],
              'solver': ['sag']}
model = Ridge()

# define the grid search
ridge_reg= GridSearchCV(model, parameters, scoring='neg_mean_squared_error',cv=5)
497/57:
parameters = {'alpha': np.arange(3, 7, 1),
              'fit_intercept': [True],
              'solver': ['sag']}
model = Ridge()

# define the grid search
ridge_reg= GridSearchCV(model, parameters, scoring='neg_mean_squared_error',cv=5)
497/58: ridge_reg.fit(X_train, y_train)
497/59: ridge_reg.best_estimator_
497/60: mean_squared_error(y_test, ridge_reg.best_estimator_.predict(X_test), squared=False)
497/61:
df_t = df_val[['id']]
df_val.drop('id', axis=1, inplace=True)
df_t['target'] = ridge_reg.best_estimator_.predict(df_val)
df_t.to_csv("ridge_reg_1.csv", index=False)
497/62:
 tuned_parameters = {'n_estimators': [1000, 1500], 
                     'max_depth': [3, 7], 
                     'min_samples_split': [2, 3],
                     'min_samples_leaf': [1, 5]
                    }
497/63: rfr = RandomForestRegressor()
497/64: from sklearn.ensemble import RandomForestRegressor
497/65: rfr = RandomForestRegressor()
497/66:
clf = GridSearchCV(rfr, tuned_parameters, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)
clf.fit(X_train, y_train)
499/12:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
499/13:
rmse = mean_squared_error(y_test, best_model.predict(X_test), squared=False)
print('RMSE: ', rmse)
499/14:
df_features_importance = pd.DataFrame(
    {'feature_names': best_model.feature_names_,
     'feature_importances': best_model.feature_importances_
    })
501/1:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
501/2:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

df_train_original = pd.read_csv('data/train.csv')

print(df_train.shape)
print(df_val.shape)
501/3: df_train
501/4:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
501/5: from sklearn.model_selection import train_test_split
501/6:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
501/7: X_train.head(5)
501/8: X_train.info()
501/9:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
501/10: df_train['Open Date'].max()
501/11: df_train['Open Date']
501/12: X_train[['Open Date']] = X_train[['Open Date']].apply(pd.to_datetime)
501/13: X_train
501/14: X_train['Open Date'].max()
501/15: own_date = date(2015, 1, 1)
501/16: own_date
501/17: own_date - X_train['Open Date']
501/18:  pd.Timestamp('today')
501/19:  pd.Timestamp(own_date)
501/20:  pd.Timestamp(own_date) - X_train['Open Date']
501/21: ( pd.Timestamp(own_date) - X_train['Open Date']).dt.days
501/22: X_train['open_days'] = ( pd.Timestamp(own_date) - X_train['Open Date']).dt.days
501/23: X_train
501/24: X_train.info()
501/25: X_train.nunique()
501/26: X_train
501/27: X_train.iloc[[:, 3:8]]
501/28: X_train.iloc[[2, 3:8]]
501/29: X_train.iloc[:, 3:8]
501/30: X_train.iloc[:, 5:12]
501/31: X_train.iloc[:, 5:20]
501/32: X_train.iloc[:, 5:23]
501/33: X_train.iloc[:, 5:26]
501/34: X_train.iloc[:, 5:28]
501/35: X_train.iloc[:, 5:25]
501/36: X_train
501/37: categorical_features_names = ['City', 'City Group', 'Type']
501/38:
X_train.drop(['Id', 'Open Date'], axis=1, inplace=True)
X_test.drop(['Id', 'Open Date'], axis=1, inplace=True)
501/39: X_train
501/40:
train_pool = Pool(X_train, 
                  label=y_train,
                  cat_features=categorical_features_names)
test_pool = Pool(X_test,
                 label=y_test,
                 cat_features=categorical_features_names)
501/41:
from sklearn.model_selection import train_test_split
from catboost import CatBoost, CatBoostRegressor, Pool
501/42:
train_pool = Pool(X_train, 
                  label=y_train,
                  cat_features=categorical_features_names)
test_pool = Pool(X_test,
                 label=y_test,
                 cat_features=categorical_features_names)
501/43:
model = CatBoostRegressor(custom_metric= ['R2', 'RMSE'], learning_rate=0.1, n_estimators=5000)
model.fit(train_pool, eval_set=test_pool, verbose=2000, plot=True)
501/44: X_train.shape
501/45: X_test.shape
501/46:
X_train['open_days'] = ( pd.Timestamp(own_date) - X_train['Open Date']).dt.days
X_test['open_days'] = ( pd.Timestamp(own_date) - X_test['Open Date'].apply(pd.to_datetime)).dt.days
501/47: X_test
501/48:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

df_train_original = pd.read_csv('data/train.csv')

print(df_train.shape)
print(df_val.shape)
501/49: df_train
501/50:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
501/51: X_train.head(5)
501/52: X_train.info()
501/53: X_train[['Open Date']] = X_train[['Open Date']].apply(pd.to_datetime)
501/54: own_date = date(2015, 1, 1)
501/55:
X_train['open_days'] = ( pd.Timestamp(own_date) - X_train['Open Date']).dt.days
X_test['open_days'] = ( pd.Timestamp(own_date) - X_test['Open Date'].apply(pd.to_datetime)).dt.days
501/56: categorical_features_names = ['City', 'City Group', 'Type']
501/57:
X_train.drop(['Id', 'Open Date'], axis=1, inplace=True)
X_test.drop(['Id', 'Open Date'], axis=1, inplace=True)
501/58:
train_pool = Pool(X_train, 
                  label=y_train,
                  cat_features=categorical_features_names)
test_pool = Pool(X_test,
                 label=y_test,
                 cat_features=categorical_features_names)
501/59: X_train.shape
501/60: X_test.shape
501/61:
model = CatBoostRegressor(custom_metric= ['R2', 'RMSE'], learning_rate=0.1, n_estimators=5000)
model.fit(train_pool, eval_set=test_pool, verbose=2000, plot=True)
501/62: #####################
501/63: model.best_score_
501/64:
from sklearn.model_selection import train_test_split
from catboost import CatBoost, CatBoostRegressor, Pool
import optuna
501/65:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test,
                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 20, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test_cbr)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    return rmse
501/66:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test,
                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 20, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test_cbr)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    return rmse
501/67:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
501/68:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from catboost import CatBoost, CatBoostRegressor, Pool
import optuna
501/69:
target_scaler = MinMaxScaler()
target_scaler.fit(y_train)
501/70:
target_scaler = MinMaxScaler()
target_scaler.fit(y_train.reshape(-1, 1))
501/71:
target_scaler = MinMaxScaler()
target_scaler.fit(y_train)
501/72:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.reshape(1, -1))
501/73:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train)
501/74:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.reshape(-1, 1))
501/75:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values)
501/76:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
501/77: target_scaler.transform(y_train)
501/78: target_scaler.transform(y_train.values.reshape(-1, 1))
501/79:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
501/80: y_train_scale.to_list()
501/81: list(y_train_scale)
501/82: y_train_scale
501/83: y_train_scale[0]
501/84: y_train_scale[:, ;]
501/85: y_train_scale[:, 0]
501/86: y_train_scale
501/87:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
501/88:
train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
501/89: X_train.shape
501/90: X_test.shape
501/91:
model = CatBoostRegressor(custom_metric= ['R2', 'RMSE'], learning_rate=0.1, n_estimators=1000)
model.fit(train_pool, eval_set=test_pool, verbose=2000, plot=True)
501/92: model.best_score_
501/93:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 20, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test_cbr)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    return rmse
501/94:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
501/95:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
501/96:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from catboost import CatBoost, CatBoostRegressor, Pool
from optuna.integration import CatBoostPruningCallback
import optuna
501/97:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
501/98:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 20, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    return rmse
501/99:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
501/100:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
501/101:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 20, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test)
    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return rmse
501/102:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
501/103:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
501/104:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
501/105:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

from catboost import CatBoost, CatBoostRegressor, Pool
from optuna.integration import CatBoostPruningCallback
import optuna
from sklearn.metrics import mean_squared_error
501/106:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 20, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test)
    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return rmse
501/107:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
501/108:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
501/109:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
501/110:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test)
    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return rmse
501/111:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
501/112:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
501/113:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
501/114: best_model.predict(X_test)
501/115: y_pred = best_model.predict(X_test)
501/116: target_scaler.inverse_transform(y_pred)
501/117: target_scaler.inverse_transform(y_pred.values)
501/118: target_scaler.inverse_transform(y_pred.reshape(1,-1))
501/119: target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
501/120: target_scaler.inverse_transform(y_pred.reshape(1,-1))
501/121: y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
501/122:
rmse = mean_squared_error(y_test, y_pred_scale, squared=False)
print('RMSE: ', rmse)
501/123: model.predict
501/124: model.predict(X_test)
501/125: df_val
501/126:
df_val['open_days'] = ( pd.Timestamp(own_date) - df_val['Open Date'].apply(pd.to_datetime)).dt.days
df_val.drop(['Id', 'Open Date'], axis=1, inplace=True)
501/127: df_val
501/128: best_model.predict(df_val)
501/129: y_val = best_model.predict(df_val)
501/130: y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
501/131: y_val_scale
501/132: df_val
501/133:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

df_train_original = pd.read_csv('data/train.csv')

print(df_train.shape)
print(df_val.shape)
501/134: df_t = df_val[['id']]
501/135: df_t = df_val[['Id']]
501/136: df_t
501/137:
df_val['open_days'] = ( pd.Timestamp(own_date) - df_val['Open Date'].apply(pd.to_datetime)).dt.days
df_val.drop(['Id', 'Open Date'], axis=1, inplace=True)
501/138: y_val = best_model.predict(df_val)
501/139: y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
501/140:
df_t['Prediction'] = y_val_scale
df_t.to_csv("catboost_1.csv", index=False)
501/141: df_t
501/142: df_t.describe
501/143: df_t.describe()
501/144:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test,
                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    return rmse
501/145:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
501/146:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
501/147:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
501/148: y_pred = best_model.predict(X_test)
501/149:
rmse = mean_squared_error(y_test, y_pred, squared=False)
print('RMSE: ', rmse)
501/150: df_t = df_val[['Id']]
501/151:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

df_train_original = pd.read_csv('data/train.csv')

print(df_train.shape)
print(df_val.shape)
501/152: df_t = df_val[['Id']]
501/153:
df_val['open_days'] = ( pd.Timestamp(own_date) - df_val['Open Date'].apply(pd.to_datetime)).dt.days
df_val.drop(['Id', 'Open Date'], axis=1, inplace=True)
501/154: y_val = best_model.predict(df_val)
501/155:
df_t['Prediction'] = y_val
df_t.to_csv("catboost_2.csv", index=False)
501/156: y_train_scale
501/157: y_train_scale[:, 0]
501/158: np.histogram(y_train_scale[:, 0], density=True)
501/159: np.histogram(y_train_scale[:, 0])
501/160: plt.hist(y_train_scale[:, 0])
501/161:
#y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
#y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
np.log1p(y_train)
501/162: plt.hist(np.log1p(y_train))
501/163:
#y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
#y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
np.log10(y_train)
501/164: plt.hist(np.log10(y_train))
501/165: plt.hist(np.log(y_train))
501/166:
#y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
#y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
np.log(y_train)
501/167:
#y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
#y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
y_train_scale = np.log(y_train)
y_test_scale = np.log(y_test)
501/168:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    return rmse
501/169:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
501/170:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
501/171:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test)
    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return rmse
501/172:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
501/173:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
501/174:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
501/175: y_pred = best_model.predict(X_test)
501/176:
#y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
np.exp(y_pred)
501/177:
rmse = mean_squared_error(y_test, np.exp(y_pred), squared=False)
print('RMSE: ', rmse)
501/178:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
df_val['open_days'] = ( pd.Timestamp(own_date) - df_val['Open Date'].apply(pd.to_datetime)).dt.days
df_val.drop(['Id', 'Open Date'], axis=1, inplace=True)
501/179: y_val = best_model.predict(df_val)
501/180:
df_t['Prediction'] = np.exp(y_val)
df_t.to_csv("catboost_3.csv", index=False)
501/181: df_t.describe()
501/182:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler

from catboost import CatBoost, CatBoostRegressor, Pool
from optuna.integration import CatBoostPruningCallback
import optuna
from sklearn.metrics import mean_squared_error
501/183:
target_scaler = StandardScaler()
target_scaler.fit(y_train.values.reshape(-1, 1))
501/184:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
#y_train_scale = np.log(y_train)
#y_test_scale = np.log(y_test)
501/185: plt.hist(y_train_scale)
501/186:
target_scaler = StandardScaler(with_mean=False, with_std=False)
target_scaler.fit(y_train.values.reshape(-1, 1))
501/187:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
#y_train_scale = np.log(y_train)
#y_test_scale = np.log(y_test)
501/188: plt.hist(y_train_scale)
501/189:
target_scaler = StandardScaler()
target_scaler.fit(y_train.values.reshape(-1, 1))
501/190:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
#y_train_scale = np.log(y_train)
#y_test_scale = np.log(y_test)
501/191: plt.hist(y_train_scale)
501/192:
target_scaler = StandardScaler(with_mean=False, with_std=False)
target_scaler.fit(y_train.values.reshape(-1, 1))
501/193:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
#y_train_scale = np.log(y_train)
#y_test_scale = np.log(y_test)
501/194: plt.hist(y_train_scale)
501/195: y_train_scale[:, 0]
501/196:
train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
501/197: X_train.shape
501/198: X_test.shape
501/199:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test)
    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return rmse
501/200:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
501/201:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
501/202:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
501/203: y_pred = best_model.predict(X_test)
501/204:
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
#np.exp(y_pred)
501/205:
rmse = mean_squared_error(y_test, np.exp(y_pred), squared=False)
print('RMSE: ', rmse)
501/206:
rmse = mean_squared_error(y_test, y_pred_scale, squared=False)
print('RMSE: ', rmse)
501/207:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
df_val['open_days'] = ( pd.Timestamp(own_date) - df_val['Open Date'].apply(pd.to_datetime)).dt.days
df_val.drop(['Id', 'Open Date'], axis=1, inplace=True)
501/208: y_val = best_model.predict(df_val)
501/209: y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
501/210:
df_t['Prediction'] = np.exp(y_val)
df_t.to_csv("catboost_4.csv", index=False)
501/211: df_t.describe()
501/212:
df_t['Prediction'] = y_val_scale
df_t.to_csv("catboost_4.csv", index=False)
501/213: df_t.describe()
501/214: #Correlation
501/215: df_train.shape
501/216: df_train
501/217:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
501/218:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
501/219: X_train
501/220:
def date_transform(df, new_date):
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date'].dt.days)
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
501/221:
def transform_date(df, new_date):
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date'].dt.days)
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
501/222:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
501/223:
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date'].dt.days)
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
501/224:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
501/225:
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
501/226:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
501/227: X_train
501/228:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
501/229:
plt.figure(figsize=(14,10))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
501/230:
plt.figure(figsize=(15,15))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
501/231:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
501/232:
corr_features = correlation(X_train, 0.85)
print('correlated features: ', len(set(corr_features)) )
501/233:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
501/234: corr_features
501/235:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
501/236:
plt.figure(figsize=(15,15))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
501/237: #Mutual information
501/238: from sklearn.feature_selection import mutual_info_regression
501/239:
# determine the mutual information
mi = mutual_info_regression(X_train, y_train)

# and make a bar  plot
mi = pd.Series(mi)
mi.index = X_train.columns
mi.sort_values(ascending=False).plot.bar(figsize=(20,6))
plt.ylabel('Mutual Information')
501/240: from scipy.stats import chi2_contingency
501/241: X_train
501/242:
chisqt = pandas.crosstab(X_train.City, y_train, margins=True)
value = np.array([chisqt.iloc[0][0:5].values,
                  chisqt.iloc[1][0:5].values])
print(chi2_contingency(value)[0:3])
501/243:
chisqt = pd.crosstab(X_train.City, y_train, margins=True)
value = np.array([chisqt.iloc[0][0:5].values,
                  chisqt.iloc[1][0:5].values])
print(chi2_contingency(value)[0:3])
501/244:
chisqt = pd.crosstab(X_train.City, y_train, margins=True)
value = np.array([chisqt.iloc[0][0:5].values,
                  chisqt.iloc[1][0:5].values])
print(chi2_contingency(value)[0:3])
501/245: chisqt
501/246: chisqt.iloc[0][0:5]
501/247: chisqt
501/248: chisqt.iloc[1][0:5]
501/249: chisqt.iloc[1]
501/250: chisqt.iloc[0]
501/251: chisqt.iloc[1]
501/252:
from scipy.stats import chi2_contingency 
from scipy import stats
501/253:
def chi2_by_hand(df, col1, col2):    
    #---create the contingency table---
    df_cont = pd.crosstab(index = df[col1], columns = df[col2])
    display(df_cont)
    #---calculate degree of freedom---
    degree_f = (df_cont.shape[0]-1) * (df_cont.shape[1]-1)
    #---sum up the totals for row and columns---
    df_cont.loc[:,'Total']= df_cont.sum(axis=1)
    df_cont.loc['Total']= df_cont.sum()
    print('---Observed (O)---')
    display(df_cont)
    #---create the expected value dataframe---
    df_exp = df_cont.copy()    
    df_exp.iloc[:,:] = np.multiply.outer(
        df_cont.sum(1).values,df_cont.sum().values) / 
        df_cont.sum().sum()            
    print('---Expected (E)---')
    display(df_exp)
        
    # calculate chi-square values
    df_chi2 = ((df_cont - df_exp)**2) / df_exp    
    df_chi2.loc[:,'Total']= df_chi2.sum(axis=1)
    df_chi2.loc['Total']= df_chi2.sum()
    
    print('---Chi-Square---')
    display(df_chi2)
    #---get chi-square score---   
    chi_square_score = df_chi2.iloc[:-1,:-1].sum().sum()
    p = stats.distributions.chi2.sf(chi_square_score, degree_f)
    return chi_square_score, degree_f, p
501/254:
def chi2_by_hand(df, col1, col2):    
    #---create the contingency table---
    df_cont = pd.crosstab(index = df[col1], columns = df[col2])
    display(df_cont)
    #---calculate degree of freedom---
    degree_f = (df_cont.shape[0]-1) * (df_cont.shape[1]-1)
    #---sum up the totals for row and columns---
    df_cont.loc[:,'Total']= df_cont.sum(axis=1)
    df_cont.loc['Total']= df_cont.sum()
    print('---Observed (O)---')
    display(df_cont)
    #---create the expected value dataframe---
    df_exp = df_cont.copy()    
    df_exp.iloc[:,:] = np.multiply.outer(
        df_cont.sum(1).values,df_cont.sum().values) \ 
        df_cont.sum().sum()            
    print('---Expected (E)---')
    display(df_exp)
        
    # calculate chi-square values
    df_chi2 = ((df_cont - df_exp)**2) / df_exp    
    df_chi2.loc[:,'Total']= df_chi2.sum(axis=1)
    df_chi2.loc['Total']= df_chi2.sum()
    
    print('---Chi-Square---')
    display(df_chi2)
    #---get chi-square score---   
    chi_square_score = df_chi2.iloc[:-1,:-1].sum().sum()
    p = stats.distributions.chi2.sf(chi_square_score, degree_f)
    return chi_square_score, degree_f, p
501/255:
def chi2_by_hand(df, col1, col2):    
    #---create the contingency table---
    df_cont = pd.crosstab(index = df[col1], columns = df[col2])
    display(df_cont)
    #---calculate degree of freedom---
    degree_f = (df_cont.shape[0]-1) * (df_cont.shape[1]-1)
    #---sum up the totals for row and columns---
    df_cont.loc[:,'Total']= df_cont.sum(axis=1)
    df_cont.loc['Total']= df_cont.sum()
    print('---Observed (O)---')
    display(df_cont)
    #---create the expected value dataframe---
    df_exp = df_cont.copy()    
    df_exp.iloc[:,:] = np.multiply.outer(df_cont.sum(1).values,df_cont.sum().values)/df_cont.sum().sum()            
    print('---Expected (E)---')
    display(df_exp)
        
    # calculate chi-square values
    df_chi2 = ((df_cont - df_exp)**2) / df_exp    
    df_chi2.loc[:,'Total']= df_chi2.sum(axis=1)
    df_chi2.loc['Total']= df_chi2.sum()
    
    print('---Chi-Square---')
    display(df_chi2)
    #---get chi-square score---   
    chi_square_score = df_chi2.iloc[:-1,:-1].sum().sum()
    p = stats.distributions.chi2.sf(chi_square_score, degree_f)
    return chi_square_score, degree_f, p
501/256: X_train
501/257: X_train[['City', 'City Group', 'Type']].nunique()
501/258: X_test[['City', 'City Group', 'Type']].nuniquenique()
501/259: X_train[['City', 'City Group', 'Type']].nunique()
501/260: X_test[['City', 'City Group', 'Type']].nunique()
501/261: X_train
501/262:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
501/263: X_test
501/264:
target_scaler = MinMaxScaler()
target_scaler.fit(y_train.values.reshape(-1, 1))
501/265:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
501/266: plt.hist(y_train_scale)
501/267: X_train
501/268: categorical_features_names = ['City Group', 'Type']
501/269:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test)
    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return rmse
501/270:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
501/271:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
501/272:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
501/273:
y_pred = best_model.predict(X_test)
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
501/274: y_pred_scale
501/275:
rmse = mean_squared_error(y_test, y_pred_scale, squared=False)
print('RMSE: ', rmse)
501/276: X_test.columns
501/277:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
df_val = transform_date(X_train, own_date)
df_val.drop(['Id', 'City'], axis=1, inplace=True)
501/278:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
df_val = transform_date(df_val, own_date)
df_val.drop(['Id', 'City'], axis=1, inplace=True)
501/279: df_val
501/280: df_val[X_train.columns]
501/281:
y_val = best_model.predict(df_val[X_train.columns])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
501/282: y_val_scale
501/283: df_t
501/284:
df_t['Prediction'] = y_val_scale
df_t.to_csv("catboost_5.csv", index=False)
501/285: df_t.describe()
501/286: df_t
501/287: df_t.sort_values('Prediction')
501/288: df_val[df_val.Id]
501/289: df_val
501/290: df_val.iloc[94524, :]
501/291:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
501/292:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
501/293: plt.hist(y_train_scale)
501/294: categorical_features_names = ['City Group', 'Type']
501/295:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test)
    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return rmse
501/296:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
501/297:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
501/298:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
501/299:
y_pred = best_model.predict(X_test)
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
501/300:
rmse = mean_squared_error(y_test, y_pred_scale, squared=False)
print('RMSE: ', rmse)
501/301: X_test.columns
501/302:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
df_val = transform_date(df_val, own_date)
df_val.drop(['Id', 'City'], axis=1, inplace=True)
501/303: df_val[X_train.columns]
501/304:
y_val = best_model.predict(df_val[X_train.columns])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
501/305: df_t.sort_values('Prediction')
501/306:
df_t['Prediction'] = y_val_scale
df_t.to_csv("catboost_5.csv", index=False)
501/307: df_t.sort_values('Prediction')
501/308: df_t.describe()
501/309:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
501/310:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
501/311:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
501/312:
plt.figure(figsize=(15,15))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
501/313:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
501/314:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
501/315: corr_features
501/316:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
501/317: #Mutual information
501/318:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
501/319: X_test
501/320:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
501/321:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
501/322: plt.hist(y_train_scale)
501/323: categorical_features_names = ['City Group', 'Type']
501/324:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test)
    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return rmse
501/325:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
501/326:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
502/1:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
502/2:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

df_train_original = pd.read_csv('data/train.csv')

print(df_train.shape)
print(df_val.shape)
502/3: df_train.head(3)
502/4: df_train.describe()
502/5:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
502/6:
### Baseline model
lr = LinearRegression()
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
502/7: #### The next part is an example of features selection.
502/8:
sel = VarianceThreshold(threshold=0.0)
sel.fit(X_train)  # fit finds the features with zero variance
sel.get_support()
constant = X_train.columns[~sel.get_support()]
len(constant)
502/9:
### only works with numerical
const_f = [feat for feat in X_train.columns if X_train[feat].std() == 0]
len(const_f)
502/10: ### works also with categorical variables
502/11:
constant_features = [feat for feat in X_train.columns if X_train[feat].nunique() == 1]
len(constant_features)
502/12:
sel = VarianceThreshold(threshold=0.02)  
sel.fit(X_train)  # fit finds the features with low variance
quasi_constant = X_train.columns[~sel.get_support()]

len(quasi_constant)
502/13:
dic_sum = df_train.sum().to_dict()
k_v_exchanged = {}
for key, val in dic_sum.items():
    if val not in k_v_exchanged:
        k_v_exchanged[val] = [key]
    else:
        k_v_exchanged[val].append(key)
502/14: max([len(v) for v in k_v_exchanged.values()])
502/15:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
502/16:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
502/17:
corr_features = correlation(X_train, 0.75)
print('correlated features: ', len(set(corr_features)) )
502/18: corr_features
502/19:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
502/20:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
### So, with selected features RMSE a little bit increase
502/21: from sklearn.feature_selection import mutual_info_regression
502/22:
# determine the mutual information
mi = mutual_info_regression(X_train, y_train)

# and make a bar  plot
mi = pd.Series(mi)
mi.index = X_train.columns
mi.sort_values(ascending=False).plot.bar(figsize=(20,6))
plt.ylabel('Mutual Information')
502/23: mi_dict = dict(sorted(mi.to_dict().items(), key=lambda item: item[1]))
502/24: unmutal_cols = list(mi_dict.keys())[0]
502/25:
X_train.drop(labels=unmutal_cols, axis=1, inplace=True)
X_test.drop(labels=unmutal_cols, axis=1, inplace=True)

X_train.shape, X_test.shape
502/26:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
### So, without the 'Id' feature RMSE doesn't change.
502/27:
## ANOVA
from sklearn.feature_selection import f_regression
502/28:
# univariate anova
univariate = f_regression(X_train.fillna(0), y_train)

# plot values
univariate = pd.Series(univariate[1])
univariate.index = X_train.columns
univariate.sort_values(ascending=False).plot.bar(figsize=(20,6))
502/29: univariate_dict = dict(sorted(univariate.to_dict().items(), key=lambda item: item[1]))
502/30: univariate_cols = list(univariate_dict.keys())[-1:]
502/31: univariate_cols
502/32:
X_train.drop(labels=univariate_cols, axis=1, inplace=True)
X_test.drop(labels=univariate_cols, axis=1, inplace=True)

X_train.shape, X_test.shape
502/33:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
#### not a big success, but I'll save this part as an example
502/34:
import mlxtend
from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS
502/35: !conda install mlxtend
502/36:
# exhaustive search
efs = EFS(LinearRegression(), min_features=1, max_features=3, scoring='neg_mean_squared_error', print_progress=True,)
efs = efs.fit(np.array(X_train), y_train)
502/37: efs.best_idx_
502/38:
selected_feat = X_train.columns[list(efs.best_idx_)]
selected_feat
502/39:
lr.fit(X_train[selected_feat], y_train)
y_predict = lr.predict(X_test[selected_feat])
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
502/40:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
502/41:
sfs = SFS(LinearRegression(),
          k_features=3, # the lower the features we want, the longer this will take
          forward=False,
          verbose=2,
          scoring='neg_mean_squared_error')

sfs = sfs.fit(np.array(X_train), y_train)
502/42:
sfs.k_feature_idx_
## The same result as before, but faster :)
502/43:
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV

from sklearn.compose import TransformedTargetRegressor
502/44:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
502/45:
X_train.drop('id', axis=1, inplace=True)
X_test.drop('id', axis=1, inplace=True)
502/46:
models=[
        LinearRegression(),
        Lasso(),
        ElasticNet(),
        Ridge(),
       ]
502/47:
### 4 different models
models=[
        LinearRegression(),
        Lasso(),
        ElasticNet(),
        Ridge(),
       ]
502/48:
lasso_params = {'alpha':[0.01, 0.1, 0.5, 1, 5, 10]}

ridge_params = {'alpha': [1, 5, 10, 15] , 
                "fit_intercept": [True, False], 
                "solver": ['svd', 'sag']
               }

elastic_params = {"max_iter": [1, 5, 10],
                  "alpha": [0.01, 0.1, 1, 10, 100],
                  "l1_ratio": np.arange(0.0, 1.0, 0.1)
                 }

models = {'OLS': GridSearchCV(LinearRegression(),param_grid={}),
           'Lasso': GridSearchCV(Lasso(), param_grid=lasso_params, scoring='neg_mean_squared_error', cv=5),
           'Ridge': GridSearchCV(Ridge(), param_grid=ridge_params, scoring='neg_mean_squared_error', cv=5),
           'Elastic': GridSearchCV(ElasticNet(), param_grid=elastic_params, scoring='neg_mean_squared_error', cv=5)
          }
502/49:
lasso_params = {'alpha':[0.01, 0.1, 0.5, 1, 5, 10]}

ridge_params = {'alpha': [1, 5, 10, 15] , 
                "fit_intercept": [True, False], 
                "solver": ['svd', 'sag']
               }

elastic_params = {"max_iter": [1, 5, 10],
                  "alpha": [0.01, 0.1, 1, 10, 100],
                  "l1_ratio": np.arange(0.0, 1.0, 0.1)
                 }

models = {'OLS': GridSearchCV(LinearRegression(),param_grid={}, scoring='neg_mean_squared_error', cv=5),
           'Lasso': GridSearchCV(Lasso(), param_grid=lasso_params, scoring='neg_mean_squared_error', cv=5),
           'Ridge': GridSearchCV(Ridge(), param_grid=ridge_params, scoring='neg_mean_squared_error', cv=5),
           'Elastic': GridSearchCV(ElasticNet(), param_grid=elastic_params, scoring='neg_mean_squared_error', cv=5)
          }
502/50: metrics = pd.DataFrame(columns=['model' , 'best_params', 'best score', 'test rmse',])
502/51: metrics
502/52:
def tune(models, X_train, X_test, y_train, y_test, flag_selected=''):
    global metrics
    for i in models2:
        clf = models2[i]
        clf.fit(X_train, y_train)
        y_pred = clf.best_estimator_.predict(X_test)
        rmse = mean_squared_error(y_test, y_pred, squared=False)
        print(i)
        print(f'RMSE: {rmse:.5f}')  
        metrics = metrics.append({
                                'model': i + ' ' + flag_selected,
                                'best_params': clf.best_params_,
                                'best score': clf.best_score_,
                                'test rmse': rmse
                                 },
                               ignore_index=True)
502/53:
### selected columns from the part below
selected_cols = ['cont1', 'cont2', 'cont7']
502/54:
tune(models, X_train, X_test, y_train, y_test)
### Also let's check a data set with selected solumns
tune(models, X_train[selected_cols], X_test[selected_cols], y_train, y_test, 'selected')
502/55:
def tune(models, X_train, X_test, y_train, y_test, flag_selected=''):
    global metrics
    for i in models:
        clf = models[i]
        clf.fit(X_train, y_train)
        y_pred = clf.best_estimator_.predict(X_test)
        rmse = mean_squared_error(y_test, y_pred, squared=False)
        print(i)
        print(f'RMSE: {rmse:.5f}')  
        metrics = metrics.append({
                                'model': i + ' ' + flag_selected,
                                'best_params': clf.best_params_,
                                'best score': clf.best_score_,
                                'test rmse': rmse
                                 },
                               ignore_index=True)
502/56:
### selected columns from the part below
selected_cols = ['cont1', 'cont2', 'cont7']
502/57:
tune(models, X_train, X_test, y_train, y_test)
### Also let's check a data set with selected solumns
tune(models, X_train[selected_cols], X_test[selected_cols], y_train, y_test, 'selected')
502/58:
def tune(models, X_train, X_test, y_train, y_test, flag_selected=''):
    global metrics
    for i in models:
        clf = models[i]
        clf.fit(X_train, y_train)
        y_pred = clf.best_estimator_.predict(X_test)
        rmse = mean_squared_error(y_test, y_pred, squared=False)
        print(i)
        print(f'RMSE: {rmse:.5f}')  
        metrics = metrics.concat({
                                'model': i + ' ' + flag_selected,
                                'best_params': clf.best_params_,
                                'best score': clf.best_score_,
                                'test rmse': rmse
                                 },
                               ignore_index=True)
502/59:
### selected columns from the part below
selected_cols = ['cont1', 'cont2', 'cont7']
502/60:
tune(models, X_train, X_test, y_train, y_test)
### Also let's check a data set with selected solumns
tune(models, X_train[selected_cols], X_test[selected_cols], y_train, y_test, 'selected')
502/61:
def tune(models, X_train, X_test, y_train, y_test, flag_selected=''):
    global metrics
    for i in models:
        clf = models[i]
        clf.fit(X_train, y_train)
        y_pred = clf.best_estimator_.predict(X_test)
        rmse = mean_squared_error(y_test, y_pred, squared=False)
        print(i)
        print(f'RMSE: {rmse:.5f}')  
        metrics = metrics.concat({
                                'model': i + ' ' + flag_selected,
                                'best_params': clf.best_params_,
                                'best score': clf.best_score_,
                                'test rmse': rmse
                                 },
                               ignore_index=True)
502/62:
### selected columns from the part below
selected_cols = ['cont1', 'cont2', 'cont7']
502/63:
tune(models, X_train, X_test, y_train, y_test)
### Also let's check a data set with selected solumns
tune(models, X_train[selected_cols], X_test[selected_cols], y_train, y_test, 'selected')
502/64:
def tune(models, X_train, X_test, y_train, y_test, flag_selected=''):
    global metrics
    for i in models:
        clf = models[i]
        clf.fit(X_train, y_train)
        y_pred = clf.best_estimator_.predict(X_test)
        rmse = mean_squared_error(y_test, y_pred, squared=False)
        print(i)
        print(f'RMSE: {rmse:.5f}')  
        metrics.loc[len(metrics.index)] = ['model': i + ' ' + flag_selected,
                                           'best_params': clf.best_params_,
                                           'best score': clf.best_score_,
                                           'test rmse': rmse
                                          ]
502/65:
def tune(models, X_train, X_test, y_train, y_test, flag_selected=''):
    global metrics
    for i in models:
        clf = models[i]
        clf.fit(X_train, y_train)
        y_pred = clf.best_estimator_.predict(X_test)
        rmse = mean_squared_error(y_test, y_pred, squared=False)
        print(i)
        print(f'RMSE: {rmse:.5f}')  
        metrics.loc[len(metrics.index)] = [i + ' ' + flag_selected,
                                           clf.best_params_,
                                           clf.best_score_,
                                           rmse
                                          ]
502/66:
### selected columns from the part below
selected_cols = ['cont1', 'cont2', 'cont7']
502/67:
tune(models, X_train, X_test, y_train, y_test)
### Also let's check a data set with selected solumns
tune(models, X_train[selected_cols], X_test[selected_cols], y_train, y_test, 'selected')
502/68: metrics
502/69: metrics = pd.DataFrame(columns=['model' , 'best_params', 'best score', 'test rmse',])
502/70:
def tune(models, X_train, X_test, y_train, y_test, flag_selected=''):
    global metrics
    for i in models:
        clf = models[i]
        clf.fit(X_train, y_train)
        y_pred = clf.best_estimator_.predict(X_test)
        rmse = mean_squared_error(y_test, y_pred, squared=False)
        print(i)
        print(f'RMSE: {rmse:.5f}')  
        metrics.loc[len(metrics.index)] = [i + ' ' + flag_selected,
                                           clf.best_params_,
                                           clf.best_score_,
                                           rmse
                                          ]
502/71:
### selected columns from the part below
selected_cols = ['cont1', 'cont2', 'cont7']
502/72:
tune(models, X_train, X_test, y_train, y_test)
### Also let's check a data set with selected solumns
tune(models, X_train[selected_cols], X_test[selected_cols], y_train, y_test, 'selected')
502/73: metrics
502/74: metrics = pd.DataFrame(columns=['model' , 'best_params', 'best score', 'test rmse',])
502/75: metrics
502/76: X_train
502/77:
lasso_params = {'alpha':[0.01, 0.1, 0.5, 1, 5, 10]}

ridge_params = {'alpha': [1, 5, 10, 15] , 
                "fit_intercept": [True, False], 
                "solver": ['svd', 'sag']
               }

elastic_params = {"max_iter": [1000, 1500, 2500],
                  "alpha": [0.01, 0.1, 1, 10, 100],
                  "l1_ratio": np.arange(0.0, 1.0, 0.1)
                 }

models = {'OLS': GridSearchCV(LinearRegression(),param_grid={}, scoring='neg_mean_squared_error', cv=5),
           'Lasso': GridSearchCV(Lasso(), param_grid=lasso_params, scoring='neg_mean_squared_error', cv=5),
           'Ridge': GridSearchCV(Ridge(), param_grid=ridge_params, scoring='neg_mean_squared_error', cv=5),
           'Elastic': GridSearchCV(ElasticNet(), param_grid=elastic_params, scoring='neg_mean_squared_error', cv=5)
          }
502/78: metrics = pd.DataFrame(columns=['model' , 'best_params', 'best score', 'test rmse',])
502/79:
def tune(models, X_train, X_test, y_train, y_test, flag_selected=''):
    global metrics
    for i in models:
        clf = models[i]
        clf.fit(X_train, y_train)
        y_pred = clf.best_estimator_.predict(X_test)
        rmse = mean_squared_error(y_test, y_pred, squared=False)
        print(i)
        print(f'RMSE: {rmse:.5f}')  
        metrics.loc[len(metrics.index)] = [i + ' ' + flag_selected,
                                           clf.best_params_,
                                           clf.best_score_,
                                           rmse
                                          ]
502/80:
### selected columns from the part below
selected_cols = ['cont1', 'cont2', 'cont7']
502/81:
lasso_params = {'alpha':[0.01, 0.1, 0.5, 1, 5, 10]}

ridge_params = {'alpha': [1, 5, 10, 15] , 
                "fit_intercept": [True, False], 
                "solver": ['svd', 'sag']
               }

elastic_params = {"max_iter": [1000, 1500, 2500],
                  "alpha": [0.01, 0.1, 1, 10, 100],
                  "l1_ratio": np.arange(0.0, 1.0, 0.1)
                 }

models = {'OLS': GridSearchCV(LinearRegression(),param_grid={}, scoring='neg_mean_squared_error', cv=5),
           'Lasso': GridSearchCV(Lasso(), param_grid=lasso_params, scoring='neg_mean_squared_error', cv=5),
           'Ridge': GridSearchCV(Ridge(), param_grid=ridge_params, scoring='neg_mean_squared_error', cv=5),
           'Elastic': GridSearchCV(ElasticNet(), param_grid=elastic_params, scoring='neg_mean_squared_error', cv=5)
          }
502/82: metrics = pd.DataFrame(columns=['model' , 'best_params', 'best score', 'test rmse',])
502/83:
def tune(models, X_train, X_test, y_train, y_test, flag_selected=''):
    global metrics
    for i in models:
        clf = models[i]
        clf.fit(X_train, y_train)
        y_pred = clf.best_estimator_.predict(X_test)
        rmse = mean_squared_error(y_test, y_pred, squared=False)
        print(i)
        print(f'RMSE: {rmse:.5f}')  
        metrics.loc[len(metrics.index)] = [i + ' ' + flag_selected,
                                           clf.best_params_,
                                           clf.best_score_,
                                           rmse
                                          ]
502/84:
### selected columns from the part below
selected_cols = ['cont1', 'cont2', 'cont7']
502/85:
tune(models, X_train, X_test, y_train, y_test)
### Also let's check a data set with selected solumns
tune(models, X_train[selected_cols], X_test[selected_cols], y_train, y_test, 'selected')
502/86:
lasso_params = {'alpha':[0.01, 0.1, 0.5, 1, 5, 10]}

ridge_params = {'alpha': [1, 5, 10, 15] , 
                "fit_intercept": [True, False], 
                "solver": ['svd', 'sag']
               }

elastic_params = {"max_iter": [10000],
                  "alpha": [0.01, 0.1, 1, 10, 100],
                  "l1_ratio": np.arange(0.0, 1.0, 0.1)
                 }

models = {'OLS': GridSearchCV(LinearRegression(),param_grid={}, scoring='neg_mean_squared_error', cv=5),
           'Lasso': GridSearchCV(Lasso(), param_grid=lasso_params, scoring='neg_mean_squared_error', cv=5),
           'Ridge': GridSearchCV(Ridge(), param_grid=ridge_params, scoring='neg_mean_squared_error', cv=5),
           'Elastic': GridSearchCV(ElasticNet(), param_grid=elastic_params, scoring='neg_mean_squared_error', cv=5)
          }
502/87: metrics = pd.DataFrame(columns=['model' , 'best_params', 'best score', 'test rmse',])
502/88:
def tune(models, X_train, X_test, y_train, y_test, flag_selected=''):
    global metrics
    for i in models:
        clf = models[i]
        clf.fit(X_train, y_train)
        y_pred = clf.best_estimator_.predict(X_test)
        rmse = mean_squared_error(y_test, y_pred, squared=False)
        print(i)
        print(f'RMSE: {rmse:.5f}')  
        metrics.loc[len(metrics.index)] = [i + ' ' + flag_selected,
                                           clf.best_params_,
                                           clf.best_score_,
                                           rmse
                                          ]
502/89:
### selected columns from the part below
selected_cols = ['cont1', 'cont2', 'cont7']
502/90:
tune(models, X_train, X_test, y_train, y_test)
### Also let's check a data set with selected solumns
tune(models, X_train[selected_cols], X_test[selected_cols], y_train, y_test, 'selected')
502/91:
lasso_params = {'alpha':[0.01, 0.1, 0.5, 1, 5, 10]}

ridge_params = {'alpha': [1, 5, 10, 15] , 
                "fit_intercept": [True, False], 
                "solver": ['svd', 'sag']
               }

elastic_params = {"max_iter": [10000],
                  "alpha": [0.01, 0.1, 1, 10, 100],
                  "l1_ratio": np.arange(0.1, 0.9, 0.1)
                 }

models = {'OLS': GridSearchCV(LinearRegression(),param_grid={}, scoring='neg_mean_squared_error', cv=5),
           'Lasso': GridSearchCV(Lasso(), param_grid=lasso_params, scoring='neg_mean_squared_error', cv=5),
           'Ridge': GridSearchCV(Ridge(), param_grid=ridge_params, scoring='neg_mean_squared_error', cv=5),
           'Elastic': GridSearchCV(ElasticNet(), param_grid=elastic_params, scoring='neg_mean_squared_error', cv=5)
          }
502/92: metrics = pd.DataFrame(columns=['model' , 'best_params', 'best score', 'test rmse',])
502/93:
def tune(models, X_train, X_test, y_train, y_test, flag_selected=''):
    global metrics
    for i in models:
        clf = models[i]
        clf.fit(X_train, y_train)
        y_pred = clf.best_estimator_.predict(X_test)
        rmse = mean_squared_error(y_test, y_pred, squared=False)
        print(i)
        print(f'RMSE: {rmse:.5f}')  
        metrics.loc[len(metrics.index)] = [i + ' ' + flag_selected,
                                           clf.best_params_,
                                           clf.best_score_,
                                           rmse
                                          ]
502/94:
### selected columns from the part below
selected_cols = ['cont1', 'cont2', 'cont7']
502/95:
tune(models, X_train, X_test, y_train, y_test)
### Also let's check a data set with selected solumns
tune(models, X_train[selected_cols], X_test[selected_cols], y_train, y_test, 'selected')
502/96: metrics.sort_values('test rmse')
502/97: metrics[metrics.model == 'Ridge']
502/98: metrics.model
502/99: metrics[metrics.model == 'Ridge ']
502/100: metrics[metrics.model == 'Ridge ']['best_params']
502/101: ### I'll try to tune hyperparameters for ridge regression
502/102:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
502/103:
X_train.drop('id', axis=1, inplace=True)
X_test.drop('id', axis=1, inplace=True)
502/104:
parameters = {'alpha': np.arange(5, 15, 1),
              'fit_intercept': [True],
              'solver': ['sag']}
model = Ridge()

# define the grid search
ridge_reg= GridSearchCV(model, parameters, scoring='neg_mean_squared_error',cv=5)
502/105: ridge_reg.fit(X_train, y_train)
502/106: ridge_reg.best_estimator_
502/107: mean_squared_error(y_test, ridge_reg.best_estimator_.predict(X_test), squared=False)
502/108:
df_t = df_val[['id']]
df_val.drop('id', axis=1, inplace=True)
df_t['target'] = ridge_reg.best_estimator_.predict(df_val)
df_t.to_csv("ridge_reg_2.csv", index=False)
502/109:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

import pickle
502/110: df_t.describe()
502/111:
# save the model to disk
filename = 'ridge_reg_tuned.sav'
pickle.dump(model, open(filename, 'wb'))
502/112: mean_squared_error(y_test, ridge_reg.best_estimator_.predict(X_test), squared=False)
502/113:
df_t = df_val[['id']]
df_val.drop('id', axis=1, inplace=True)
df_t['target'] = ridge_reg.best_estimator_.predict(df_val)
df_t.to_csv("ridge_reg_2.csv", index=False)
506/1:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

import pickle
506/2:
df_train = pd.read_csv('data/train.csv')

#df_train_original = pd.read_csv('data/train.csv')

print(df_train.shape)
print(df_val.shape)
506/3: #### The next part is an example of features selection.
506/4:
sel = VarianceThreshold(threshold=0.0)
sel.fit(X_train)  # fit finds the features with zero variance
sel.get_support()
constant = X_train.columns[~sel.get_support()]
len(constant)
506/5:
### only works with numerical
const_f = [feat for feat in X_train.columns if X_train[feat].std() == 0]
len(const_f)
507/1:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

import pickle
507/2:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

#df_train_original = pd.read_csv('data/train.csv')

print(df_train.shape)
print(df_val.shape)
507/3: df_train.head(3)
507/4: df_train.describe()
507/5:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
507/6:
### Baseline model
lr = LinearRegression()
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
507/7: #### The next part is an example of features selection.
507/8:
sel = VarianceThreshold(threshold=0.0)
sel.fit(X_train)  # fit finds the features with zero variance
sel.get_support()
constant = X_train.columns[~sel.get_support()]
len(constant)
507/9:
### only works with numerical
const_f = [feat for feat in X_train.columns if X_train[feat].std() == 0]
len(const_f)
507/10: ### works also with categorical variables
507/11:
constant_features = [feat for feat in X_train.columns if X_train[feat].nunique() == 1]
len(constant_features)
507/12:
sel = VarianceThreshold(threshold=0.02)  
sel.fit(X_train)  # fit finds the features with low variance
quasi_constant = X_train.columns[~sel.get_support()]

len(quasi_constant)
507/13:
dic_sum = df_train.sum().to_dict()
k_v_exchanged = {}
for key, val in dic_sum.items():
    if val not in k_v_exchanged:
        k_v_exchanged[val] = [key]
    else:
        k_v_exchanged[val].append(key)
507/14: max([len(v) for v in k_v_exchanged.values()])
507/15:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
507/16:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
507/17:
corr_features = correlation(X_train, 0.75)
print('correlated features: ', len(set(corr_features)) )
507/18: corr_features
507/19:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
507/20:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
### So, with selected features RMSE a little bit increase
507/21: from sklearn.feature_selection import mutual_info_regression
507/22:
# determine the mutual information
mi = mutual_info_regression(X_train, y_train)

# and make a bar  plot
mi = pd.Series(mi)
mi.index = X_train.columns
mi.sort_values(ascending=False).plot.bar(figsize=(20,6))
plt.ylabel('Mutual Information')
507/23: mi_dict = dict(sorted(mi.to_dict().items(), key=lambda item: item[1]))
507/24: unmutal_cols = list(mi_dict.keys())[0]
507/25:
X_train.drop(labels=unmutal_cols, axis=1, inplace=True)
X_test.drop(labels=unmutal_cols, axis=1, inplace=True)

X_train.shape, X_test.shape
507/26:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
### So, without the 'Id' feature RMSE doesn't change.
507/27:
## ANOVA
from sklearn.feature_selection import f_regression
507/28:
# univariate anova
univariate = f_regression(X_train.fillna(0), y_train)

# plot values
univariate = pd.Series(univariate[1])
univariate.index = X_train.columns
univariate.sort_values(ascending=False).plot.bar(figsize=(20,6))
507/29: univariate_dict = dict(sorted(univariate.to_dict().items(), key=lambda item: item[1]))
507/30: univariate_cols = list(univariate_dict.keys())[-1:]
507/31: univariate_cols
507/32:
X_train.drop(labels=univariate_cols, axis=1, inplace=True)
X_test.drop(labels=univariate_cols, axis=1, inplace=True)

X_train.shape, X_test.shape
507/33:
### model
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
#### not a big success, but I'll save this part as an example
507/34:
import mlxtend
from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS
507/35:
# exhaustive search
efs = EFS(LinearRegression(), min_features=1, max_features=3, scoring='neg_mean_squared_error', print_progress=True,)
efs = efs.fit(np.array(X_train), y_train)
507/36: efs.best_idx_
507/37:
selected_feat = X_train.columns[list(efs.best_idx_)]
selected_feat
507/38:
lr.fit(X_train[selected_feat], y_train)
y_predict = lr.predict(X_test[selected_feat])
rmse = mean_squared_error(y_test, y_predict, squared=False)
print(f'RMSE: {rmse:.5f}')
507/39:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
507/40:
sfs = SFS(LinearRegression(),
          k_features=3, # the lower the features we want, the longer this will take
          forward=False,
          verbose=2,
          scoring='neg_mean_squared_error')

sfs = sfs.fit(np.array(X_train), y_train)
507/41:
sfs.k_feature_idx_
## The same result as before, but faster :)
507/42:
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV

from sklearn.compose import TransformedTargetRegressor
507/43:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
507/44:
X_train.drop('id', axis=1, inplace=True)
X_test.drop('id', axis=1, inplace=True)
507/45:
### 4 different models
models=[
        LinearRegression(),
        Lasso(),
        ElasticNet(),
        Ridge(),
       ]
507/46:
lasso_params = {'alpha':[0.01, 0.1, 0.5, 1, 5, 10]}

ridge_params = {'alpha': [1, 5, 10, 15] , 
                "fit_intercept": [True, False], 
                "solver": ['svd', 'sag']
               }

elastic_params = {"max_iter": [10000],
                  "alpha": [0.01, 0.1, 1, 10, 100],
                  "l1_ratio": np.arange(0.1, 0.9, 0.1)
                 }

models = {'OLS': GridSearchCV(LinearRegression(),param_grid={}, scoring='neg_mean_squared_error', cv=5),
           'Lasso': GridSearchCV(Lasso(), param_grid=lasso_params, scoring='neg_mean_squared_error', cv=5),
           'Ridge': GridSearchCV(Ridge(), param_grid=ridge_params, scoring='neg_mean_squared_error', cv=5),
           'Elastic': GridSearchCV(ElasticNet(), param_grid=elastic_params, scoring='neg_mean_squared_error', cv=5)
          }
507/47: metrics = pd.DataFrame(columns=['model' , 'best_params', 'best score', 'test rmse',])
507/48:
def tune(models, X_train, X_test, y_train, y_test, flag_selected=''):
    global metrics
    for i in models:
        clf = models[i]
        clf.fit(X_train, y_train)
        y_pred = clf.best_estimator_.predict(X_test)
        rmse = mean_squared_error(y_test, y_pred, squared=False)
        print(i)
        print(f'RMSE: {rmse:.5f}')  
        metrics.loc[len(metrics.index)] = [i + ' ' + flag_selected,
                                           clf.best_params_,
                                           clf.best_score_,
                                           rmse
                                          ]
507/49:
### selected columns from the part below
selected_cols = ['cont1', 'cont2', 'cont7']
507/50:
tune(models, X_train, X_test, y_train, y_test)
### Also let's check a data set with selected solumns
tune(models, X_train[selected_cols], X_test[selected_cols], y_train, y_test, 'selected')
507/51: metrics.sort_values('test rmse')
507/52: ### I'll try to tune hyperparameters for ridge regression
507/53:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
507/54:
X_train.drop('id', axis=1, inplace=True)
X_test.drop('id', axis=1, inplace=True)
507/55:
parameters = {'alpha': np.arange(5, 15, 1),
              'fit_intercept': [True],
              'solver': ['sag']}
model = Ridge()

# define the grid search
ridge_reg= GridSearchCV(model, parameters, scoring='neg_mean_squared_error',cv=5)
507/56: ridge_reg.fit(X_train, y_train)
507/57: ridge_reg.best_estimator_
507/58:
# save the model to disk
filename = 'ridge_reg_tuned.sav'
pickle.dump(model, open(filename, 'wb'))
507/59: mean_squared_error(y_test, ridge_reg.best_estimator_.predict(X_test), squared=False)
507/60:
df_t = df_val[['id']]
df_val.drop('id', axis=1, inplace=True)
df_t['target'] = ridge_reg.best_estimator_.predict(df_val)
df_t.to_csv("ridge_reg_2.csv", index=False)
507/61: df_t.describe()
507/62: from sklearn.ensemble import RandomForestRegressor
507/63:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
507/64: from sklearn.model_selection import cross_val_score
507/65:
rf_model = RandomForestRegressor(max_depth=10, n_estimators=750)
scores = cross_val_score(rf_model, X_train, y_train, cv=5)
507/66:
rf_model = RandomForestRegressor(max_depth=10, n_estimators=750, scoring='neg_mean_squared_error')
scores = cross_val_score(rf_model, X_train, y_train, cv=5)
507/67:
rf_model = RandomForestRegressor(max_depth=10, n_estimators=750)
scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')
507/68:
rf_model = RandomForestRegressor(max_depth=10, n_estimators=750)
rf_model.fit(X_train,y_train)
503/1:
import pandas as pd
import numpy as np
import catboost
import optuna
503/2:
import pandas as pd
import numpy as np
import catboost
import optuna
503/3:
from catboost import CatBoostRegressor, Pool
from optuna.integration import CatBoostPruningCallback
from sklearn.metrics import mean_squared_error

from sklearn.model_selection import train_test_split
503/4:
import os
import matplotlib.pyplot as plt#visualization
%matplotlib inline
import seaborn as sns#visualization
import plotly.offline as py #visualization
py.init_notebook_mode(connected=True)#visualization
import plotly.graph_objs as go#visualization
import plotly.tools as tls#visualization
import plotly.figure_factory as ff
from plotly.offline import init_notebook_mode, iplot
508/1:
import pandas as pd
import numpy as np
import catboost
import optuna
508/2:
from catboost import CatBoostRegressor, Pool
from optuna.integration import CatBoostPruningCallback
from sklearn.metrics import mean_squared_error

from sklearn.model_selection import train_test_split
509/1:
import pandas as pd
import numpy as np
import catboost
import optuna
509/2:
from catboost import CatBoostRegressor, Pool
from optuna.integration import CatBoostPruningCallback
from sklearn.metrics import mean_squared_error

from sklearn.model_selection import train_test_split
509/3:
import pandas as pd
import numpy as np
import catboost
import optuna
509/4:
from catboost import CatBoostRegressor, Pool
from optuna.integration import CatBoostPruningCallback
from sklearn.metrics import mean_squared_error

from sklearn.model_selection import train_test_split
509/5:
import os
import matplotlib.pyplot as plt#visualization
%matplotlib inline
import seaborn as sns#visualization
#import plotly.offline as py #visualization
#py.init_notebook_mode(connected=True)#visualization
#import plotly.graph_objs as go#visualization
#import plotly.tools as tls#visualization
#import plotly.figure_factory as ff
#from plotly.offline import init_notebook_mode, iplot
509/6:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

df_train_original = pd.read_csv('data/train.csv')

print(df_train.shape)
print(df_val.shape)
509/7:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target', 'id'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
509/8:
def objective(trial):
    global cbr
    X_train_cbr = X_train
    X_test_cbr = X_test
    y_train_cbr = y_train
    y_test_cbr = y_test
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 20, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        X_train_cbr,
        y_train_cbr,
        eval_set=[(X_test_cbr, y_test_cbr)],
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test_cbr)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    return rmse
509/9:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
509/10:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
510/1:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
510/2:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler

from catboost import CatBoost, CatBoostRegressor, Pool
from optuna.integration import CatBoostPruningCallback
import optuna
from sklearn.metrics import mean_squared_error
510/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

##df_train_original = pd.read_csv('data/train.csv')

print(df_train.shape)
print(df_val.shape)
510/4: df_train
510/5:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
510/6:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=0)

X_train.shape, X_test.shape
510/7: X_train.head(5)
510/8: X_train.info()
510/9: own_date = date(2015, 1, 1)
510/10:
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
510/11:
X_train['open_days'] = ( pd.Timestamp(own_date) - X_train['Open Date']).dt.days
X_test['open_days'] = ( pd.Timestamp(own_date) - X_test['Open Date'].apply(pd.to_datetime)).dt.days
510/12:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
510/13: categorical_features_names = ['City', 'City Group', 'Type']
510/14:
X_train.drop(['Id', 'Open Date'], axis=1, inplace=True)
X_test.drop(['Id', 'Open Date'], axis=1, inplace=True)
510/15:
X_train.drop(['Id'], axis=1, inplace=True)
X_test.drop(['Id'], axis=1, inplace=True)
510/16:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
510/17:
target_scaler = StandardScaler(with_mean=False, with_std=False)
target_scaler.fit(y_train.values.reshape(-1, 1))
510/18:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
510/19:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
#y_train_scale = np.log(y_train)
#y_test_scale = np.log(y_test)
510/20: plt.hist(y_train_scale)
510/21:
train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
510/22:
model = CatBoostRegressor(custom_metric= ['R2', 'RMSE'], learning_rate=0.1, n_estimators=1000)
model.fit(train_pool, eval_set=test_pool, verbose=2000, plot=True)
510/23: model.best_score_
510/24: model.best_iteration_
510/25: df_val
510/26: #####################
510/27:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test)
    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return rmse
510/28:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
510/29:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
510/30:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
510/31: y_pred = best_model.predict(X_test)
510/32:
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
#np.exp(y_pred)
510/33:
rmse = mean_squared_error(y_test, y_pred_scale, squared=False)
print('RMSE: ', rmse)
510/34:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
510/35:
df_val = transform_date(df_val, own_date)
df_val.drop(['Id'], axis=1, inplace=True)
510/36: df_val
510/37:
y_val = best_model.predict(df_val)
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
510/38:
df_t['Prediction'] = y_val_scale
df_t.to_csv("catboost_1.csv", index=False)
510/39: y_val_scale
510/40: y_val_scale[:]
510/41: y_val_scale.tolist()
510/42:
df_t['Prediction'] = y_val_scale.tolist()
df_t.to_csv("catboost_1.csv", index=False)
510/43: y_val_scale.tolist()
510/44:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
510/45:
df_val = transform_date(df_val, own_date)
df_val.drop(['Id'], axis=1, inplace=True)
510/46:
y_val = best_model.predict(df_val)
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
510/47: y_val_scale.tolist()
510/48:
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("catboost_1.csv", index=False)
510/49: df_t.describe()
511/1:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
511/2:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler

from catboost import CatBoost, CatBoostRegressor, Pool
from optuna.integration import CatBoostPruningCallback
import optuna
from sklearn.metrics import mean_squared_error
511/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

##df_train_original = pd.read_csv('data/train.csv')

print(df_train.shape)
print(df_val.shape)
511/4: df_train
511/5:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
511/6: X_train.head(5)
511/7: X_train.info()
511/8: own_date = date(2015, 1, 1)
511/9:
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
511/10:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
511/11: categorical_features_names = ['City', 'City Group', 'Type']
511/12:
X_train.drop(['Id'], axis=1, inplace=True)
X_test.drop(['Id'], axis=1, inplace=True)
511/13:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
511/14:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
511/15: plt.hist(y_train_scale)
511/16:
train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
511/17:
model = CatBoostRegressor(custom_metric= ['R2', 'RMSE'], learning_rate=0.1, n_estimators=1000)
model.fit(train_pool, eval_set=test_pool, verbose=2000, plot=True)
511/18: model.best_score_
511/19: model.best_iteration_
511/20: #####################
511/21:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test)
    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return rmse
511/22:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
511/23:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
509/11:
rmse = mean_squared_error(y_test, best_model.predict(X_test), squared=False)
print('RMSE: ', rmse)
509/12:
rmse = mean_squared_error(y_test, study.user_attrs["best_booster"].predict(X_test), squared=False)
print('RMSE: ', rmse)
509/13:
model = CatBoostRegressor()      # parameters not required.
model.load_model('catboost')
509/14:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
509/15: best_model.save_model('catboost')
509/16:
rmse = mean_squared_error(y_test, best_model.predict(X_test), squared=False)
print('RMSE: ', rmse)
509/17:
df_features_importance = pd.DataFrame(
    {'feature_names': best_model.feature_names_,
     'feature_importances': best_model.feature_importances_
    })
509/18: df_features_importance.sort_values('feature_importances', ascending=False)
509/19: df_val
509/20:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
509/21:
df_t = df_val[['id']]
df_val.drop('id', axis=1, inplace=True)
df_t.insert(1, "target", df_val.tolist(), True)
df_t.to_csv("catboost_reg.csv", index=False)
509/22: best_model.predict(df_val)
509/23: y_val = best_model.predict(df_val)
509/24:
#df_t = df_val[['id']]
#df_val.drop('id', axis=1, inplace=True)
df_t.insert(1, "target", y_val.tolist(), True)
df_t.to_csv("catboost_reg.csv", index=False)
509/25: df_t.describe
509/26: df_t.describe()
511/24:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
511/25: y_pred = best_model.predict(X_test)
511/26:
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
#np.exp(y_pred)
511/27:
rmse = mean_squared_error(y_test, y_pred_scale, squared=False)
print('RMSE: ', rmse)
511/28:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
511/29:
df_val = transform_date(df_val, own_date)
df_val.drop(['Id'], axis=1, inplace=True)
511/30:
y_val = best_model.predict(df_val)
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
511/31: y_val_scale.tolist()
511/32:
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("catboost_1.csv", index=False)
511/33: df_t.describe()
511/34: best_model.save_model('catboost_model')
507/69:
filename = 'random_forest_reg.sav'
pickle.dump(rf_model, open(filename, 'wb'))
511/35:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
511/36:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
511/37:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
511/38:
plt.figure(figsize=(15,15))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
511/39:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
511/40:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
511/41:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
511/42:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
511/43:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
511/44:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
511/45: plt.hist(y_train_scale)
511/46: categorical_features_names = ['City Group', 'Type']
511/47:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=100),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test)
    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return rmse
511/48:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
511/49:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
507/70:
filename = 'random_forest_reg.sav'
pickle.dump(rf_model, open(filename, 'wb'))
507/71: mean_squared_error(y_test, rf_model.predict(X_test), squared=False)
507/72: X_train
507/73: X_train
507/74: rf_model.predict(df_val)
507/75: df_val
507/76:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['target'], axis=1),  # drop the target
    df_train['target'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
507/77:
X_train.drop('id', axis=1, inplace=True)
X_test.drop('id', axis=1, inplace=True)
507/78: X_train
507/79:
rf_model = RandomForestRegressor(max_depth=10, n_estimators=750)
rf_model.fit(X_train,y_train)
511/50:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
511/51:
y_pred = best_model.predict(X_test)
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
511/52:
rmse = mean_squared_error(y_test, y_pred_scale, squared=False)
print('RMSE: ', rmse)
511/53:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
df_val = transform_date(df_val, own_date)
df_val.drop(['Id', 'City'], axis=1, inplace=True)
511/54: y_train
511/55:
y_val = best_model.predict(df_val[X_train.columns])
#y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
511/56: y_val
511/57:
y_val = best_model.predict(df_val[X_train.columns])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
511/58: y_val_scale
511/59: df_t.describe()
511/60:
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("catboost_preproc.csv", index=False)
511/61: df_t.describe()
511/62: best_model.save_model('catboost_preproc')
511/63: X_train
511/64: categorical_features_names
511/65: X_train[categorical_features_names]
511/66: sns.boxplot(data=X_train, x="City Group", y=y_train)
511/67: best_model.feature_importances_
511/68:
df_features_importance = pd.DataFrame(
    {'feature_names': best_model.feature_names_,
     'feature_importances': best_model.feature_importances_
    })
511/69: df_features_importance.sort_values('feature_importances', ascending=False)
511/70: df_features_importance.sort_values('feature_importances', ascending=False)[:10]
511/71:
fig, axes = plt.subplots(1, 2, figsize=(10, 5), sharey=True)
fig.suptitle('Initial Pokemon - 1st Generation')

# Bulbasaur
sns.boxplot(ax=axes[0], data=X_train, x="City Group", y=y_train)
axes[0].set_title('X train')

# Charmander
sns.barplot(ax=axes[0], data=X_train, x="City Group", y=y_train)
axes[1].set_title('X test')
511/72:
fig, axes = plt.subplots(1, 2, figsize=(8, 3), sharey=True)
fig.suptitle('Initial Pokemon - 1st Generation')

# Bulbasaur
sns.boxplot(ax=axes[0], data=X_train, x="City Group", y=y_train)
axes[0].set_title('X train')

# Charmander
sns.barplot(ax=axes[1], data=X_test, x="City Group", y=y_train)
axes[1].set_title('X test')
511/73:
fig, axes = plt.subplots(1, 2, figsize=(8, 3), sharey=True)
fig.suptitle('Initial Pokemon - 1st Generation')

# Bulbasaur
sns.boxplot(ax=axes[0], data=X_train, x="City Group", y=y_train)
axes[0].set_title('X train')

# Charmander
sns.barplot(ax=axes[1], data=X_test, x="City Group", y=y_test)
axes[1].set_title('X test')
511/74: X_train
511/75:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.3,
    stratify=Meta[['City Group', 'Type']])

X_train.shape, X_test.shape
511/76:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.3,
    stratify=df_train[['City Group', 'Type']])

X_train.shape, X_test.shape
511/77:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.3,
    stratify=df_train[['City Group']])

X_train.shape, X_test.shape
511/78:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
    stratify=df_train[['City Group']])

X_train.shape, X_test.shape
511/79: X_train
511/80:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
    stratify=df_train[['Type']])

X_train.shape, X_test.shape
511/81:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
    stratify=df_train[['City Group']])

X_train.shape, X_test.shape
511/82: X_train
511/83: df_train
511/84: df_train.Type.value_counts()
511/85: df_train[df_train.Type == 'DT']
511/86: df_val
511/87: df_val.Type.value_counts()
511/88: df_train.Type.value_counts()
511/89: df_train.head(1)
511/90:
dict_type = {'FC': 1, 'IL': 2, 'DT': 2, 'MB': 2}
df_train['Type'] = df_train['Type'].map(dict_type)
511/91:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
    stratify=df_train[['City Group', 'Type']])

X_train.shape, X_test.shape
511/92: df_val.Type.value_counts()
511/93:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
511/94:
plt.figure(figsize=(15,15))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
511/95:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
511/96:
dict_type = {'FC': 1, 'IL': 2, 'DT': 2, 'MB': 2}
df_train['Type'] = df_train['Type'].map(dict_type)
511/97:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
    stratify=df_train[['City Group', 'Type']])

X_train.shape, X_test.shape
511/98:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
511/99:
plt.figure(figsize=(15,15))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
511/100:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
511/101: df_train
511/102: df_train.City.value_counts()
511/103: df_val.City.value_counts()
511/104:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
511/105:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
511/106:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
511/107:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
511/108:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
511/109: plt.hist(y_train_scale)
511/110: categorical_features_names = ['City Group', 'Type']
511/111:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=100),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test)
    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return rmse
511/112:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
511/113:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
511/114:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
511/115: best_model.save_model('catboost_preproc')
511/116:
df_features_importance = pd.DataFrame(
    {'feature_names': best_model.feature_names_,
     'feature_importances': best_model.feature_importances_
    })
511/117: df_features_importance.sort_values('feature_importances', ascending=False)[:10]
511/118:
y_pred = best_model.predict(X_test)
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
511/119:
rmse = mean_squared_error(y_test, y_pred_scale, squared=False)
print('RMSE: ', rmse)
511/120:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
df_val = transform_date(df_val, own_date)
df_val.drop(['Id', 'City'], axis=1, inplace=True)
511/121:
y_val = best_model.predict(df_val[X_train.columns])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
511/122:
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("catboost_preproc.csv", index=False)
#Score: 1809394.10357
#Private score: 1854047.39565
511/123: df_t.describe()
511/124:
dict_type = {'FC': 1, 'IL': 2, 'DT': 2, 'MB': 2}
df_train['Type'] = df_train['Type'].map(dict_type)
df_val['Type'] = df_val['Type'].map(dict_type)
511/125:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
511/126:
dict_type = {'FC': 1, 'IL': 2, 'DT': 2, 'MB': 2}
df_train['Type'] = df_train['Type'].map(dict_type)
df_val['Type'] = df_val['Type'].map(dict_type)
511/127:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
    stratify=df_train[['City Group', 'Type']])

X_train.shape, X_test.shape
511/128:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
511/129:
plt.figure(figsize=(15,15))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
511/130:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
511/131:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
511/132:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
511/133:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
511/134:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
511/135:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
511/136: plt.hist(y_train_scale)
511/137: categorical_features_names = ['City Group', 'Type']
511/138:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=100),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test)
    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return rmse
511/139:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
511/140:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
511/141:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
511/142: best_model.save_model('catboost_preproc')
511/143:
df_features_importance = pd.DataFrame(
    {'feature_names': best_model.feature_names_,
     'feature_importances': best_model.feature_importances_
    })
511/144: df_features_importance.sort_values('feature_importances', ascending=False)[:10]
511/145:
y_pred = best_model.predict(X_test)
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
511/146:
rmse = mean_squared_error(y_test, y_pred_scale, squared=False)
print('RMSE: ', rmse)
511/147:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
df_val = transform_date(df_val, own_date)
df_val.drop(['Id', 'City'], axis=1, inplace=True)
df_val['Type'] = df_val['Type'].map(dict_type)
511/148: df_val
511/149:
y_val = best_model.predict(df_val[X_train.columns])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
511/150:
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("catboost_preproc.csv", index=False)
#Score: 1809394.10357
#Private score: 1854047.39565
511/151: df_t.describe()
511/152:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
511/153:
dict_type = {'FC': 1, 'IL': 2, 'DT': 2, 'MB': 2}
df_train['Type'] = df_train['Type'].map(dict_type)
511/154:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
    stratify=df_train[['City Group']])

X_train.shape, X_test.shape
511/155:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
511/156:
plt.figure(figsize=(15,15))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
511/157:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
511/158:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
511/159:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
511/160:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
511/161:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
511/162:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
511/163: plt.hist(y_train_scale)
511/164: categorical_features_names = ['City Group', 'Type']
511/165:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=100),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test)
    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return rmse
511/166:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
511/167:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
511/168:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
511/169: best_model.save_model('catboost_preproc')
511/170:
df_features_importance = pd.DataFrame(
    {'feature_names': best_model.feature_names_,
     'feature_importances': best_model.feature_importances_
    })
511/171: df_features_importance.sort_values('feature_importances', ascending=False)[:10]
511/172:
y_pred = best_model.predict(X_test)
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
511/173:
rmse = mean_squared_error(y_test, y_pred_scale, squared=False)
print('RMSE: ', rmse)
511/174:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
df_val = transform_date(df_val, own_date)
df_val.drop(['Id', 'City'], axis=1, inplace=True)
df_val['Type'] = df_val['Type'].map(dict_type)
511/175:
y_val = best_model.predict(df_val[X_train.columns])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
511/176:
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("catboost_preproc.csv", index=False)
#Score: 1809394.10357
#Private score: 1854047.39565
511/177: df_train
511/178: df_train['City Group'].value_counts()
511/179: df_train
511/180:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
    stratify=df_train[['City Group', 'Type']])

X_train.shape, X_test.shape
511/181:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
511/182:
plt.figure(figsize=(15,15))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
511/183:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
511/184:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
511/185:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
511/186:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
511/187:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
511/188:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
511/189: plt.hist(y_train_scale)
511/190: categorical_features_names = ['City Group', 'Type']
511/191:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=100),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test)
    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return rmse
511/192: X_train
511/193:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
511/194:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
507/80: y_val = rf_model.predict()
507/81: df_val
507/82: X_train
507/83: y_val = rf_model.predict(df_val)
507/84: y_val
507/85:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['id']]
df_val.drop('id', axis=1, inplace=True)
df_t['target'] = random_forest_cv.best_estimator_.predict(df_val)
df_t.to_csv("rf_1.csv", index=False)
507/86:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['id']]
df_val.drop('id', axis=1, inplace=True)
df_t['target'] = y_val
df_t.to_csv("rf_1.csv", index=False)
507/87:
df_t = df_val[['id']]
df_val.drop('id', axis=1, inplace=True)
df_t.insert(1, "target", y_val.tolist(), True)
df_t.to_csv("catboost_reg_1.csv", index=False)
507/88:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['id']]
df_val.drop('id', axis=1, inplace=True)
df_t.insert(1, "target", y_val.tolist(), True)
df_t.to_csv("catboost_reg_1.csv", index=False)
507/89: df_t.describe()
507/90:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['id']]
df_val.drop('id', axis=1, inplace=True)
df_t.insert(1, "target", y_val.tolist(), True)
df_t.to_csv("random_forest.csv", index=False)
511/195:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
511/196: best_model.save_model('catboost_preproc')
511/197:
df_features_importance = pd.DataFrame(
    {'feature_names': best_model.feature_names_,
     'feature_importances': best_model.feature_importances_
    })
511/198: df_features_importance.sort_values('feature_importances', ascending=False)[:10]
511/199:
y_pred = best_model.predict(X_test)
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
511/200:
rmse = mean_squared_error(y_test, y_pred_scale, squared=False)
print('RMSE: ', rmse)
511/201:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
df_val = transform_date(df_val, own_date)
df_val.drop(['Id', 'City'], axis=1, inplace=True)
df_val['Type'] = df_val['Type'].map(dict_type)
511/202: y_val
511/203: df_val
511/204: df_val
511/205:
y_val = best_model.predict(df_val[X_train.columns])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
511/206:
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("catboost_preproc.csv", index=False)
#Score: 1809394.10357
#Private score: 1854047.39565
511/207: df_t.describe()
511/208:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
511/209:
dict_type = {'FC': 1, 'IL': 2, 'DT': 2, 'MB': 2}
df_train['Type'] = df_train['Type'].map(dict_type)
511/210:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
 #   stratify=df_train[['City Group', 'Type']]
)

X_train.shape, X_test.shape
511/211:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
511/212:
plt.figure(figsize=(15,15))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
511/213:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
511/214:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
511/215:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
511/216:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
511/217:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
511/218:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
511/219: plt.hist(y_train_scale)
511/220: categorical_features_names = ['City Group', 'Type']
511/221:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 10000, 25000, step=100),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test)
    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return rmse
511/222:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
511/223:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
507/91: df_t.describe()
513/1:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
513/2:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler

from catboost import CatBoost, CatBoostRegressor, Pool
from optuna.integration import CatBoostPruningCallback
import optuna
from sklearn.metrics import mean_squared_error
513/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

##df_train_original = pd.read_csv('data/train.csv')

print(df_train.shape)
print(df_val.shape)
513/4: df_train
513/5:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
513/6: X_train.head(5)
513/7: X_train.info()
513/8: own_date = date(2015, 1, 1)
513/9:
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
513/10:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
513/11: categorical_features_names = ['City', 'City Group', 'Type']
513/12:
X_train.drop(['Id'], axis=1, inplace=True)
X_test.drop(['Id'], axis=1, inplace=True)
513/13:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
513/14:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
513/15: plt.hist(y_train_scale)
513/16:
train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
513/17:
model = CatBoostRegressor(custom_metric= ['R2', 'RMSE'], learning_rate=0.1, n_estimators=1000)
model.fit(train_pool, eval_set=test_pool, verbose=2000, plot=True)
513/18: model.best_score_
513/19: model.best_iteration_
513/20: #####################
513/21:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.4, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 200, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test)
    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return rmse
513/22:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
513/23:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
513/24:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
513/25: y_pred = best_model.predict(X_test)
513/26:
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
#np.exp(y_pred)
513/27:
rmse = mean_squared_error(y_test, y_pred_scale, squared=False)
print('RMSE: ', rmse)
513/28:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
513/29:
df_val = transform_date(df_val, own_date)
df_val.drop(['Id'], axis=1, inplace=True)
513/30:
y_val = best_model.predict(df_val)
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
513/31:
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("catboost_1.csv", index=False)
513/32: df_t.describe()
513/33: best_model.save_model('catboost_model')
513/34:
#Score: 1879155.85679
#Public score: 1929288.10434
## with target scaling, minmax
#Score: 1847132.03702
#Public score: 1866521.17688
## with target log
#Score: 1911926.31450
#Public score: 1872417.51804
## StandartScale
#Score: 1853802.48131
#Public score: 1815061.21828
513/35:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
513/36:
dict_type = {'FC': 1, 'IL': 2, 'DT': 3, 'MB': 3}
df_train['Type'] = df_train['Type'].map(dict_type)
513/37:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
 #   stratify=df_train[['City Group', 'Type']]
)

X_train.shape, X_test.shape
513/38:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
513/39:
plt.figure(figsize=(15,15))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
513/40:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
513/41:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
513/42:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
513/43:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
513/44:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
513/45:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
513/46: plt.hist(y_train_scale)
513/47: categorical_features_names = ['City Group', 'Type']
513/48:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 5000, 25000, step=100),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.5, 1, step=0.1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 1),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
    #    'max_bin': trial.suggest_int('max_bin', 100, 400),
        'reg_lambda': trial.suggest_loguniform("reg_lambda", 1e-8, 100.0),
        'random_strength': trial.suggest_uniform('random_strength',10,50),
        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test)
    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return rmse
513/49:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
513/50:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
513/51:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 5000, 25000, step=100),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.5, 1, step=0.1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 1),
    #    'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
    #    'max_bin': trial.suggest_int('max_bin', 100, 400),
        'reg_lambda': trial.suggest_loguniform("reg_lambda", 1e-8, 100.0),
        'random_strength': trial.suggest_uniform('random_strength',10,50),
        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test)
    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return rmse
513/52:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
513/53:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
513/54:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
513/55: best_model.save_model('catboost_preproc')
513/56:
df_features_importance = pd.DataFrame(
    {'feature_names': best_model.feature_names_,
     'feature_importances': best_model.feature_importances_
    })
513/57: df_features_importance.sort_values('feature_importances', ascending=False)[:10]
513/58:
y_pred = best_model.predict(X_test)
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
513/59:
rmse = mean_squared_error(y_test, y_pred_scale, squared=False)
print('RMSE: ', rmse)
513/60:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
df_val = transform_date(df_val, own_date)
df_val.drop(['Id', 'City'], axis=1, inplace=True)
df_val['Type'] = df_val['Type'].map(dict_type)
513/61:
y_val = best_model.predict(df_val[X_train.columns])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
513/62:
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("catboost_preproc.csv", index=False)
#Score: 1809394.10357
#Private score: 1854047.39565
513/63: df_t.describe()
513/64:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 5000, 25000, step=100),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.5, 1, step=0.1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 1),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 100, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test)
    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return rmse
513/65:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
513/66:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
513/67:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
513/68:
dict_type = {'FC': 1, 'IL': 2, 'DT': 2, 'MB': 2}
df_train['Type'] = df_train['Type'].map(dict_type)
513/69:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
 #   stratify=df_train[['City Group', 'Type']]
)

X_train.shape, X_test.shape
513/70:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
513/71:
plt.figure(figsize=(15,15))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
513/72:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
513/73:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
513/74:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
513/75:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
513/76:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
513/77:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
513/78: plt.hist(y_train_scale)
513/79: categorical_features_names = ['City Group', 'Type']
513/80:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 5000, 25000, step=100),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.5, 1, step=0.1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 1),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 100, 400),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test)
    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return rmse
513/81:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
513/82:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
514/1:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
514/2:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler

from catboost import CatBoost, CatBoostRegressor, Pool
from optuna.integration import CatBoostPruningCallback
import optuna
from sklearn.metrics import mean_squared_error
514/3:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler

from catboost import CatBoost, CatBoostRegressor, Pool
from optuna.integration import CatBoostPruningCallback
import optuna
from sklearn.metrics import mean_squared_error
514/4:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

##df_train_original = pd.read_csv('data/train.csv')

print(df_train.shape)
print(df_val.shape)
514/5: df_train.head(5)
514/6:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
514/7: df_train.info()
514/8:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
514/9: own_date = date(2015, 1, 1)
514/10:
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
514/11:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
514/12: categorical_features_names = ['City', 'City Group', 'Type']
514/13:
X_train.drop(['Id'], axis=1, inplace=True)
X_test.drop(['Id'], axis=1, inplace=True)
514/14:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
514/15:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
514/16: plt.hist(y_train_scale)
514/17:
train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
514/18:
model = CatBoostRegressor(custom_metric= ['R2', 'RMSE'], learning_rate=0.1, n_estimators=1000)
model.fit(train_pool, eval_set=test_pool, verbose=2000, plot=True)
514/19: model.best_score_
514/20: model.best_iteration_
514/21: #There is a very bad result on the validation set. One of the main problems with this dataset is that it's too small and the model overfits very fast.
514/22: #####################
514/23:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=100),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.5, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 1, 64000),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test)
    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return rmse
514/24:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
514/25:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
514/26:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
514/27: y_pred = best_model.predict(X_test)
514/28:
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
#np.exp(y_pred)
514/29:
rmse = mean_squared_error(y_test, y_pred_scale, squared=False)
print('RMSE: ', rmse)
514/30:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
514/31:
df_val = transform_date(df_val, own_date)
df_val.drop(['Id'], axis=1, inplace=True)
514/32:
y_val = best_model.predict(df_val)
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
514/33:
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("catboost_1.csv", index=False)
514/34: df_t.describe()
514/35: best_model.save_model('catboost_model')
514/36:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
514/37:
from sklearn.model_selection import LeaveOneOut
from sklearn.model_selection import cross_val_score
514/38:
#dict_type = {'FC': 1, 'IL': 2, 'DT': 2, 'MB': 2}
#df_train['Type'] = df_train['Type'].map(dict_type)
514/39:
#X_train, X_test, y_train, y_test = train_test_split(
#    df_train.drop(labels=['revenue'], axis=1),  # drop the target
#    df_train['revenue'],  # just the target
#    test_size=0.2,
# #   stratify=df_train[['City Group', 'Type']]
#)
#
#X_train.shape, X_test.shape
514/40:
#X_train = transform_date(X_train, own_date)
#X_test = transform_date(X_test, own_date)
514/41: df_train
514/42: df_train_trans = transform_date(df_train, own_date)
514/43:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
 #   stratify=df_train[['City Group', 'Type']]
)

X_train.shape, X_test.shape
514/44:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
514/45:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
514/46:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
 #   stratify=df_train[['City Group', 'Type']]
)

X_train.shape, X_test.shape
514/47:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
514/48:
plt.figure(figsize=(15,15))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
514/49:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
514/50:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
514/51:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
514/52:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
514/53:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
514/54:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
514/55: plt.hist(y_train_scale)
514/56: categorical_features_names = ['City Group', 'Type']
514/57: X_train
514/58: X_train.columns
514/59: selected_columns = X_train.columns
514/60: selected_columns
514/61:
def objective(trial):
#    global cbr
#    train_pool = Pool(X_train, 
#                  label=y_train_scale,
#                  cat_features=categorical_features_names)
#    test_pool = Pool(X_test,
#                 label=y_test_scale,
#                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=100),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.5, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 1, 64000),
        'eval_metric': 'RMSE'
    }
    
    cv_dataset = Pool(data=df_train_trans,
                      label=y_train_scale,
                      cat_features=[0, 1]
                     )
    
#    cbr = CatBoostRegressor(**param)

#    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    scores = cv(cv_dataset,
            params,
            fold_count=5)
    
 #   cbr.fit(
 #       train_pool, eval_set=test_pool,
 #       verbose=1,
 #       early_stopping_rounds=100,
 #       callbacks=[pruning_callback],
    )
#    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
#    pruning_callback.check_pruned()
    
#    y_pred = cbr.predict(X_test)
#    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return mean(scores)
514/62:
def objective(trial):
#    global cbr
#    train_pool = Pool(X_train, 
#                  label=y_train_scale,
#                  cat_features=categorical_features_names)
#    test_pool = Pool(X_test,
#                 label=y_test_scale,
#                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=100),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.5, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 1, 64000),
        'eval_metric': 'RMSE'
    }
    
    cv_dataset = Pool(data=df_train_trans,
                      label=y_train_scale,
                      cat_features=[0, 1])
    
#    cbr = CatBoostRegressor(**param)

#    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    scores = cv(cv_dataset,
            params,
            fold_count=5)
    
 #   cbr.fit(
 #       train_pool, eval_set=test_pool,
 #       verbose=1,
 #       early_stopping_rounds=100,
 #       callbacks=[pruning_callback],
    )
#    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
#    pruning_callback.check_pruned()
    
#    y_pred = cbr.predict(X_test)
#    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return mean(scores)
514/63:
def objective(trial):
#    global cbr
#    train_pool = Pool(X_train, 
#                  label=y_train_scale,
#                  cat_features=categorical_features_names)
#    test_pool = Pool(X_test,
#                 label=y_test_scale,
#                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=100),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.5, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 1, 64000),
        'eval_metric': 'RMSE'
    }
    
    cv_dataset = Pool(data=df_train_trans,
                      label=y_train_scale,
                      cat_features=[0, 1])
    
#    cbr = CatBoostRegressor(**param)

#    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    scores = cv(cv_dataset,
            params,
            fold_count=5)
    
 #   cbr.fit(
 #       train_pool, eval_set=test_pool,
 #       verbose=1,
 #       early_stopping_rounds=100,
 #       callbacks=[pruning_callback],
 #   )
#    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
#    pruning_callback.check_pruned()
    
#    y_pred = cbr.predict(X_test)
#    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return mean(scores)
514/64:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600)
514/65: df_train_trans
514/66:
def objective(trial):
#    global cbr
#    train_pool = Pool(X_train, 
#                  label=y_train_scale,
#                  cat_features=categorical_features_names)
#    test_pool = Pool(X_test,
#                 label=y_test_scale,
#                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=100),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.5, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 1, 64000),
        'eval_metric': 'RMSE'
    }
    
    cv_dataset = Pool(data=df_train_trans[selected_columns],
                      label=df_train_trans.revenue,
                      cat_features=[0, 1])
    
#    cbr = CatBoostRegressor(**param)

#    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    scores = cv(cv_dataset,
            params,
            fold_count=5)
    
 #   cbr.fit(
 #       train_pool, eval_set=test_pool,
 #       verbose=1,
 #       early_stopping_rounds=100,
 #       callbacks=[pruning_callback],
 #   )
#    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
#    pruning_callback.check_pruned()
    
#    y_pred = cbr.predict(X_test)
#    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return mean(scores)
514/67:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
514/68:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600)
514/69:
from sklearn.model_selection import LeaveOneOut
from sklearn.model_selection import cross_val_score
from catboost import Pool, cv
514/70:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600)
514/71:
def objective(trial):
#    global cbr
#    train_pool = Pool(X_train, 
#                  label=y_train_scale,
#                  cat_features=categorical_features_names)
#    test_pool = Pool(X_test,
#                 label=y_test_scale,
#                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=100),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.5, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 1, 64000),
        'eval_metric': 'RMSE'
    }
    
    cv_dataset = Pool(data=df_train_trans[selected_columns],
                      label=df_train_trans.revenue,
                      cat_features=[0, 1])
    
#    cbr = CatBoostRegressor(**param)

#    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    scores = cv(cv_dataset,
            param,
            fold_count=5)
    
 #   cbr.fit(
 #       train_pool, eval_set=test_pool,
 #       verbose=1,
 #       early_stopping_rounds=100,
 #       callbacks=[pruning_callback],
 #   )
#    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
#    pruning_callback.check_pruned()
    
#    y_pred = cbr.predict(X_test)
#    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return mean(scores)
514/72:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
514/73:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600)
514/74:
def objective(trial):
#    global cbr
#    train_pool = Pool(X_train, 
#                  label=y_train_scale,
#                  cat_features=categorical_features_names)
#    test_pool = Pool(X_test,
#                 label=y_test_scale,
#                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=100),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.5, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 1, 64000),
        'loss_function': 'RMSE'
    }
    
    cv_dataset = Pool(data=df_train_trans[selected_columns],
                      label=df_train_trans.revenue,
                      cat_features=[0, 1])
    
#    cbr = CatBoostRegressor(**param)

#    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    scores = cv(cv_dataset,
            param,
            fold_count=5)
    
 #   cbr.fit(
 #       train_pool, eval_set=test_pool,
 #       verbose=1,
 #       early_stopping_rounds=100,
 #       callbacks=[pruning_callback],
 #   )
#    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
#    pruning_callback.check_pruned()
    
#    y_pred = cbr.predict(X_test)
#    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return mean(scores)
514/75:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
514/76:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600)
514/77:
def objective(trial):
    global cbr
#    train_pool = Pool(X_train, 
#                  label=y_train_scale,
#                  cat_features=categorical_features_names)
#    test_pool = Pool(X_test,
#                 label=y_test_scale,
#                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 1000, 25000, step=100),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.5, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 1, 64000),
        'loss_function': 'RMSE'
    }
    
    cv_dataset = Pool(data=df_train_trans[selected_columns],
                      label=df_train_trans.revenue,
                      cat_features=[0, 1])
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    scores = cv(cv_dataset,
            param,
            fold_count=5)
    
 #   cbr.fit(
 #       train_pool, eval_set=test_pool,
 #       verbose=1,
 #       early_stopping_rounds=100,
 #       callbacks=[pruning_callback],
 #   )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
#    y_pred = cbr.predict(X_test)
#    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return mean(scores)
514/78:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
514/79:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
514/80:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
514/81:
def objective(trial):
    global cbr
#    train_pool = Pool(X_train, 
#                  label=y_train_scale,
#                  cat_features=categorical_features_names)
#    test_pool = Pool(X_test,
#                 label=y_test_scale,
#                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 100, 1500, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.5, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 1, 32000),
        'loss_function': 'RMSE'
    }
    
    cv_dataset = Pool(data=df_train_trans[selected_columns],
                      label=df_train_trans.revenue,
                      cat_features=[0, 1])
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    scores = cv(cv_dataset,
            param,
            fold_count=5)
    
 #   cbr.fit(
 #       train_pool, eval_set=test_pool,
 #       verbose=1,
 #       early_stopping_rounds=100,
 #       callbacks=[pruning_callback],
 #   )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
#    y_pred = cbr.predict(X_test)
#    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return mean(scores)
514/82:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
514/83:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=300, callbacks=[callback])
514/84:
def objective(trial):
    global cbr
#    train_pool = Pool(X_train, 
#                  label=y_train_scale,
#                  cat_features=categorical_features_names)
#    test_pool = Pool(X_test,
#                 label=y_test_scale,
#                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 100, 1500, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.5, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 1, 32000),
        'loss_function': 'RMSE'
    }
    
    cv_dataset = Pool(data=df_train_trans[selected_columns],
                      label=df_train_trans.revenue,
                      cat_features=[0, 1])
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    scores = cv(cv_dataset,
            param,
            fold_count=5)
    
 #   cbr.fit(
 #       train_pool, eval_set=test_pool,
 #       verbose=1,
 #       early_stopping_rounds=100,
 #       callbacks=[pruning_callback],
 #   )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
#    y_pred = cbr.predict(X_test)
#    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return np.mean(scores)
514/85:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=300, callbacks=[callback])
514/86:
def objective(trial):
    global cbr
#    train_pool = Pool(X_train, 
#                  label=y_train_scale,
#                  cat_features=categorical_features_names)
#    test_pool = Pool(X_test,
#                 label=y_test_scale,
#                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 10, 150, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.5, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 1, 32000),
        'loss_function': 'RMSE'
    }
    
    cv_dataset = Pool(data=df_train_trans[selected_columns],
                      label=df_train_trans.revenue,
                      cat_features=[0, 1])
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    scores = cv(cv_dataset,
            param,
            fold_count=5)
    
 #   cbr.fit(
 #       train_pool, eval_set=test_pool,
 #       verbose=1,
 #       early_stopping_rounds=100,
 #       callbacks=[pruning_callback],
 #   )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
#    y_pred = cbr.predict(X_test)
#    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return scores['test-RMSE-mean']
514/87: np.mean
514/88:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
514/89:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=300, callbacks=[callback])
514/90:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=1, timeout=300, callbacks=[callback])
514/91:
from sklearn.model_selection import LeaveOneOut
from sklearn.model_selection import cross_val_score
from catboost import Pool, cv
514/92:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
514/93:
#dict_type = {'FC': 1, 'IL': 2, 'DT': 2, 'MB': 2}
#df_train['Type'] = df_train['Type'].map(dict_type)
514/94:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
 #   stratify=df_train[['City Group', 'Type']]
)

X_train.shape, X_test.shape
514/95:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
514/96: df_train_trans = transform_date(df_train, own_date)
514/97:
plt.figure(figsize=(15,15))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
514/98:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
514/99:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
514/100:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
514/101:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
514/102:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
514/103:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
514/104: plt.hist(y_train_scale)
514/105: categorical_features_names = ['City Group', 'Type']
514/106: selected_columns = X_train.columns
514/107:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
    
    param = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 50, 2500, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.5, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 1, 64000),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test)
    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return rmse
514/108:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
514/109:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
514/110:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
514/111:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
    
    param = {
      #  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 50, 2500, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.5, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 1, 64000),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test)
    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return rmse
514/112:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
514/113:
study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
514/114:
best_model=study.user_attrs["best_booster"]
print(best_model.get_params())
514/115: best_model.save_model('catboost_preproc')
514/116:
df_features_importance = pd.DataFrame(
    {'feature_names': best_model.feature_names_,
     'feature_importances': best_model.feature_importances_
    })
514/117: df_features_importance.sort_values('feature_importances', ascending=False)[:10]
514/118:
y_pred = best_model.predict(X_test)
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
514/119:
rmse = mean_squared_error(y_test, y_pred_scale, squared=False)
print('RMSE: ', rmse)
514/120:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
df_val = transform_date(df_val, own_date)
df_val.drop(['Id', 'City'], axis=1, inplace=True)
df_val['Type'] = df_val['Type'].map(dict_type)
514/121:
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("catboost_preproc.csv", index=False)
#Score: 1809394.10357
#Private score: 1854047.39565
514/122:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
df_val = transform_date(df_val, own_date)
df_val.drop(['Id', 'City'], axis=1, inplace=True)
df_val['Type'] = transform_date(df_val)
514/123:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
df_val = transform_date(df_val, own_date)
df_val.drop(['Id', 'City'], axis=1, inplace=True)
df_val['Type'] = transform_date(df_val, own_date)
514/124:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
df_val = transform_date(df_val, own_date)
df_val.drop(['Id', 'City'], axis=1, inplace=True)
#df_val['Type'] = df_val['Type'].map(dict_type)
514/125:
y_val = best_model.predict(df_val[X_train.columns])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
514/126:
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("catboost_preproc.csv", index=False)
#Score: 1809394.10357
#Private score: 1854047.39565
514/127: df_t.describe()
514/128: ### So, the next idea was to use cross-val because O have a very small dataset
514/129: X_train
514/130: cv = LeaveOneOut()
514/131:
cat_features = [0,1]
params = {'max_depth': [8, 10, 12, 14],
          'n_estimators': [70, 100, 120, 150]
          'l2_leaf_reg': [4.15],
          'subsample': [0.7, 0.8, 0.85],
          'colsample_bylevel': [0.55, 0.62, 0.67],
          'max_bin': [35000, 43000, 50000]
         }
cb = CatBoostRegressor(loss_function='RMSE', iterations = 200, learning_rate=0.03)
cb_model = GridSearchCV(cb, params, scoring='neg_mean_squared_error', cv = cv)
cb_model.fit(X_train, Y_train, cat_features)
514/132:
cat_features = [0,1]
params = {'max_depth': [8, 10, 12, 14],
          'n_estimators': [70, 100, 120, 150],
          'l2_leaf_reg': [4.15],
          'subsample': [0.7, 0.8, 0.85],
          'colsample_bylevel': [0.55, 0.62, 0.67],
          'max_bin': [35000, 43000, 50000]
         }
cb = CatBoostRegressor(loss_function='RMSE', iterations = 200, learning_rate=0.03)
cb_model = GridSearchCV(cb, params, scoring='neg_mean_squared_error', cv = cv)
cb_model.fit(X_train, Y_train, cat_features)
514/133:
from sklearn.model_selection import LeaveOneOut
from sklearn.model_selection import GridSearchCV
from catboost import Pool, cv
514/134:
cat_features = [0,1]
params = {'max_depth': [8, 10, 12, 14],
          'n_estimators': [70, 100, 120, 150],
          'l2_leaf_reg': [4.15],
          'subsample': [0.7, 0.8, 0.85],
          'colsample_bylevel': [0.55, 0.62, 0.67],
          'max_bin': [35000, 43000, 50000]
         }
cb = CatBoostRegressor(loss_function='RMSE', iterations = 200, learning_rate=0.03)
cb_model = GridSearchCV(cb, params, scoring='neg_mean_squared_error', cv = cv)
cb_model.fit(X_train, Y_train, cat_features)
514/135:
cat_features = [0,1]
params = {'max_depth': [8, 10, 12, 14],
          'n_estimators': [70, 100, 120, 150],
          'l2_leaf_reg': [4.15],
          'subsample': [0.7, 0.8, 0.85],
          'colsample_bylevel': [0.55, 0.62, 0.67],
          'max_bin': [35000, 43000, 50000]
         }
cb = CatBoostRegressor(loss_function='RMSE', iterations = 200, learning_rate=0.03)
cb_model = GridSearchCV(cb, params, scoring='neg_mean_squared_error', cv = cv)
cb_model.fit(X_train, y_train_scale, cat_features)
514/136:
cat_features = [0,1]
params = {'max_depth': [8, 10, 12, 14],
          'n_estimators': [70, 100, 120, 150],
          'l2_leaf_reg': [4.15],
          'subsample': [0.7, 0.8, 0.85],
          'colsample_bylevel': [0.55, 0.62, 0.67],
          'max_bin': [35000, 43000, 50000]
         }
cb = CatBoostRegressor(loss_function='RMSE', iterations = 200, learning_rate=0.03)
cb_model = GridSearchCV(cb, params, scoring='neg_mean_squared_error', cv = cv)
cb_model.fit(X_train, y_train_scale, cat_features = cat_features)
514/137: own_cv = LeaveOneOut()
514/138:
cat_features = [0,1]
params = {'max_depth': [8, 10, 12, 14],
          'n_estimators': [70, 100, 120, 150],
          'l2_leaf_reg': [4.15],
          'subsample': [0.7, 0.8, 0.85],
          'colsample_bylevel': [0.55, 0.62, 0.67],
          'max_bin': [35000, 43000, 50000]
         }
cb = CatBoostRegressor(loss_function='RMSE', iterations = 200, learning_rate=0.03)
cb_model = GridSearchCV(cb, params, scoring='neg_mean_squared_error', cv = own_cv)
cb_model.fit(X_train, y_train_scale, cat_features = cat_features)
514/139:
cat_features = [0,1]
params = {'max_depth': [8, 10, 12, 14],
          'n_estimators': [70, 100, 120, 150],
          'l2_leaf_reg': [4.15],
          'subsample': [0.7, 0.8, 0.85],
          'colsample_bylevel': [0.55, 0.62, 0.67],
          'max_bin': [35000, 43000, 50000]
         }
cb = CatBoostRegressor(loss_function='RMSE', learning_rate=0.03)
cb_model = GridSearchCV(cb, params, scoring='neg_mean_squared_error', cv = own_cv)
cb_model.fit(X_train, y_train_scale, cat_features = cat_features)
514/140:
from sklearn.model_selection import LeaveOneOut
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from catboost import Pool, cv
514/141:
cat_features = [0,1]
params = {'max_depth': [8, 10, 12, 14],
          'n_estimators': [70, 100, 120, 150],
          'l2_leaf_reg': [4.15],
          'subsample': [0.7, 0.8, 0.85],
          'colsample_bylevel': [0.55, 0.62, 0.67],
          'max_bin': [35000, 43000, 50000]
         }
cb = CatBoostRegressor(loss_function='RMSE', learning_rate=0.03)
cb_model = RandomizedSearchCV(cb, params, scoring='neg_mean_squared_error', cv = own_cv)
cb_model.fit(X_train, y_train_scale, cat_features = cat_features)
514/142: cb_model.best_estimator_
514/143: cb_model.best_params_
514/144: cb_model.best_score_
514/145: best_model.predict(X_test)
514/146:
y_pred = best_model.predict(X_test)
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
514/147: y_pred_scale
514/148:
rmse = mean_squared_error(y_test, y_pred_scale, squared=False)
print('RMSE: ', rmse)
514/149: best_model_cv = cb_model.best_estimator_
514/150:
y_pred = best_model_cv.predict(X_test)
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
514/151:
rmse = mean_squared_error(y_test, y_pred_scale, squared=False)
print('RMSE: ', rmse)
514/152:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
df_val = transform_date(df_val, own_date)
df_val.drop(['Id', 'City'], axis=1, inplace=True)
#df_val['Type'] = df_val['Type'].map(dict_type)
514/153:
y_val = best_model_cv.predict(df_val[X_train.columns])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
514/154:
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("catboost_preproc_cv.csv", index=False)
#Score: 1857535.40677
#Public score: 1823507.04070
514/155: df_t.describe()
514/156:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
 #   stratify=df_train[['City Group', 'Type']]
)

X_train.shape, X_test.shape
514/157:
cat_features = [0,1]
params = {'max_depth': [14],
          'n_estimators': [120, 150],
          'l2_leaf_reg': [4.15],
          'subsample': [0.7],
          'colsample_bylevel': [0.62],
          'max_bin': [43000, 50000]
         }
cb = CatBoostRegressor(loss_function='RMSE', learning_rate=0.03)
cb_model = RandomizedSearchCV(cb, params, scoring='neg_mean_squared_error', cv = own_cv)
cb_model.fit(X_train, y_train_scale, cat_features = cat_features)
514/158: X_train
514/159: X_train
514/160:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
 #   stratify=df_train[['City Group', 'Type']]
)

X_train.shape, X_test.shape
514/161:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
514/162:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
514/163:
#dict_type = {'FC': 1, 'IL': 2, 'DT': 2, 'MB': 2}
#df_train['Type'] = df_train['Type'].map(dict_type)
514/164:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
 #   stratify=df_train[['City Group', 'Type']]
)

X_train.shape, X_test.shape
514/165:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
514/166: df_train_trans = transform_date(df_train, own_date)
514/167:
plt.figure(figsize=(15,15))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
514/168:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
514/169:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
514/170:
cat_features = [0,1]
params = {'max_depth': [14],
          'n_estimators': [120, 150],
          'l2_leaf_reg': [4.15],
          'subsample': [0.7],
          'colsample_bylevel': [0.62],
          'max_bin': [43000, 50000]
         }
cb = CatBoostRegressor(loss_function='RMSE', learning_rate=0.03)
cb_model = RandomizedSearchCV(cb, params, scoring='neg_mean_squared_error', cv = own_cv)
cb_model.fit(X_train, y_train_scale, cat_features = cat_features)
514/171: X_train
514/172:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
514/173:
#dict_type = {'FC': 1, 'IL': 2, 'DT': 2, 'MB': 2}
#df_train['Type'] = df_train['Type'].map(dict_type)
514/174:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
 #   stratify=df_train[['City Group', 'Type']]
)

X_train.shape, X_test.shape
514/175:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
514/176: df_train_trans = transform_date(df_train, own_date)
514/177:
plt.figure(figsize=(15,15))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
514/178:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
514/179:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
514/180:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
514/181:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
514/182: plt.hist(y_train_scale)
514/183: X_train
514/184:
cat_features = [0,1]
params = {'max_depth': [14],
          'n_estimators': [120, 150],
          'l2_leaf_reg': [4.15],
          'subsample': [0.7],
          'colsample_bylevel': [0.62],
          'max_bin': [43000, 50000]
         }
cb = CatBoostRegressor(loss_function='RMSE', learning_rate=0.03)
cb_model = RandomizedSearchCV(cb, params, scoring='neg_mean_squared_error', cv = own_cv)
cb_model.fit(X_train, y_train_scale, cat_features = cat_features)
514/185: cb_model.best_params_
514/186: best_model_cv = cb_model.best_estimator_
514/187: cb_model.best_score_
514/188:
y_pred = best_model_cv.predict(X_test)
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
514/189:
rmse = mean_squared_error(y_test, y_pred_scale, squared=False)
print('RMSE: ', rmse)
514/190:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
df_val = transform_date(df_val, own_date)
df_val.drop(['Id', 'City'], axis=1, inplace=True)
#df_val['Type'] = df_val['Type'].map(dict_type)
514/191:
y_val = best_model_cv.predict(df_val[X_train.columns])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
514/192:
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("catboost_preproc_cv.csv", index=False)
#Score: 1857535.40677
#Public score: 1823507.04070
514/193:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
514/194:
#dict_type = {'FC': 1, 'IL': 2, 'DT': 2, 'MB': 2}
#df_train['Type'] = df_train['Type'].map(dict_type)
514/195:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
 #   stratify=df_train[['City Group', 'Type']]
)

X_train.shape, X_test.shape
514/196:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
514/197: df_train_trans = transform_date(df_train, own_date)
514/198:
plt.figure(figsize=(15,15))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
514/199:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
514/200:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
514/201:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
514/202: X_train
514/203:
from sklearn.model_selection import LeaveOneOut
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from catboost import Pool, cv
514/204:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
514/205:
#dict_type = {'FC': 1, 'IL': 2, 'DT': 2, 'MB': 2}
#df_train['Type'] = df_train['Type'].map(dict_type)
514/206:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
 #   stratify=df_train[['City Group', 'Type']]
)

X_train.shape, X_test.shape
514/207:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
514/208: df_train_trans = transform_date(df_train, own_date)
514/209:
plt.figure(figsize=(15,15))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
514/210:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
514/211:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
514/212:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
514/213:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
514/214:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
514/215:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
514/216: plt.hist(y_train_scale)
514/217: categorical_features_names = ['City Group', 'Type']
514/218: X_train
514/219:
cat_features = [0,1]
params = {'max_depth': [12, 14],
          'n_estimators': [120, 150, 200],
          'l2_leaf_reg': [4.15],
 #         'subsample': [0.7],
 #         'colsample_bylevel': [0.62],
 #         'max_bin': [43000]
         }
cb = CatBoostRegressor(loss_function='RMSE', learning_rate=0.03)
cb_model = RandomizedSearchCV(cb, params, scoring='neg_mean_squared_error', cv = own_cv)
cb_model.fit(X_train, y_train_scale, cat_features = cat_features)
514/220: cb_model.best_params_
514/221: best_model_cv = cb_model.best_estimator_
514/222: cb_model.best_score_
514/223:
y_pred = best_model_cv.predict(X_test)
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
514/224:
rmse = mean_squared_error(y_test, y_pred_scale, squared=False)
print('RMSE: ', rmse)
514/225:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
df_val = transform_date(df_val, own_date)
df_val.drop(['Id', 'City'], axis=1, inplace=True)
#df_val['Type'] = df_val['Type'].map(dict_type)
514/226:
y_val = best_model_cv.predict(df_val[X_train.columns])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
514/227:
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("catboost_preproc_cv.csv", index=False)
#Score: 1875729.69768
#Public score: 1759117.93052
514/228: X_train
514/229: best_model_cv
514/230: best_model_cv.save_model('catboost_tune_cv')
514/231:
my_model = CatBoostRegressor()      # parameters not required.
modelmy_model.load_model('catboost_tune_cv')
514/232:
my_model = CatBoostRegressor()      # parameters not required.
my_model.load_model('catboost_tune_cv')
515/1:
#study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
#study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
515/2:
#plt.figure(figsize=(15,15))
#sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":8})
##sns.set(font_scale = .7)
516/1:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
516/2:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler

from catboost import CatBoost, CatBoostRegressor, Pool
from optuna.integration import CatBoostPruningCallback
import optuna
from sklearn.metrics import mean_squared_error
516/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
516/4: df_train.head(5)
516/5: df_train.info()
516/6:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
516/7: own_date = date(2015, 1, 1)
516/8:
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
516/9:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
516/10: categorical_features_names = ['City', 'City Group', 'Type']
516/11:
X_train.drop(['Id'], axis=1, inplace=True)
X_test.drop(['Id'], axis=1, inplace=True)
516/12:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
516/13:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
516/14: plt.hist(y_train_scale)
516/15: plt.hist(y_train_scale, figsize=(8,8))
516/16:
plt.figure(figsize=(5,5))
plt.hist(y_train_scale)
516/17:
plt.figure(figsize=(5,4))
plt.hist(y_train_scale)
516/18:
train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
516/19:
model = CatBoostRegressor(custom_metric= ['R2', 'RMSE'], learning_rate=0.1, n_estimators=1000)
model.fit(train_pool, eval_set=test_pool, verbose=2000, plot=True)
516/20:
model = CatBoostRegressor(custom_metric= ['R2', 'RMSE'], learning_rate=0.1, n_estimators=1000)
model.fit(train_pool, eval_set=test_pool, verbose=2000, plot=False)
516/21: model.best_score_
516/22: #####################
516/23:
‚Ññstudy = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
‚Ññstudy.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
516/24:
#study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
#study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
516/25:
#study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
#study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
516/26:
from sklearn.model_selection import LeaveOneOut
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from catboost import Pool, cv
516/27:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
516/28:
#dict_type = {'FC': 1, 'IL': 2, 'DT': 2, 'MB': 2}
#df_train['Type'] = df_train['Type'].map(dict_type)
516/29:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
 #   stratify=df_train[['City Group', 'Type']]
)

X_train.shape, X_test.shape
516/30:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
516/31: df_train_trans = transform_date(df_train, own_date)
516/32:
plt.figure(figsize=(15,15))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
516/33:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
516/34:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
516/35:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
516/36:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
516/37:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
516/38:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
516/39:
plt.figure(figsize=(5,4))
plt.hist(y_train_scale)
516/40: categorical_features_names = ['City Group', 'Type']
516/41: selected_columns = X_train.columns
516/42:
def objective(trial):
    global cbr
    train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
    test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
    
    param = {
      #  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 50, 2500, step=1),
        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),
        'subsample': trial.suggest_float('subsample', 0.5, 1),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 0.99),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 15.0),
        'max_bin': trial.suggest_int('max_bin', 1, 64000),
        'eval_metric': 'RMSE'
    }
    
    cbr = CatBoostRegressor(**param)

    pruning_callback = CatBoostPruningCallback(trial, "RMSE")
    
    cbr.fit(
        train_pool, eval_set=test_pool,
        verbose=1,
        early_stopping_rounds=100,
        callbacks=[pruning_callback],
    )
    trial.set_user_attr(key="best_booster", value=cbr)
    # evoke pruning manually.
    pruning_callback.check_pruned()
    
    y_pred = cbr.predict(X_test)
    rmse = mean_squared_error(y_test_scale, y_pred, squared=False)
    return rmse
516/43:
def callback(study, trial):
    if study.best_trial.number == trial.number:
        study.set_user_attr(key="best_booster", value=trial.user_attrs["best_booster"])
516/44:
#study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="minimize")
#study.optimize(objective, n_trials=5, timeout=600, callbacks=[callback])
516/45: print(X_train.shape)
516/46: print(X_train.shape, X_test.shape)
516/47:
## hyperparameters from the part above
cat_features = [0,1]
params = {'max_depth': [12, 14],
          'n_estimators': [120, 150, 200],
          'l2_leaf_reg': [4.15],
 #         'subsample': [0.7],
 #         'colsample_bylevel': [0.62],
 #         'max_bin': [43000]
         }
#cb = CatBoostRegressor(loss_function='RMSE', learning_rate=0.03)
#cb_model = RandomizedSearchCV(cb, params, scoring='neg_mean_squared_error', cv = own_cv)
#cb_model.fit(X_train, y_train_scale, cat_features = cat_features)
518/1:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
518/2:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler

from catboost import CatBoost, CatBoostRegressor, Pool
from optuna.integration import CatBoostPruningCallback
import optuna
from sklearn.metrics import mean_squared_error
518/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
518/4: df_train.head(5)
518/5: df_train.info()
518/6:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
518/7: own_date = date(2015, 1, 1)
518/8:
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
518/9:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
518/10: categorical_features_names = ['City', 'City Group', 'Type']
518/11:
X_train.drop(['Id'], axis=1, inplace=True)
X_test.drop(['Id'], axis=1, inplace=True)
518/12:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
518/13:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
518/14:
plt.figure(figsize=(5,4))
plt.hist(y_train_scale)
518/15:
train_pool = Pool(X_train, 
                  label=y_train_scale,
                  cat_features=categorical_features_names)
test_pool = Pool(X_test,
                 label=y_test_scale,
                 cat_features=categorical_features_names)
518/16:
model = CatBoostRegressor(custom_metric= ['R2', 'RMSE'], learning_rate=0.1, n_estimators=1000)
model.fit(train_pool, eval_set=test_pool, verbose=2000, plot=False)
518/17: model.best_score_
518/18:
# There is a very bad result on the validation set. One of the main problems with this dataset is 
# that it's too small and the model overfits very fast. This was clearly visible on the graph.
518/19:
from sklearn.model_selection import LeaveOneOut
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from catboost import Pool, cv
518/20:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
518/21:
#dict_type = {'FC': 1, 'IL': 2, 'DT': 2, 'MB': 2}
#df_train['Type'] = df_train['Type'].map(dict_type)
518/22:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.01,
 #   stratify=df_train[['City Group', 'Type']]
)

X_train.shape, X_test.shape
518/23:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
518/24: df_train_trans = transform_date(df_train, own_date)
518/25:
plt.figure(figsize=(15,15))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
518/26:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
518/27:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
518/28:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
518/29:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
518/30:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
518/31:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
518/32:
plt.figure(figsize=(5,4))
plt.hist(y_train_scale)
518/33: categorical_features_names = ['City Group', 'Type']
518/34: own_cv = LeaveOneOut()
518/35:
## hyperparameters from the part above
cat_features = [0,1]
params = {'max_depth': [12, 14],
          'n_estimators': [120, 150, 200],
          'l2_leaf_reg': [4.15],
 #         'subsample': [0.7],
 #         'colsample_bylevel': [0.62],
 #         'max_bin': [43000]
         }
cb = CatBoostRegressor(loss_function='RMSE', learning_rate=0.03)
cb_model = RandomizedSearchCV(cb, params, scoring='neg_mean_squared_error', cv = own_cv)
cb_model.fit(X_train, y_train_scale, cat_features = cat_features)
518/36: print(X_train.shape)
518/37: print(X_train.shape, X_test.shape)
518/38: df_train.shape
518/39: df_train_trans = transform_date(df_train)
518/40: df_train_trans = transform_date(df_train, own_date)
518/41: df_train_trans
518/42: df_train_trans
518/43: df_train_trans[X_train.columns]
518/44: df_all = df_train_trans[X_train.columns]
518/45: own_cv = LeaveOneOut()
518/46: df_train_trans
518/47: df_train_trans.revenue
518/48: target_scaler(df_train_trans.revenue)
518/49: target_scaler.transform(df_train_trans.revenue)
518/50: target_scaler.transform(df_train_trans.revenue.reshape(1,-1))
518/51: target_scaler.transform(df_train_trans.revenue.values.reshape(1,-1))
518/52: target_scaler.transform(df_train_trans['revenue'].values.reshape(1,-1))
518/53: y_all = df_train_trans['revenue']
518/54: y_all
518/55: target_scaler.transform(y_all.values.reshape(1,-1))
518/56: y_all_scale = target_scaler.transform(y_all.values.reshape(-1, 1))
518/57: own_cv = LeaveOneOut()
518/58: print(X_train.shape, X_test.shape)
518/59:
## hyperparameters from the part above
cat_features = [0,1]
params = {'max_depth': [12, 14],
          'n_estimators': [120, 150, 200],
          'l2_leaf_reg': [4.15],
 #         'subsample': [0.7],
 #         'colsample_bylevel': [0.62],
 #         'max_bin': [43000]
         }
cb = CatBoostRegressor(loss_function='RMSE', learning_rate=0.03)
cb_model = RandomizedSearchCV(cb, params, scoring='neg_mean_squared_error', cv = own_cv)
cb_model.fit(df_all, y_all_scale, cat_features = cat_features)
518/60: cb_model.best_params_
518/61: best_model_cv = cb_model.best_estimator_
518/62: cb_model.best_score_
518/63:
y_pred = best_model_cv.predict(X_test)
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
518/64:
rmse = mean_squared_error(y_test, y_pred_scale, squared=False)
print('RMSE: ', rmse)
518/65:
## Dataset for validation
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
df_val = transform_date(df_val, own_date)
df_val.drop(['Id', 'City'], axis=1, inplace=True)
#df_val['Type'] = df_val['Type'].map(dict_type)
518/66:
y_val = best_model_cv.predict(df_val[X_train.columns])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
518/67:
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("catboost_preproc_cv_all.csv", index=False)
#Public Score: 1781416.09894
#Private score: 1809288.28069
521/1:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
521/2:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
521/3: df_train
521/4: df_train['City Group'].value_counts()
521/5: df_val['City Group'].value_counts()
521/6: df_train
521/7: df_train['Type'].value_counts()
521/8: df_val['Type'].value_counts()
521/9: df_train['City Group', 'Type'].value_counts()
521/10: df_train.groupby(['City Group', 'Type']).value_counts()
521/11: df_train.groupby(['City Group', 'Type'])['revenue'].value_counts()
521/12: df_train.groupby(['City Group', 'Type'])['revenue'].mean()
521/13: df_train.groupby(['City Group', 'Type'])['revenue'].min()
521/14: df_train.groupby(['City Group', 'Type'])['revenue'].max()
521/15: df_train.groupby(['City Group', 'Type'])['revenue'].max()
521/16: df_train.groupby(['City Group', 'Type'])['revenue'].mean()
521/17: df_train.groupby(['City Group'])['revenue'].mean()
521/18:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
521/19: df_train
521/20:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=40)

X_train.shape, X_test.shape
521/21:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
521/22:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
521/23:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=40)

X_train.shape, X_test.shape
521/24: X_train
521/25: ### PREPROC + EDA
521/26:
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
521/27:
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
521/28:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
521/29: own_date = date(2015, 1, 1)
521/30:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
521/31: X_train
521/32: df_train
521/33: df_train[df_train['City Group'] == 'Big Cities']
525/1:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
525/2:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
525/3: df_train.head(5)
525/4:
fig,ax = plt.subplots(1,2,figsize=(6,3))
sns.countplot(df_train.Type, ax=ax[0])
ax[0].set_title('Train set')
sns.countplot(test_data.Type, ax=ax[1])
ax[1].set_title('Test set')
525/5:
fig,ax = plt.subplots(1,2,figsize=(6,3))
sns.countplot(df_train.Type, ax=ax[0])
ax[0].set_title('Train set')
sns.countplot(df_val.Type, ax=ax[1])
ax[1].set_title('Test set')
525/6:
fig,ax = plt.subplots(1,2,figsize=(8,3))
sns.countplot(df_train.Type, ax=ax[0])
ax[0].set_title('Train set')
sns.countplot(df_val.Type, ax=ax[1])
ax[1].set_title('Test set')
525/7:
f, axes = plt.subplots(1, 2, figsize=(12,6))
sns.countplot(x='Type', data=df_train, ax=axes[0])
sns.countplot(x='Type', data=df_val, ax=axes[1])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')

#plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
axes[0].legend(labels=['0', '1'])
axes[1].legend(labels=['0', '1'])
axes[2].legend(labels=['0', '1'])

plt.show()
525/8:
f, axes = plt.subplots(1, 2, figsize=(12,6))
sns.countplot(x='Type', data=df_train, ax=axes[0])
sns.countplot(x='Type', data=df_val, ax=axes[1])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')

#plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])
axes[0].legend(labels=['0', '1'])
axes[1].legend(labels=['0', '1'])

plt.show()
525/9:
f, axes = plt.subplots(1, 2, figsize=(8,4))
sns.countplot(x='Type', data=df_train, ax=axes[0])
sns.countplot(x='Type', data=df_val, ax=axes[1])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')

plt.show()
525/10:
f, axes = plt.subplots(1, 2, figsize=(10,4))
sns.countplot(x='Type', data=df_train, ax=axes[0])
sns.countplot(x='Type', data=df_val, ax=axes[1])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')

plt.show()
525/11:
f, axes = plt.subplots(2, 2, figsize=(10,4))
sns.countplot(x='City Group', data=df_train, ax=axes[0])
sns.countplot(x='City Group', data=df_val, ax=axes[1])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')

sns.countplot(x='Type', data=df_train, ax=axes[2])
sns.countplot(x='Type', data=df_val, ax=axes[3])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')

plt.show()
525/12:
f, axes = plt.subplots(2, 2, figsize=(10,4))
sns.countplot(x='City Group', data=df_train, ax=axes[0])
sns.countplot(x='City Group', data=df_val, ax=axes[1])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')

sns.countplot(x='Type', data=df_train, ax=axes[2])
sns.countplot(x='Type', data=df_val, ax=axes[3])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')

plt.show()
525/13:
f, axes = plt.subplots(2, 2, figsize=(10,4))
sns.countplot(x='City', data=df_train, ax=axes[0])
sns.countplot(x='City Group', data=df_val, ax=axes[1])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')

sns.countplot(x='Type', data=df_train, ax=axes[2])
sns.countplot(x='Type', data=df_val, ax=axes[3])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')

plt.show()
525/14:
f, axes = plt.subplots(2, 2, figsize=(10,4))
sns.countplot(x='City Group', data=df_train, ax=axes[0])
sns.countplot(x='City Group', data=df_val, ax=axes[1])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')

sns.countplot(x='Type', data=df_train, ax=axes[2])
sns.countplot(x='Type', data=df_val, ax=axes[3])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')

plt.show()
525/15:
f, axes = plt.subplots(2, 2, figsize=(10,4))
sns.countplot(x='Type', data=df_train, ax=axes[0])
sns.countplot(x='Type', data=df_val, ax=axes[1])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')

sns.countplot(x='Type', data=df_train, ax=axes[2])
sns.countplot(x='Type', data=df_val, ax=axes[3])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')

plt.show()
525/16:
f, axes = plt.subplots(2, 2, figsize=(10,4))
sns.countplot(x='Type', data=df_train, ax=axes[0, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')

sns.countplot(x='Type', data=df_train, ax=axes[2])
sns.countplot(x='Type', data=df_val, ax=axes[3])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')

plt.show()
525/17:
f, axes = plt.subplots(2, 2, figsize=(10,4))
sns.countplot(x='Type', data=df_train, ax=axes[0, 0])
sns.countplot(x='Type', data=df_val, ax=axes[0, 1])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')

sns.countplot(x='Type', data=df_train, ax=axes[2])
sns.countplot(x='Type', data=df_val, ax=axes[3])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')

plt.show()
525/18:
f, axes = plt.subplots(2, 2, figsize=(10,4))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[0].set_title('GPE location')
axes[1].set_title('GPE text')

plt.show()
525/19:
f, axes = plt.subplots(2, 2, figsize=(10,4))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('GPE location')
axes[0, 1].set_title('GPE text')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('GPE location')
axes[1, 1].set_title('GPE text')

plt.show()
525/20:
f, axes = plt.subplots(2, 2, figsize=(10,10))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('GPE location')
axes[0, 1].set_title('GPE text')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('GPE location')
axes[1, 1].set_title('GPE text')

plt.show()
525/21:
f, axes = plt.subplots(2, 2, figsize=(8,8))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('GPE location')
axes[0, 1].set_title('GPE text')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('GPE location')
axes[1, 1].set_title('GPE text')

plt.show()
525/22:
f, axes = plt.subplots(2, 2, figsize=(10,10))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('GPE location')
axes[0, 1].set_title('GPE text')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('GPE location')
axes[1, 1].set_title('GPE text')

plt.show()
525/23:
f, axes = plt.subplots(2, 2, figsize=(10,10))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('Train')
axes[1, 1].set_title('Test')

plt.show()
525/24:
f, axes = plt.subplots(2, 2, figsize=(10,8))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('Train')
axes[1, 1].set_title('Test')

plt.show()
525/25:
f, axes = plt.subplots(2, 2, figsize=(10,10))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('Train')
axes[1, 1].set_title('Test')

plt.show()
525/26:
f, axes = plt.subplots(2, 2, figsize=(12,10))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('Train')
axes[1, 1].set_title('Test')

plt.show()
525/27: So, there is a new type 'MB' in test data and there is only one restaurant with DT type in the train dataset.
525/28: #So, there is a new type 'MB' in test data and there is only one restaurant with DT type in the train dataset.
525/29: df_train
525/30: sns.boxplot(data=df_train, x="revenue", y="City Group")
525/31: sns.boxplot(data=df_train, x="revenue", y="City Group", orient=vert)
525/32: sns.boxplot(data=df_train, x="revenue", y="City Group", orient='h')
525/33: sns.boxplot(data=df_train, x="revenue", y="City Group", orient='v')
525/34: sns.boxplot(data=df_train, x="City Group", y="revenue")
525/35:
f, axes = plt.subplots(2, 1, figsize=(12,10))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
525/36:
f, axes = plt.subplots(1, 2, figsize=(12,10))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
525/37:
f, axes = plt.subplots(1, 2, figsize=(12,5))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
525/38:
f, axes = plt.subplots(1, 2, figsize=(12,4))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
525/39:
f, axes = plt.subplots(1, 2, figsize=(12,4))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_val, x="City Group", y="revenue", ax=axes[1])
525/40:
f, axes = plt.subplots(1, 2, figsize=(12,4))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
#sns.boxplot(data=df_val, x="City Group", y="revenue", ax=axes[1])
525/41:
f, axes = plt.subplots(1, 2, figsize=(12,4))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_val, x="Type", y="revenue", ax=axes[1])
525/42:
f, axes = plt.subplots(1, 2, figsize=(12,4))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", ax=axes[1])
525/43:
f, axes = plt.subplots(1, 2, figsize=(12,4))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", hue="City Group", ax=axes[1])
525/44:
#f, axes = plt.subplots(1, 2, figsize=(12,4))
#sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", hue="City Group")
525/45:
f, axes = plt.subplots(1, 2, figsize=(12,4))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
525/46:
f, axes = plt.subplots(1, 2, figsize=(12,4))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", ax=axes[1])
525/47: own_date = date(2015, 1, 1)
525/48:
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
525/49:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
525/50:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=40)

X_train.shape, X_test.shape
525/51:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
525/52: X_train
525/53: X_train.dtype
525/54: X_train.dtypes
525/55: X_train
525/56: rfr = RandomForestRegressor(n_estimators=150)
525/57: X_train.iloc[:, 3:]
525/58: X_train.iloc[:, 4:]
525/59: rfr.fit(X_train.iloc[:, 4:], y_train)
525/60:
rfr.fit(X_train.iloc[:, 4:], y_train)
rfr.predict(X_test)
525/61:
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
525/62:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
525/63: mean_squared_error(y_test, y_pred)
525/64: y_pred
525/65: y_test
525/66: mean_squared_error(y_test, y_pred, squared=False)
525/67: ### One-hot encoding
525/68: from sklearn.preprocessing import OneHotEncoder
525/69:
tmp = pd.get_dummies(X_train)

print(tmp.shape)

tmp.head()
525/70: X_train
525/71:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
525/72:
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
525/73:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=40)

X_train.shape, X_test.shape
525/74:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
525/75:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
525/76: X_train
525/77:
rfr = RandomForestRegressor(n_estimators=150)
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
525/78: mean_squared_error(y_test, y_pred, squared=False)
525/79: from sklearn.preprocessing import OneHotEncoder
525/80:
tmp = pd.get_dummies(X_train)

print(tmp.shape)

tmp.head()
525/81:
ohe = OneHotEncoder(handle_unknown='ignore')

enc.fit(X_train)
525/82:
ohe = OneHotEncoder(handle_unknown='ignore')

ohe.fit(X_train)
525/83: ohe.transform(X_train)
525/84: ohe.categories_
525/85: ohe.categories
525/86: pd.DataFrame(ohe.transform(X_train)).head()
525/87: ohe.get_feature_names
525/88: X_train
525/89: cat_f = ['City Group', 'Type']
525/90:
ohe = OneHotEncoder(handle_unknown='ignore')

ohe.fit(X_train[cat_f])
525/91: ohe.get_feature_names
525/92: ohe.transform(X_train)
525/93: ohe.transform(X_train[cat_f])
525/94: X_train_oh = ohe.transform(X_train[cat_f])
525/95: pd.DataFrame(X_train_oh).head()
525/96:
ohe = OneHotEncoder()
ohe.fit(X_train[cat_f])
525/97: ohe.get_feature_names
525/98: X_train_oh = ohe.transform(X_train[cat_f])
525/99: pd.DataFrame(X_train_oh).head()
525/100: X_train
525/101: X_train[cat_f]
525/102:
encoder = OneHotEncoder(categories='auto',
                       drop='first', # to return k-1, use drop=false to return k dummies
                       sparse=False,
                       handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train[cat_f])
525/103: encoder.get_feature_names
525/104: X_train_oh = encoder.transform(X_train[cat_f])
525/105: X_train_oh = encoder.transform(X_train[cat_f])
525/106: pd.DataFrame(X_train_oh).head()
525/107: X_train[cat_f]
525/108: X_train[~cat_f]
525/109: X_train[X_train.columns.isin(cat_f)]
525/110:

X_train
525/111: X_train[X_train.columns.isin([cat_f])]
525/112: X_train.iloc[:, 2:]
525/113: pd.concat(pd.DataFrame(X_train_oh), X_train.iloc[:, 2:])
525/114: X_train_oh
525/115: pd.DataFrame(X_train_oh)
525/116: X_train_oh = pd.DataFrame(X_train_oh)
525/117: pd.concat(X_train_oh, X_train.iloc[:, 2:])
525/118: pd.concat([X_train_oh, X_train.iloc[:, 2:]], axis=1)
525/119: pd.concat([X_train_oh, X_train.iloc[:, 2:]], axis=1)
525/120: X_all = pd.concat([X_train_oh, X_train.iloc[:, 2:]], axis=1)
525/121: ### One Hot Encoding
525/122: cat_f = ['City Group', 'Type']
525/123: from sklearn.preprocessing import OneHotEncoder
525/124:
encoder = OneHotEncoder(categories='auto',
                       drop='first', # to return k-1, use drop=false to return k dummies
                       sparse=False,
                       handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train[cat_f])
525/125:
X_train_oh = encoder.transform(X_train[cat_f])
X_train_oh = pd.DataFrame(X_train_oh)
525/126: X_all = pd.concat([X_train_oh, X_train.iloc[:, 2:]], axis=1)
525/127: pd.DataFrame(X_train_oh).head()
525/128: X_all
525/129: X_train.shape
525/130: X_train_oh.shape
525/131: X_all = pd.concat([X_train_oh, X_train.iloc[:, 2:]])
525/132: X_all
525/133: X_all = pd.concat([X_train_oh, X_train.iloc[:, 2:]], axis=1)
525/134: X_all
525/135: X_all.P29.isnull()
525/136: X_all[X_all.P29.isnull()]
525/137: X_train
525/138: X_train_oh
525/139: X_train
525/140: X_train.reset_index()
525/141: X_train.reset_index(inplace=True)
525/142: X_train.shape
525/143: X_train_oh.shape
525/144:
X_train_oh = encoder.transform(X_train[cat_f])
X_train_oh = pd.DataFrame(X_train_oh)
525/145: X_all = pd.concat([X_train_oh, X_train.iloc[:, 2:]], axis=1)
525/146: X_train_oh
525/147: X_all
525/148: X_test
525/149:
X_train.reset_index(inplace=True)
X_test.reset_index(inplace=True)
525/150:
X_train.reset_index(inplace=True)
X_test.reset_index(inplace=True)
525/151:
encoder = OneHotEncoder(categories='auto',
                       drop='first', # to return k-1, use drop=false to return k dummies
                       sparse=False,
                       handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train[cat_f])
525/152:
X_train_oh = encoder.transform(X_train[cat_f])
X_train_oh = pd.DataFrame(X_train_oh)

X_test_oh = encoder.transform(X_test[cat_f])
X_test_oh = pd.DataFrame(X_test_oh)
525/153: X_all = pd.concat([X_train_oh, X_train.iloc[:, 2:]], axis=1)
525/154:
rfr.fit(X_all, y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
525/155:
rfr.fit(X_all, y_train)
y_pred = rfr.predict(X_test_oh)
525/156:
X_train_all = pd.concat([X_train_oh, X_train.iloc[:, 2:]], axis=1)
X_test_all = pd.concat([X_test_oh, X_test.iloc[:, 2:]], axis=1)
525/157:
rfr.fit(X_all, y_train)
y_pred = rfr.predict(X_test_oh)
525/158:
rfr.fit(X_all, X_train_all)
y_pred = rfr.predict(X_test_all)
525/159:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
525/160: X_train_all
525/161:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
525/162: df_train.head(5)
525/163:
f, axes = plt.subplots(2, 2, figsize=(12,10))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('Train')
axes[1, 1].set_title('Test')

plt.show()
525/164: #So, there is a new type 'MB' in test data and there is only one restaurant with DT type in the train dataset.
525/165: df_train
525/166:
f, axes = plt.subplots(1, 2, figsize=(12,4))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", ax=axes[1])
525/167:
#f, axes = plt.subplots(1, 2, figsize=(12,4))
#sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", hue="City Group")
525/168: own_date = date(2015, 1, 1)
525/169:
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
525/170:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=40)

X_train.shape, X_test.shape
525/171:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
525/172:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
525/173:
rfr = RandomForestRegressor(n_estimators=150)
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
525/174: mean_squared_error(y_test, y_pred, squared=False)
525/175: ### One Hot Encoding
525/176: cat_f = ['City Group', 'Type']
525/177: from sklearn.preprocessing import OneHotEncoder
525/178:
X_train.reset_index(inplace=True)
X_test.reset_index(inplace=True)
525/179:
encoder = OneHotEncoder(categories='auto',
                       drop='first', # to return k-1, use drop=false to return k dummies
                       sparse=False,
                       handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train[cat_f])
525/180: X_train
527/1:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
527/2:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
527/3: df_train.head(5)
527/4:
f, axes = plt.subplots(2, 2, figsize=(12,10))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('Train')
axes[1, 1].set_title('Test')

plt.show()
527/5: #So, there is a new type 'MB' in test data and there is only one restaurant with DT type in the train dataset.
527/6: df_train
527/7:
f, axes = plt.subplots(1, 2, figsize=(12,4))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", ax=axes[1])
527/8:
#f, axes = plt.subplots(1, 2, figsize=(12,4))
#sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", hue="City Group")
527/9: own_date = date(2015, 1, 1)
527/10:
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
527/11:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=40)

X_train.shape, X_test.shape
527/12:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
527/13:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
527/14: X_train
527/15:
rfr = RandomForestRegressor(n_estimators=150)
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
527/16: mean_squared_error(y_test, y_pred, squared=False)
527/17: ### One Hot Encoding
527/18: cat_f = ['City Group', 'Type']
527/19: from sklearn.preprocessing import OneHotEncoder
527/20:
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
527/21: X_train
527/22:
encoder = OneHotEncoder(categories='auto',
                       drop='first', # to return k-1, use drop=false to return k dummies
                       sparse=False,
                       handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train[cat_f])
527/23: X_train
527/24:
X_train_oh = encoder.transform(X_train[cat_f])
X_train_oh = pd.DataFrame(X_train_oh)

X_test_oh = encoder.transform(X_test[cat_f])
X_test_oh = pd.DataFrame(X_test_oh)
527/25: X_train_oh
527/26:
X_train_all = pd.concat([X_train_oh, X_train.iloc[:, 2:]], axis=1)
X_test_all = pd.concat([X_test_oh, X_test.iloc[:, 2:]], axis=1)
527/27: X_train_all
527/28: X_test_all
527/29: X_train_all
527/30:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
527/31: encoder.get_feature_names()
527/32: encoder.get_feature_names_out()
527/33:
X_train_oh = encoder.transform(X_train[cat_f])
X_train_oh = pd.DataFrame(X_train_oh)
X_train_oh.columns = encoder.get_feature_names_out()

X_test_oh = encoder.transform(X_test[cat_f])
X_test_oh = pd.DataFrame(X_test_oh)
X_test_oh.columns = encoder.get_feature_names_out()
527/34:
X_train_all = pd.concat([X_train_oh, X_train.iloc[:, 2:]], axis=1)
X_test_all = pd.concat([X_test_oh, X_test.iloc[:, 2:]], axis=1)
527/35:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
527/36: y_pred
527/37: mean_squared_error(y_test, y_pred, squared=False)
527/38: dict_result = {}
527/39: dict_result['no preproc'] = mean_squared_error(y_test, y_pred, squared=False)
527/40: dict_result['one hot encoder'] = mean_squared_error(y_test, y_pred, squared=False)
527/41: dict_result
527/42: pd.DataFrame.from_dict(dict_result)
527/43: pd.DataFrame.from_dict(dict_result, orient='index')
527/44:
rfr = RandomForestRegressor(n_estimators=150)
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
527/45: mean_squared_error(y_test, y_pred, squared=False)
527/46: dict_result['no preproc'] = mean_squared_error(y_test, y_pred, squared=False)
527/47: dict_result = {}
527/48:
rfr = RandomForestRegressor(n_estimators=150)
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
527/49: mean_squared_error(y_test, y_pred, squared=False)
527/50: dict_result['no preproc'] = mean_squared_error(y_test, y_pred, squared=False)
527/51:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OneHotEncoder
527/52:
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
527/53:
encoder = OneHotEncoder(categories='auto',
                       drop='first', # to return k-1, use drop=false to return k dummies
                       sparse=False,
                       handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train[cat_f])
527/54:
X_train_oh = encoder.transform(X_train[cat_f])
X_train_oh = pd.DataFrame(X_train_oh)
X_train_oh.columns = encoder.get_feature_names_out()

X_test_oh = encoder.transform(X_test[cat_f])
X_test_oh = pd.DataFrame(X_test_oh)
X_test_oh.columns = encoder.get_feature_names_out()
527/55:
X_train_all = pd.concat([X_train_oh, X_train.iloc[:, 2:]], axis=1)
X_test_all = pd.concat([X_test_oh, X_test.iloc[:, 2:]], axis=1)
527/56:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
527/57: mean_squared_error(y_test, y_pred, squared=False)
527/58: dict_result['one hot encoder'] = mean_squared_error(y_test, y_pred, squared=False)
527/59: pd.DataFrame.from_dict(dict_result, orient='index')
527/60:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
527/61:
le = LabelEncoder()
le.fit(X_train[cat_f])
527/62:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from collections import defaultdict
527/63: d = defaultdict(LabelEncoder)
527/64:
# Encoding the variable
train_transformed = X_train[cat_f].apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
test_transformed = X_train[cat_f].apply(lambda x: d[x.name].transform(x))
527/65: train_transformed
527/66:
X_train_all = pd.concat([train_transformed, X_train.iloc[:, 2:]], axis=1)
X_test_all = pd.concat([test_transformed, X_test.iloc[:, 2:]], axis=1)
527/67: X_train_all
527/68:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
527/69: X_train_all
527/70: X_train_all.isnull()
527/71: X_train_all.isnull().sum()
527/72: X_train_all.isna().sum()
527/73:
rfr.fit(X_train_all, y_train)
#y_pred = rfr.predict(X_test_all)
527/74:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
527/75: X_test_all
527/76: test_transformed
527/77:
# Encoding the variable
train_transformed = X_train[cat_f].apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
test_transformed = X_test[cat_f].apply(lambda x: d[x.name].transform(x))
527/78:
X_train_all = pd.concat([train_transformed, X_train.iloc[:, 2:]], axis=1)
X_test_all = pd.concat([test_transformed, X_test.iloc[:, 2:]], axis=1)
527/79: test_transformed
527/80:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
527/81: mean_squared_error(y_test, y_pred, squared=False)
527/82: dict_result['label encoder'] = mean_squared_error(y_test, y_pred, squared=False)
527/83: pd.DataFrame.from_dict(dict_result, orient='index')
527/84: X_train
527/85: X_train['revenue'] = y_train
527/86: X_train.groupby('City Group')['revenue'].mean()
527/87: X_train.groupby('City Group')['revenue'].median()
527/88: X_train.groupby('City Group')['revenue'].mean()
527/89: X_train.groupby('Type')['revenue'].mean()
527/90: X_train.groupby(['City Group', 'Type'])['revenue'].mean()
527/91: X_train.groupby(['City Group', 'Type'])['revenue'].mean().reset_index()
527/92:
ordered_labels = X_train.groupby(['City Group'])['survived'].mean().to_dict()
ordered_labels
527/93:
ordered_labels = X_train.groupby(['City Group'])['revenue'].mean().to_dict()
ordered_labels
527/94: X_train
527/95:
X_train['City Group'] = X_train['City Group'].map(ordered_labels)
X_test['City Group'] = X_test['City Group'].map(ordered_labels)
527/96: X_train
527/97: X_train.drop(['Type', 'revenue'], axis=1, inplace=True)
527/98: X_train
527/99: X_test
527/100: X_test.drop(['Type'], axis=1, inplace=True)
527/101:
rfr.fit(X_train, y_train)
y_pred = rfr.predict(X_test)
527/102: X_train_mean_encode = X_train[:]
527/103:
X_train_mean_encode = X_train[:]
X_test_mean_encode = X_test[:]
527/104:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from collections import defaultdict
527/105:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
527/106: df_train.head(5)
527/107:
f, axes = plt.subplots(2, 2, figsize=(12,10))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('Train')
axes[1, 1].set_title('Test')

plt.show()
527/108: #So, there is a new type 'MB' in test data and there is only one restaurant with DT type in the train dataset.
527/109: df_train
527/110:
f, axes = plt.subplots(1, 2, figsize=(12,4))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", ax=axes[1])
527/111:
#f, axes = plt.subplots(1, 2, figsize=(12,4))
#sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", hue="City Group")
527/112: own_date = date(2015, 1, 1)
527/113:
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
527/114:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=40)

X_train.shape, X_test.shape
527/115:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
527/116:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
527/117: dict_result = {}
527/118:
rfr = RandomForestRegressor(n_estimators=150)
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
527/119: mean_squared_error(y_test, y_pred, squared=False)
527/120: dict_result['no preproc'] = mean_squared_error(y_test, y_pred, squared=False)
527/121: cat_f = ['City Group', 'Type']
527/122:
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
527/123:
encoder = OneHotEncoder(categories='auto',
                       drop='first', # to return k-1, use drop=false to return k dummies
                       sparse=False,
                       handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train[cat_f])
527/124:
X_train_oh = encoder.transform(X_train[cat_f])
X_train_oh = pd.DataFrame(X_train_oh)
X_train_oh.columns = encoder.get_feature_names_out()

X_test_oh = encoder.transform(X_test[cat_f])
X_test_oh = pd.DataFrame(X_test_oh)
X_test_oh.columns = encoder.get_feature_names_out()
527/125:
X_train_all = pd.concat([X_train_oh, X_train.iloc[:, 2:]], axis=1)
X_test_all = pd.concat([X_test_oh, X_test.iloc[:, 2:]], axis=1)
527/126:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
527/127: mean_squared_error(y_test, y_pred, squared=False)
527/128: dict_result['one hot encoder'] = mean_squared_error(y_test, y_pred, squared=False)
527/129: d = defaultdict(LabelEncoder)
527/130:
# Encoding the variable
train_transformed = X_train[cat_f].apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
test_transformed = X_test[cat_f].apply(lambda x: d[x.name].transform(x))
527/131:
X_train_all = pd.concat([train_transformed, X_train.iloc[:, 2:]], axis=1)
X_test_all = pd.concat([test_transformed, X_test.iloc[:, 2:]], axis=1)
527/132:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
527/133: mean_squared_error(y_test, y_pred, squared=False)
527/134: dict_result['label encoder'] = mean_squared_error(y_test, y_pred, squared=False)
527/135:
X_train_mean_encode = X_train[:]
X_test_mean_encode = X_test[:]
527/136: X_train_mean_encode['revenue'] = y_train
527/137: X_train_mean_encode['revenue'] = y_train
527/138: X_train_mean_encode.groupby('City Group')['revenue'].mean()
527/139:
ordered_labels = X_train_mean_encode.groupby(['City Group'])['revenue'].mean().to_dict()
ordered_labels
527/140:
X_train_mean_encode['City Group'] = X_train_mean_encode['City Group'].map(ordered_labels)
X_test_mean_encode['City Group'] = X_test_mean_encode['City Group'].map(ordered_labels)
527/141:
X_trainX_train_mean_encode.drop(['Type', 'revenue'], axis=1, inplace=True)
X_test_mean_encode.drop(['Type'], axis=1, inplace=True)
527/142:
X_train_mean_encode.drop(['Type', 'revenue'], axis=1, inplace=True)
X_test_mean_encode.drop(['Type'], axis=1, inplace=True)
527/143: X_train_mean_encode
527/144: X_train
527/145:
rfr.fit(X_train_mean_encode, y_train)
y_pred = rfr.predict(X_test_mean_encode)
527/146: mean_squared_error(y_test, y_pred, squared=False)
527/147: dict_result['mean encoder'] = mean_squared_error(y_test, y_pred, squared=False)
527/148: pd.DataFrame.from_dict(dict_result, orient='index')
530/1:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from collections import defaultdict
530/2:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
530/3: df_train.head(5)
530/4:
f, axes = plt.subplots(2, 2, figsize=(12,10))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('Train')
axes[1, 1].set_title('Test')

plt.show()
530/5: #So, there is a new type 'MB' in test data and there is only one restaurant with DT type in the train dataset.
530/6: df_train
530/7:
f, axes = plt.subplots(1, 2, figsize=(12,4))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", ax=axes[1])
530/8:
#f, axes = plt.subplots(1, 2, figsize=(12,4))
#sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", hue="City Group")
530/9: own_date = date(2015, 1, 1)
530/10:
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
530/11:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=40)

X_train.shape, X_test.shape
530/12:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
530/13:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
530/14: dict_result = {}
530/15:
rfr = RandomForestRegressor(n_estimators=150)
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
530/16: mean_squared_error(y_test, y_pred, squared=False)
530/17: dict_result['no preproc'] = mean_squared_error(y_test, y_pred, squared=False)
530/18: cat_f = ['City Group', 'Type']
530/19:
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
530/20:
encoder = OneHotEncoder(categories='auto',
                       drop='first', # to return k-1, use drop=false to return k dummies
                       sparse=False,
                       handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train[cat_f])
530/21:
X_train_oh = encoder.transform(X_train[cat_f])
X_train_oh = pd.DataFrame(X_train_oh)
X_train_oh.columns = encoder.get_feature_names_out()

X_test_oh = encoder.transform(X_test[cat_f])
X_test_oh = pd.DataFrame(X_test_oh)
X_test_oh.columns = encoder.get_feature_names_out()
530/22:
X_train_all = pd.concat([X_train_oh, X_train.iloc[:, 2:]], axis=1)
X_test_all = pd.concat([X_test_oh, X_test.iloc[:, 2:]], axis=1)
530/23:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
530/24: mean_squared_error(y_test, y_pred, squared=False)
530/25: dict_result['one hot encoder'] = mean_squared_error(y_test, y_pred, squared=False)
530/26: d = defaultdict(LabelEncoder)
530/27:
# Encoding the variable
train_transformed = X_train[cat_f].apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
test_transformed = X_test[cat_f].apply(lambda x: d[x.name].transform(x))
530/28:
X_train_all = pd.concat([train_transformed, X_train.iloc[:, 2:]], axis=1)
X_test_all = pd.concat([test_transformed, X_test.iloc[:, 2:]], axis=1)
530/29:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
530/30: mean_squared_error(y_test, y_pred, squared=False)
530/31: dict_result['label encoder'] = mean_squared_error(y_test, y_pred, squared=False)
530/32:
X_train_mean_encode = X_train[:]
X_test_mean_encode = X_test[:]
530/33: X_train_mean_encode['revenue'] = y_train
530/34: X_train_mean_encode.groupby('City Group')['revenue'].mean()
530/35:
ordered_labels = X_train_mean_encode.groupby(['City Group'])['revenue'].mean().to_dict()
ordered_labels
530/36:
X_train_mean_encode['City Group'] = X_train_mean_encode['City Group'].map(ordered_labels)
X_test_mean_encode['City Group'] = X_test_mean_encode['City Group'].map(ordered_labels)
530/37:
X_train_mean_encode.drop(['Type', 'revenue'], axis=1, inplace=True)
X_test_mean_encode.drop(['Type'], axis=1, inplace=True)
530/38:
rfr.fit(X_train_mean_encode, y_train)
y_pred = rfr.predict(X_test_mean_encode)
530/39: mean_squared_error(y_test, y_pred, squared=False)
530/40: dict_result['mean encoder'] = mean_squared_error(y_test, y_pred, squared=False)
530/41: pd.DataFrame.from_dict(dict_result, orient='index')
530/42: X_train_mean_encode.loc[:, 'revenue'] = y_train
530/43: X_train_mean_encode.groupby('City Group')['revenue'].mean()
530/44: X_train_mean_encode
530/45:
X_train_mean_encode = X_train[:]
X_test_mean_encode = X_test[:]
530/46: X_train_mean_encode.loc[:, 'revenue'] = y_train
530/47: X_train_mean_encode.groupby('City Group')['revenue'].mean()
530/48:
X_train_mean_encode = X_train.copy()
X_test_mean_encode = X_test[:]
530/49: X_train_mean_encode.loc[:, 'revenue'] = y_train
530/50:
X_train_mean_encode = X_train.copy()
X_test_mean_encode = X_test.copy()
530/51: X_train_mean_encode.loc[:, 'revenue'] = y_train
530/52: X_train_mean_encode.groupby('City Group')['revenue'].mean()
530/53:
ordered_labels = X_train_mean_encode.groupby(['City Group'])['revenue'].mean().to_dict()
ordered_labels
530/54:
X_train_mean_encode['City Group'] = X_train_mean_encode['City Group'].map(ordered_labels)
X_test_mean_encode['City Group'] = X_test_mean_encode['City Group'].map(ordered_labels)
530/55:
X_train_mean_encode.drop(['Type', 'revenue'], axis=1, inplace=True)
X_test_mean_encode.drop(['Type'], axis=1, inplace=True)
530/56:
rfr.fit(X_train_mean_encode, y_train)
y_pred = rfr.predict(X_test_mean_encode)
530/57: mean_squared_error(y_test, y_pred, squared=False)
530/58:
X_train_mean_encode = X_train.copy()
X_test_mean_encode = X_test.copy()
530/59: X_train_mean_encode.loc[:, 'revenue'] = y_train
530/60: X_train_mean_encode.groupby('City Group')['revenue'].mean()
530/61:
ordered_labels = X_train_mean_encode.groupby(['City Group'])['revenue'].mean().to_dict()
ordered_labels
530/62:
X_train_mean_encode['City Group'] = X_train_mean_encode['City Group'].map(ordered_labels)
X_test_mean_encode['City Group'] = X_test_mean_encode['City Group'].map(ordered_labels)
530/63:
X_train_mean_encode.drop(['Type', 'revenue'], axis=1, inplace=True)
X_test_mean_encode.drop(['Type'], axis=1, inplace=True)
530/64:
rfr.fit(X_train_mean_encode, y_train)
y_pred = rfr.predict(X_test_mean_encode)
530/65: mean_squared_error(y_test, y_pred, squared=False)
530/66:
X_train_mean_encode = X_train.copy()
X_test_mean_encode = X_test.copy()
530/67: X_train_mean_encode.loc[:, 'revenue'] = y_train
530/68: X_train_mean_encode.groupby('City Group')['revenue'].mean()
530/69:
ordered_labels = X_train_mean_encode.groupby(['City Group'])['revenue'].mean().to_dict()
ordered_labels
530/70:
X_train_mean_encode['City Group'] = X_train_mean_encode['City Group'].map(ordered_labels)
X_test_mean_encode['City Group'] = X_test_mean_encode['City Group'].map(ordered_labels)
530/71:
X_train_mean_encode.drop(['Type', 'revenue'], axis=1, inplace=True)
X_test_mean_encode.drop(['Type'], axis=1, inplace=True)
530/72:
rfr.fit(X_train_mean_encode, y_train)
y_pred = rfr.predict(X_test_mean_encode)
530/73: mean_squared_error(y_test, y_pred, squared=False)
530/74:
X_train_mean_encode = X_train.copy()
X_test_mean_encode = X_test.copy()
530/75: X_train_mean_encode.loc[:, 'revenue'] = y_train
530/76: X_train_mean_encode.groupby('City Group')['revenue'].mean()
530/77:
ordered_labels = X_train_mean_encode.groupby(['City Group'])['revenue'].mean().to_dict()
ordered_labels
530/78:
X_train_mean_encode['City Group'] = X_train_mean_encode['City Group'].map(ordered_labels)
X_test_mean_encode['City Group'] = X_test_mean_encode['City Group'].map(ordered_labels)
530/79:
X_train_mean_encode.drop(['Type', 'revenue'], axis=1, inplace=True)
X_test_mean_encode.drop(['Type'], axis=1, inplace=True)
530/80:
rfr.fit(X_train_mean_encode, y_train)
y_pred = rfr.predict(X_test_mean_encode)
530/81: mean_squared_error(y_test, y_pred, squared=False)
531/1:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from collections import defaultdict
531/2:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
531/3: df_train.head(5)
531/4:
f, axes = plt.subplots(2, 2, figsize=(12,10))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('Train')
axes[1, 1].set_title('Test')

plt.show()
531/5: #So, there is a new type 'MB' in test data and there is only one restaurant with DT type in the train dataset.
531/6: df_train
531/7:
f, axes = plt.subplots(1, 2, figsize=(12,4))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", ax=axes[1])
531/8:
#f, axes = plt.subplots(1, 2, figsize=(12,4))
#sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", hue="City Group")
531/9: own_date = date(2015, 1, 1)
531/10:
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
531/11:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=40)

X_train.shape, X_test.shape
531/12:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
531/13:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
531/14: dict_result = {}
531/15:
rfr = RandomForestRegressor(n_estimators=200)
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
531/16: mean_squared_error(y_test, y_pred, squared=False)
531/17: dict_result['no preproc'] = mean_squared_error(y_test, y_pred, squared=False)
531/18: cat_f = ['City Group', 'Type']
531/19:
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
531/20:
encoder = OneHotEncoder(categories='auto',
                       drop='first', # to return k-1, use drop=false to return k dummies
                       sparse=False,
                       handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train[cat_f])
531/21:
X_train_oh = encoder.transform(X_train[cat_f])
X_train_oh = pd.DataFrame(X_train_oh)
X_train_oh.columns = encoder.get_feature_names_out()

X_test_oh = encoder.transform(X_test[cat_f])
X_test_oh = pd.DataFrame(X_test_oh)
X_test_oh.columns = encoder.get_feature_names_out()
531/22:
X_train_all = pd.concat([X_train_oh, X_train.iloc[:, 2:]], axis=1)
X_test_all = pd.concat([X_test_oh, X_test.iloc[:, 2:]], axis=1)
531/23:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
531/24: mean_squared_error(y_test, y_pred, squared=False)
531/25: dict_result['one hot encoder'] = mean_squared_error(y_test, y_pred, squared=False)
531/26: d = defaultdict(LabelEncoder)
531/27:
# Encoding the variable
train_transformed = X_train[cat_f].apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
test_transformed = X_test[cat_f].apply(lambda x: d[x.name].transform(x))
531/28:
X_train_all = pd.concat([train_transformed, X_train.iloc[:, 2:]], axis=1)
X_test_all = pd.concat([test_transformed, X_test.iloc[:, 2:]], axis=1)
531/29:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
531/30: mean_squared_error(y_test, y_pred, squared=False)
531/31: dict_result['label encoder'] = mean_squared_error(y_test, y_pred, squared=False)
531/32:
X_train_mean_encode = X_train.copy()
X_test_mean_encode = X_test.copy()
531/33: X_train_mean_encode.loc[:, 'revenue'] = y_train
531/34: X_train_mean_encode.groupby('City Group')['revenue'].mean()
531/35:
ordered_labels = X_train_mean_encode.groupby(['City Group'])['revenue'].mean().to_dict()
ordered_labels
531/36:
X_train_mean_encode['City Group'] = X_train_mean_encode['City Group'].map(ordered_labels)
X_test_mean_encode['City Group'] = X_test_mean_encode['City Group'].map(ordered_labels)
531/37:
X_train_mean_encode.drop(['Type', 'revenue'], axis=1, inplace=True)
X_test_mean_encode.drop(['Type'], axis=1, inplace=True)
531/38:
rfr.fit(X_train_mean_encode, y_train)
y_pred = rfr.predict(X_test_mean_encode)
531/39: mean_squared_error(y_test, y_pred, squared=False)
531/40: dict_result['mean encoder'] = mean_squared_error(y_test, y_pred, squared=False)
531/41:
rfr.fit(X_train_mean_encode, y_train)
y_pred = rfr.predict(X_test_mean_encode)
531/42: mean_squared_error(y_test, y_pred, squared=False)
531/43:
rfr.fit(X_train_mean_encode, y_train)
y_pred = rfr.predict(X_test_mean_encode)
531/44: mean_squared_error(y_test, y_pred, squared=False)
531/45: dict_result['mean encoder'] = mean_squared_error(y_test, y_pred, squared=False)
531/46: pd.DataFrame.from_dict(dict_result, orient='index')
532/1:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from collections import defaultdict
532/2:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
532/3: df_train.head(5)
532/4:
f, axes = plt.subplots(2, 2, figsize=(12,10))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('Train')
axes[1, 1].set_title('Test')

plt.show()
532/5: #So, there is a new type 'MB' in test data and there is only one restaurant with DT type in the train dataset.
532/6: df_train
532/7:
f, axes = plt.subplots(1, 2, figsize=(12,4))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", ax=axes[1])
532/8:
#f, axes = plt.subplots(1, 2, figsize=(12,4))
#sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", hue="City Group")
532/9: own_date = date(2015, 1, 1)
532/10:
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
532/11:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=40)

X_train.shape, X_test.shape
532/12:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
532/13:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
532/14: dict_result = {}
532/15:
rfr = RandomForestRegressor(n_estimators=200)
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
532/16: mean_squared_error(y_test, y_pred, squared=False)
532/17: dict_result['no preproc'] = mean_squared_error(y_test, y_pred, squared=False)
532/18: cat_f = ['City Group', 'Type']
532/19:
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
532/20:
encoder = OneHotEncoder(categories='auto',
                       drop='first', # to return k-1, use drop=false to return k dummies
                       sparse=False,
                       handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train[cat_f])
532/21:
X_train_oh = encoder.transform(X_train[cat_f])
X_train_oh = pd.DataFrame(X_train_oh)
X_train_oh.columns = encoder.get_feature_names_out()

X_test_oh = encoder.transform(X_test[cat_f])
X_test_oh = pd.DataFrame(X_test_oh)
X_test_oh.columns = encoder.get_feature_names_out()
532/22:
X_train_all = pd.concat([X_train_oh, X_train.iloc[:, 2:]], axis=1)
X_test_all = pd.concat([X_test_oh, X_test.iloc[:, 2:]], axis=1)
532/23:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
532/24: mean_squared_error(y_test, y_pred, squared=False)
532/25: dict_result['one hot encoder'] = mean_squared_error(y_test, y_pred, squared=False)
532/26: d = defaultdict(LabelEncoder)
532/27:
# Encoding the variable
train_transformed = X_train[cat_f].apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
test_transformed = X_test[cat_f].apply(lambda x: d[x.name].transform(x))
532/28:
X_train_all = pd.concat([train_transformed, X_train.iloc[:, 2:]], axis=1)
X_test_all = pd.concat([test_transformed, X_test.iloc[:, 2:]], axis=1)
532/29:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
532/30: mean_squared_error(y_test, y_pred, squared=False)
532/31: dict_result['label encoder'] = mean_squared_error(y_test, y_pred, squared=False)
532/32:
X_train_mean_encode = X_train.copy()
X_test_mean_encode = X_test.copy()
532/33: X_train_mean_encode.loc[:, 'revenue'] = y_train
532/34: X_train_mean_encode.groupby('City Group')['revenue'].mean()
532/35:
ordered_labels = X_train_mean_encode.groupby(['City Group'])['revenue'].mean().to_dict()
ordered_labels
532/36:
X_train_mean_encode['City Group'] = X_train_mean_encode['City Group'].map(ordered_labels)
X_test_mean_encode['City Group'] = X_test_mean_encode['City Group'].map(ordered_labels)
532/37:
X_train_mean_encode.drop(['Type', 'revenue'], axis=1, inplace=True)
X_test_mean_encode.drop(['Type'], axis=1, inplace=True)
532/38:
rfr.fit(X_train_mean_encode, y_train)
y_pred = rfr.predict(X_test_mean_encode)
532/39: mean_squared_error(y_test, y_pred, squared=False)
532/40: dict_result['mean encoder'] = mean_squared_error(y_test, y_pred, squared=False)
532/41: pd.DataFrame.from_dict(dict_result, orient='index')
532/42:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from collections import defaultdict

from sklearn.linear_model import LinearRegression
532/43:
#rfr = RandomForestRegressor(n_estimators=200)
rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
532/44: mean_squared_error(y_test, y_pred, squared=False)
532/45: mean_squared_error(y_test, y_pred, squared=False)
532/46: dict_result['no preproc'] = mean_squared_error(y_test, y_pred, squared=False)
532/47: cat_f = ['City Group', 'Type']
532/48:
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
532/49:
encoder = OneHotEncoder(categories='auto',
                       drop='first', # to return k-1, use drop=false to return k dummies
                       sparse=False,
                       handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train[cat_f])
532/50:
X_train_oh = encoder.transform(X_train[cat_f])
X_train_oh = pd.DataFrame(X_train_oh)
X_train_oh.columns = encoder.get_feature_names_out()

X_test_oh = encoder.transform(X_test[cat_f])
X_test_oh = pd.DataFrame(X_test_oh)
X_test_oh.columns = encoder.get_feature_names_out()
532/51:
X_train_all = pd.concat([X_train_oh, X_train.iloc[:, 2:]], axis=1)
X_test_all = pd.concat([X_test_oh, X_test.iloc[:, 2:]], axis=1)
532/52:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
532/53: mean_squared_error(y_test, y_pred, squared=False)
532/54: dict_result['one hot encoder'] = mean_squared_error(y_test, y_pred, squared=False)
532/55: d = defaultdict(LabelEncoder)
532/56:
# Encoding the variable
train_transformed = X_train[cat_f].apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
test_transformed = X_test[cat_f].apply(lambda x: d[x.name].transform(x))
532/57:
X_train_all = pd.concat([train_transformed, X_train.iloc[:, 2:]], axis=1)
X_test_all = pd.concat([test_transformed, X_test.iloc[:, 2:]], axis=1)
532/58:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
532/59: mean_squared_error(y_test, y_pred, squared=False)
532/60: dict_result['label encoder'] = mean_squared_error(y_test, y_pred, squared=False)
532/61:
X_train_mean_encode = X_train.copy()
X_test_mean_encode = X_test.copy()
532/62: X_train_mean_encode.loc[:, 'revenue'] = y_train
532/63: X_train_mean_encode.groupby('City Group')['revenue'].mean()
532/64:
ordered_labels = X_train_mean_encode.groupby(['City Group'])['revenue'].mean().to_dict()
ordered_labels
532/65:
X_train_mean_encode['City Group'] = X_train_mean_encode['City Group'].map(ordered_labels)
X_test_mean_encode['City Group'] = X_test_mean_encode['City Group'].map(ordered_labels)
532/66:
X_train_mean_encode.drop(['Type', 'revenue'], axis=1, inplace=True)
X_test_mean_encode.drop(['Type'], axis=1, inplace=True)
532/67:
rfr.fit(X_train_mean_encode, y_train)
y_pred = rfr.predict(X_test_mean_encode)
532/68: mean_squared_error(y_test, y_pred, squared=False)
532/69: dict_result['mean encoder'] = mean_squared_error(y_test, y_pred, squared=False)
532/70: pd.DataFrame.from_dict(dict_result, orient='index')
533/1:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from collections import defaultdict

from sklearn.linear_model import LinearRegression
533/2:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
533/3: df_train.head(5)
533/4:
f, axes = plt.subplots(2, 2, figsize=(12,10))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('Train')
axes[1, 1].set_title('Test')

plt.show()
533/5: #So, there is a new type 'MB' in test data and there is only one restaurant with DT type in the train dataset.
533/6: df_train
533/7:
f, axes = plt.subplots(1, 2, figsize=(12,4))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", ax=axes[1])
533/8:
#f, axes = plt.subplots(1, 2, figsize=(12,4))
#sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", hue="City Group")
533/9: own_date = date(2015, 1, 1)
533/10:
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
533/11:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=40)

X_train.shape, X_test.shape
533/12:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
533/13:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
533/14: dict_result = {}
533/15:
rfr = RandomForestRegressor(n_estimators=200)
#rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
533/16: mean_squared_error(y_test, y_pred, squared=False)
533/17: dict_result['no preproc'] = mean_squared_error(y_test, y_pred, squared=False)
533/18: cat_f = ['City Group', 'Type']
533/19:
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
533/20:
encoder = OneHotEncoder(categories='auto',
                       drop='first', # to return k-1, use drop=false to return k dummies
                       sparse=False,
                       handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train[cat_f])
533/21:
X_train_oh = encoder.transform(X_train[cat_f])
X_train_oh = pd.DataFrame(X_train_oh)
X_train_oh.columns = encoder.get_feature_names_out()

X_test_oh = encoder.transform(X_test[cat_f])
X_test_oh = pd.DataFrame(X_test_oh)
X_test_oh.columns = encoder.get_feature_names_out()
533/22:
X_train_all = pd.concat([X_train_oh, X_train.iloc[:, 2:]], axis=1)
X_test_all = pd.concat([X_test_oh, X_test.iloc[:, 2:]], axis=1)
533/23:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
533/24: mean_squared_error(y_test, y_pred, squared=False)
533/25: dict_result['one hot encoder'] = mean_squared_error(y_test, y_pred, squared=False)
533/26: d = defaultdict(LabelEncoder)
533/27:
# Encoding the variable
train_transformed = X_train[cat_f].apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
test_transformed = X_test[cat_f].apply(lambda x: d[x.name].transform(x))
533/28:
X_train_all = pd.concat([train_transformed, X_train.iloc[:, 2:]], axis=1)
X_test_all = pd.concat([test_transformed, X_test.iloc[:, 2:]], axis=1)
533/29:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
533/30: mean_squared_error(y_test, y_pred, squared=False)
533/31: dict_result['label encoder'] = mean_squared_error(y_test, y_pred, squared=False)
533/32:
X_train_mean_encode = X_train.copy()
X_test_mean_encode = X_test.copy()
533/33: X_train_mean_encode.loc[:, 'revenue'] = y_train
533/34: X_train_mean_encode.groupby('City Group')['revenue'].mean()
533/35:
ordered_labels = X_train_mean_encode.groupby(['City Group'])['revenue'].mean().to_dict()
ordered_labels
533/36:
X_train_mean_encode['City Group'] = X_train_mean_encode['City Group'].map(ordered_labels)
X_test_mean_encode['City Group'] = X_test_mean_encode['City Group'].map(ordered_labels)
533/37:
X_train_mean_encode.drop(['Type', 'revenue'], axis=1, inplace=True)
X_test_mean_encode.drop(['Type'], axis=1, inplace=True)
533/38:
rfr.fit(X_train_mean_encode, y_train)
y_pred = rfr.predict(X_test_mean_encode)
533/39: mean_squared_error(y_test, y_pred, squared=False)
533/40: dict_result['mean encoder'] = mean_squared_error(y_test, y_pred, squared=False)
533/41: pd.DataFrame.from_dict(dict_result, orient='index')
534/1:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from collections import defaultdict

from sklearn.linear_model import LinearRegression
534/2:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
534/3: df_train.head(5)
534/4:
f, axes = plt.subplots(2, 2, figsize=(12,10))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('Train')
axes[1, 1].set_title('Test')

plt.show()
534/5: #So, there is a new type 'MB' in test data and there is only one restaurant with DT type in the train dataset.
534/6: df_train
534/7:
f, axes = plt.subplots(1, 2, figsize=(12,4))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", ax=axes[1])
534/8:
#f, axes = plt.subplots(1, 2, figsize=(12,4))
#sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", hue="City Group")
534/9: own_date = date(2015, 1, 1)
534/10:
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
534/11:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=40)

X_train.shape, X_test.shape
534/12:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
534/13:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
534/14: dict_result = {}
534/15:
rfr = RandomForestRegressor(n_estimators=200)
#rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
534/16: mean_squared_error(y_test, y_pred, squared=False)
534/17: dict_result['no preproc'] = mean_squared_error(y_test, y_pred, squared=False)
534/18: cat_f = ['City Group', 'Type']
534/19:
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
534/20:
encoder = OneHotEncoder(categories='auto',
                       drop='first', # to return k-1, use drop=false to return k dummies
                       sparse=False,
                       handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train[cat_f])
534/21:
X_train_oh = encoder.transform(X_train[cat_f])
X_train_oh = pd.DataFrame(X_train_oh)
X_train_oh.columns = encoder.get_feature_names_out()

X_test_oh = encoder.transform(X_test[cat_f])
X_test_oh = pd.DataFrame(X_test_oh)
X_test_oh.columns = encoder.get_feature_names_out()
534/22:
X_train_all = pd.concat([X_train_oh, X_train.iloc[:, 2:]], axis=1)
X_test_all = pd.concat([X_test_oh, X_test.iloc[:, 2:]], axis=1)
534/23:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
534/24: mean_squared_error(y_test, y_pred, squared=False)
534/25: dict_result['one hot encoder'] = mean_squared_error(y_test, y_pred, squared=False)
534/26: d = defaultdict(LabelEncoder)
534/27:
# Encoding the variable
train_transformed = X_train[cat_f].apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
test_transformed = X_test[cat_f].apply(lambda x: d[x.name].transform(x))
534/28:
X_train_all = pd.concat([train_transformed, X_train.iloc[:, 2:]], axis=1)
X_test_all = pd.concat([test_transformed, X_test.iloc[:, 2:]], axis=1)
534/29:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
534/30: mean_squared_error(y_test, y_pred, squared=False)
534/31: dict_result['label encoder'] = mean_squared_error(y_test, y_pred, squared=False)
534/32:
X_train_mean_encode = X_train.copy()
X_test_mean_encode = X_test.copy()
534/33: X_train_mean_encode.loc[:, 'revenue'] = y_train
534/34: X_train_mean_encode.groupby('City Group')['revenue'].mean()
534/35:
ordered_labels = X_train_mean_encode.groupby(['City Group'])['revenue'].mean().to_dict()
ordered_labels
534/36:
X_train_mean_encode['City Group'] = X_train_mean_encode['City Group'].map(ordered_labels)
X_test_mean_encode['City Group'] = X_test_mean_encode['City Group'].map(ordered_labels)
534/37:
X_train_mean_encode.drop(['Type', 'revenue'], axis=1, inplace=True)
X_test_mean_encode.drop(['Type'], axis=1, inplace=True)
534/38:
rfr.fit(X_train_mean_encode, y_train)
y_pred = rfr.predict(X_test_mean_encode)
534/39: mean_squared_error(y_test, y_pred, squared=False)
534/40: dict_result['mean encoder'] = mean_squared_error(y_test, y_pred, squared=False)
534/41: pd.DataFrame.from_dict(dict_result, orient='index')
534/42:
rfr = RandomForestRegressor(n_estimators=10)
#rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
534/43: mean_squared_error(y_test, y_pred, squared=False)
534/44: dict_result['no preproc'] = mean_squared_error(y_test, y_pred, squared=False)
534/45:
rfr = RandomForestRegressor(n_estimators=10)
#rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
534/46: mean_squared_error(y_test, y_pred, squared=False)
534/47:
rfr = RandomForestRegressor(n_estimators=10)
#rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
534/48: mean_squared_error(y_test, y_pred, squared=False)
534/49:
rfr = RandomForestRegressor(n_estimators=10)
#rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
534/50: mean_squared_error(y_test, y_pred, squared=False)
534/51:
rfr = RandomForestRegressor(n_estimators=10)
#rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
534/52: mean_squared_error(y_test, y_pred, squared=False)
534/53:
rfr = RandomForestRegressor(n_estimators=1000)
#rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
534/54: mean_squared_error(y_test, y_pred, squared=False)
534/55:
rfr = RandomForestRegressor(n_estimators=1000)
#rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
534/56: mean_squared_error(y_test, y_pred, squared=False)
534/57:
rfr = RandomForestRegressor(n_estimators=1000)
#rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
534/58: mean_squared_error(y_test, y_pred, squared=False)
534/59:
rfr = RandomForestRegressor(n_estimators=1000)
#rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
534/60: mean_squared_error(y_test, y_pred, squared=False)
534/61:
rfr = RandomForestRegressor(n_estimators=1000)
#rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
534/62: mean_squared_error(y_test, y_pred, squared=False)
534/63:
rfr = RandomForestRegressor(n_estimators=1000)
#rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
534/64: mean_squared_error(y_test, y_pred, squared=False)
534/65:
rfr = RandomForestRegressor(n_estimators=1000)
#rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
534/66: mean_squared_error(y_test, y_pred, squared=False)
534/67:
rfr = RandomForestRegressor(n_estimators=1000)
#rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
534/68: mean_squared_error(y_test, y_pred, squared=False)
534/69:
#rfr = RandomForestRegressor(n_estimators=1000)
rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
534/70: mean_squared_error(y_test, y_pred, squared=False)
534/71:
#rfr = RandomForestRegressor(n_estimators=1000)
rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
534/72: mean_squared_error(y_test, y_pred, squared=False)
534/73:
#rfr = RandomForestRegressor(n_estimators=1000)
rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
534/74: mean_squared_error(y_test, y_pred, squared=False)
534/75:
#rfr = RandomForestRegressor(n_estimators=1000)
rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
534/76: mean_squared_error(y_test, y_pred, squared=False)
535/1:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from collections import defaultdict

from sklearn.linear_model import LinearRegression
535/2:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
535/3: df_train.head(5)
535/4:
f, axes = plt.subplots(2, 2, figsize=(12,10))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('Train')
axes[1, 1].set_title('Test')

plt.show()
535/5: #So, there is a new type 'MB' in test data and there is only one restaurant with DT type in the train dataset.
535/6: df_train
535/7:
f, axes = plt.subplots(1, 2, figsize=(12,4))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", ax=axes[1])
535/8:
#f, axes = plt.subplots(1, 2, figsize=(12,4))
#sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", hue="City Group")
535/9: own_date = date(2015, 1, 1)
535/10:
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
535/11:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=40)

X_train.shape, X_test.shape
535/12:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
535/13:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
535/14: dict_result = {}
535/15:
#rfr = RandomForestRegressor(n_estimators=1000)
rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
535/16: mean_squared_error(y_test, y_pred, squared=False)
535/17: dict_result['no preproc'] = mean_squared_error(y_test, y_pred, squared=False)
535/18: cat_f = ['City Group', 'Type']
535/19:
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
535/20:
encoder = OneHotEncoder(categories='auto',
                       drop='first', # to return k-1, use drop=false to return k dummies
                       sparse=False,
                       handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train[cat_f])
535/21:
X_train_oh = encoder.transform(X_train[cat_f])
X_train_oh = pd.DataFrame(X_train_oh)
X_train_oh.columns = encoder.get_feature_names_out()

X_test_oh = encoder.transform(X_test[cat_f])
X_test_oh = pd.DataFrame(X_test_oh)
X_test_oh.columns = encoder.get_feature_names_out()
535/22:
X_train_all = pd.concat([X_train_oh, X_train.iloc[:, 2:]], axis=1)
X_test_all = pd.concat([X_test_oh, X_test.iloc[:, 2:]], axis=1)
535/23:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
535/24: mean_squared_error(y_test, y_pred, squared=False)
535/25: dict_result['one hot encoder'] = mean_squared_error(y_test, y_pred, squared=False)
535/26: d = defaultdict(LabelEncoder)
535/27:
# Encoding the variable
train_transformed = X_train[cat_f].apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
test_transformed = X_test[cat_f].apply(lambda x: d[x.name].transform(x))
535/28:
X_train_all = pd.concat([train_transformed, X_train.iloc[:, 2:]], axis=1)
X_test_all = pd.concat([test_transformed, X_test.iloc[:, 2:]], axis=1)
535/29:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
535/30: mean_squared_error(y_test, y_pred, squared=False)
535/31: dict_result['label encoder'] = mean_squared_error(y_test, y_pred, squared=False)
535/32:
X_train_mean_encode = X_train.copy()
X_test_mean_encode = X_test.copy()
535/33: X_train_mean_encode.loc[:, 'revenue'] = y_train
535/34: X_train_mean_encode.groupby('City Group')['revenue'].mean()
535/35:
ordered_labels = X_train_mean_encode.groupby(['City Group'])['revenue'].mean().to_dict()
ordered_labels
535/36:
X_train_mean_encode['City Group'] = X_train_mean_encode['City Group'].map(ordered_labels)
X_test_mean_encode['City Group'] = X_test_mean_encode['City Group'].map(ordered_labels)
535/37:
X_train_mean_encode.drop(['Type', 'revenue'], axis=1, inplace=True)
X_test_mean_encode.drop(['Type'], axis=1, inplace=True)
535/38:
rfr.fit(X_train_mean_encode, y_train)
y_pred = rfr.predict(X_test_mean_encode)
535/39: mean_squared_error(y_test, y_pred, squared=False)
535/40: dict_result['mean encoder'] = mean_squared_error(y_test, y_pred, squared=False)
535/41: pd.DataFrame.from_dict(dict_result, orient='index')
536/1:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from collections import defaultdict

from sklearn.linear_model import LinearRegression
536/2:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
536/3: df_train.head(5)
536/4:
f, axes = plt.subplots(2, 2, figsize=(12,10))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('Train')
axes[1, 1].set_title('Test')

plt.show()
536/5: #So, there is a new type 'MB' in test data and there is only one restaurant with DT type in the train dataset.
536/6: df_train
536/7:
f, axes = plt.subplots(1, 2, figsize=(12,4))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", ax=axes[1])
536/8:
#f, axes = plt.subplots(1, 2, figsize=(12,4))
#sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", hue="City Group")
536/9: own_date = date(2015, 1, 1)
536/10:
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
536/11:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=40)

X_train.shape, X_test.shape
536/12:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
536/13:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
536/14: dict_result = {}
536/15:
#rfr = RandomForestRegressor(n_estimators=1000)
rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
536/16: mean_squared_error(y_test, y_pred, squared=False)
536/17: dict_result['no preproc'] = mean_squared_error(y_test, y_pred, squared=False)
536/18: cat_f = ['City Group', 'Type']
536/19:
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
536/20:
encoder = OneHotEncoder(categories='auto',
                       drop='first', # to return k-1, use drop=false to return k dummies
                       sparse=False,
                       handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train[cat_f])
536/21:
X_train_oh = encoder.transform(X_train[cat_f])
X_train_oh = pd.DataFrame(X_train_oh)
X_train_oh.columns = encoder.get_feature_names_out()

X_test_oh = encoder.transform(X_test[cat_f])
X_test_oh = pd.DataFrame(X_test_oh)
X_test_oh.columns = encoder.get_feature_names_out()
536/22:
X_train_all = pd.concat([X_train_oh, X_train.iloc[:, 2:]], axis=1)
X_test_all = pd.concat([X_test_oh, X_test.iloc[:, 2:]], axis=1)
536/23:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
536/24: mean_squared_error(y_test, y_pred, squared=False)
536/25: dict_result['one hot encoder'] = mean_squared_error(y_test, y_pred, squared=False)
536/26: d = defaultdict(LabelEncoder)
536/27:
# Encoding the variable
train_transformed = X_train[cat_f].apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
test_transformed = X_test[cat_f].apply(lambda x: d[x.name].transform(x))
536/28:
X_train_all = pd.concat([train_transformed, X_train.iloc[:, 2:]], axis=1)
X_test_all = pd.concat([test_transformed, X_test.iloc[:, 2:]], axis=1)
536/29:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
536/30: mean_squared_error(y_test, y_pred, squared=False)
536/31: dict_result['label encoder'] = mean_squared_error(y_test, y_pred, squared=False)
536/32:
X_train_mean_encode = X_train.copy()
X_test_mean_encode = X_test.copy()
536/33: X_train_mean_encode.loc[:, 'revenue'] = y_train
536/34: X_train_mean_encode.groupby('City Group')['revenue'].mean()
536/35:
ordered_labels = X_train_mean_encode.groupby(['City Group'])['revenue'].mean().to_dict()
ordered_labels
536/36:
X_train_mean_encode['City Group'] = X_train_mean_encode['City Group'].map(ordered_labels)
X_test_mean_encode['City Group'] = X_test_mean_encode['City Group'].map(ordered_labels)
536/37:
X_train_mean_encode.drop(['Type', 'revenue'], axis=1, inplace=True)
X_test_mean_encode.drop(['Type'], axis=1, inplace=True)
536/38:
rfr.fit(X_train_mean_encode, y_train)
y_pred = rfr.predict(X_test_mean_encode)
536/39: mean_squared_error(y_test, y_pred, squared=False)
536/40: dict_result['mean encoder'] = mean_squared_error(y_test, y_pred, squared=False)
536/41: pd.DataFrame.from_dict(dict_result, orient='index')
537/1:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from collections import defaultdict

from sklearn.linear_model import LinearRegression
537/2:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
537/3: df_train.head(5)
537/4:
f, axes = plt.subplots(2, 2, figsize=(12,10))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('Train')
axes[1, 1].set_title('Test')

plt.show()
537/5: #So, there is a new type 'MB' in test data and there is only one restaurant with DT type in the train dataset.
537/6: df_train
537/7:
f, axes = plt.subplots(1, 2, figsize=(12,4))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", ax=axes[1])
537/8:
#f, axes = plt.subplots(1, 2, figsize=(12,4))
#sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", hue="City Group")
537/9: own_date = date(2015, 1, 1)
537/10:
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
537/11:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
537/12:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
537/13:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
537/14: dict_result = {}
537/15:
#rfr = RandomForestRegressor(n_estimators=1000)
rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
537/16: mean_squared_error(y_test, y_pred, squared=False)
537/17: dict_result['no preproc'] = mean_squared_error(y_test, y_pred, squared=False)
537/18: cat_f = ['City Group', 'Type']
537/19:
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
537/20:
encoder = OneHotEncoder(categories='auto',
                       drop='first', # to return k-1, use drop=false to return k dummies
                       sparse=False,
                       handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train[cat_f])
537/21:
X_train_oh = encoder.transform(X_train[cat_f])
X_train_oh = pd.DataFrame(X_train_oh)
X_train_oh.columns = encoder.get_feature_names_out()

X_test_oh = encoder.transform(X_test[cat_f])
X_test_oh = pd.DataFrame(X_test_oh)
X_test_oh.columns = encoder.get_feature_names_out()
537/22:
X_train_all = pd.concat([X_train_oh, X_train.iloc[:, 2:]], axis=1)
X_test_all = pd.concat([X_test_oh, X_test.iloc[:, 2:]], axis=1)
537/23:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
537/24: mean_squared_error(y_test, y_pred, squared=False)
537/25: dict_result['one hot encoder'] = mean_squared_error(y_test, y_pred, squared=False)
537/26: d = defaultdict(LabelEncoder)
537/27:
# Encoding the variable
train_transformed = X_train[cat_f].apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
test_transformed = X_test[cat_f].apply(lambda x: d[x.name].transform(x))
537/28:
X_train_all = pd.concat([train_transformed, X_train.iloc[:, 2:]], axis=1)
X_test_all = pd.concat([test_transformed, X_test.iloc[:, 2:]], axis=1)
537/29:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
537/30: mean_squared_error(y_test, y_pred, squared=False)
537/31: dict_result['label encoder'] = mean_squared_error(y_test, y_pred, squared=False)
537/32:
X_train_mean_encode = X_train.copy()
X_test_mean_encode = X_test.copy()
537/33: X_train_mean_encode.loc[:, 'revenue'] = y_train
537/34: X_train_mean_encode.groupby('City Group')['revenue'].mean()
537/35:
ordered_labels = X_train_mean_encode.groupby(['City Group'])['revenue'].mean().to_dict()
ordered_labels
537/36:
X_train_mean_encode['City Group'] = X_train_mean_encode['City Group'].map(ordered_labels)
X_test_mean_encode['City Group'] = X_test_mean_encode['City Group'].map(ordered_labels)
537/37:
X_train_mean_encode.drop(['Type', 'revenue'], axis=1, inplace=True)
X_test_mean_encode.drop(['Type'], axis=1, inplace=True)
537/38:
rfr.fit(X_train_mean_encode, y_train)
y_pred = rfr.predict(X_test_mean_encode)
537/39: mean_squared_error(y_test, y_pred, squared=False)
537/40: dict_result['mean encoder'] = mean_squared_error(y_test, y_pred, squared=False)
537/41: pd.DataFrame.from_dict(dict_result, orient='index')
538/1:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from collections import defaultdict

from sklearn.linear_model import LinearRegression
538/2:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
538/3: df_train.head(5)
538/4:
f, axes = plt.subplots(2, 2, figsize=(12,10))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('Train')
axes[1, 1].set_title('Test')

plt.show()
538/5: #So, there is a new type 'MB' in test data and there is only one restaurant with DT type in the train dataset.
538/6: df_train
538/7:
f, axes = plt.subplots(1, 2, figsize=(12,4))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", ax=axes[1])
538/8:
#f, axes = plt.subplots(1, 2, figsize=(12,4))
#sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", hue="City Group")
538/9: own_date = date(2015, 1, 1)
538/10:
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
538/11:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
    random_state=20)

X_train.shape, X_test.shape
538/12:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
538/13:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
538/14: dict_result = {}
538/15:
#rfr = RandomForestRegressor(n_estimators=1000)
rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
538/16: mean_squared_error(y_test, y_pred, squared=False)
538/17: dict_result['no preproc'] = mean_squared_error(y_test, y_pred, squared=False)
538/18: cat_f = ['City Group', 'Type']
538/19:
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
538/20:
encoder = OneHotEncoder(categories='auto',
                       drop='first', # to return k-1, use drop=false to return k dummies
                       sparse=False,
                       handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train[cat_f])
538/21:
X_train_oh = encoder.transform(X_train[cat_f])
X_train_oh = pd.DataFrame(X_train_oh)
X_train_oh.columns = encoder.get_feature_names_out()

X_test_oh = encoder.transform(X_test[cat_f])
X_test_oh = pd.DataFrame(X_test_oh)
X_test_oh.columns = encoder.get_feature_names_out()
538/22:
X_train_all = pd.concat([X_train_oh, X_train.iloc[:, 2:]], axis=1)
X_test_all = pd.concat([X_test_oh, X_test.iloc[:, 2:]], axis=1)
538/23:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
538/24: mean_squared_error(y_test, y_pred, squared=False)
538/25: dict_result['one hot encoder'] = mean_squared_error(y_test, y_pred, squared=False)
538/26: d = defaultdict(LabelEncoder)
538/27:
# Encoding the variable
train_transformed = X_train[cat_f].apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
test_transformed = X_test[cat_f].apply(lambda x: d[x.name].transform(x))
538/28:
X_train_all = pd.concat([train_transformed, X_train.iloc[:, 2:]], axis=1)
X_test_all = pd.concat([test_transformed, X_test.iloc[:, 2:]], axis=1)
538/29:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
538/30: mean_squared_error(y_test, y_pred, squared=False)
538/31: dict_result['label encoder'] = mean_squared_error(y_test, y_pred, squared=False)
538/32:
X_train_mean_encode = X_train.copy()
X_test_mean_encode = X_test.copy()
538/33: X_train_mean_encode.loc[:, 'revenue'] = y_train
538/34: X_train_mean_encode.groupby('City Group')['revenue'].mean()
538/35:
ordered_labels = X_train_mean_encode.groupby(['City Group'])['revenue'].mean().to_dict()
ordered_labels
538/36:
X_train_mean_encode['City Group'] = X_train_mean_encode['City Group'].map(ordered_labels)
X_test_mean_encode['City Group'] = X_test_mean_encode['City Group'].map(ordered_labels)
538/37:
X_train_mean_encode.drop(['Type', 'revenue'], axis=1, inplace=True)
X_test_mean_encode.drop(['Type'], axis=1, inplace=True)
538/38:
rfr.fit(X_train_mean_encode, y_train)
y_pred = rfr.predict(X_test_mean_encode)
538/39: mean_squared_error(y_test, y_pred, squared=False)
538/40: dict_result['mean encoder'] = mean_squared_error(y_test, y_pred, squared=False)
538/41: pd.DataFrame.from_dict(dict_result, orient='index')
538/42:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from collections import defaultdict

from sklearn.linear_model import LinearRegression
538/43: r2_score(y_test, y_pred, squared=False)
538/44: r2_score(y_test, y_pred)
538/45:
rfr = RandomForestRegressor(n_estimators=1000)
#rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
538/46: r2_score(y_test, y_pred)
538/47:
rfr = RandomForestRegressor(n_estimators=150)
#rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
538/48: r2_score(y_test, y_pred)
538/49: r2_score(y_test, y_pred)
538/50: dict_result['no preproc'] = mean_squared_error(y_test, y_pred, squared=False)
538/51: cat_f = ['City Group', 'Type']
538/52:
X_train.reset_index(inplace=True, drop=True)
X_test.reset_index(inplace=True, drop=True)
538/53:
encoder = OneHotEncoder(categories='auto',
                       drop='first', # to return k-1, use drop=false to return k dummies
                       sparse=False,
                       handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train[cat_f])
538/54:
X_train_oh = encoder.transform(X_train[cat_f])
X_train_oh = pd.DataFrame(X_train_oh)
X_train_oh.columns = encoder.get_feature_names_out()

X_test_oh = encoder.transform(X_test[cat_f])
X_test_oh = pd.DataFrame(X_test_oh)
X_test_oh.columns = encoder.get_feature_names_out()
538/55:
X_train_all = pd.concat([X_train_oh, X_train.iloc[:, 2:]], axis=1)
X_test_all = pd.concat([X_test_oh, X_test.iloc[:, 2:]], axis=1)
538/56:
rfr.fit(X_train_all, y_train)
y_pred = rfr.predict(X_test_all)
538/57: r2_score(y_test, y_pred)
538/58:
rfr = RandomForestRegressor(n_estimators=150)
#rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
538/59: y_pred
538/60: y_test
538/61: X_train.iloc[:, 4:]
538/62: X_train.iloc[:, 4:-1]
538/63:
rfr = RandomForestRegressor(n_estimators=150)
#rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:-1], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:-1])
538/64: r2_score(y_test, y_pred)
538/65:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from collections import defaultdict

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
538/66:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
538/67:
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
y_test_scale = target_scaler.transform(y_test.values.reshape(-1, 1))
538/68: dict_result = {}
538/69: X_train.iloc[:, 4:-1]
538/70:
rfr = RandomForestRegressor(n_estimators=150)
#rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
538/71: r2_score(y_test, y_pred)
538/72: y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
538/73: dict_result = {}
538/74: X_train.iloc[:, 4:-1]
538/75:
rfr = RandomForestRegressor(n_estimators=150)
#rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
538/76: r2_score(y_test, y_pred)
538/77: dict_result['no preproc'] = mean_squared_error(y_test, y_pred, squared=False)
538/78: y_train
538/79:
rfr = RandomForestRegressor(n_estimators=150)
#rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train_scale)
y_pred = rfr.predict(X_test.iloc[:, 4:])
538/80: y_train
538/81: y_train_scale
538/82: y_train_scale.to_list()
538/83: y_train_scale[:]
538/84: y_train_scale[:, 1]
538/85: y_train_scale[1, 1]
538/86: y_train_scale[1]
538/87: list(y_train_scale)
538/88: y_train_scale.values()
538/89: y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
538/90: y_train_scale
538/91: y_train_scale[:]
538/92: y_train_scale[0]
538/93: y_train_scale[0,:]
538/94: y_train_scale[:,0]
538/95:
rfr = RandomForestRegressor(n_estimators=150)
#rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train_scale[:,0])
y_pred = rfr.predict(X_test.iloc[:, 4:])
538/96: y_pred
538/97: target_scaler.inverse_transform(y_pred)
538/98: target_scaler.inverse_transform(y_pred.reshape(-1,1))
538/99: y_test
538/100: target_scaler.inverse_transform(y_pred.reshape(-1,1))[:,0]
538/101: y_pred = target_scaler.inverse_transform(y_pred.reshape(-1,1))[:,0]
538/102: r2_score(y_test, y_pred)
538/103:
rfr = RandomForestRegressor(n_estimators=1050)
#rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train_scale[:,0])
y_pred = rfr.predict(X_test.iloc[:, 4:])
538/104: y_pred = target_scaler.inverse_transform(y_pred.reshape(-1,1))[:,0]
538/105: r2_score(y_test, y_pred)
538/106:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
538/107:
plt.figure(figsize=(13,13))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
538/108:
plt.figure(figsize=(13,13))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":5})
#sns.set(font_scale = .7)
538/109:
#rfr = RandomForestRegressor(n_estimators=1050)
rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train_scale[:,0])
y_pred = rfr.predict(X_test.iloc[:, 4:])
538/110: r2_score(y_test, y_pred)
538/111:
#rfr = RandomForestRegressor(n_estimators=1050)
rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
538/112: r2_score(y_test, y_pred)
538/113:
#rfr = RandomForestRegressor(n_estimators=1050)
rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 4:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 4:])
538/114: r2_score(y_test, y_pred)
538/115: dict_result['no preproc'] = mean_squared_error(y_test, y_pred, squared=False)
538/116:
plt.figure(figsize=(13,13))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":5})
#sns.set(font_scale = .7)
538/117:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
538/118:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
538/119: corr_features
538/120:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
538/121: X_train
538/122: X_train.iloc[:, 4:]
538/123: X_train.iloc[:, 2:]
538/124:
#rfr = RandomForestRegressor(n_estimators=1050)
rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 2:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 2:])
538/125: r2_score(y_test, y_pred)
538/126: #dict_result['no preproc'] = mean_squared_error(y_test, y_pred, squared=False)
538/127:
plt.figure(figsize=(13,13))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":5})
#sns.set(font_scale = .7)
538/128:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
538/129:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
538/130: corr_features
538/131:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
538/132: X_train
538/133:
rfr.fit(X_train.iloc[:, 2:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 2:])
538/134: r2_score(y_test, y_pred)
538/135: X_train.iloc[:, 2:]
538/136:
rfr.fit(X_train.iloc[:, 2:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 2:])
538/137: r2_score(y_test, y_pred)
540/1:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from collections import defaultdict

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
540/2:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
540/3: df_train.head(5)
540/4:
f, axes = plt.subplots(2, 2, figsize=(12,10))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('Train')
axes[1, 1].set_title('Test')

plt.show()
540/5: #So, there is a new type 'MB' in test data and there is only one restaurant with DT type in the train dataset.
540/6: df_train
540/7:
f, axes = plt.subplots(1, 2, figsize=(12,4))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", ax=axes[1])
540/8:
#f, axes = plt.subplots(1, 2, figsize=(12,4))
#sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", hue="City Group")
540/9: own_date = date(2015, 1, 1)
540/10:
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
540/11:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
    random_state=20)

X_train.shape, X_test.shape
540/12:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
540/13:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
540/14:
#target_scaler = MinMaxScaler(feature_range = (0,1))
#target_scaler.fit(y_train.values.reshape(-1, 1))
540/15: #y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
540/16: dict_result = {}
540/17:
#rfr = RandomForestRegressor(n_estimators=1050)
rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 2:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 2:])
540/18: r2_score(y_test, y_pred)
540/19: #dict_result['no preproc'] = mean_squared_error(y_test, y_pred, squared=False)
540/20:
plt.figure(figsize=(13,13))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":5})
#sns.set(font_scale = .7)
540/21:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
540/22:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
540/23: corr_features
540/24:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
540/25:
rfr.fit(X_train.iloc[:, 2:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 2:])
540/26: r2_score(y_test, y_pred)
540/27:
corr_features = correlation(X_train, 0.8)
print('correlated features: ', len(set(corr_features)) )
540/28: corr_features
540/29:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
540/30:
rfr.fit(X_train.iloc[:, 2:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 2:])
540/31: r2_score(y_test, y_pred)
540/32:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
540/33:
sfs = SFS(LinearRegression(),
          k_features=3, # the lower the features we want, the longer this will take
          forward=False,
          verbose=2,
          scoring='r2')

sfs = sfs.fit(np.array(X_test.iloc[:, 2:]), y_train)
540/34:
sfs = SFS(LinearRegression(),
          k_features=3, # the lower the features we want, the longer this will take
          forward=False,
          verbose=2,
          scoring='r2')

sfs = sfs.fit(np.array(X_train.iloc[:, 2:]), y_train)
540/35: sfs.k_feature_idx_
540/36: pd.DataFrame.from_dict(sfs.get_metric_dict()).T
540/37:
sfs = SFS(LinearRegression(),
          k_features=1, # the lower the features we want, the longer this will take
          forward=False,
          verbose=2,
          scoring='r2')

sfs = sfs.fit(np.array(X_train.iloc[:, 2:]), y_train)
540/38: sfs.k_feature_idx_
540/39: pd.DataFrame.from_dict(sfs.get_metric_dict()).T
540/40: X_train.columns[list(sfs.best_idx_)]
540/41: X_train.columns[list(sfs.k_feature_idx_)]
540/42:
sfs = SFS(LinearRegression(),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          verbose=2,
          scoring='r2')

sfs = sfs.fit(np.array(X_train.iloc[:, 2:]), y_train)
540/43: sfs.k_feature_idx_
540/44: pd.DataFrame.from_dict(sfs.get_metric_dict()).T
540/45: X_train.columns[list(sfs.k_feature_idx_)]
540/46: X = X_train['P1', 'P3']
540/47: X = X_train[['P1', 'P3']]
540/48:
rfr.fit(X, y_train)
y_pred = rfr.predict(X_test[['P1', 'P3']])
540/49: r2_score(y_test, y_pred)
540/50:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=20)

X_train.shape, X_test.shape
540/51:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
540/52:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
540/53:
#target_scaler = MinMaxScaler(feature_range = (0,1))
#target_scaler.fit(y_train.values.reshape(-1, 1))
540/54: #y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
540/55: dict_result = {}
540/56:
#rfr = RandomForestRegressor(n_estimators=1050)
rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 2:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 2:])
540/57: r2_score(y_test, y_pred)
540/58: #dict_result['no preproc'] = mean_squared_error(y_test, y_pred, squared=False)
540/59:
plt.figure(figsize=(13,13))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":5})
#sns.set(font_scale = .7)
540/60:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
540/61:
corr_features = correlation(X_train, 0.8)
print('correlated features: ', len(set(corr_features)) )
540/62: corr_features
540/63:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
540/64:
rfr.fit(X_train.iloc[:, 2:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 2:])
540/65: r2_score(y_test, y_pred)
540/66:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
540/67:
sfs = SFS(LinearRegression(),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          verbose=2,
          scoring='r2')

sfs = sfs.fit(np.array(X_train.iloc[:, 2:]), y_train)
540/68: sfs.k_feature_idx_
540/69: pd.DataFrame.from_dict(sfs.get_metric_dict()).T
540/70: X_train.columns[list(sfs.k_feature_idx_)]
540/71: X = X_train[['P1', 'P3']]
540/72:
rfr.fit(X, y_train)
y_pred = rfr.predict(X_test[['P1', 'P3']])
540/73: r2_score(y_test, y_pred)
541/1:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from collections import defaultdict

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
541/2:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
541/3: df_train.head(5)
541/4:
f, axes = plt.subplots(2, 2, figsize=(12,10))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('Train')
axes[1, 1].set_title('Test')

plt.show()
541/5: #So, there is a new type 'MB' in test data and there is only one restaurant with DT type in the train dataset.
541/6: df_train
541/7:
f, axes = plt.subplots(1, 2, figsize=(12,4))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", ax=axes[1])
541/8:
#f, axes = plt.subplots(1, 2, figsize=(12,4))
#sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", hue="City Group")
541/9: own_date = date(2015, 1, 1)
541/10:
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
541/11:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=20)

X_train.shape, X_test.shape
541/12:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
541/13:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
541/14:
#target_scaler = MinMaxScaler(feature_range = (0,1))
#target_scaler.fit(y_train.values.reshape(-1, 1))
541/15: #y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
541/16: dict_result = {}
541/17:
#rfr = RandomForestRegressor(n_estimators=1050)
rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 2:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 2:])
541/18: r2_score(y_test, y_pred)
541/19: #dict_result['no preproc'] = mean_squared_error(y_test, y_pred, squared=False)
541/20:
plt.figure(figsize=(13,13))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":5})
#sns.set(font_scale = .7)
541/21:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
541/22:
sfs = SFS(LinearRegression(),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          verbose=2,
          scoring='r2')

sfs = sfs.fit(np.array(X_train.iloc[:, 2:]), y_train)
541/23: sfs.k_feature_idx_
541/24: pd.DataFrame.from_dict(sfs.get_metric_dict()).T
541/25: rfr.intercept_
541/26: df_train
541/27: df_train.revenue.info()
541/28: df_train.revenue.describe()
541/29:
df_train.revenue.describe()
sns.boxplot(data=df_train, x="Type")
541/30:
df_train.revenue.describe()
sns.boxplot(data=df_train, y="Type")
541/31:
df_train.revenue.describe()
sns.boxplot(data=df_train, y="revenue")
541/32:
def remove_outlier(df_in, col_name):
    q1 = df_in[col_name].quantile(0.25)
    q3 = df_in[col_name].quantile(0.75)
    iqr = q3-q1 #Interquartile range
    fence_low  = q1-1.5*iqr
    fence_high = q3+1.5*iqr
    df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]
    return df_out
541/33: df_remove = remove_outlier(df_train, 'revenue')
541/34:
df_train.revenue.describe()
sns.boxplot(data=df_remove, y="revenue")
541/35: df_remove.shape
541/36: df_train.shape
541/37:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=20)

X_train.shape, X_test.shape
541/38:
X_train, X_test, y_train, y_test = train_test_split(
    df_remove.drop(labels=['revenue'], axis=1),  # drop the target
    df_remove['revenue'],  # just the target
    test_size=0.1,
    random_state=20)

X_train.shape, X_test.shape
541/39:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
541/40:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
541/41:
#target_scaler = MinMaxScaler(feature_range = (0,1))
#target_scaler.fit(y_train.values.reshape(-1, 1))
541/42: #y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
541/43: dict_result = {}
541/44:
#rfr = RandomForestRegressor(n_estimators=1050)
rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 2:], y_train)
y_pred = rfr.predict(X_test.iloc[:, 2:])
541/45: rfr.intercept_
541/46: r2_score(y_test, y_pred)
541/47: #dict_result['no preproc'] = mean_squared_error(y_test, y_pred, squared=False)
541/48:
plt.figure(figsize=(13,13))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":5})
#sns.set(font_scale = .7)
541/49:
sfs = SFS(LinearRegression(),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          verbose=2,
          scoring='r2')

sfs = sfs.fit(np.array(X_train.iloc[:, 2:]), y_train)
541/50: sfs.k_feature_idx_
541/51: pd.DataFrame.from_dict(sfs.get_metric_dict()).T
541/52: X_train.columns[1, 14, 18, 19, 22, 23, 24, 25]
541/53: list(sfs.k_feature_idx_)
541/54: X_train.columns[[1, 14, 18, 19, 22, 23, 24, 25]]
541/55: X_train.iloc[:, 2:]
541/56:
X_train = X_train.iloc[:, 2:]
X_test = X_test.iloc[:, 2:]
541/57:
sfs = SFS(LinearRegression(),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          verbose=2,
          scoring='r2')

sfs = sfs.fit(np.array(X_train), y_train)
541/58: sfs.k_feature_idx_
541/59: pd.DataFrame.from_dict(sfs.get_metric_dict()).T
541/60: list(sfs.k_feature_idx_)
541/61: X_train.columns[[1, 14, 18, 19, 22, 23, 24, 25]]
541/62: X = X_train[['P2', 'P15', 'P19', 'P20', 'P23', 'P24', 'P25', 'P26']]
541/63:
rfr.fit(X, y_train)
y_pred = rfr.predict(X_test[['P2', 'P15', 'P19', 'P20', 'P23', 'P24', 'P25', 'P26']])
541/64: r2_score(y_test, y_pred)
541/65:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
df_val = transform_date(df_val, own_date)
df_val[['P2', 'P15', 'P19', 'P20', 'P23', 'P24', 'P25', 'P26']]
#df_val['Type'] = df_val['Type'].map(dict_type)
541/66:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
df_val = transform_date(df_val, own_date)
df_val = df_val[['P2', 'P15', 'P19', 'P20', 'P23', 'P24', 'P25', 'P26']]
#df_val['Type'] = df_val['Type'].map(dict_type)
541/67: df_val
541/68: y_val = rfr.predict(df_val)
541/69: y_val
541/70:
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("lin_reg.csv", index=False)
541/71:
df_t.insert(1, "Prediction", y_val.tolist(), True)
df_t.to_csv("lin_reg.csv", index=False)
541/72: df_t
541/73: df_t.describe
541/74: df_t.describe()
541/75:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
541/76: y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
541/77:
plt.figure(figsize=(5,4))
plt.hist(y_train_scale)
541/78:
#rfr = RandomForestRegressor(n_estimators=1050)
rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 2:], y_train_scale)
y_pred = rfr.predict(X_test.iloc[:, 2:])
541/79: rfr.intercept_
541/80: r2_score(y_test, y_pred)
541/81: y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
541/82: r2_score(y_test, y_pred_scale)
541/83: sfs.k_feature_idx_
541/84: pd.DataFrame.from_dict(sfs.get_metric_dict()).T
541/85: 1, 14, 18, 19, 22, 23, 24, 25
541/86: list(sfs.k_feature_idx_)
541/87: X_train.columns[[1, 14, 18, 19, 22, 23, 24, 25]]
541/88: X = X_train[['P2', 'P15', 'P19', 'P20', 'P23', 'P24', 'P25', 'P26']]
541/89:
rfr.fit(X, y_train)
y_pred = rfr.predict(X_test[['P2', 'P15', 'P19', 'P20', 'P23', 'P24', 'P25', 'P26']])
541/90: y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
541/91: r2_score(y_test, y_pred_scale)
541/92:
rfr.fit(X, y_train_scale)
y_pred = rfr.predict(X_test[['P2', 'P15', 'P19', 'P20', 'P23', 'P24', 'P25', 'P26']])
541/93: y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
541/94: r2_score(y_test, y_pred_scale)
541/95:
sfs = SFS(LinearRegression(),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          verbose=2,
          scoring='r2')

sfs = sfs.fit(np.array(X_train), y_train_scale)
541/96: sfs.k_feature_idx_
541/97: pd.DataFrame.from_dict(sfs.get_metric_dict()).T
541/98: 1, 14, 18, 19, 22, 23, 24, 25
541/99: list(sfs.k_feature_idx_)
541/100: X_train.columns[[1, 14, 18, 19, 22, 23, 24, 25]]
541/101: X_train.columns[[1, 14, 18, 19, 22, 23, 24, 25]]
541/102: X = X_train[['P2', 'P15', 'P19', 'P20', 'P23', 'P24', 'P25', 'P26']]
541/103:
rfr.fit(X, y_train_scale)
y_pred = rfr.predict(X_test[['P2', 'P15', 'P19', 'P20', 'P23', 'P24', 'P25', 'P26']])
541/104: y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
541/105: r2_score(y_test, y_pred_scale)
541/106:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
df_val = transform_date(df_val, own_date)
df_val = df_val[['P2', 'P15', 'P19', 'P20', 'P23', 'P24', 'P25', 'P26']]
#df_val['Type'] = df_val['Type'].map(dict_type)
541/107: y_val = rfr.predict(df_val)
541/108:
y_val = rfr.predict(df_val)
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
541/109:
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("lin_reg.csv", index=False)
541/110: df_t.describe()
541/111:
#rfr = RandomForestRegressor(n_estimators=1050)
rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 2:], y_train_scale)
y_pred = rfr.predict(X_test.iloc[:, 2:])
541/112: rfr.intercept_
541/113: y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
541/114: r2_score(y_test, y_pred_scale)
541/115:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
541/116:
X_train = X_train.iloc[:, 2:]
X_test = X_test.iloc[:, 2:]
541/117:
X_train, X_test, y_train, y_test = train_test_split(
    df_remove.drop(labels=['revenue'], axis=1),  # drop the target
    df_remove['revenue'],  # just the target
    test_size=0.1,
    random_state=20)

X_train.shape, X_test.shape
541/118:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
541/119:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
541/120:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
541/121: y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
541/122: dict_result = {}
541/123:
plt.figure(figsize=(5,4))
plt.hist(y_train_scale)
541/124: X_train
541/125:
#rfr = RandomForestRegressor(n_estimators=1050)
rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 2:], y_train_scale)
y_pred = rfr.predict(X_test.iloc[:, 2:])
541/126: rfr.intercept_
541/127: y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
541/128: r2_score(y_test, y_pred_scale)
541/129: #dict_result['no preproc'] = mean_squared_error(y_test, y_pred, squared=False)
541/130:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
541/131:
X_train = X_train.iloc[:, 2:]
X_test = X_test.iloc[:, 2:]
541/132:
sfs = SFS(LinearRegression(),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          verbose=2,
          scoring='r2')

sfs = sfs.fit(np.array(X_train), y_train_scale)
541/133: sfs.k_feature_idx_
541/134: pd.DataFrame.from_dict(sfs.get_metric_dict()).T
541/135: 1, 14, 18, 19, 22, 23, 24, 25
541/136: list(sfs.k_feature_idx_)
541/137: X_train.columns[[1, 14, 18, 19, 22, 23, 24, 25]]
541/138: X = X_train[['P2', 'P15', 'P19', 'P20', 'P23', 'P24', 'P25', 'P26']]
541/139:
rfr.fit(X, y_train_scale)
y_pred = rfr.predict(X_test[['P2', 'P15', 'P19', 'P20', 'P23', 'P24', 'P25', 'P26']])
541/140: y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
541/141: r2_score(y_test, y_pred_scale)
541/142:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
#df_val = transform_date(df_val, own_date)
df_val = df_val[['P2', 'P15', 'P19', 'P20', 'P23', 'P24', 'P25', 'P26']]
#df_val['Type'] = df_val['Type'].map(dict_type)
541/143:
y_val = rfr.predict(df_val)
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
541/144:
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("lin_reg.csv", index=False)
541/145: df_t.describe()
541/146: df_t
541/147: df_t[df_t['Prediction'] < 0]
541/148:
rfr = RandomForestRegressor(n_estimators=150)
#rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 2:], y_train_scale)
y_pred = rfr.predict(X_test.iloc[:, 2:])
541/149: rfr.intercept_
541/150: y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
541/151: r2_score(y_test, y_pred_scale)
541/152: #dict_result['no preproc'] = mean_squared_error(y_test, y_pred, squared=False)
541/153:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
541/154:
X_train = X_train.iloc[:, 2:]
X_test = X_test.iloc[:, 2:]
541/155:
sfs = SFS(LinearRegression(),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          verbose=2,
          scoring='r2')

sfs = sfs.fit(np.array(X_train), y_train_scale)
541/156:
sfs = SFS(RandomForestRegressor(n_estimators=150),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          verbose=2,
          scoring='r2')

sfs = sfs.fit(np.array(X_train), y_train_scale)
541/157: sfs.k_feature_idx_
541/158: pd.DataFrame.from_dict(sfs.get_metric_dict()).T
541/159: r2_score(y_test, y_pred_scale)
541/160: X = X_train[['P2', 'P15', 'P19', 'P20', 'P23', 'P24', 'P25', 'P26']]
541/161:
rfr = RandomForestRegressor(n_estimators=150)
#rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 2:], y_train_scale)
y_pred = rfr.predict(X_test.iloc[:, 2:])
541/162:
#rfr = RandomForestRegressor(n_estimators=150)
rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 2:], y_train_scale)
y_pred = rfr.predict(X_test.iloc[:, 2:])
541/163: rfr.intercept_
541/164: y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
541/165: r2_score(y_test, y_pred_scale)
541/166: #dict_result['no preproc'] = mean_squared_error(y_test, y_pred, squared=False)
541/167:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
541/168:
X_train = X_train.iloc[:, 2:]
X_test = X_test.iloc[:, 2:]
541/169:
sfs = SFS(RandomForestRegressor(n_estimators=150),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          verbose=2,
          scoring='r2')

sfs = sfs.fit(np.array(X_train), y_train_scale)
541/170:
sfs = SFS(LinearRegression(),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          verbose=2,
          scoring='r2')

sfs = sfs.fit(np.array(X_train), y_train_scale)
541/171: sfs.k_feature_idx_
541/172: pd.DataFrame.from_dict(sfs.get_metric_dict()).T
541/173:
X_train, X_test, y_train, y_test = train_test_split(
    df_remove.drop(labels=['revenue'], axis=1),  # drop the target
    df_remove['revenue'],  # just the target
    test_size=0.1,
    random_state=20)

X_train.shape, X_test.shape
541/174:
X_train = transform_date(X_train, own_date)
X_test = transform_date(X_test, own_date)
541/175:
X_train.drop(['Id', 'City'], axis=1, inplace=True)
X_test.drop(['Id', 'City'], axis=1, inplace=True)
541/176:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
541/177: y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
541/178: dict_result = {}
541/179:
plt.figure(figsize=(5,4))
plt.hist(y_train_scale)
541/180:
#rfr = RandomForestRegressor(n_estimators=150)
rfr = LinearRegression()
rfr.fit(X_train.iloc[:, 2:], y_train_scale)
y_pred = rfr.predict(X_test.iloc[:, 2:])
541/181: rfr.intercept_
541/182: y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
541/183: r2_score(y_test, y_pred_scale)
541/184: #dict_result['no preproc'] = mean_squared_error(y_test, y_pred, squared=False)
541/185:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
541/186:
X_train = X_train.iloc[:, 2:]
X_test = X_test.iloc[:, 2:]
541/187:
sfs = SFS(LinearRegression(),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          verbose=2,
          scoring='r2')

sfs = sfs.fit(np.array(X_train), y_train_scale)
541/188: sfs.k_feature_idx_
541/189: pd.DataFrame.from_dict(sfs.get_metric_dict()).T
541/190: X_train.columns[[1, 14, 18, 19, 22, 23, 24, 25]]
541/191: X = X_train[['P2', 'P15', 'P19', 'P20', 'P23', 'P24', 'P25', 'P26']]
541/192:
rfr.fit(X, y_train_scale)
y_pred = rfr.predict(X_test[['P2', 'P15', 'P19', 'P20', 'P23', 'P24', 'P25', 'P26']])
541/193: y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
541/194: r2_score(y_test, y_pred_scale)
541/195:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
#df_val = transform_date(df_val, own_date)
df_val = df_val[['P2', 'P15', 'P19', 'P20', 'P23', 'P24', 'P25', 'P26']]
#df_val['Type'] = df_val['Type'].map(dict_type)
541/196:
y_val = rfr.predict(df_val)
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
541/197: df_t[df_t < 0] = 0
541/198: df_t[df_t['Prediction'] < 0]
541/199: df_t.describe()
541/200:
#df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("lin_reg.csv", index=False)
541/201: df_t.describe()
541/202:
y_val = rfr.predict(df_val)
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
541/203:
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t[df_t < 0] = 0
df_t.to_csv("lin_reg.csv", index=False)
541/204: df_t.describe()
541/205: X_train
541/206:
rfr.fit(X_train, y_train_scale)
y_pred = rfr.predict(X_test)
541/207: y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
541/208: r2_score(y_test, y_pred_scale)
541/209:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
#df_val = transform_date(df_val, own_date)
df_val = df_val[X_train.columns]
#df_val['Type'] = df_val['Type'].map(dict_type)
541/210: X_train.columns
541/211:
df_val = pd.read_csv('data/test.csv')
df_t = df_val[['Id']]
df_val = transform_date(df_val, own_date)
df_val = df_val[X_train.columns]
#df_val['Type'] = df_val['Type'].map(dict_type)
541/212:
y_val = rfr.predict(df_val)
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
541/213:
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t[df_t < 0] = 0
df_t.to_csv("lin_reg_all.csv", index=False)
541/214: df_t.describe()
542/1:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from collections import defaultdict

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
542/2:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
542/3: df_train.head(5)
542/4:
f, axes = plt.subplots(2, 2, figsize=(12,10))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('Train')
axes[1, 1].set_title('Test')

plt.show()
542/5: #So, there is a new type 'MB' in test data and there is only one restaurant with DT type in the train dataset.
542/6: df_train
542/7:
f, axes = plt.subplots(1, 2, figsize=(12,4))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", ax=axes[1])
542/8:
#f, axes = plt.subplots(1, 2, figsize=(12,4))
#sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", hue="City Group")
542/9: own_date = date(2015, 1, 1)
542/10:
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
542/11:
df_train.revenue.describe()
sns.boxplot(data=df_remove, y="revenue")
542/12:
df_train.revenue.describe()
sns.boxplot(data=df_train, y="revenue")
542/13:
df_train.revenue.describe()
sns.boxplot(data=df_train, y="revenue", figsize=(5,5))
542/14:
df_train.revenue.describe()
sns.set(rc={'figure.figsize':(5,5)})
sns.boxplot(data=df_train, y="revenue")
542/15:
df_train.revenue.describe()
sns.set(rc={'figure.figsize':(5,3)})
sns.boxplot(data=df_train, y="revenue")
542/16: df_train = transform_date(df_train, own_date)
542/17: df_train
542/18: sns.jointplot(df_train.open_days, df_train.revenue)
542/19: sns.pairplot(df_train)
542/20:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from collections import defaultdict

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
542/21:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
542/22: df_train.head(5)
542/23:
f, axes = plt.subplots(2, 2, figsize=(12,10))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('Train')
axes[1, 1].set_title('Test')

plt.show()
542/24: #So, there is a new type 'MB' in test data and there is only one restaurant with DT type in the train dataset.
542/25: df_train
542/26:
f, axes = plt.subplots(1, 2, figsize=(12,4))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", ax=axes[1])
542/27:
#f, axes = plt.subplots(1, 2, figsize=(12,4))
#sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", hue="City Group")
542/28: own_date = date(2015, 1, 1)
542/29:
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
542/30: df_train = transform_date(df_train, own_date)
542/31: df_train
542/32: df_train.columnsmnsumns
542/33: df_train.columns
542/34: df_train[['P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'revenue']]
542/35: sns.pairplot(df_train[['P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'revenue']])
542/36: df_train.columns
542/37: sns.pairplot(df_train[['P10', 'P11', 'P12', 'P13', 'P14', 'P15', 'P16', 'P17', 'P18', 'P19', 'revenue']])
542/38: sns.pairplot(df_train[['P20', 'P21', 'P22', 'P23', 'P24', 'P25', 'P26', 'P27', 'P28', 'P29', 'revenue']])
542/39: sns.pairplot(df_train[['P30', 'P31', 'P32', 'P33', 'P34', 'P35', 'P36', 'P37', 'revenue']])
545/1:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from collections import defaultdict

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
545/2:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
545/3: df_train.head(5)
545/4:
f, axes = plt.subplots(2, 2, figsize=(12,10))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('Train')
axes[1, 1].set_title('Test')

plt.show()
545/5: #So, there is a new type 'MB' in test data and there is only one restaurant with DT type in the train dataset.
545/6: df_train
545/7:
f, axes = plt.subplots(1, 2, figsize=(12,4))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", ax=axes[1])
545/8:
#f, axes = plt.subplots(1, 2, figsize=(12,4))
#sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", hue="City Group")
545/9: own_date = date(2015, 1, 1)
545/10:
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
545/11: df_train = transform_date(df_train, own_date)
545/12: df_train
545/13: df_train.columns
545/14:
P_cols = ['P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'P11', 
          'P12', 'P13', 'P14', 'P15', 'P16','P17', 'P18', 'P19', 'P20', 'P21',
          'P22', 'P23', 'P24', 'P25', 'P26', 'P27', 'P28', 'P29', 'P30', 'P31', 
          'P32', 'P33', 'P34', 'P35', 'P36', 'P37']
545/15: sns.pairplot(df_train[['P10', 'P11', 'P12', 'P13', 'P14', 'P15', 'P16', 'P17', 'P18', 'P19', 'revenue']])
545/16: df_test
545/17: df_train['P10']
545/18: df_train['P10'].hist()
545/19: sns.pairplot(df_train['P10'], df_train.revenue)
545/20: sns.pairplot(df_train['P10', 'revenue'])
545/21: sns.pairplot(df_train[['P10', 'revenue']])
545/22: sns.regplot(x=df_train['P10'], y=df_train.revenue, data=tips)
545/23: sns.regplot(x=df_train['P10'], y=df_train.revenue)
545/24: sns.histplot(df_train['P10'])
545/25: sns.regplot(x=df_train['P10'], y=df_train.revenue, x_jitter=.1)
545/26: sns.regplot(x=df_train['P10'], y=df_train.revenue, x_jitter=.3)
545/27: sns.regplot(x=df_train['P10'], y=df_train.revenue, x_jitter=.2)
545/28: sns.regplot(x=df_train['P10'], y=df_train.revenue, x_jitter=.1)
545/29: sns.regplot(x=df_train['P10'], y=df_train.revenue, x_estimator=np.mean)
545/30: sns.regplot(x=df_train['P10'], y=df_train.revenue, x_jitter=.1, x_estimator=np.mean)
545/31: sns.regplot(x=df_train['P10'], y=df_train.revenue, x_jitter=.1)
545/32: sns.regplot(x=df_train['P10'], y=df_train.revenue, x_jitter=.1, logx=True)
545/33: sns.regplot(x=df_train['P10'], y=df_train.revenue, x_jitter=.1, logy=True)
545/34: sns.regplot(x=df_train['P10'], y=df_train.revenue, x_jitter=.1)
545/35: sns.regplot(x=df_train['P10'], y=df_train.revenue, x_jitter=.1, x_estimator=np.max)
545/36: sns.regplot(x=df_train['P10'], y=df_train.revenue, x_jitter=.1, x_estimator=np.mean)
545/37: sns.regplot(x=df_train['P10'], y=df_train.revenue, x_jitter=.1, x_estimator=np.mode)
545/38: sns.regplot(x=df_train['P10'], y=df_train.revenue, x_jitter=.1, x_estimator=np.median)
545/39: sns.regplot(x=df_train['P10'], y=df_train.revenue, x_jitter=.1, x_estimator=np.mean)
545/40: P_cols = ['P1', 'P2', 'P3', 'P4']
545/41:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 3, figsize=(15,6))
    sns.histplot(df_train[p_col], ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.1, x=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.1, x_estimator=np.mean, x=axes[2])
    axes[0].set_title('Train')
    axes[1].set_title('Test')
   # plt.show()
545/42:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 3, figsize=(15,6))
    sns.histplot(df_train[p_col], ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.1, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.1, x_estimator=np.mean, ax=axes[2])
    axes[0].set_title('Train')
    axes[1].set_title('Test')
   # plt.show()
545/43:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(15,4))
    sns.histplot(df_train[p_col], ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.1, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.1, x_estimator=np.mean, ax=axes[2])
    axes[0].set_title('Train')
    axes[1].set_title('Test')
   # plt.show()
545/44:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(15,4))
    sns.histplot(df_train[p_col], ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.1, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.1, x_estimator=np.mean, ax=axes[2])
    axes[0].set_title('Train')
    axes[1].set_title('Test')
    plt.grid()
   # plt.show()
545/45:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(15,4))
    sns.histplot(df_train[p_col], ax=axes[0])
    sns.set_style("darkgrid")
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.1, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.1, x_estimator=np.mean, ax=axes[2])
    axes[0].set_title('Train')
    axes[1].set_title('Test')
   # plt.show()
545/46:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(15,4))
        sns.set_style("darkgrid")
    sns.histplot(df_train[p_col], ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.1, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.1, x_estimator=np.mean, ax=axes[2])
    axes[0].set_title('Train')
    axes[1].set_title('Test')
   # plt.show()
545/47:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(15,4))
    sns.set_style("darkgrid")
    sns.histplot(df_train[p_col], ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.1, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.1, x_estimator=np.mean, ax=axes[2])
    axes[0].set_title('Train')
    axes[1].set_title('Test')
   # plt.show()
545/48:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(15,4))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.1, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.1, x_estimator=np.mean, ax=axes[2])
    axes[0].set_title('Train')
    axes[1].set_title('Test')
   # plt.show()
545/49:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(15,4))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.1, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.1, x_estimator=np.mean, ax=axes[2])
    ax.grid(b=True, which='major', color='red', linewidth=0.075)
    ax.grid(b=True, which='minor', color='black', linewidth=0.075)
    axes[0].set_title('Train')
    axes[1].set_title('Test')
   # plt.show()
545/50:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(15,4))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.1, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.1, x_estimator=np.mean, ax=axes[2])
    axes.grid(b=True, which='major', color='red', linewidth=0.075)
    axes.grid(b=True, which='minor', color='black', linewidth=0.075)
    axes[0].set_title('Train')
    axes[1].set_title('Test')
   # plt.show()
545/51:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(15,4))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.1, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.1, x_estimator=np.mean, ax=axes[2])

    axes[0].set_title('Train')
    axes[1].set_title('Test')
   # plt.show()
545/52:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(15,4))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.05, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_estimator=np.mean, ax=axes[2])

    axes[0].set_title('Train')
    axes[1].set_title('Test')
   # plt.show()
545/53: df_val
545/54:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(15,4))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.05, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_estimator=np.mean, ax=axes[2])
    sns.histplot(df_val[p_col], ax=axes[3], color='orange')
    

    axes[0].set_title('Train')
    axes[1].set_title('Test')
   # plt.show()
545/55:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(15,4))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.05, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_estimator=np.mean, ax=axes[2])
    sns.histplot(df_val[p_col], ax=axes[3], color='green')
    

    axes[0].set_title('Train')
    axes[1].set_title('Test')
   # plt.show()
545/56:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(15,4))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.05, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_estimator=np.mean, ax=axes[2])
    sns.histplot(df_val[p_col], ax=axes[3], color='red')
    

    axes[0].set_title('Train')
    axes[1].set_title('Test')
   # plt.show()
545/57:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(15,4))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.05, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_estimator=np.mean, ax=axes[2])
    sns.histplot(df_val[p_col], ax=axes[3], binwidth = 1, color='red')
    

    axes[0].set_title('Train')
    axes[1].set_title('Test')
   # plt.show()
545/58:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(15,4))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.05, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_estimator=np.mean, ax=axes[2])
    sns.histplot(df_val[p_col], ax=axes[3], binwidth = 0.7, color='red')
    

    axes[0].set_title('Train')
    axes[1].set_title('Test')
   # plt.show()
545/59: df_val['P1'].value_counts()
545/60:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(15,4))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.05, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_estimator=np.mean, ax=axes[2])
    sns.histplot(df_val[p_col], ax=axes[3], binwidth = 0.5, color='red')
    

    axes[0].set_title('Train')
    axes[1].set_title('Test')
   # plt.show()
545/61:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(15,4))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], binwidth = 0.5, ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.05, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_estimator=np.mean, ax=axes[2])
    sns.histplot(df_val[p_col], ax=axes[3], binwidth = 0.5, color='red')
    

    axes[0].set_title('Train')
    axes[1].set_title('Test')
   # plt.show()
545/62:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(15,4))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], binwidth = 0.5, ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.05, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_estimator=np.mean, ax=axes[2])
    sns.histplot(df_val[p_col], ax=axes[3], binwidth = 0.5, color='red')
    

    axes[0].set_title(f'Train {p_col}')
    axes[1].set_title(f'{p_col} and revenue')
    axes[2].set_title(f'{p_col} and revenue (mean)')
    axes[3].set_title(f'Test {p_col}' )
    
    
   # plt.show()
545/63:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(18,4))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], binwidth = 0.5, ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.05, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_estimator=np.mean, ax=axes[2])
    sns.histplot(df_val[p_col], ax=axes[3], binwidth = 0.5, color='red')
    

    axes[0].set_title(f'Train {p_col}')
    axes[1].set_title(f'{p_col} and revenue')
    axes[2].set_title(f'{p_col} and revenue (mean)')
    axes[3].set_title(f'Test {p_col}' )
    
    
   # plt.show()
545/64:
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("output.pdf")
545/65:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from collections import defaultdict

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
545/66:
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("output.pdf")
545/67:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(18,4))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], binwidth = 0.5, ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.05, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_estimator=np.mean, ax=axes[2])
    sns.histplot(df_val[p_col], ax=axes[3], binwidth = 0.5, color='red')
    

    axes[0].set_title(f'Train {p_col}')
    axes[1].set_title(f'{p_col} and revenue')
    axes[2].set_title(f'{p_col} and revenue (mean)')
    axes[3].set_title(f'Test {p_col}' )
    pdf.savefig( fig )
    
pdf.close()    
   # plt.show()
545/68:
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("output.pdf")
545/69: df_val['P2'].value_counts()
545/70:
P_cols = ['P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'P11', 
          'P12', 'P13', 'P14', 'P15', 'P16','P17', 'P18', 'P19', 'P20', 'P21',
          'P22', 'P23', 'P24', 'P25', 'P26', 'P27', 'P28', 'P29', 'P30', 'P31', 
          'P32', 'P33', 'P34', 'P35', 'P36', 'P37']
545/71:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(18,5))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], binwidth = 0.5, ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.05, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_estimator=np.mean, ax=axes[2])
    sns.histplot(df_val[p_col], ax=axes[3], binwidth = 0.5, color='red')
    

    axes[0].set_title(f'Train {p_col}')
    axes[1].set_title(f'{p_col} and revenue')
    axes[2].set_title(f'{p_col} and revenue (mean)')
    axes[3].set_title(f'Test {p_col}' )
    pdf.savefig( fig )
    
pdf.close()    
   # plt.show()
545/72:
P_cols = ['P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'P11', 
          'P12', 'P13', 'P14', 'P15', 'P16','P17', 'P18', 'P19', 'P20', 'P21',
          'P22', 'P23', 'P24', 'P25', 'P26', 'P27', 'P28', 'P29', 'P30', 'P31', 
          'P32', 'P33', 'P34', 'P35', 'P36', 'P37', 'open_days']
545/73:
f, axes = plt.subplots(2, 2, figsize=(12,10))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')
545/74: df_val['P2'].value_counts()
545/75:
f, axes = plt.subplots(2, 2, figsize=(12,10))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('Train')
axes[1, 1].set_title('Test')

plt.show()
545/76:
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("output.pdf")
545/77:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(18,5))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], binwidth = 0.5, ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.05, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_estimator=np.mean, ax=axes[2])
    sns.histplot(df_val[p_col], ax=axes[3], binwidth = 0.5, color='red')
    

    axes[0].set_title(f'Train {p_col}')
    axes[1].set_title(f'{p_col} and revenue')
    axes[2].set_title(f'{p_col} and revenue (mean)')
    axes[3].set_title(f'Test {p_col}' )
    pdf.savefig( fig )
    
pdf.close()    
   # plt.show()
545/78:
P_cols = ['P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'P11', 
          'P12', 'P13', 'P14', 'P15', 'P16','P17', 'P18', 'P19', 'P20', 'P21',
          'P22', 'P23', 'P24', 'P25', 'P26', 'P27', 'P28', 'P29', 'P30', 'P31', 
          'P32', 'P33', 'P34', 'P35', 'P36', 'P37']
545/79:
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("output.pdf")
545/80:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
545/81: df_val
545/82:
#df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
545/83:
P_cols = ['P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'P11', 
          'P12', 'P13', 'P14', 'P15', 'P16','P17', 'P18', 'P19', 'P20', 'P21',
          'P22', 'P23', 'P24', 'P25', 'P26', 'P27', 'P28', 'P29', 'P30', 'P31', 
          'P32', 'P33', 'P34', 'P35', 'P36', 'P37']
545/84:
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("output.pdf")
545/85: df_val
545/86:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(18,5))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], binwidth = 0.5, ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.05, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_estimator=np.mean, ax=axes[2])
    sns.histplot(df_val[p_col], ax=axes[3], binwidth = 0.5, color='red')
    

    axes[0].set_title(f'Train {p_col}')
    axes[1].set_title(f'{p_col} and revenue')
    axes[2].set_title(f'{p_col} and revenue (mean)')
    axes[3].set_title(f'Test {p_col}' )
    pdf.savefig( fig )
    
### Open days and revenue

fig, axes = plt.subplots(1, 3, figsize=(18,5))
sns.set_style("whitegrid")
sns.histplot(df_train['open_days'], ax=axes[0])
sns.regplot(x=df_train['open_days'], y=df_train.revenue, ax=axes[1])
sns.histplot(df_val[p_col], ax=axes[2], color='red')

axes[0].set_title(f'Train. Hist of open days')
axes[1].set_title(f'Open days and revenue')
axes[3].set_title(f'Test. Hist of open days')
pdf.savefig( fig )
    
pdf.close()
545/87:
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("output.pdf")
545/88:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(18,5))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], binwidth = 0.5, ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.05, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_estimator=np.mean, ax=axes[2])
    sns.histplot(df_val[p_col], ax=axes[3], binwidth = 0.5, color='red')
    

    axes[0].set_title(f'Train {p_col}')
    axes[1].set_title(f'{p_col} and revenue')
    axes[2].set_title(f'{p_col} and revenue (mean)')
    axes[3].set_title(f'Test {p_col}' )
    pdf.savefig( fig )
    
### Open days and revenue

fig, axes = plt.subplots(1, 3, figsize=(18,5))
sns.set_style("whitegrid")
sns.histplot(df_train['open_days'], ax=axes[0])
sns.regplot(x=df_train['open_days'], y=df_train.revenue, ax=axes[1])
sns.histplot(df_val['open_days'], ax=axes[2], color='red')

axes[0].set_title(f'Train. Hist of open days')
axes[1].set_title(f'Open days and revenue')
axes[3].set_title(f'Test. Hist of open days')
pdf.savefig( fig )
    
pdf.close()
545/89:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(18,5))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], binwidth = 0.5, ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.05, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_estimator=np.mean, ax=axes[2])
    sns.histplot(df_val[p_col], ax=axes[3], binwidth = 0.5, color='red')
    

    axes[0].set_title(f'Train {p_col}')
    axes[1].set_title(f'{p_col} and revenue')
    axes[2].set_title(f'{p_col} and revenue (mean)')
    axes[3].set_title(f'Test {p_col}' )
    pdf.savefig( fig )
    
### Open days and revenue

fig, axes = plt.subplots(1, 3, figsize=(18,5))
sns.set_style("whitegrid")
sns.histplot(df_train['open_days'], kde=True, ax=axes[0])
sns.regplot(x=df_train['open_days'], y=df_train.revenue, ax=axes[1])
sns.histplot(df_val['open_days'], kde=True, ax=axes[2], color='red')

axes[0].set_title(f'Train. Hist of open days')
axes[1].set_title(f'Open days and revenue')
axes[3].set_title(f'Test. Hist of open days')
pdf.savefig( fig )
    
pdf.close()
545/90:
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("output.pdf")
545/91:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(18,5))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], binwidth = 0.5, ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.05, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_estimator=np.mean, ax=axes[2])
    sns.histplot(df_val[p_col], ax=axes[3], binwidth = 0.5, color='red')
    

    axes[0].set_title(f'Train {p_col}')
    axes[1].set_title(f'{p_col} and revenue')
    axes[2].set_title(f'{p_col} and revenue (mean)')
    axes[3].set_title(f'Test {p_col}' )
    pdf.savefig( fig )
    
### Open days and revenue

fig, axes = plt.subplots(1, 3, figsize=(18,5))
sns.set_style("whitegrid")
sns.histplot(df_train['open_days'], kde=True, ax=axes[0])
sns.regplot(x=df_train['open_days'], y=df_train.revenue, ax=axes[1])
sns.histplot(df_val['open_days'], kde=True, ax=axes[2], color='red')

axes[0].set_title(f'Train. Hist of open days')
axes[1].set_title(f'Open days and revenue')
axes[3].set_title(f'Test. Hist of open days')
pdf.savefig( fig )
    
pdf.close()
545/92:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
545/93:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
    random_state=20)

X_train.shape, X_test.shape
545/94: X_train
545/95:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['Id', 'City', 'revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
    random_state=20)

X_train.shape, X_test.shape
545/96: X_train
545/97:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
545/98: neigh = KNeighborsRegressor(metric = 'hamming',n_neighbors=2)
545/99: neigh.fit(X_train, y_train)
545/100:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['Id', 'City', 'City Group', 'Type', 'revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
    random_state=20)

X_train.shape, X_test.shape
545/101: X_train
545/102:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['Id', 'City', 'City Group', 'Type', 'open_days', 'revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
    random_state=20)

X_train.shape, X_test.shape
545/103: X_train
545/104: neigh = KNeighborsRegressor(metric = 'hamming',n_neighbors=2)
545/105: neigh = KNeighborsRegressor(metric = 'hamming',n_neighbors=20)
545/106: neigh
545/107: neigh.predict(X_test)
545/108: neigh.fit(X_train, y_train)
545/109: neigh.predict(X_test)
545/110: r2_score(y_test, neigh.predict(X_test))
545/111: neigh = KNeighborsRegressor(metric = 'hamming',n_neighbors=10)
545/112: neigh.fit(X_train, y_train)
545/113: neigh.predict(X_test)
545/114: r2_score(y_test, neigh.predict(X_test))
545/115: neigh = KNeighborsRegressor(metric = 'hamming',n_neighbors=5)
545/116: neigh.fit(X_train, y_train)
545/117: neigh.predict(X_test)
545/118: r2_score(y_test, neigh.predict(X_test))
545/119: neigh = KNeighborsRegressor(metric = 'hamming',n_neighbors=7)
545/120: neigh.fit(X_train, y_train)
545/121: neigh.predict(X_test)
545/122: r2_score(y_test, neigh.predict(X_test))
545/123:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
545/124: y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
545/125:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=7),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          verbose=10,
          scoring='r2')

sfs = sfs.fit(np.array(X_train), y_train_scale)
545/126: pd.DataFrame.from_dict(sfs.get_metric_dict()).T
545/127:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=10),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          verbose=10,
          scoring='r2')

sfs = sfs.fit(np.array(X_train), y_train_scale)
545/128: pd.DataFrame.from_dict(sfs.get_metric_dict()).T
545/129: df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
545/130: df_k
545/131: df_k['k'] = 10
545/132: df_k
545/133:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=1),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          verbose=10,
          scoring='r2')

sfs = sfs.fit(np.array(X_train), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
545/134: df_k
545/135: result
545/136:
frames = [df_k, df_k]

result = pd.concat(frames)
545/137: result
545/138:
frames = [df_k]

result = pd.concat(frames)
545/139: result
545/140:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=1),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          verbose=10,
          scoring='r2')

sfs = sfs.fit(np.array(X_train), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
545/141: df_k_all
545/142:
for k in enumerate(10):
    print(k)
545/143:
for k in range(1, 10):
    print(k)
545/144:
for k in range(2, 25):
    print(k)
545/145:
for k in range(2, 30):
    print(k)
545/146:
for k in range(2, 30):
    sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=k),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          verbose=10,
          scoring='r2')

    sfs = sfs.fit(np.array(X_train), y_train_scale)
    df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
    df_k['k'] = k
    frames = [df_k_all, df_k]
    df_k_all = pd.concat(frames)
    print(k)
545/147:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=1),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          verbose=0,
          scoring='r2')

sfs = sfs.fit(np.array(X_train), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
545/148: df_k_all
545/149:
for k in range(2, 30):
    sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=k),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          verbose=0,
          scoring='r2')

    sfs = sfs.fit(np.array(X_train), y_train_scale)
    df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
    df_k['k'] = k
    frames = [df_k_all, df_k]
    df_k_all = pd.concat(frames)
    print(k)
545/150: df_k_all
545/151: df_k_all.sort_values('avg_score')
545/152: df_k_all.sort_values('avg_score', ascending=False)
545/153: df_train
545/154: df_train.revenue
545/155: df_train.sort_values('revenue')
545/156: df_train_remove = df_train[df_train.revenue <= 10000000]
545/157: df_train_remove.sort_values('revenue')
545/158:
X_train, X_test, y_train, y_test = train_test_split(
    df_train_remove.drop(labels=['Id', 'City', 'City Group', 'Type', 'open_days', 'revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
    random_state=20)

X_train.shape, X_test.shape
545/159:
X_train, X_test, y_train, y_test = train_test_split(
    df_train_remove.drop(labels=['Id', 'City', 'City Group', 'Type', 'open_days', 'revenue'], axis=1),  # drop the target
    df_train_remove['revenue'],  # just the target
    test_size=0.2,
    random_state=20)

X_train.shape, X_test.shape
545/160:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
545/161: y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
545/162:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=1),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          verbose=0,
          scoring=['r2', 'neg_root_mean_squared_error'])

sfs = sfs.fit(np.array(X_train), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
545/163:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=1),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          verbose=0,
          scoring='neg_root_mean_squared_error')

sfs = sfs.fit(np.array(X_train), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
545/164:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=1),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          verbose=0,
          scoring='r2')

sfs = sfs.fit(np.array(X_train), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
545/165: df_k_all
545/166:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=1),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          verbose=0,
          scoring='neg_root_mean_squared_error')

sfs = sfs.fit(np.array(X_train), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
545/167: df_k_all
545/168:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=1),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          verbose=0,
          scoring='r2')

sfs = sfs.fit(np.array(X_train), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
545/169: df_k_all
545/170:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
545/171: y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
545/172:
X_train, X_test, y_train, y_test = train_test_split(
    df_train_remove.drop(labels=['Id', 'City', 'City Group', 'Type', 'open_days', 'revenue'], axis=1),  # drop the target
    df_train_remove['revenue'],  # just the target
    test_size=0.2,
    random_state=20)

X_train.shape, X_test.shape
545/173:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
545/174: y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
545/175:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=1),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          verbose=0,
          scoring='r2')

sfs = sfs.fit(np.array(X_train), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
545/176:
for k in range(2, 30):
    sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=k),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          verbose=0,
          scoring='r2')

    sfs = sfs.fit(np.array(X_train), y_train_scale)
    df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
    df_k['k'] = k
    frames = [df_k_all, df_k]
    df_k_all = pd.concat(frames)
    print(k)
545/177: df_k_all.sort_values('avg_score', ascending=False)
545/178: df_k_all.sort_values('avg_score', ascending=False)[:20]
545/179: df_k_all.sort_values('avg_score', ascending=False)[0]
545/180: df_k_all.sort_values('avg_score', ascending=False)[1]
545/181: df_k_all.sort_values('avg_score', ascending=False)[:1]
545/182: df_k_all.sort_values('avg_score', ascending=False)[:1]['feature_idx']
545/183: df_k_all.sort_values('avg_score', ascending=False).iloc[0, 1]
545/184: df_k_all.sort_values('avg_score', ascending=False).iloc[0, 0]
545/185: df_k_all.sort_values('avg_score', ascending=False).iloc[0, 0].to_list()
545/186: df_k_all.sort_values('avg_score', ascending=False).iloc[0, 0]
545/187: list(df_k_all.sort_values('avg_score', ascending=False).iloc[0, 0])
545/188: f_indexes = list(df_k_all.sort_values('avg_score', ascending=False).iloc[0, 0])
545/189: X_train.columns[f_indexes]
545/190: list(X_train.columns[f_indexes])
545/191: colmns_selected = list(X_train.columns[f_indexes])
545/192: y_test
545/193:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=2)
knn_reg.fit(X_train[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/194: mean_squared_error(y_test, y_pred_scale, squared=False)
545/195: r2_score(y_test, y_pred_scale)
545/196: df_k_all.sort_values('avg_score', ascending=False).iloc[5, 0]
545/197: f_indexes = list(df_k_all.sort_values('avg_score', ascending=False).iloc[5, 0])
545/198: colmns_selected = list(X_train.columns[f_indexes])
545/199:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=8)
knn_reg.fit(X_train[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/200: mean_squared_error(y_test, y_pred_scale, squared=False)
545/201: r2_score(y_test, y_pred_scale)
545/202:
for k in range(2, 30):
    sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=k),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=20
          verbose=0,
          scoring='r2')

    sfs = sfs.fit(np.array(X_train), y_train_scale)
    df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
    df_k['k'] = k
    frames = [df_k_all, df_k]
    df_k_all = pd.concat(frames)
    print(k)
545/203:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=1),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=20,
          verbose=0,
          scoring='r2')

sfs = sfs.fit(np.array(X_train), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
545/204: df_k_all.sort_values('avg_score', ascending=False)[:20]
545/205:
for k in range(2, 30):
    sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=k),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=20,
          verbose=0,
          scoring='r2')

    sfs = sfs.fit(np.array(X_train), y_train_scale)
    df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
    df_k['k'] = k
    frames = [df_k_all, df_k]
    df_k_all = pd.concat(frames)
    print(k)
545/206: df_k_all.sort_values('avg_score', ascending=False)[:20]
545/207: f_indexes = list(df_k_all.sort_values('avg_score', ascending=False).iloc[0, 0])
545/208: colmns_selected = list(X_train.columns[f_indexes])
545/209:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=3)
knn_reg.fit(X_train[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/210: mean_squared_error(y_test, y_pred_scale, squared=False)
545/211: r2_score(y_test, y_pred_scale)
545/212: X_train.columns[f_indexes]
545/213:
y_val = knn_reg.predict(df_val.columns[f_indexes])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
545/214:
y_val = knn_reg.predict(df_val[colmns_selected])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
545/215: y_val_scale
545/216:
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t[df_t < 0] = 0
df_t.to_csv("lin_reg_all.csv", index=False)
545/217:
df_t = df_val[['Id']]
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t[df_t < 0] = 0
df_t.to_csv("lin_reg_all.csv", index=False)
545/218: df_t
545/219: df_t.describe()
545/220:
df_t = df_val[['Id']]
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("knn.csv", index=False)
545/221: df_t.describe()
545/222:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['Id', 'City', 'City Group', 'Type', 'open_days', 'revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
    random_state=20)

X_train.shape, X_test.shape
545/223:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
545/224: y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
545/225:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=1),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=20,
          verbose=0,
          scoring='r2')

sfs = sfs.fit(np.array(X_train), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
545/226:
for k in range(2, 30):
    sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=k),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=20,
          verbose=0,
          scoring='r2')

    sfs = sfs.fit(np.array(X_train), y_train_scale)
    df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
    df_k['k'] = k
    frames = [df_k_all, df_k]
    df_k_all = pd.concat(frames)
    print(k)
545/227:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=1),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=20,
          verbose=0,
          scoring='r2')

sfs = sfs.fit(np.array(X_train), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
545/228:
for k in range(2, 15):
    sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=k),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=20,
          verbose=0,
          scoring='r2')

    sfs = sfs.fit(np.array(X_train), y_train_scale)
    df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
    df_k['k'] = k
    frames = [df_k_all, df_k]
    df_k_all = pd.concat(frames)
    print(k)
545/229: df_k_all.sort_values('avg_score', ascending=False)[:20]
545/230: f_indexes = list(df_k_all.sort_values('avg_score', ascending=False).iloc[0, 0])
545/231: colmns_selected = list(X_train.columns[f_indexes])
545/232:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=3)
knn_reg.fit(X_train[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/233: mean_squared_error(y_test, y_pred_scale, squared=False)
545/234: r2_score(y_test, y_pred_scale)
545/235:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=8)
knn_reg.fit(X_train[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/236: mean_squared_error(y_test, y_pred_scale, squared=False)
545/237: r2_score(y_test, y_pred_scale)
545/238: X_train.columns[f_indexes]
545/239:
y_val = knn_reg.predict(df_val[colmns_selected])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
545/240: y_val_scale
545/241:
df_t = df_val[['Id']]
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("knn.csv", index=False)
545/242: df_t.describe()
545/243: ##df_train_remove = df_train[df_train.revenue <= 10000000]
545/244:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['Id', 'City', 'open_days', 'revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.2,
    random_state=20)

X_train.shape, X_test.shape
545/245: d = defaultdict(LabelEncoder)
545/246: X_train
545/247:
# Encoding the variable
train_transformed = X_train.apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
test_transformed = X_test.apply(lambda x: d[x.name].transform(x))
545/248:
# Encoding the variable
train_transformed = X_train.apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
#test_transformed = X_test.apply(lambda x: d[x.name].transform(x))
545/249: d
545/250: d['City Group']
545/251:
# Encoding the variable
train_transformed = X_train.apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
#test_transformed = X_test.apply(lambda x: d[x.name].transform(x))
545/252: d['City Group']
545/253: d['City Group'].transform(X_test['City Group'])
545/254: X_train
545/255: d['P1'].transform(X_test['P1'])
545/256: X_train[X_train==4.5]
545/257:
# Encoding the variable
train_transformed = X_train.apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
#test_transformed = X_test.apply(lambda x: d[x.name].transform(x))
545/258: d['P1'].transform(X_test['P1'])
545/259:
# Encoding the variable
train_transformed = X_train.apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
test_transformed = X_test.apply(lambda x: d[x.name].transform(x))
545/260: d = defaultdict(OrdinalEncoder(handle_unknown='ignore', unknown_value=np.nan))
545/261:
# Encoding the variable
train_transformed = X_train.apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
test_transformed = X_test.apply(lambda x: d[x.name].transform(x))
545/262: d = defaultdict(OrdinalEncoder())
545/263: from sklearn.preprocessing import OrdinalEncoder
545/264: d = defaultdict(OrdinalEncoder())
545/265: d = defaultdict(OrdinalEncoder)
545/266:
# Encoding the variable
train_transformed = X_train.apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
test_transformed = X_test.apply(lambda x: d[x.name].transform(x))
545/267:
# Encoding the variable
train_transformed = X_train.apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
#test_transformed = X_test.apply(lambda x: d[x.name].transform(x))
545/268: d['P1'].transform(X_test['P1'])
545/269: d = defaultdict(OrdinalEncoder)
545/270:
# Encoding the variable
train_transformed = X_train.apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
#test_transformed = X_test.apply(lambda x: d[x.name].transform(x))
545/271: ord_e = OrdinalEncoder()
545/272: ord_e.fit(X_train)
545/273: ord_e.fit_transform(X_train)
545/274: ord_e.fit_transform(X_train).shape
545/275: ord_e.fit_transform(X_train)
545/276: ord_e.transform(X_test)
545/277: ord_e = OrdinalEncoder(handle_unknown='ignore', unknown_value=np.nan)
545/278: ord_e.fit_transform(X_train)
545/279: ord_e.transform(X_test)
545/280: ord_e = OrdinalEncoder(handle_unknown='ignore')
545/281: ord_e.fit_transform(X_train)
545/282: ord_e = OrdinalEncoder(unknown_value=np.nan)
545/283: ord_e.fit_transform(X_train)
545/284: ord_e = OrdinalEncoder(handle_unknown=use_encoded_value, unknown_value=np.nan)
545/285: ord_e.fit_transform(X_train)
545/286: ord_e = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)
545/287: ord_e.fit_transform(X_train)
545/288: ord_e.transform(X_test)
545/289: pd.DataFrame.from_records(ord_e.transform(X_test))
545/290: ddf = pd.DataFrame.from_records(ord_e.transform(X_test))
545/291: ddf
545/292: ddf['38']
545/293: ddf[38]
545/294: ddf[38].max
545/295: ddf[38].max()
545/296: ddf.apply(lambda column: column.replace(0, max(column)), axis=0)
545/297: ddf.apply(lambda column: column.replace(np.NAN, max(column)), axis=0)
545/298: ddf.apply(lambda column: column.replace(np.NAN, max(column)+1), axis=0)
545/299: X_train = ord_e.fit_transform(X_train)
545/300: ddf = pd.DataFrame.from_records(ord_e.transform(X_test))
545/301: X_test_tranform = pd.DataFrame.from_records(ord_e.transform(X_test))
545/302: X_test_tranform = ddf.apply(lambda column: column.replace(np.NAN, max(column)+1), axis=0)
545/303:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
545/304: y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
545/305: X_train
545/306:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=1),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=2,
          verbose=0,
          scoring='r2')

sfs = sfs.fit(np.array(X_train), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
545/307: df_k_all.sort_values('avg_score', ascending=False)[:20]
545/308:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['Id', 'City', 'open_days', 'revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=20)

X_train.shape, X_test.shape
545/309: from sklearn.preprocessing import OrdinalEncoder
545/310: ord_e = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)
545/311: X_train = ord_e.fit_transform(X_train)
545/312: X_test_tranform = pd.DataFrame.from_records(ord_e.transform(X_test))
545/313: X_test_tranform = ddf.apply(lambda column: column.replace(np.NAN, max(column)+1), axis=0)
545/314:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
545/315: y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
545/316: X_train
545/317:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=1),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=2,
          verbose=0,
          scoring='r2')

sfs = sfs.fit(np.array(X_train), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
545/318:
for k in range(2, 15):
    sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=k),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=20,
          verbose=0,
          scoring='r2')

    sfs = sfs.fit(np.array(X_train), y_train_scale)
    df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
    df_k['k'] = k
    frames = [df_k_all, df_k]
    df_k_all = pd.concat(frames)
    print(k)
545/319:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=1),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=20,
          verbose=0,
          scoring='r2')

sfs = sfs.fit(np.array(X_train), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
545/320:
for k in range(2, 15):
    sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=k),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=20,
          verbose=0,
          scoring='r2')

    sfs = sfs.fit(np.array(X_train), y_train_scale)
    df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
    df_k['k'] = k
    frames = [df_k_all, df_k]
    df_k_all = pd.concat(frames)
    print(k)
545/321: df_k_all.sort_values('avg_score', ascending=False)[:20]
545/322: f_indexes = list(df_k_all.sort_values('avg_score', ascending=False).iloc[0, 0])
545/323: colmns_selected = list(X_train.columns[f_indexes])
545/324: ##df_train_remove = df_train[df_train.revenue <= 10000000]
545/325:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['Id', 'City', 'open_days', 'revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=20)

X_train.shape, X_test.shape
545/326: from sklearn.preprocessing import OrdinalEncoder
545/327: ord_e = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)
545/328: X_train = ord_e.fit_transform(X_train)
545/329: X_test_tranform = pd.DataFrame.from_records(ord_e.transform(X_test))
545/330: X_test_tranform = ddf.apply(lambda column: column.replace(np.NAN, max(column)+1), axis=0)
545/331: X_test_tranform
545/332:
X_train = ord_e.fit_transform(X_train)
X_train = pd.DataFrame(X_train)
545/333: X_test_tranform = pd.DataFrame.from_records(ord_e.transform(X_test))
545/334: X_test_tranform = ddf.apply(lambda column: column.replace(np.NAN, max(column)+1), axis=0)
545/335:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['Id', 'City', 'open_days', 'revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=20)

X_train.shape, X_test.shape
545/336: from sklearn.preprocessing import OrdinalEncoder
545/337: ord_e = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)
545/338:
X_train = ord_e.fit_transform(X_train)
X_train = pd.DataFrame(X_train)
545/339: X_test_tranform = pd.DataFrame.from_records(ord_e.transform(X_test))
545/340: X_test_tranform = ddf.apply(lambda column: column.replace(np.NAN, max(column)+1), axis=0)
545/341: X_test_tranform
545/342: X_train
545/343: from sklearn.preprocessing import OrdinalEncoder
545/344: ord_e = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)
545/345:
X_train_transform = ord_e.fit_transform(X_train)
X_train = pd.DataFrame(X_train)
545/346:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['Id', 'City', 'open_days', 'revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=20)

X_train.shape, X_test.shape
545/347: from sklearn.preprocessing import OrdinalEncoder
545/348: ord_e = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)
545/349:
X_train_transform = ord_e.fit_transform(X_train)
X_train_transform = pd.DataFrame(X_train_transform)
545/350: X_test_transform = pd.DataFrame.from_records(ord_e.transform(X_test))
545/351: X_test_transform = ddf.apply(lambda column: column.replace(np.NAN, max(column)+1), axis=0)
545/352: X_train
545/353: X_train_transform
545/354: X_train
545/355:
X_train_transform.columns = X_train.columns
X_test_transform.columns = X_train.columns
545/356: X_train_transform
545/357:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
545/358: y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
545/359: X_train
545/360:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=1),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=20,
          verbose=0,
          scoring='r2')

sfs = sfs.fit(np.array(X_train_transform), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
545/361: df_k_all.sort_values('avg_score', ascending=False)[:20]
545/362: df_k_all.sort_values('avg_score', ascending=False)
545/363:
for k in range(2, 15):
    sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=k),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=20,
          verbose=0,
          scoring='r2')

    sfs = sfs.fit(np.array(X_train_transform), y_train_scale)
    df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
    df_k['k'] = k
    frames = [df_k_all, df_k]
    df_k_all = pd.concat(frames)
    print(k)
545/364: df_k_all.sort_values('avg_score', ascending=False)
545/365: df_k_all.sort_values('avg_score', ascending=False)
545/366: f_indexes = list(df_k_all.sort_values('avg_score', ascending=False).iloc[0, 0])
545/367: colmns_selected = list(X_train.columns[f_indexes])
545/368:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=6)
knn_reg.fit(X_train[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/369: mean_squared_error(y_test, y_pred_scale, squared=False)
545/370: r2_score(y_test, y_pred_scale)
545/371: X_train.columns[f_indexes]
545/372:
y_val = knn_reg.predict(df_val[colmns_selected])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
545/373: y_val_scale
545/374:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val))
df_val_transform = ddf.apply(lambda column: column.replace(np.NAN, max(column)+1), axis=0)
545/375:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[X_train.columns]))
df_val_transform = ddf.apply(lambda column: column.replace(np.NAN, max(column)+1), axis=0)
545/376:
X_train_transform.columns = X_train.columns
X_test_transform.columns = X_train.columns
df_val_transform = X_train.columns
545/377: df_val_transform
545/378:
X_train_transform = ord_e.fit_transform(X_train)
X_train_transform = pd.DataFrame(X_train_transform)
545/379:
X_test_transform = pd.DataFrame.from_records(ord_e.transform(X_test))
X_test_transform = X_test_transform.apply(lambda column: column.replace(np.NAN, max(column)+1), axis=0)
545/380:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[X_train.columns]))
df_val_transform = df_val_transform.apply(lambda column: column.replace(np.NAN, max(column)+1), axis=0)
545/381: df_val_transform
545/382:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['Id', 'City', 'open_days', 'revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=20)

X_train.shape, X_test.shape
545/383: from sklearn.preprocessing import OrdinalEncoder
545/384:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['Id', 'City', 'open_days', 'revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=0)

X_train.shape, X_test.shape
545/385: from sklearn.preprocessing import OrdinalEncoder
545/386: ord_e = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)
545/387:
X_train_transform = ord_e.fit_transform(X_train)
X_train_transform = pd.DataFrame(X_train_transform)
545/388:
X_test_transform = pd.DataFrame.from_records(ord_e.transform(X_test))
X_test_transform = X_test_transform.apply(lambda column: column.replace(np.NAN, max(column)+1), axis=0)
545/389:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[X_train.columns]))
df_val_transform = df_val_transform.apply(lambda column: column.replace(np.NAN, max(column)+1), axis=0)
545/390:
X_train_transform.columns = X_train.columns
X_test_transform.columns = X_train.columns
df_val_transform = X_train.columns
545/391:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
545/392: y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
545/393:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=1),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=20,
          verbose=0,
          scoring='r2')

sfs = sfs.fit(np.array(X_train_transform), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
545/394: df_k_all.sort_values('avg_score', ascending=False)
545/395:
for k in range(2, 15):
    sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=k),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=20,
          verbose=0,
          scoring='r2')

    sfs = sfs.fit(np.array(X_train_transform), y_train_scale)
    df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
    df_k['k'] = k
    frames = [df_k_all, df_k]
    df_k_all = pd.concat(frames)
    print(k)
545/396: df_k_all.sort_values('avg_score', ascending=False)
545/397: df_k_all.sort_values('avg_score', ascending=False)[:20]
545/398: f_indexes = list(df_k_all.sort_values('avg_score', ascending=False).iloc[0, 0])
545/399: colmns_selected = list(X_train.columns[f_indexes])
545/400:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=6)
knn_reg.fit(X_train[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/401: mean_squared_error(y_test, y_pred_scale, squared=False)
545/402: r2_score(y_test, y_pred_scale)
545/403:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=5)
knn_reg.fit(X_train[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/404: mean_squared_error(y_test, y_pred_scale, squared=False)
545/405: r2_score(y_test, y_pred_scale)
545/406:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=6)
knn_reg.fit(X_train[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/407: mean_squared_error(y_test, y_pred_scale, squared=False)
545/408: r2_score(y_test, y_pred_scale)
545/409:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=7)
knn_reg.fit(X_train[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/410: mean_squared_error(y_test, y_pred_scale, squared=False)
545/411: r2_score(y_test, y_pred_scale)
545/412:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=8)
knn_reg.fit(X_train[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/413: mean_squared_error(y_test, y_pred_scale, squared=False)
545/414: r2_score(y_test, y_pred_scale)
545/415:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=7)
knn_reg.fit(X_train[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/416: mean_squared_error(y_test, y_pred_scale, squared=False)
545/417: r2_score(y_test, y_pred_scale)
545/418:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=6)
knn_reg.fit(X_train[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/419: mean_squared_error(y_test, y_pred_scale, squared=False)
545/420: r2_score(y_test, y_pred_scale)
545/421:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=5)
knn_reg.fit(X_train[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/422: mean_squared_error(y_test, y_pred_scale, squared=False)
545/423: r2_score(y_test, y_pred_scale)
545/424:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=6)
knn_reg.fit(X_train[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/425: mean_squared_error(y_test, y_pred_scale, squared=False)
545/426: r2_score(y_test, y_pred_scale)
545/427: X_train.columns[f_indexes]
545/428:
y_val = knn_reg.predict(df_val[colmns_selected])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
545/429: y_val_scale
545/430:
df_t = df_val[['Id']]
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("knn.csv", index=False)
545/431: df_t.describe()
545/432:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=5)
knn_reg.fit(X_train[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/433: mean_squared_error(y_test, y_pred_scale, squared=False)
545/434: r2_score(y_test, y_pred_scale)
545/435: X_train.columns[f_indexes]
545/436:
y_val = knn_reg.predict(df_val[colmns_selected])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
545/437: y_val_scale
545/438:
df_t = df_val[['Id']]
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("knn.csv", index=False)
545/439: df_t.describe()
545/440: f_indexes = list(df_k_all.sort_values('avg_score', ascending=False).iloc[4, 0])
545/441: colmns_selected = list(X_train.columns[f_indexes])
545/442:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=10)
knn_reg.fit(X_train[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/443: mean_squared_error(y_test, y_pred_scale, squared=False)
545/444: r2_score(y_test, y_pred_scale)
545/445: X_train.columns[f_indexes]
545/446:
y_val = knn_reg.predict(df_val[colmns_selected])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
545/447: y_val_scale
545/448:
df_t = df_val[['Id']]
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("knn.csv", index=False)
545/449: df_t.describe()
545/450: df_k_all.sort_values('std_err', ascending=False)[:20]
545/451: df_k_all.sort_values('std_err', ascending=True)[:20]
545/452: df_k_all.sort_values('std_dev', ascending=True)[:20]
545/453: df_k_all.sort_values('std_err', ascending=True)[:20]
545/454: f_indexes = list(df_k_all.sort_values('std_err', ascending=True).iloc[0, 0])
545/455: colmns_selected = list(X_train.columns[f_indexes])
545/456:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=10)
knn_reg.fit(X_train[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/457: colmns_selected = list(X_train.columns[f_indexes])
545/458:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=9)
knn_reg.fit(X_train[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/459: mean_squared_error(y_test, y_pred_scale, squared=False)
545/460: r2_score(y_test, y_pred_scale)
545/461: X_train.columns[f_indexes]
545/462:
y_val = knn_reg.predict(df_val[colmns_selected])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
545/463: y_val_scale
545/464:
df_t = df_val[['Id']]
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("knn.csv", index=False)
545/465: df_t.describe()
545/466:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=1),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=20,
          verbose=0,
          scoring='neg_root_mean_squared_error')

sfs = sfs.fit(np.array(X_train_transform), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
545/467: df_k_all.sort_values('avg_score', ascending=False)[:20]
545/468:
for k in range(2, 15):
    sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=k),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=20,
          verbose=0,
          scoring='neg_root_mean_squared_error')

    sfs = sfs.fit(np.array(X_train_transform), y_train_scale)
    df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
    df_k['k'] = k
    frames = [df_k_all, df_k]
    df_k_all = pd.concat(frames)
    print(k)
545/469: df_k_all.sort_values('avg_score', ascending=False)[:20]
545/470: df_k_all.sort_values('avg_score', ascending=False)[:20]
545/471: df_k_all.sort_values('std_err', ascending=True)[:20]
545/472: df_k_all.sort_values('avg_score', ascending=False)[:20]
545/473: f_indexes = list(df_k_all.sort_values('avg_score', ascending=False).iloc[0, 0])
545/474: f_indexes = list(df_k_all.sort_values('avg_score', ascending=False).iloc[1, 0])
545/475: colmns_selected = list(X_train.columns[f_indexes])
545/476:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=8)
knn_reg.fit(X_train[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/477: mean_squared_error(y_test, y_pred_scale, squared=False)
545/478: r2_score(y_test, y_pred_scale)
545/479: X_train.columns[f_indexes]
545/480:
y_val = knn_reg.predict(df_val[colmns_selected])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
545/481: y_val_scale
545/482:
df_t = df_val[['Id']]
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("knn.csv", index=False)
545/483: df_t.describe()
545/484: f_indexes = list(df_k_all.sort_values('avg_score', ascending=False).iloc[0, 0])
545/485: colmns_selected = list(X_train.columns[f_indexes])
545/486:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=4)
knn_reg.fit(X_train[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/487: f_indexes = list(df_k_all.sort_values('avg_score', ascending=False).iloc[0, 0])
545/488: colmns_selected = list(X_train.columns[f_indexes])
545/489:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=4)
knn_reg.fit(X_train[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/490: colmns_selected
545/491:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=4)
knn_reg.fit(X_train[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/492: X_train[colmns_selected]
545/493:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=4)
knn_reg.fit(X_train_transform[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test_transform[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/494: mean_squared_error(y_test, y_pred_scale, squared=False)
545/495: r2_score(y_test, y_pred_scale)
545/496: df_k_all.sort_values('avg_score', ascending=False)[:20]
545/497: f_indexes = list(df_k_all.sort_values('avg_score', ascending=False).iloc[1, 0])
545/498: colmns_selected = list(X_train.columns[f_indexes])
545/499:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=8)
knn_reg.fit(X_train_transform[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test_transform[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/500: mean_squared_error(y_test, y_pred_scale, squared=False)
545/501: r2_score(y_test, y_pred_scale)
545/502: X_train.columns[f_indexes]
545/503: f_indexes = list(df_k_all.sort_values('avg_score', ascending=False).iloc[1, 0])
545/504: colmns_selected = list(X_train.columns[f_indexes])
545/505:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=8)
knn_reg.fit(X_train_transform[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test_transform[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/506: mean_squared_error(y_test, y_pred_scale, squared=False)
545/507: r2_score(y_test, y_pred_scale)
545/508: X_train.columns[f_indexes]
545/509: f_indexes = list(df_k_all.sort_values('avg_score', ascending=False).iloc[2, 0])
545/510: colmns_selected = list(X_train.columns[f_indexes])
545/511:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=8)
knn_reg.fit(X_train_transform[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test_transform[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/512: mean_squared_error(y_test, y_pred_scale, squared=False)
545/513: r2_score(y_test, y_pred_scale)
545/514: f_indexes = list(df_k_all.sort_values('avg_score', ascending=False).iloc[1, 0])
545/515: colmns_selected = list(X_train.columns[f_indexes])
545/516:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=8)
knn_reg.fit(X_train_transform[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test_transform[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/517: mean_squared_error(y_test, y_pred_scale, squared=False)
545/518: r2_score(y_test, y_pred_scale)
545/519: X_train.columns[f_indexes]
545/520:
y_val = knn_reg.predict(df_val[colmns_selected])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
545/521: y_val_scale
545/522:
df_t = df_val[['Id']]
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("knn.csv", index=False)
545/523: df_t.describe()
545/524: df_k_all.sort_values('std_err', ascending=True)[:20]
545/525: f_indexes = list(df_k_all.sort_values('avg_score', ascending=False).iloc[0, 0])
545/526: colmns_selected = list(X_train.columns[f_indexes])
545/527:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=4)
knn_reg.fit(X_train_transform[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test_transform[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/528: mean_squared_error(y_test, y_pred_scale, squared=False)
545/529: r2_score(y_test, y_pred_scale)
545/530: X_train.columns[f_indexes]
545/531: f_indexes = list(df_k_all.sort_values('std_err', ascending=True).iloc[0, 0])
545/532: colmns_selected = list(X_train.columns[f_indexes])
545/533:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=4)
knn_reg.fit(X_train_transform[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test_transform[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/534: mean_squared_error(y_test, y_pred_scale, squared=False)
545/535: r2_score(y_test, y_pred_scale)
545/536: X_train.columns[f_indexes]
545/537:
y_val = knn_reg.predict(df_val[colmns_selected])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
545/538: y_val_scale
545/539:
df_t = df_val[['Id']]
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("knn.csv", index=False)
545/540: df_t.describe()
545/541: df_k_all.sort_values('avg_score', ascending=False)[:20]
545/542: f_indexes = list(df_k_all.sort_values('std_err', ascending=True).iloc[0, 0])
545/543: colmns_selected = list(X_train.columns[f_indexes])
545/544:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=4)
knn_reg.fit(X_train_transform[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test_transform[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/545: mean_squared_error(y_test, y_pred_scale, squared=False)
545/546: r2_score(y_test, y_pred_scale)
545/547: X_train.columns[f_indexes]
545/548:
y_val = knn_reg.predict(df_val[colmns_selected])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
545/549: y_val_scale
545/550: f_indexes = list(df_k_all.sort_values('avg_score', ascending=False).iloc[0, 0])
545/551: colmns_selected = list(X_train.columns[f_indexes])
545/552:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=4)
knn_reg.fit(X_train_transform[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test_transform[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/553: mean_squared_error(y_test, y_pred_scale, squared=False)
545/554: r2_score(y_test, y_pred_scale)
545/555: X_train.columns[f_indexes]
545/556:
y_val = knn_reg.predict(df_val[colmns_selected])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
545/557:
y_val = knn_reg.predict(df_val_transform[colmns_selected])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
545/558: df_val_transform
545/559:
X_test_transform = pd.DataFrame.from_records(ord_e.transform(X_test))
X_test_transform = X_test_transform.apply(lambda column: column.replace(np.NAN, max(column)+1), axis=0)
545/560:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[X_train.columns]))
df_val_transform = df_val_transform.apply(lambda column: column.replace(np.NAN, max(column)+1), axis=0)
545/561: df_val_transform
545/562:
X_train_transform.columns = X_train.columns
X_test_transform.columns = X_train.columns
df_val_transform.columns = X_train.columns
545/563:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
545/564: y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
545/565:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=1),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=20,
          verbose=0,
          scoring='neg_root_mean_squared_error')

sfs = sfs.fit(np.array(X_train_transform), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
545/566:
for k in range(2, 15):
    sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=k),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=20,
          verbose=0,
          scoring='neg_root_mean_squared_error')

    sfs = sfs.fit(np.array(X_train_transform), y_train_scale)
    df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
    df_k['k'] = k
    frames = [df_k_all, df_k]
    df_k_all = pd.concat(frames)
    print(k)
545/567: df_val_transform
545/568: df_k_all.sort_values('avg_score', ascending=False)[:20]
545/569: f_indexes = list(df_k_all.sort_values('avg_score', ascending=False).iloc[0, 0])
545/570: colmns_selected = list(X_train.columns[f_indexes])
545/571:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=4)
knn_reg.fit(X_train_transform[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test_transform[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/572: mean_squared_error(y_test, y_pred_scale, squared=False)
545/573: r2_score(y_test, y_pred_scale)
545/574: X_train.columns[f_indexes]
545/575: df_val_transform
545/576:
y_val = knn_reg.predict(df_val_transform[colmns_selected])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
545/577: y_val_scale
545/578:
df_t = df_val[['Id']]
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("knn.csv", index=False)
545/579: df_t.describe()
545/580:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=1),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=25,
          verbose=0,
          scoring='neg_root_mean_squared_error')

sfs = sfs.fit(np.array(X_train_transform), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
545/581:
for k in range(2, 15):
    sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=k),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=25,
          verbose=0,
          scoring='neg_root_mean_squared_error')

    sfs = sfs.fit(np.array(X_train_transform), y_train_scale)
    df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
    df_k['k'] = k
    frames = [df_k_all, df_k]
    df_k_all = pd.concat(frames)
    print(k)
545/582: df_k_all.sort_values('avg_score', ascending=False)[:20]
545/583: f_indexes = list(df_k_all.sort_values('avg_score', ascending=False).iloc[0, 0])
545/584: colmns_selected = list(X_train.columns[f_indexes])
545/585:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=4)
knn_reg.fit(X_train_transform[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test_transform[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
545/586: mean_squared_error(y_test, y_pred_scale, squared=False)
545/587: r2_score(y_test, y_pred_scale)
545/588: X_train.columns[f_indexes]
545/589: df_val_transform
545/590: df_val_transform.columns[f_indexes]
545/591: df_val_transform[colmns_selected]
545/592: df_val_transform[colmns_selected].head(2)
545/593:
y_val = knn_reg.predict(df_val_transform[colmns_selected])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
545/594: y_val_scale
545/595:
df_t = df_val[['Id']]
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("knn.csv", index=False)
545/596: df_t.describe()
545/597: df_k_all.sort_values('avg_score', ascending=False)
548/1:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
548/2:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
548/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
548/4: df_train.head(5)
548/5:
f, axes = plt.subplots(2, 2, figsize=(12,10))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('Train')
axes[1, 1].set_title('Test')

plt.show()
548/6: #So, there is a new type 'MB' in test data and there is only one restaurant with DT type in the train dataset.
548/7: df_train
548/8:
f, axes = plt.subplots(1, 2, figsize=(12,4))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", ax=axes[1])
548/9:
#f, axes = plt.subplots(1, 2, figsize=(12,4))
#sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", hue="City Group")
548/10:
### add a function for counting open days for each restaurant
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
548/11:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
548/12:
### I don't know anything about P columns, so I want to check them in both datasets
P_cols = ['P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'P11', 
          'P12', 'P13', 'P14', 'P15', 'P16','P17', 'P18', 'P19', 'P20', 'P21',
          'P22', 'P23', 'P24', 'P25', 'P26', 'P27', 'P28', 'P29', 'P30', 'P31', 
          'P32', 'P33', 'P34', 'P35', 'P36', 'P37']
548/13:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("output.pdf")
548/14:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(18,5))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], binwidth = 0.5, ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.05, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_estimator=np.mean, ax=axes[2])
    sns.histplot(df_val[p_col], ax=axes[3], binwidth = 0.5, color='red')
    

    axes[0].set_title(f'Train {p_col}')
    axes[1].set_title(f'{p_col} and revenue')
    axes[2].set_title(f'{p_col} and revenue (mean)')
    axes[3].set_title(f'Test {p_col}' )
    pdf.savefig( fig )
    
### Open days and revenue
fig, axes = plt.subplots(1, 3, figsize=(18,5))
sns.set_style("whitegrid")
sns.histplot(df_train['open_days'], kde=True, ax=axes[0])
sns.regplot(x=df_train['open_days'], y=df_train.revenue, ax=axes[1])
sns.histplot(df_val['open_days'], kde=True, ax=axes[2], color='red')

axes[0].set_title(f'Train. Hist of open days')
axes[1].set_title(f'Open days and revenue')
axes[3].set_title(f'Test. Hist of open days')
pdf.savefig( fig )
    
pdf.close()
548/15:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(18,5))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], binwidth = 0.5, ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.05, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_estimator=np.mean, ax=axes[2])
    sns.histplot(df_val[p_col], ax=axes[3], binwidth = 0.5, color='red')
    

    axes[0].set_title(f'Train {p_col}')
    axes[1].set_title(f'{p_col} and revenue')
    axes[2].set_title(f'{p_col} and revenue (mean)')
    axes[3].set_title(f'Test {p_col}' )
    pdf.savefig( fig )
    
### Open days and revenue
fig, axes = plt.subplots(1, 3, figsize=(18,5))
sns.set_style("whitegrid")
sns.histplot(df_train['open_days'], kde=True, ax=axes[0])
sns.regplot(x=df_train['open_days'], y=df_train.revenue, ax=axes[1])
sns.histplot(df_val['open_days'], kde=True, ax=axes[2], color='red')

axes[0].set_title(f'Train. Hist of open days')
axes[1].set_title(f'Open days and revenue')
axes[2].set_title(f'Test. Hist of open days')
pdf.savefig( fig )

### close file
pdf.close()
548/16: df_train_remove.sort_values('revenue')
548/17:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['Id', 'City', 'City Group', 'Type', 'open_days', 'revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=0)

X_train.shape, X_test.shape
548/18:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
548/19: y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
548/20: X_train
548/21:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['Id', 'City', 'open_days', 'revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=0)

X_train.shape, X_test.shape
548/22: from sklearn.preprocessing import OrdinalEncoder
548/23: ord_e = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)
548/24:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['Id', 'open_days', 'revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=0)

X_train.shape, X_test.shape
548/25: from sklearn.preprocessing import OrdinalEncoder
548/26: ord_e = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)
548/27:
X_train_transform = ord_e.fit_transform(X_train[['City', 'City Group', 'Type',]])
X_train_transform = pd.DataFrame(X_train_transform)
548/28:
X_test_transform = pd.DataFrame.from_records(ord_e.transform(X_test[['City', 'City Group', 'Type',]]))
X_test_transform = X_test_transform.apply(lambda column: column.replace(np.NAN, max(column)+1), axis=0)
548/29:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[['City', 'City Group', 'Type',]]))
df_val_transform = df_val_transform.apply(lambda column: column.replace(np.NAN, max(column)+1), axis=0)
548/30: X_train_transform
548/31: cat_cols_label_enc = ['City', 'City Group', 'Type',]
548/32:
X_train_transform = ord_e.fit_transform(X_train[cat_cols_label_enc])
X_train_transform = pd.DataFrame(X_train_transform)
548/33:
X_test_transform = pd.DataFrame.from_records(ord_e.transform(X_test[cat_cols_label_enc]))
X_test_transform = X_test_transform.apply(lambda column: column.replace(np.NAN, max(column)+1), axis=0)
548/34:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[cat_cols_label_enc]))
df_val_transform = df_val_transform.apply(lambda column: column.replace(np.NAN, max(column)+1), axis=0)
548/35:
X_train_transform.columns = cat_cols_label_enc
X_test_transform.columns = cat_cols_label_enc
df_val_transform.columns = cat_cols_label_enc
548/36: X_train_transform
548/37: X_train_transform
548/38:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
548/39: y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
548/40: X_train
548/41: X_train.drop(cat_cols_label_enc, axis=1)
548/42:
X_train.drop(cat_cols_label_enc, axis=1, inplace=True)
X_test.drop(cat_cols_label_enc, axis=1, inplace=True)
df_val.drop(cat_cols_label_enc, axis=1, inplace=True)
548/43: X_train
548/44: X_train_transform
548/45:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['Id', 'open_days', 'revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=0)

X_train.shape, X_test.shape
548/46: from sklearn.preprocessing import OrdinalEncoder
548/47: ord_e = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)
548/48: cat_cols_label_enc = ['City', 'City Group', 'Type',]
548/49:
X_train_transform = ord_e.fit_transform(X_train[cat_cols_label_enc])
X_train_transform = pd.DataFrame(X_train_transform)
548/50:
X_test_transform = pd.DataFrame.from_records(ord_e.transform(X_test[cat_cols_label_enc]))
X_test_transform = X_test_transform.apply(lambda column: column.replace(np.NAN, max(column)+1), axis=0)
548/51:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[cat_cols_label_enc]))
df_val_transform = df_val_transform.apply(lambda column: column.replace(np.NAN, max(column)+1), axis=0)
548/52:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
548/53: y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
548/54:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
548/55: from sklearn.preprocessing import OrdinalEncoder
548/56: ord_e = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)
548/57: cat_cols_label_enc = ['City', 'City Group', 'Type',]
548/58:
X_train_transform = ord_e.fit_transform(X_train[cat_cols_label_enc])
X_train_transform = pd.DataFrame(X_train_transform)
548/59:
X_test_transform = pd.DataFrame.from_records(ord_e.transform(X_test[cat_cols_label_enc]))
X_test_transform = X_test_transform.apply(lambda column: column.replace(np.NAN, max(column)+1), axis=0)
548/60:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[cat_cols_label_enc]))
df_val_transform = df_val_transform.apply(lambda column: column.replace(np.NAN, max(column)+1), axis=0)
548/61:
X_train_transform.columns = cat_cols_label_enc
X_test_transform.columns = cat_cols_label_enc
df_val_transform.columns = cat_cols_label_enc
548/62:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
548/63: y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
548/64: X_train
548/65:  X_train.iloc[:, 2:]]
548/66:  X_train.iloc[:, 2:]
548/67:  X_train.iloc[:, 3:]
548/68:
X_train_all = pd.concat([X_train_transform, X_train.iloc[:, 3:]], axis=1)
#X_test_all = pd.concat([test_transformed, X_test.iloc[:, 2:]], axis=1)
548/69: X_train_all
548/70: X_train_transform
548/71: X_train.iloc[:, 3:]
548/72:
X_train_all = pd.concat([X_train_transform, X_train.iloc[:, 3:]].reset_index(), axis=1)
#X_test_all = pd.concat([test_transformed, X_test.iloc[:, 2:]], axis=1)
548/73:
X_train_all = pd.concat([X_train_transform, X_train.iloc[:, 3:].reset_index()], axis=1)
#X_test_all = pd.concat([test_transformed, X_test.iloc[:, 2:]], axis=1)
548/74: X_train_all
548/75:
X_train_all = pd.concat([X_train_transform, X_train.iloc[:, 3:].reset_index()], axis=1)
X_test_all = pd.concat([X_test_transform, X_test.iloc[:, 3:].reset_index()], axis=1)
548/76: X_train_all
548/77:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=1),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=25,
          verbose=0,
          scoring='neg_root_mean_squared_error')

sfs = sfs.fit(np.array(X_train), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
548/78:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=1),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=25,
          verbose=0,
          scoring='neg_root_mean_squared_error')

sfs = sfs.fit(np.array(X_train_all), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
548/79: df_k_all.sort_values('avg_score', ascending=False)
548/80:
for k in range(2, 15):
    sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=k),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=25,
          verbose=0,
          scoring='neg_root_mean_squared_error')

    sfs = sfs.fit(np.array(X_train_transform), y_train_scale)
    df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
    df_k['k'] = k
    frames = [df_k_all, df_k]
    df_k_all = pd.concat(frames)
    print(k)
548/81: df_k_all.sort_values('avg_score', ascending=False)
548/82:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=1),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=25,
          verbose=0,
          scoring='r2')

sfs = sfs.fit(np.array(X_train_all), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
548/83:
for k in range(2, 15):
    sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=k),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=25,
          verbose=0,
          scoring='r2')

    sfs = sfs.fit(np.array(X_train_transform), y_train_scale)
    df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
    df_k['k'] = k
    frames = [df_k_all, df_k]
    df_k_all = pd.concat(frames)
    print(k)
548/84: df_k_all.sort_values('avg_score', ascending=False)
548/85:
for k in range(2, 15):
    sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=k),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=25,
          verbose=0,
          scoring='neg_root_mean_squared_error')

    sfs = sfs.fit(np.array(X_train_all), y_train_scale)
    df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
    df_k['k'] = k
    frames = [df_k_all, df_k]
    df_k_all = pd.concat(frames)
    print(k)
548/86: df_k_all.sort_values('avg_score', ascending=False)
548/87:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=1),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=25,
          verbose=0,
          scoring='r2')

sfs = sfs.fit(np.array(X_train_all), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
548/88:
for k in range(2, 15):
    sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=k),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=25,
          verbose=0,
          scoring='r2')

    sfs = sfs.fit(np.array(X_train_all), y_train_scale)
    df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
    df_k['k'] = k
    frames = [df_k_all, df_k]
    df_k_all = pd.concat(frames)
    print(k)
548/89: df_k_all.sort_values('avg_score', ascending=False)
548/90: f_indexes = list(df_k_all.sort_values('avg_score', ascending=False).iloc[0, 0])
548/91: colmns_selected = list(X_train.columns[f_indexes])
548/92: f_indexes
548/93: colmns_selected = list(X_train_all.columns[f_indexes])
548/94: colmns_selected
548/95: X_train_all
548/96:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=4)
knn_reg.fit(X_train_transform[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test_transform[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
548/97:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=4)
knn_reg.fit(X_train_all[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test_all[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
548/98: mean_squared_error(y_test, y_pred_scale, squared=False)
548/99: r2_score(y_test, y_pred_scale)
548/100: X_train_all.columns[f_indexes]
548/101: df_val_transform[colmns_selected].head(2)
548/102: df_val_transform
548/103: df_val_transform
548/104: df_val
548/105: df_val.iloc[:, 4:]
548/106: df_val.iloc[:, 5:]
548/107:
X_train_all = pd.concat([X_train_transform, X_train.iloc[:, 3:].reset_index()], axis=1)
X_test_all = pd.concat([X_test_transform, X_test.iloc[:, 3:].reset_index()], axis=1)
df_val_all = pd.concat([df_val_transform, df_val.iloc[:, 5:].reset_index()], axis=1)
548/108: df_val_all
548/109: X_train_all
548/110:
X_train_all = pd.concat([X_train_transform, X_train.iloc[:, 3:].reset_index(drop=True)], axis=1)
X_test_all = pd.concat([X_test_transform, X_test.iloc[:, 3:].reset_index()], axis=1)
df_val_all = pd.concat([df_val_transform, df_val.iloc[:, 5:].reset_index()], axis=1)
548/111: X_train_all
548/112:
X_train_all = pd.concat([X_train_transform, X_train.iloc[:, 3:].reset_index(drop=True)], axis=1)
X_test_all = pd.concat([X_test_transform, X_test.iloc[:, 3:].reset_index(drop=True)], axis=1)
df_val_all = pd.concat([df_val_transform, df_val.iloc[:, 5:].reset_index(drop=True)], axis=1)
548/113:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=1),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=25,
          verbose=0,
          scoring='r2')

sfs = sfs.fit(np.array(X_train_all), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
548/114:
for k in range(2, 15):
    sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=k),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=25,
          verbose=0,
          scoring='r2')

    sfs = sfs.fit(np.array(X_train_all), y_train_scale)
    df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
    df_k['k'] = k
    frames = [df_k_all, df_k]
    df_k_all = pd.concat(frames)
    print(k)
548/115: df_k_all.sort_values('avg_score', ascending=False)
548/116: f_indexes = list(df_k_all.sort_values('avg_score', ascending=False).iloc[0, 0])
548/117: colmns_selected = list(X_train_all.columns[f_indexes])
548/118:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=4)
knn_reg.fit(X_train_all[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test_all[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
548/119: mean_squared_error(y_test, y_pred_scale, squared=False)
548/120: r2_score(y_test, y_pred_scale)
548/121: X_train_all.columns[f_indexes]
548/122:
y_val = knn_reg.predict(df_val_transform[colmns_selected])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
548/123:
y_val = knn_reg.predict(df_val_all[colmns_selected])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
548/124: y_val_scale
548/125:
df_t = df_val[['Id']]
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("knn.csv", index=False)
548/126: df_t.describe()
548/127:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=1),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=25,
          verbose=0,
          scoring='neg_root_mean_squared_error')

sfs = sfs.fit(np.array(X_train_all), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
548/128:
for k in range(2, 15):
    sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=k),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=25,
          verbose=0,
          scoring='neg_root_mean_squared_error')

    sfs = sfs.fit(np.array(X_train_all), y_train_scale)
    df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
    df_k['k'] = k
    frames = [df_k_all, df_k]
    df_k_all = pd.concat(frames)
    print(k)
548/129: df_k_all.sort_values('avg_score', ascending=False)
548/130: df_k_all.sort_values('avg_score', ascending=False)[:20]
548/131: f_indexes = list(df_k_all.sort_values('avg_score', ascending=False).iloc[0, 0])
548/132: colmns_selected = list(X_train_all.columns[f_indexes])
548/133:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=4)
knn_reg.fit(X_train_all[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test_all[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
548/134: mean_squared_error(y_test, y_pred_scale, squared=False)
548/135: r2_score(y_test, y_pred_scale)
548/136: X_train_all.columns[f_indexes]
548/137: df_val_all
548/138:
y_val = knn_reg.predict(df_val_all[colmns_selected])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
548/139: y_val_scale
548/140:
df_t = df_val[['Id']]
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("knn.csv", index=False)
548/141: df_t.describe()
548/142:
X_test_transform = pd.DataFrame.from_records(ord_e.transform(X_test[cat_cols_label_enc]))
X_test_transform = X_test_transform.apply(lambda column: column.replace(np.NAN, max(column)+1), axis=0)
548/143:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[cat_cols_label_enc]))
df_val_transform = df_val_transform.apply(lambda column: column.replace(np.NAN, max(column)+1), axis=0)
548/144:
X_train_transform.columns = cat_cols_label_enc
X_test_transform.columns = cat_cols_label_enc
df_val_transform.columns = cat_cols_label_enc
548/145:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
548/146: y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
548/147:
#X_train.drop(cat_cols_label_enc, axis=1, inplace=True)
#X_test.drop(cat_cols_label_enc, axis=1, inplace=True)
#df_val.drop(cat_cols_label_enc, axis=1, inplace=True)
548/148:
X_train_all = pd.concat([X_train_transform, X_train.iloc[:, 3:].reset_index(drop=True)], axis=1)
X_test_all = pd.concat([X_test_transform, X_test.iloc[:, 3:].reset_index(drop=True)], axis=1)
df_val_all = pd.concat([df_val_transform, df_val.iloc[:, 5:].reset_index(drop=True)], axis=1)
548/149: df_val_all
548/150: df_val_transform
548/151: df_val_transform
548/152: df_val_transform.City.isna()
548/153:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[cat_cols_label_enc]))
df_val_transform = df_val_transform.apply(lambda column: column.replace(np.nan, max(column)+1), axis=0)
548/154: df_val_transform.City.isna()
548/155: df_val_transform.City
548/156: df_val_transform.
548/157: df_val_transform
548/158:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[cat_cols_label_enc]))
df_val_transform = df_val_transform.apply(lambda column: column.replace(np.NaN, max(column)+1), axis=0)
548/159: df_val_transform
548/160:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[cat_cols_label_enc]))
df_val_transform = df_val_transform.apply(lambda column: column.replace(np.NaN, max(column)+1), axis=0)
548/161:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[cat_cols_label_enc]))
#df_val_transform = df_val_transform.apply(lambda column: column.replace(np.NaN, max(column)+1), axis=0)
548/162: df_val_transform
548/163:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[cat_cols_label_enc]))
df_val_transform = df_val_transform.apply(lambda column: column.replace(np.NaN, max(column)+1), axis=0)
548/164: df_val_transform
548/165:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[cat_cols_label_enc]))
df_val_transform = df_val_transform.apply(lambda column: column.replace(np.NaN, max(column)+1), axis=1)
548/166: df_val_transform
548/167: X_test_transform
548/168: ord_e = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.NaN)
548/169: cat_cols_label_enc = ['City', 'City Group', 'Type',]
548/170:
X_train_transform = ord_e.fit_transform(X_train[cat_cols_label_enc])
X_train_transform = pd.DataFrame(X_train_transform)
548/171:
X_test_transform = pd.DataFrame.from_records(ord_e.transform(X_test[cat_cols_label_enc]))
X_test_transform = X_test_transform.apply(lambda column: column.replace(np.NAN, max(column)+1), axis=0)
548/172:
X_test_transform = pd.DataFrame.from_records(ord_e.transform(X_test[cat_cols_label_enc]))
X_test_transform = X_test_transform.apply(lambda column: column.replace(np.NaN, max(column)+1), axis=0)
548/173:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[cat_cols_label_enc]))
df_val_transform = df_val_transform.apply(lambda column: column.replace(np.NaN, max(column)+1), axis=0)
548/174: X_test_transform
548/175: df_val_transform
548/176: df_val
548/177: df_val_transform
548/178:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[cat_cols_label_enc]))
df_val_transform = df_val_transform.apply(lambda column: column.replace(np.NaN, 'nn'), axis=0)
548/179: df_val_transform
548/180:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[cat_cols_label_enc]))
df_val_transform = df_val_transform.apply(lambda column: column.replace(np.NaN, max(column)), axis=0)
548/181: df_val_transform
548/182:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[cat_cols_label_enc]))
df_val_transform = df_val_transform.apply(lambda column: column.replace(np.NaN, 111), axis=0)
548/183: df_val_transform
548/184:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[cat_cols_label_enc]))
df_val_transform = df_val_transform.apply(lambda column: column.replace(np.NaN, max(column)), axis=0)
548/185: df_val_transform
548/186: X_test_transform
548/187: X_test_transform.isna()
548/188: X_test_transform.isna().sum()
548/189: df_val_transform.isna().sum()
548/190: df_val_transform
548/191:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[cat_cols_label_enc]))
#df_val_transform = df_val_transform.apply(lambda column: column.replace(np.NaN, max(column)), axis=0)
548/192: df_val_transform
548/193: df_val_transform.isna().sum()
548/194: df_val_transform
548/195: df_val_transform.max
548/196: df_val_transform.max()
548/197:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[cat_cols_label_enc]))
df_val_transform = df_val_transform.apply(lambda column: column.replace(np.NaN, max(column)), axis=0)
548/198: df_val_transform
548/199: df_val_transform.max()
548/200: df_val_transform.isna()
548/201: df_val_transform.isna().sum()
548/202:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[cat_cols_label_enc]))
#df_val_transform = df_val_transform.apply(lambda column: column.replace(np.NaN, max(column)), axis=0)
548/203: df_val_transform.isna().sum()
548/204: df_val_transform.max()
548/205: df_val_transform
548/206:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[cat_cols_label_enc]))
df_val_transform = df_val_transform.apply(lambda column: column.replace(np.NaN, max(column)+1), axis=0)
548/207: df_val_transform
548/208: df_val_transform.max()
548/209: df_val_transform
548/210:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[cat_cols_label_enc]))
df_val_transform = df_val_transform.apply(lambda column: column.replace(np.NaN, max(column)+1), axis=1)
548/211: df_val_transform
549/1:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
549/2:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
549/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
549/4: df_train.head(5)
549/5:
f, axes = plt.subplots(2, 2, figsize=(12,10))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('Train')
axes[1, 1].set_title('Test')

plt.show()
549/6: #So, there is a new type 'MB' in test data and there is only one restaurant with DT type in the train dataset.
549/7:
f, axes = plt.subplots(1, 2, figsize=(12,4))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", ax=axes[1])
549/8:
#f, axes = plt.subplots(1, 2, figsize=(12,4))
#sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", hue="City Group")
549/9:
### add a function for counting open days for each restaurant
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
549/10:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
549/11:
### I don't know anything about P columns, so I want to check them in both datasets
P_cols = ['P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'P11', 
          'P12', 'P13', 'P14', 'P15', 'P16','P17', 'P18', 'P19', 'P20', 'P21',
          'P22', 'P23', 'P24', 'P25', 'P26', 'P27', 'P28', 'P29', 'P30', 'P31', 
          'P32', 'P33', 'P34', 'P35', 'P36', 'P37']
549/12:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("output.pdf")
549/13:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(18,5))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], binwidth = 0.5, ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.05, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_estimator=np.mean, ax=axes[2])
    sns.histplot(df_val[p_col], ax=axes[3], binwidth = 0.5, color='red')
    

    axes[0].set_title(f'Train {p_col}')
    axes[1].set_title(f'{p_col} and revenue')
    axes[2].set_title(f'{p_col} and revenue (mean)')
    axes[3].set_title(f'Test {p_col}' )
    pdf.savefig( fig )
    
### Open days and revenue
fig, axes = plt.subplots(1, 3, figsize=(18,5))
sns.set_style("whitegrid")
sns.histplot(df_train['open_days'], kde=True, ax=axes[0])
sns.regplot(x=df_train['open_days'], y=df_train.revenue, ax=axes[1])
sns.histplot(df_val['open_days'], kde=True, ax=axes[2], color='red')

axes[0].set_title(f'Train. Hist of open days')
axes[1].set_title(f'Open days and revenue')
axes[2].set_title(f'Test. Hist of open days')
pdf.savefig( fig )

### close file
pdf.close()
549/14: df_train_remove.sort_values('revenue')
549/15:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['Id', 'open_days', 'revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=0)

X_train.shape, X_test.shape
549/16: from sklearn.preprocessing import OrdinalEncoder
549/17: ord_e = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.NaN)
549/18: cat_cols_label_enc = ['City', 'City Group', 'Type',]
549/19:
X_train_transform = ord_e.fit_transform(X_train[cat_cols_label_enc])
X_train_transform = pd.DataFrame(X_train_transform)
549/20:
X_test_transform = pd.DataFrame.from_records(ord_e.transform(X_test[cat_cols_label_enc]))
X_test_transform = X_test_transform.apply(lambda column: column.replace(np.NaN, max(column)+1), axis=0)
549/21:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[cat_cols_label_enc]))
df_val_transform = df_val_transform.apply(lambda column: column.replace(np.NaN, max(column)+1), axis=0)
549/22: df_val_transform
549/23: X_test_transform
549/24: X_test_transform.max()
549/25: df_val_transform.max()
549/26:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[cat_cols_label_enc]))
#df_val_transform = df_val_transform.apply(lambda column: column.replace(np.NaN, max(column)+1), axis=0)
549/27: df_val_transform.max()
549/28:
X_test_transform = pd.DataFrame.from_records(ord_e.transform(X_test[cat_cols_label_enc]))
#X_test_transform = X_test_transform.apply(lambda column: column.replace(np.NaN, max(column)+1), axis=0)
549/29:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[cat_cols_label_enc]))
#df_val_transform = df_val_transform.apply(lambda column: column.replace(np.NaN, max(column)+1), axis=0)
549/30: df_val_transform.max()
549/31: X_test_transform.max()
549/32:
X_test_transform = pd.DataFrame.from_records(ord_e.transform(X_test[cat_cols_label_enc]))
X_test_transform = X_test_transform.apply(lambda column: column.replace(np.NaN, max(column)+1), axis=0)
549/33:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[cat_cols_label_enc]))
df_val_transform = df_val_transform.apply(lambda column: column.replace(np.NaN, max(column)+1), axis=0)
549/34: X_test_transform.max()
549/35: df_val_transform
549/36:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[cat_cols_label_enc]))
df_val_transform = df_val_transform.apply(lambda column: column.replace(np.NaN, 100, axis=0)
549/37: df_val_transform
549/38:
df_val_transform = pd.DataFrame.from_records(ord_e.transform(df_val[cat_cols_label_enc]))
df_val_transform = df_val_transform.apply(lambda column: column.replace(np.NaN, 100), axis=0)
549/39: df_val_transform
549/40:
X_train_transform.columns = cat_cols_label_enc
X_test_transform.columns = cat_cols_label_enc
df_val_transform.columns = cat_cols_label_enc
549/41:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
549/42: y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
549/43:
#X_train.drop(cat_cols_label_enc, axis=1, inplace=True)
#X_test.drop(cat_cols_label_enc, axis=1, inplace=True)
#df_val.drop(cat_cols_label_enc, axis=1, inplace=True)
549/44:
X_train_all = pd.concat([X_train_transform, X_train.iloc[:, 3:].reset_index(drop=True)], axis=1)
X_test_all = pd.concat([X_test_transform, X_test.iloc[:, 3:].reset_index(drop=True)], axis=1)
df_val_all = pd.concat([df_val_transform, df_val.iloc[:, 5:].reset_index(drop=True)], axis=1)
549/45:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=1),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=25,
          verbose=0,
          scoring='neg_root_mean_squared_error')

sfs = sfs.fit(np.array(X_train_all), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
549/46:
for k in range(2, 15):
    sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=k),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=25,
          verbose=0,
          scoring='neg_root_mean_squared_error')

    sfs = sfs.fit(np.array(X_train_all), y_train_scale)
    df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
    df_k['k'] = k
    frames = [df_k_all, df_k]
    df_k_all = pd.concat(frames)
    print(k)
549/47: df_k_all.sort_values('avg_score', ascending=False)[:20]
549/48: f_indexes = list(df_k_all.sort_values('avg_score', ascending=False).iloc[0, 0])
549/49: colmns_selected = list(X_train_all.columns[f_indexes])
549/50:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=3)
knn_reg.fit(X_train_all[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test_all[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
549/51: mean_squared_error(y_test, y_pred_scale, squared=False)
549/52: r2_score(y_test, y_pred_scale)
549/53: X_train_all.columns[f_indexes]
549/54: df_val_transform
549/55:
y_val = knn_reg.predict(df_val_all[colmns_selected])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
549/56: y_val_scale
549/57:
df_t = df_val[['Id']]
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("knn.csv", index=False)
549/58: df_t.describe()
549/59: df_val_all
549/60: df_val_all[colmns_selected]
549/61:
sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=1),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=20,
          verbose=0,
          scoring='r2')

sfs = sfs.fit(np.array(X_train_all), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
549/62:
for k in range(2, 15):
    sfs = SFS(KNeighborsRegressor(metric = 'hamming',n_neighbors=k),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=20,
          verbose=0,
          scoring='r2')

    sfs = sfs.fit(np.array(X_train_all), y_train_scale)
    df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
    df_k['k'] = k
    frames = [df_k_all, df_k]
    df_k_all = pd.concat(frames)
    print(k)
549/63: df_k_all.sort_values('avg_score', ascending=False)[:20]
549/64: f_indexes = list(df_k_all.sort_values('avg_score', ascending=False).iloc[0, 0])
549/65: colmns_selected = list(X_train_all.columns[f_indexes])
549/66:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=6)
knn_reg.fit(X_train_all[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test_all[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
549/67: mean_squared_error(y_test, y_pred_scale, squared=False)
549/68: r2_score(y_test, y_pred_scale)
549/69: X_train_all.columns[f_indexes]
549/70: df_val_all[colmns_selected]
549/71: df_val_all
549/72: df_val
549/73: df_val.iloc[:, 5:]
549/74:
X_train_all = pd.concat([X_train_transform, X_train.iloc[:, 3:].reset_index(drop=True)], axis=1)
X_test_all = pd.concat([X_test_transform, X_test.iloc[:, 3:].reset_index(drop=True)], axis=1)
df_val_all = pd.concat([df_val_transform, df_val.iloc[:, 4:].reset_index(drop=True)], axis=1)
549/75: df_val_all
549/76:
y_val = knn_reg.predict(df_val_all[colmns_selected])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
549/77: y_val_scale
549/78:
df_t = df_val[['Id']]
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("knn.csv", index=False)
549/79: df_t.describe()
549/80: df_k_all.sort_values('avg_score', ascending=False)[:10]
549/81: colmns_selected
549/82: df_k_all.sort_values('std_err', ascending=True)[:10]
549/83: df_k_all.sort_values('std_err', ascending=True)
549/84: df_k_all.sort_values('std_err', ascending=True)[:10]
549/85: f_indexes = list(df_k_all.sort_values('std_err', ascending=True).iloc[0, 0])
549/86: colmns_selected = list(X_train_all.columns[f_indexes])
549/87: colmns_selected
549/88:
knn_reg = KNeighborsRegressor(metric = 'hamming',n_neighbors=8)
knn_reg.fit(X_train_all[colmns_selected], y_train_scale)
y_pred = knn_reg.predict(X_test_all[colmns_selected])
y_pred_scale = target_scaler.inverse_transform(y_pred.reshape(1,-1))[0]
549/89: mean_squared_error(y_test, y_pred_scale, squared=False)
549/90: r2_score(y_test, y_pred_scale)
549/91:
y_val = knn_reg.predict(df_val_all[colmns_selected])
y_val_scale = target_scaler.inverse_transform(y_val.reshape(1,-1))[0]
549/92: y_val_scale
549/93:
df_t = df_val[['Id']]
df_t.insert(1, "Prediction", y_val_scale.tolist(), True)
df_t.to_csv("knn.csv", index=False)
549/94:
df_t.describe()
## Score: 1819676.86175
## Private score: 2018429.34415
549/95: df_k_all.sort_values('avg_score', ascending=False)[:10]
549/96: df_k_all.sort_values('std_err', ascending=True)[:10]
549/97: ### Feature selection + KNN regression
549/98: cat_cols_label_enc
549/99:
def find_category_mappings(df, variable, target):

    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable
                                 ])[target].mean().sort_values().index

    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, variable, ordinal_mapping):

    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
549/100:
def find_category_mappings(df, variable, target):

    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable
                                 ])[target].mean().sort_values().index

    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, variable, ordinal_mapping):

    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
549/101:
for variable in cat_cols_label_enc:

    mappings = find_category_mappings(X_train, variable, 'revenue')

    integer_encode(X_train, X_test, variable, mappings)
549/102: X_train
549/103:
for variable in cat_cols_label_enc:

    mappings = find_category_mappings(X_train, variable, y_train_scale)

    integer_encode(X_train, X_test, variable, mappings)
549/104:
for variable in cat_cols_label_enc:

    mappings = find_category_mappings(X_train, variable, y_train_scale.reshape(-1, 1))

    integer_encode(X_train, X_test, variable, mappings)
549/105: y_train_scale
549/106: y_train_scale[:]
549/107: y_train_scale[0]
549/108: y_train_scale[:, 0]
549/109:
for variable in cat_cols_label_enc:

    mappings = find_category_mappings(X_train, variable, y_train_scale[:, 0])

    integer_encode(X_train, X_test, variable, mappings)
549/110: X_train['y_scale'] = y_train_scale[:, 0]
549/111: cat_cols_label_enc
549/112:
for variable in cat_cols_label_enc:

    mappings = find_category_mappings(X_train, variable, 'y_scale')

    integer_encode(X_train, X_test, variable, mappings)
549/113: X_train
549/114: X_train.groupby(['City Group'])['y_scale'].mean().plot()
549/115: X_train.groupby(['City'])['y_scale'].mean().plot()
551/1:
import numpy as np
import pandas as pd
from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
551/2:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
551/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
551/4: df_train.head(5)
551/5:
f, axes = plt.subplots(2, 2, figsize=(12,10))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('Train')
axes[1, 1].set_title('Test')

plt.show()
551/6: #So, there is a new type 'MB' in test data and there is only one restaurant with DT type in the train dataset.
551/7:
f, axes = plt.subplots(1, 2, figsize=(12,4))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", ax=axes[1])
551/8:
#f, axes = plt.subplots(1, 2, figsize=(12,4))
#sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", hue="City Group")
551/9:
### add a function for counting open days for each restaurant
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
551/10:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
551/11:
### I don't know anything about P columns, so I want to check them in both datasets
P_cols = ['P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'P11', 
          'P12', 'P13', 'P14', 'P15', 'P16','P17', 'P18', 'P19', 'P20', 'P21',
          'P22', 'P23', 'P24', 'P25', 'P26', 'P27', 'P28', 'P29', 'P30', 'P31', 
          'P32', 'P33', 'P34', 'P35', 'P36', 'P37']
551/12:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("output.pdf")
551/13:
for p_col in P_cols:
 #   plt.figure()
    fig, axes = plt.subplots(1, 4, figsize=(18,5))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], binwidth = 0.5, ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.05, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_estimator=np.mean, ax=axes[2])
    sns.histplot(df_val[p_col], ax=axes[3], binwidth = 0.5, color='red')
    

    axes[0].set_title(f'Train {p_col}')
    axes[1].set_title(f'{p_col} and revenue')
    axes[2].set_title(f'{p_col} and revenue (mean)')
    axes[3].set_title(f'Test {p_col}' )
    pdf.savefig( fig )
    
### Open days and revenue
fig, axes = plt.subplots(1, 3, figsize=(18,5))
sns.set_style("whitegrid")
sns.histplot(df_train['open_days'], kde=True, ax=axes[0])
sns.regplot(x=df_train['open_days'], y=df_train.revenue, ax=axes[1])
sns.histplot(df_val['open_days'], kde=True, ax=axes[2], color='red')

axes[0].set_title(f'Train. Hist of open days')
axes[1].set_title(f'Open days and revenue')
axes[2].set_title(f'Test. Hist of open days')
pdf.savefig( fig )

### close file
pdf.close()
551/14: ##df_train_remove = df_train[df_train.revenue <= 10000000]
551/15:
X_train, X_test, y_train, y_test = train_test_split(
    df_train.drop(labels=['Id', 'open_days', 'revenue'], axis=1),  # drop the target
    df_train['revenue'],  # just the target
    test_size=0.1,
    random_state=0)

X_train.shape, X_test.shape
551/16:
def find_category_mappings(df, variable, target):

    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable
                                 ])[target].mean().sort_values().index

    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, variable, ordinal_mapping):

    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
551/17: df_train
551/18: X_train
551/19: cat_features = ['City', 'City Group', 'Type']
551/20: cat_features = cat_features
551/21: cat_features = ['City', 'City Group', 'Type']
551/22:
for variable in cat_features:

    mappings = find_category_mappings(X_train, variable, 'SalePrice')

    integer_encode(X_train, X_test, variable, mappings)
551/23: df_train
551/24: len(df_train)
551/25: len(df_train)/10
551/26: np.floor(len(df_train)/10)
551/27:
percentage = 10
np.floor(len(df_train)*percentage/100)
551/28:
percentage = 15
np.floor(len(df_train)*percentage/100)
551/29:
percentage = 15
bounder = np.floor(len(df_train)*percentage/100)
551/30:
X_train = df_train[bounder:, :]
X_test = df_train[:bounder, :]
551/31:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]
551/32:
X_train = df_train.iloc[10:, :]
X_test = df_train.iloc[:bounder, :]
551/33: bounder
551/34:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
551/35: bounder
551/36:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]
551/37: X_train
551/38: X_test
551/39: X_train
551/40:
def find_category_mappings(df, variable, target):

    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable
                                 ])[target].mean().sort_values().index

    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, variable, ordinal_mapping):

    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
551/41: X_train
551/42: cat_features = ['City', 'City Group', 'Type']
551/43:
for variable in cat_features:

    mappings = find_category_mappings(X_train, variable, 'SalePrice')

    integer_encode(X_train, X_test, variable, mappings)
551/44:
for variable in cat_features:

    mappings = find_category_mappings(X_train, variable, 'revenue')

    integer_encode(X_train, X_test, variable, mappings)
551/45:
def find_category_mappings(df, variable, target):

    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable
                                 ])[target].mean().sort_values().index

    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, variable, ordinal_mapping):

    train[variable] = train[[variable]].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
551/46: cat_features = ['City', 'City Group', 'Type']
551/47:
for variable in cat_features:

    mappings = find_category_mappings(X_train, variable, 'revenue')

    integer_encode(X_train, X_test, variable, mappings)
551/48:
def find_category_mappings(df, variable, target):

    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable
                                 ])[target].mean().sort_values().index

    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, variable, ordinal_mapping):

    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
551/49: cat_features = ['City', 'City Group', 'Type']
551/50:
for variable in cat_features:

    mappings = find_category_mappings(X_train, variable, 'revenue')

    integer_encode(X_train, X_test, variable, mappings)
551/51:
def find_category_mappings(df, variable, target):

    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable
                                 ])[target].mean().sort_values().index

    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, variable, ordinal_mapping):

    train[variable] = train.loc[:, variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
551/52: cat_features = ['City', 'City Group', 'Type']
551/53:
for variable in cat_features:

    mappings = find_category_mappings(X_train, variable, 'revenue')

    integer_encode(X_train, X_test, variable, mappings)
551/54:
def find_category_mappings(df, variable, target):

    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable
                                 ])[target].mean().sort_values().index

    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, variable, ordinal_mapping):

    train.loc[:, variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
551/55: cat_features = ['City', 'City Group', 'Type']
551/56:
for variable in cat_features:

    mappings = find_category_mappings(X_train, variable, 'revenue')

    integer_encode(X_train, X_test, variable, mappings)
551/57:
def find_category_mappings(df, variable, target):

    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable
                                 ])[target].mean().sort_values().index

    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, variable, ordinal_mapping):

    train.loc[:, variable] = train.loc[:, variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
551/58: cat_features = ['City', 'City Group', 'Type']
551/59:
for variable in cat_features:

    mappings = find_category_mappings(X_train, variable, 'revenue')

    integer_encode(X_train, X_test, variable, mappings)
551/60: X_train
551/61:
# let's inspect the newly created monotonic relationship
# between the variables and the target

for var in cat_features:
    
    fig = plt.figure()
    fig = X_train.groupby([var])['revenue'].mean().plot()
    fig.set_title('Monotonic relationship between {} and SalePrice'.format(var))
    fig.set_ylabel('Mean SalePrice')
    plt.show()
551/62:
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
551/63:
for variable in cat_features:

    mappings = find_category_mappings(X_train, variable, 'revenue')

    integer_encode(X_train, X_test, variable, mappings)
551/64: X_train.columns
551/65:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'P11', 'P12', 'P13', 'P14', 
                'P15', 'P16', 'P17', 'P18', 'P19', 'P20', 'P21', 'P22', 'P23', 'P24', 'P25', 'P26',
                'P27', 'P28', 'P29', 'P30', 'P31', 'P32', 'P33', 'P34', 'P35', 'P36', 'P37']
551/66:
for variable in cat_features:

    mappings = find_category_mappings(X_train, variable, 'revenue')

    integer_encode(X_train, X_test, variable, mappings)
551/67:
# let's inspect the newly created monotonic relationship
# between the variables and the target

for var in cat_features:
    
    fig = plt.figure()
    fig = X_train.groupby([var])['revenue'].mean().plot()
    fig.set_title('Monotonic relationship between {} and SalePrice'.format(var))
    fig.set_ylabel('Mean SalePrice')
    plt.show()
551/68: X_train
551/69: X_train[cat_features]
551/70:
y_train = X_train.pop('revenue')
y_test = X_test.pop('revenue')
551/71:
sfs = SFS(KNeighborsRegressor(LinearRegression()),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=10,
          verbose=0,
          scoring='r2')

sfs = sfs.fit(np.array(X_train_all), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
551/72:
sfs = SFS(KNeighborsRegressor(LinearRegression()),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=10,
          verbose=0,
          scoring='r2')

sfs = sfs.fit(np.array(X_train), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
551/73: df_k_all.sort_values('std_err', ascending=True)[:10]
551/74:
sfs = SFS(LinearRegression(),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=10,
          verbose=0,
          scoring='r2')

sfs = sfs.fit(np.array(X_train), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df_k['k'] = 1
frames = [df_k]
df_k_all = pd.concat(frames)
551/75: df_k_all
551/76:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train, y_train)
551/77: regressor.get_params()
551/78:
print('Linear Model Coeff (m)', regressor.coef_)
print('Linear Model Coeff (b)', regressor.intercept_)
551/79: y_predict = regressor.predict(X_test)
551/80:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train[cat_features], y_train)
551/81: regressor.get_params()
551/82:
print('Linear Model Coeff (m)', regressor.coef_)
print('Linear Model Coeff (b)', regressor.intercept_)
551/83: y_predict = regressor.predict(X_test[cat_features])
551/84: X_test
551/85: X_test.fillna(-1, inplace=True)
551/86: y_predict = regressor.predict(X_test[cat_features])
551/87:
plt.scatter(y_test, y_predict, color = 'r')
plt.ylabel('Model Predictions')
plt.xlabel('True (ground truth)')
551/88: y_predict
551/89: y_true
551/90: y_test
551/91:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]
551/92:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(X_train.revenue)
X_train.revenue = target_scaler.transform(X_train.revenue)
X_test.revenue = target_scaler.transform(X_test.revenue)
551/93: X_train.revenue
551/94:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(X_train.revenue)
X_train.revenue = target_scaler.transform(X_train.revenue.reshape(-1, 1))
X_test.revenue = target_scaler.transform(X_test.revenue.reshape(-1, 1))
551/95:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(X_train.revenue.reshape(-1, 1))
X_train.revenue = target_scaler.transform(X_train.revenue.reshape(-1, 1))
X_test.revenue = target_scaler.transform(X_test.revenue.reshape(-1, 1))
551/96:
def find_category_mappings(df, variable, target):

    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable
                                 ])[target].mean().sort_values().index

    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, variable, ordinal_mapping):

    train.loc[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
551/97: X_train.columns
551/98:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'P11', 'P12', 'P13', 'P14', 
                'P15', 'P16', 'P17', 'P18', 'P19', 'P20', 'P21', 'P22', 'P23', 'P24', 'P25', 'P26',
                'P27', 'P28', 'P29', 'P30', 'P31', 'P32', 'P33', 'P34', 'P35', 'P36', 'P37']
551/99:
for variable in cat_features:

    mappings = find_category_mappings(X_train, variable, 'revenue')

    integer_encode(X_train, X_test, variable, mappings)
551/100:
# let's inspect the newly created monotonic relationship
# between the variables and the target

for var in cat_features:
    
    fig = plt.figure()
    fig = X_train.groupby([var])['revenue'].mean().plot()
    fig.set_title('Monotonic relationship between {} and SalePrice'.format(var))
    fig.set_ylabel('Mean SalePrice')
    plt.show()
551/101:
y_train = X_train.pop('revenue')
y_test = X_test.pop('revenue')
551/102:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]
551/103:
def find_category_mappings(df, variable, target):

    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable
                                 ])[target].mean().sort_values().index

    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, variable, ordinal_mapping):

    train.loc[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
551/104: X_train.columns
551/105:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'P11', 'P12', 'P13', 'P14', 
                'P15', 'P16', 'P17', 'P18', 'P19', 'P20', 'P21', 'P22', 'P23', 'P24', 'P25', 'P26',
                'P27', 'P28', 'P29', 'P30', 'P31', 'P32', 'P33', 'P34', 'P35', 'P36', 'P37']
551/106:
for variable in cat_features:

    mappings = find_category_mappings(X_train, variable, 'revenue')

    integer_encode(X_train, X_test, variable, mappings)
551/107:
# let's inspect the newly created monotonic relationship
# between the variables and the target

for var in cat_features:
    
    fig = plt.figure()
    fig = X_train.groupby([var])['revenue'].mean().plot()
    fig.set_title('Monotonic relationship between {} and SalePrice'.format(var))
    fig.set_ylabel('Mean SalePrice')
    plt.show()
551/108:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]
551/109:
def find_category_mappings(df, variable, target):

    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable
                                 ])[target].mean().sort_values().index

    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, variable, ordinal_mapping):

    train.loc[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
551/110: X_train.columns
551/111:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'P11', 'P12', 'P13', 'P14', 
                'P15', 'P16', 'P17', 'P18', 'P19', 'P20', 'P21', 'P22', 'P23', 'P24', 'P25', 'P26',
                'P27', 'P28', 'P29', 'P30', 'P31', 'P32', 'P33', 'P34', 'P35', 'P36', 'P37']
551/112:
for variable in cat_features:

    mappings = find_category_mappings(X_train, variable, 'revenue')

    integer_encode(X_train, X_test, variable, mappings)
551/113: X_train
551/114:
# let's inspect the newly created monotonic relationship
# between the variables and the target

for var in cat_features:
    
    fig = plt.figure()
    fig = X_train.groupby([var])['revenue'].mean().plot()
    fig.set_title('Monotonic relationship between {} and SalePrice'.format(var))
    fig.set_ylabel('Mean SalePrice')
    plt.show()
551/115:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]
551/116:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
551/117:
def find_category_mappings(df, variable, target):

    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable
                                 ])[target].mean().sort_values().index

    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, variable, ordinal_mapping):

    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
551/118: X_train.columns
551/119:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'P11', 'P12', 'P13', 'P14', 
                'P15', 'P16', 'P17', 'P18', 'P19', 'P20', 'P21', 'P22', 'P23', 'P24', 'P25', 'P26',
                'P27', 'P28', 'P29', 'P30', 'P31', 'P32', 'P33', 'P34', 'P35', 'P36', 'P37']
551/120:
for variable in cat_features:

    mappings = find_category_mappings(X_train, variable, 'revenue')

    integer_encode(X_train, X_test, variable, mappings)
551/121: X_train
551/122:
# let's inspect the newly created monotonic relationship
# between the variables and the target

for var in cat_features:
    
    fig = plt.figure()
    fig = X_train.groupby([var])['revenue'].mean().plot()
    fig.set_title('Monotonic relationship between {} and SalePrice'.format(var))
    fig.set_ylabel('Mean SalePrice')
    plt.show()
551/123:
y_train = X_train.pop('revenue')
y_test = X_test.pop('revenue')
551/124:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
551/125:
plt.figure(figsize=(14,10))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
551/126:
sfs = SFS(LinearRegression(),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=10,
          verbose=0,
          scoring='neg_mean_squared_error ')

sfs = sfs.fit(np.array(X_train), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
551/127:
sfs = SFS(LinearRegression(),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=10,
          verbose=0,
          scoring='neg_mean_squared_error')

sfs = sfs.fit(np.array(X_train), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
551/128: df_k
551/129: df_k.sort_values('avg_score', ascending=False)[:10]
551/130:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
551/131:
sfs = SFS(LinearRegression(),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=10,
          verbose=0,
          scoring='neg_mean_squared_error')

sfs = sfs.fit(np.array(X_train), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
551/132: df_k.sort_values('avg_score', ascending=False)[:10]
551/133:
sfs = SFS(LinearRegression(),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=10,
          verbose=0,
          scoring='neg_mean_squared_error')

sfs = sfs.fit(np.array(X_train[cat_features]), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
551/134: X_train
551/135:
cat_features = [
                #'City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'P11', 'P12', 'P13', 'P14', 
                'P15', 'P16', 'P17', 'P18', 'P19', 'P20', 'P21', 'P22', 'P23', 'P24', 'P25', 'P26',
                'P27', 'P28', 'P29', 'P30', 'P31', 'P32', 'P33', 'P34', 'P35', 'P36', 'P37']
551/136:
sfs = SFS(LinearRegression(),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=10,
          verbose=0,
          scoring='neg_mean_squared_error')

sfs = sfs.fit(np.array(X_train[cat_features]), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
551/137: df_k.sort_values('avg_score', ascending=False)[:10]
551/138:
plt.scatter(y_test, y_predict, color = 'r')
plt.ylabel('Model Predictions')
plt.xlabel('True (ground truth)')
551/139:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train[cat_features], y_train)
551/140: regressor.get_params()
551/141:
print('Linear Model Coeff (m)', regressor.coef_)
print('Linear Model Coeff (b)', regressor.intercept_)
551/142: X_test.fillna(-1, inplace=True)
551/143: y_predict = regressor.predict(X_test[cat_features])
551/144:
plt.scatter(y_test, y_predict, color = 'r')
plt.ylabel('Model Predictions')
plt.xlabel('True (ground truth)')
551/145: y_predict
551/146: y_test
554/1:
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
554/2:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
554/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
554/4: df_train.head(5)
554/5:
f, axes = plt.subplots(2, 2, figsize=(12,10))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('Train')
axes[1, 1].set_title('Test')

plt.show()
554/6: #So, there is a new type 'MB' in test data and there is only one restaurant with DT type in the train dataset.
554/7:
f, axes = plt.subplots(1, 2, figsize=(12,4))
sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", ax=axes[1])
554/8:
#f, axes = plt.subplots(1, 2, figsize=(12,4))
#sns.boxplot(data=df_train, x="City Group", y="revenue", ax=axes[0])
sns.boxplot(data=df_train, x="Type", y="revenue", hue="City Group")
554/9:
### add a function for counting open days for each restaurant
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
554/10:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
554/11:
### I don't know anything about P columns, so I want to check them in both datasets
P_cols = ['P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'P11', 
          'P12', 'P13', 'P14', 'P15', 'P16','P17', 'P18', 'P19', 'P20', 'P21',
          'P22', 'P23', 'P24', 'P25', 'P26', 'P27', 'P28', 'P29', 'P30', 'P31', 
          'P32', 'P33', 'P34', 'P35', 'P36', 'P37']
554/12:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("output.pdf")
554/13:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
554/14: bounder
554/15:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
554/16:
def find_category_mappings(df, variable, target):

    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable
                                 ])[target].mean().sort_values().index

    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, variable, ordinal_mapping):

    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
554/17: X_train.columns
554/18:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'P11', 'P12', 'P13', 'P14', 
                'P15', 'P16', 'P17', 'P18', 'P19', 'P20', 'P21', 'P22', 'P23', 'P24', 'P25', 'P26',
                'P27', 'P28', 'P29', 'P30', 'P31', 'P32', 'P33', 'P34', 'P35', 'P36', 'P37']
554/19:
for variable in cat_features:

    mappings = find_category_mappings(X_train, variable, 'revenue')

    integer_encode(X_train, X_test, variable, mappings)
554/20: X_train
554/21: from sklearn.feature_selection import mutual_info_regression
554/22:
# determine the mutual information
mi = mutual_info_regression(X_train, y_train)

# and make a bar  plot
mi = pd.Series(mi)
mi.index = X_train.columns
mi.sort_values(ascending=False).plot.bar(figsize=(20,6))
plt.ylabel('Mutual Information')
554/23:
y_train = X_train.pop('revenue')
y_test = X_test.pop('revenue')
554/24:
# determine the mutual information
mi = mutual_info_regression(X_train[cat_features], y_train)

# and make a bar  plot
mi = pd.Series(mi)
mi.index = X_train[cat_features].columns
mi.sort_values(ascending=False).plot.bar(figsize=(20,6))
plt.ylabel('Mutual Information')
554/25:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
554/26:
# determine the mutual information
mi = mutual_info_regression(X_train[cat_features], y_train)

# and make a bar  plot
mi = pd.Series(mi)
mi.index = X_train[cat_features].columns
mi.sort_values(ascending=False).plot.bar(figsize=(20,6))
plt.ylabel('Mutual Information')
554/27:
cat_features = [#'City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'P11', 'P12', 'P13', 'P14', 
                'P15', 'P16', 'P17', 'P18', 'P19', 'P20', 'P21', 'P22', 'P23', 'P24', 'P25', 'P26',
                'P27', 'P28', 'P29', 'P30', 'P31', 'P32', 'P33', 'P34', 'P35', 'P36', 'P37']
554/28:
# determine the mutual information
mi = mutual_info_regression(X_train[cat_features], y_train)

# and make a bar  plot
mi = pd.Series(mi)
mi.index = X_train[cat_features].columns
mi.sort_values(ascending=False).plot.bar(figsize=(20,6))
plt.ylabel('Mutual Information')
560/1:
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
560/2:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
560/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
560/4: df_train.head(5)
560/5:
f, axes = plt.subplots(2, 2, figsize=(12,10))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('Train')
axes[1, 1].set_title('Test')

plt.show()
560/6:
f, axes = plt.subplots(2, 2, figsize=(6,5))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('Train')
axes[1, 1].set_title('Test')

plt.show()
560/7:
f, axes = plt.subplots(2, 2, figsize=(10,5))
sns.countplot(x='City Group', data=df_train, ax=axes[0, 0])
sns.countplot(x='City Group', data=df_val, ax=axes[0, 1])
axes[0, 0].set_title('Train')
axes[0, 1].set_title('Test')

sns.countplot(x='Type', data=df_train, ax=axes[1, 0])
sns.countplot(x='Type', data=df_val, ax=axes[1, 1])
axes[1, 0].set_title('Train')
axes[1, 1].set_title('Test')

plt.show()
560/8: #So, there is a new type 'MB' in test data and there is only one restaurant with DT type in the train dataset.
560/9:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("linear.pdf")
560/10: df_train.columns
560/11: df_train.columns.iloc[:, 2:]
560/12: df_train.columns[2:]
560/13: df_train.columns[2:-1]
560/14:
for col in df_train.columns[2:-1]:    
    fig = plt.figure()
    fig = df_train.groupby([var])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
560/15:
for col in df_train.columns[2:-1]:    
    fig = plt.figure()
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
560/16:
for col in df_train.columns[2:-1]:    
    fig, axes = plt.figure(1, 2, figsize=(12,4))
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
560/17:
for col in df_train.columns[2:-1]:    
    fig, axes = plt.figure(figsize=(12,4))
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
560/18:
for col in df_train.columns[2:-1]:    
    fig = plt.figure()

    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
560/19:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(1,1))

    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
560/20:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(5,5))

    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
560/21:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(6,6))

    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
560/22:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("linear.pdf")
560/23:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(7,7))
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
560/24:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("linear.pdf")
560/25:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(7,7))
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
pdf.close()
560/26: ## Train test split
560/27:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
560/28: bounder
560/29:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
560/30:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
560/31:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'P11', 'P12', 'P13', 'P14', 
                'P15', 'P16', 'P17', 'P18', 'P19', 'P20', 'P21', 'P22', 'P23', 'P24', 'P25', 'P26',
                'P27', 'P28', 'P29', 'P30', 'P31', 'P32', 'P33', 'P34', 'P35', 'P36', 'P37']
560/32:
for feature in cat_features:
    mappings = find_category_mappings(X_train, feature, 'revenue')
    integer_encode(X_train, X_test, feature, mappings)
560/33:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
pdf.close()
560/34:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
560/35: X_test
560/36: X_test.isna().sum()
560/37: X_test.isna().sum() > 0
560/38: X_test[X_test.isna().sum() > 0]
560/39: X_test.isna().sum() > 0
560/40: X_test.isna()
560/41: X_test.isna()
560/42: X_test[X_test.isna()]
560/43: X_test.isna()
560/44: X_test.loc[:, X_test.isna().any()]
560/45: X_test.loc[:, X_test.isna().any()]
560/46: X_test.fillna(-1, inplace=True)
560/47:
# Check X_test after features encoding
X_test.loc[:, X_test.isna().any()]
560/48: X_test.fillna(-1, inplace=True)
560/49: X_train
560/50:
plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
560/51:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
560/52: X_train
560/53: drop_cols = ['Id', 'Open Date']
560/54:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
560/55:
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
560/56:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
560/57: df_train
560/58: df_val
560/59:
df_train.drop(['Id'], axis=1, inplace=True)
df_val.drop(['Id'], axis=1, inplace=True)
560/60:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
560/61:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
560/62: X_train.
560/63: X_train
560/64:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
560/65:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
560/66:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
560/67:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'P11', 'P12', 'P13', 'P14', 
                'P15', 'P16', 'P17', 'P18', 'P19', 'P20', 'P21', 'P22', 'P23', 'P24', 'P25', 'P26',
                'P27', 'P28', 'P29', 'P30', 'P31', 'P32', 'P33', 'P34', 'P35', 'P36', 'P37']
560/68:
for feature in cat_features:
    mappings = find_category_mappings(X_train, feature, 'revenue')
    integer_encode(X_train, X_test, feature, mappings)
560/69:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
560/70:
# Check X_test after features encoding
X_test.loc[:, X_test.isna().any()]
560/71: X_test.fillna(-1, inplace=True)
560/72:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=True, annot_kws={"size":8})
#sns.set(font_scale = .7)
560/73:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=False, annot_kws={"size":8})
#sns.set(font_scale = .7)
560/74:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
560/75:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
560/76: X_train
560/77:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=False, annot_kws={"size":8})
#sns.set(font_scale = .7)
560/78:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
560/79: df_train.head(5)
560/80:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("linear.pdf")
560/81:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(7,7))
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
pdf.close()
560/82:
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
560/83:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
560/84:
df_train.drop(['Id'], axis=1, inplace=True)
df_val.drop(['Id'], axis=1, inplace=True)
560/85:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
560/86:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
560/87: X_train
560/88:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=False, annot_kws={"size":8})
#sns.set(font_scale = .7)
560/89:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
560/90:
corr_features = correlation(X_train, 0.85)
print('correlated features: ', len(set(corr_features)) )
560/91:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
560/92: corr_features
560/93:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
560/94: X_train
560/95:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=False, annot_kws={"size":8})
#sns.set(font_scale = .7)
560/96:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
560/97:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'P11', 'P12', 'P13', 'P14', 
                'P15', 'P16', 'P17', 'P18', 'P19', 'P20', 'P21', 'P22', 'P23', 'P24', 'P25', 'P26',
                'P27', 'P28', 'P29', 'P30', 'P31', 'P32', 'P33', 'P34', 'P35', 'P36', 'P37']
560/98: X_train.columns
560/99:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
560/100:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
560/101:
for feature in cat_features:
    mappings = find_category_mappings(X_train, feature, 'revenue')
    integer_encode(X_train, X_test, feature, mappings)
560/102:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
560/103:
# Check X_test after features encoding
X_test.loc[:, X_test.isna().any()]
560/104:
# Check X_test after features encoding
X_test.loc[:, X_test.isna().any()].head(3)
560/105: X_test.fillna(-1, inplace=True)
560/106: X_train
560/107:
y_train = X_train.pop('revenue')
y_test = X_test.pop('revenue')
560/108:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train[cat_features], y_train)
560/109: regressor.get_params()
560/110:
print('Linear Model Coeff (m)', regressor.coef_)
print('Linear Model Coeff (b)', regressor.intercept_)
560/111: y_predict = regressor.predict(X_test[cat_features])
560/112:
plt.scatter(y_test, y_predict, color = 'r')
plt.ylabel('Model Predictions')
plt.xlabel('True (ground truth)')
560/113: mean_squared_error(y_test, y_predict)
560/114: y_test
560/115: y_predict
561/1:
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
561/2:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
561/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
561/4: df_train.head(5)
561/5:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("linear.pdf")
561/6:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(7,7))
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
pdf.close()
561/7:
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
561/8:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
561/9:
df_train.drop(['Id'], axis=1, inplace=True)
df_val.drop(['Id'], axis=1, inplace=True)
561/10:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
561/11:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
561/12: X_train
561/13: X_train.iloc[2:-2]
561/14: X_train.iloc[4:-3]
561/15: X_train.iloc[:, 4:-2]
561/16: X_train.iloc[:, 3:-1]
561/17: X_train.iloc[:, 3:-2]
561/18:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train.iloc[:, 3:-2], X_train.revenue)
561/19: regressor.predict(X_test.iloc[:, 3:-2])
561/20:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
mean_squared_error(X_test.revenue, y_pred)
561/21:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
mean_squared_error(X_test.revenue, y_pred, squared=True)
561/22:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
mean_squared_error(X_test.revenue, y_pred, squared=False)
561/23:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
mean_squared_error(X_test.revenue, y_pred, squared=False)
561/24:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=False, annot_kws={"size":8})
#sns.set(font_scale = .7)
561/25:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
561/26:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
561/27:
X_train.drop(labels=corr_features, axis=1, inplace=True)
X_test.drop(labels=corr_features, axis=1, inplace=True)

X_train.shape, X_test.shape
561/28: X_train.columns
561/29:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
561/30:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
561/31:
for feature in cat_features:
    mappings = find_category_mappings(X_train, feature, 'revenue')
    integer_encode(X_train, X_test, feature, mappings)
561/32:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
561/33:
# Check X_test after features encoding
X_test.loc[:, X_test.isna().any()].head(3)
561/34: X_test.fillna(-1, inplace=True)
561/35:
y_train = X_train.pop('revenue')
y_test = X_test.pop('revenue')
561/36:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train[cat_features], y_train)
561/37: regressor.get_params()
561/38:
print('Linear Model Coeff (m)', regressor.coef_)
print('Linear Model Coeff (b)', regressor.intercept_)
561/39: y_predict = regressor.predict(X_test[cat_features])
561/40:
plt.scatter(y_test, y_predict, color = 'r')
plt.ylabel('Model Predictions')
plt.xlabel('True (ground truth)')
561/41: mean_squared_error(y_test, y_predict)
561/42: mean_squared_error(y_test, y_predict, squared=False)
561/43: ###
561/44:
sfs = SFS(LinearRegression(),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=10,
          verbose=0,
          scoring='neg_mean_squared_error')

sfs = sfs.fit(np.array(X_train[cat_features]), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
561/45: df_k.sort_values('avg_score', ascending=False)[:10]
561/46:
sfs = SFS(LinearRegression(),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=10,
          verbose=0,
          scoring='r2')

sfs = sfs.fit(np.array(X_train[cat_features]), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
561/47: df_k.sort_values('avg_score', ascending=False)[:10]
561/48:
sfs = SFS(LinearRegression(),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=10,
          verbose=0,
          scoring='neg_mean_squared_error')

sfs = sfs.fit(np.array(X_train[cat_features]), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
561/49: df_k.sort_values('avg_score', ascending=False)[:10]
561/50:
sfs = SFS(LinearRegression(),
          k_features=2, # the lower the features we want, the longer this will take
          forward=False,
          cv=10,
          verbose=0,
          scoring='neg_mean_squared_error')

sfs = sfs.fit(np.array(X_train[cat_features]), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
561/51: df_k.sort_values('avg_score', ascending=False)[:10]
561/52: df_k.sort_values('std_err', ascending=True)[:10]
561/53: df_k.sort_values('std_err', ascending=True)[:5]
561/54: df_k.sort_values('avg_score', ascending=False)[:5]
561/55:
f_indexes = list(df_k.sort_values('std_err', ascending=True).iloc[0, 0])
colmns_selected = list(X_train.columns[f_indexes])
561/56: colmns_selected
561/57:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train[colmns_selected], y_train)
561/58: y_predict = regressor.predict(X_test[cat_features])
561/59: y_predict = regressor.predict(X_test[colmns_selected])
561/60: mean_squared_error(y_test, y_predict, squared=False)
561/61:
y_predict = regressor.predict(X_test[colmns_selected])
mean_squared_error(y_test, y_predict, squared=False)
561/62: X_train
561/63:
sfs = SFS(RandomForestRegressor(n_estimators=20,
                                random_state=0,
                                max_depth=3),
          min_features=1,
          scoring='neg_mean_squared_error',
          print_progress=True,
          cv=2)

sfs = sfs.fit(np.array(X_train[cat_features]), y_train)
561/64:
sfs = SFS(RandomForestRegressor(n_estimators=20,
                                random_state=0,
                                max_depth=3),
          k_features=1,
          scoring='neg_mean_squared_error',
          print_progress=True,
          cv=2)

sfs = sfs.fit(np.array(X_train[cat_features]), y_train)
561/65:
sfs = SFS(RandomForestRegressor(n_estimators=20,
                                random_state=0,
                                max_depth=3),
          k_features=1,
          scoring='neg_mean_squared_error',
          cv=2)

sfs = sfs.fit(np.array(X_train[cat_features]), y_train)
561/66:
sfs = SFS(RandomForestRegressor(n_estimators=20,
                                random_state=0,
                                max_depth=3),
          k_features=1,
          scoring='neg_mean_squared_error',
          cv=2)

sfs = sfs.fit(np.array(X_train[cat_features]), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
561/67: df_k.sort_values('avg_score', ascending=False)[:5]
561/68:
sfs = SFS(RandomForestRegressor(n_estimators=20,
                                random_state=0,
                                max_depth=3),
          k_features=2,
          scoring='neg_mean_squared_error',
          cv=2)

sfs = sfs.fit(np.array(X_train[cat_features]), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
561/69: df_k.sort_values('avg_score', ascending=False)[:5]
561/70:
sfs = SFS(RandomForestRegressor(n_estimators=20,
                                random_state=0,
                                max_depth=3),
          k_features=2,
          forward=False,
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(np.array(X_train[cat_features]), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
561/71: df_k.sort_values('avg_score', ascending=False)[:5]
561/72:
f_indexes = list(df_k.sort_values('std_err', ascending=True).iloc[0, 0])
colmns_selected = list(X_train.columns[f_indexes])
561/73: colmns_selected
561/74:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
colmns_selected = list(X_train.columns[f_indexes])
561/75: colmns_selected
561/76:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train[colmns_selected], y_train)
561/77:
y_predict = regressor.predict(X_test[colmns_selected])
mean_squared_error(y_test, y_predict, squared=False)
561/78:
sfs = SFS(LinearRegression(),
          k_features=2
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(np.array(X_train[cat_features]), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
561/79:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(np.array(X_train[cat_features]), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
561/80: df_k.sort_values('avg_score', ascending=False)[:5]
561/81:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
colmns_selected = list(X_train.columns[f_indexes])
561/82: colmns_selected
561/83: X_train[cat_features]
561/84: X_train
561/85: X_train.columns
561/86: X_train[cat_features]
561/87: X_train
561/88: X_train[cat_features]
561/89:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(np.array(X_train), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
561/90: df_k.sort_values('avg_score', ascending=False)[:5]
561/91:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
colmns_selected = list(X_train.columns[f_indexes])
561/92: colmns_selected
561/93:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train[colmns_selected], y_train)
561/94:
y_predict = regressor.predict(X_test[colmns_selected])
mean_squared_error(y_test, y_predict, squared=False)
561/95:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=20)

sfs = sfs.fit(np.array(X_train), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
561/96: df_k.sort_values('avg_score', ascending=False)[:5]
561/97:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
colmns_selected = list(X_train.columns[f_indexes])
561/98: colmns_selected
561/99:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train[colmns_selected], y_train)
561/100:
y_predict = regressor.predict(X_test[colmns_selected])
mean_squared_error(y_test, y_predict, squared=False)
561/101:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(np.array(X_train), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
561/102: df_k.sort_values('avg_score', ascending=False)[:5]
561/103:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
colmns_selected = list(X_train.columns[f_indexes])
561/104: colmns_selected
561/105:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train[colmns_selected], y_train)
561/106:
y_predict = regressor.predict(X_test[colmns_selected])
mean_squared_error(y_test, y_predict, squared=False)
561/107:
scaler = StandardScaler()
scaler.fit(X_train)
561/108: from sklearn.feature_selection import SelectFromModel
561/109:
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import Lasso
561/110:
scaler = StandardScaler()
scaler.fit(X_train)
561/111:
sel_ = SelectFromModel(Lasso(alpha=1000, random_state=10))
sel_.fit(scaler.transform(X_train), y_train)
561/112: sel_.get_support()
561/113: X_train
561/114:
sel_ = SelectFromModel(Lasso(alpha=10000, random_state=10))
sel_.fit(scaler.transform(X_train), y_train)
561/115: sel_.get_support()
561/116:
sel_ = SelectFromModel(Lasso(alpha=100000, random_state=10))
sel_.fit(scaler.transform(X_train), y_train)
561/117: sel_.get_support()
561/118:
sel_ = SelectFromModel(Lasso(alpha=1000000, random_state=10))
sel_.fit(scaler.transform(X_train), y_train)
561/119: sel_.get_support()
561/120:
sel_ = SelectFromModel(Lasso(alpha=100000, random_state=10))
sel_.fit(scaler.transform(X_train), y_train)
561/121: sel_.get_support()
561/122:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(np.array(scaler.transform(X_train)), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
561/123: df_k.sort_values('avg_score', ascending=False)[:5]
561/124:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
colmns_selected = list(X_train.columns[f_indexes])
561/125: colmns_selected
561/126:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(scaler.transform(X_train[colmns_selected]), y_train)
561/127:
scaler = StandardScaler()
scaler.fit(X_train[colmns_selected])
561/128:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(scaler.transform(X_train[colmns_selected]), y_train)
561/129:
y_predict = regressor.predict(scaler.transform(X_test[colmns_selected]))
mean_squared_error(y_test, y_predict, squared=False)
561/130:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(np.array(scaler.transform(X_train)), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
561/131:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(np.array(X_train), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
561/132: df_k.sort_values('avg_score', ascending=False)[:5]
561/133:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
colmns_selected = list(X_train.columns[f_indexes])
561/134: colmns_selected
561/135:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train[colmns_selected], y_train)
561/136:
y_predict = regressor.predict(X_test[colmns_selected])
mean_squared_error(y_test, y_predict, squared=False)
561/137: ####
561/138:
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import Lasso
561/139:
scaler = StandardScaler()
scaler.fit(X_train)
561/140:
sel_ = SelectFromModel(Lasso(alpha=100000, random_state=10))
sel_.fit(scaler.transform(X_train), y_train)
561/141: sel_.get_support()
561/142: selected_feat = X_train.columns[(sel_.get_support())]
561/143: selected_feat
561/144:
sel_ = SelectFromModel(Lasso(alpha=2000000, random_state=10))
sel_.fit(scaler.transform(X_train), y_train)
561/145: sel_.get_support()
561/146:
sel_ = SelectFromModel(Lasso(alpha=150000, random_state=10))
sel_.fit(scaler.transform(X_train), y_train)
561/147: sel_.get_support()
561/148: selected_feat = X_train.columns[(sel_.get_support())]
561/149: selected_feat
561/150: columns_selected = X_train.columns[(sel_.get_support())]
561/151: columns_selected
561/152:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train[columns_selected], y_train)
561/153:
y_predict = regressor.predict(X_test[colmns_selected])
mean_squared_error(y_test, y_predict, squared=False)
561/154:
y_predict = regressor.predict(X_test[columns_selected])
mean_squared_error(y_test, y_predict, squared=False)
561/155:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
colmns_selected = list(X_train.columns[f_indexes])
print(colmns_selected)
561/156:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(np.array(X_train), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
561/157: df_k.sort_values('avg_score', ascending=False)[:5]
561/158:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train.columns[f_indexes])
print(columns_selected)
561/159:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train[colmns_selected], y_train)
561/160:
y_predict = regressor.predict(X_test[colmns_selected])
mean_squared_error(y_test, y_predict, squared=False)
561/161:
sel_ = SelectFromModel(Lasso(alpha=150000, random_state=10))
sel_.fit(X_train, y_train)
561/162: sel_.get_support()
561/163:
sel_ = SelectFromModel(Lasso(alpha=50000, random_state=10))
sel_.fit(X_train, y_train)
561/164: sel_.get_support()
561/165: columns_selected = X_train.columns[(sel_.get_support())]
561/166: columns_selected
561/167:
#scaler = StandardScaler()
#scaler.fit(X_train)
561/168:
sel_ = SelectFromModel(Lasso(alpha=50000, random_state=10))
sel_.fit(X_train, y_train)
561/169: sel_.get_support()
561/170: columns_selected = X_train.columns[(sel_.get_support())]
561/171: columns_selected
561/172:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train[columns_selected], y_train)
561/173:
y_predict = regressor.predict(X_test[columns_selected])
mean_squared_error(y_test, y_predict, squared=False)
561/174:
y_predict = regressor.predict(X_test[columns_selected])
mean_squared_error(y_test, y_predict, squared=False)
561/175:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train.columns[f_indexes])
print(columns_selected)
561/176:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train[colmns_selected], y_train)
561/177:
y_predict = regressor.predict(X_test[colmns_selected])
mean_squared_error(y_test, y_predict, squared=False)
561/178: ####
561/179:
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import Lasso
561/180:
#scaler = StandardScaler()
#scaler.fit(X_train)
561/181:
sel_ = SelectFromModel(Lasso(alpha=50000, random_state=10))
sel_.fit(X_train, y_train)
561/182:
sel_ = SelectFromModel(Lasso(alpha=100000, random_state=10))
sel_.fit(X_train, y_train)
561/183: sel_.get_support()
561/184: columns_selected = X_train.columns[(sel_.get_support())]
561/185: columns_selected
561/186:
sel_ = SelectFromModel(Lasso(alpha=1000000, random_state=10))
sel_.fit(X_train, y_train)
561/187: sel_.get_support()
561/188: columns_selected = X_train.columns[(sel_.get_support())]
561/189: columns_selected
561/190:
sel_ = SelectFromModel(Lasso(alpha=500000, random_state=10))
sel_.fit(X_train, y_train)
561/191: sel_.get_support()
561/192: columns_selected = X_train.columns[(sel_.get_support())]
561/193: columns_selected
561/194:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train[columns_selected], y_train)
561/195:
y_predict = regressor.predict(X_test[columns_selected])
mean_squared_error(y_test, y_predict, squared=False)
561/196:
sel_ = SelectFromModel(Lasso(alpha=100000, random_state=10))
sel_.fit(X_train, y_train)
561/197: sel_.get_support()
561/198: columns_selected = X_train.columns[(sel_.get_support())]
561/199: columns_selected
561/200:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train[columns_selected], y_train)
561/201:
y_predict = regressor.predict(X_test[columns_selected])
mean_squared_error(y_test, y_predict, squared=False)
561/202:
sel_ = SelectFromModel(Lasso(alpha=10000, random_state=10))
sel_.fit(X_train, y_train)
561/203: sel_.get_support()
561/204: columns_selected = X_train.columns[(sel_.get_support())]
561/205: columns_selected
561/206:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train[columns_selected], y_train)
561/207:
y_predict = regressor.predict(X_test[columns_selected])
mean_squared_error(y_test, y_predict, squared=False)
561/208:
sel_ = SelectFromModel(Lasso(alpha=100000, random_state=10))
sel_.fit(X_train, y_train)
561/209: sel_.get_support()
561/210: columns_selected = X_train.columns[(sel_.get_support())]
561/211: columns_selected
561/212:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train[columns_selected], y_train)
561/213:
y_predict = regressor.predict(X_test[columns_selected])
mean_squared_error(y_test, y_predict, squared=False)
561/214: X_train
561/215: X_train['revenue'] = y_train
561/216: X_train
561/217: X_train.groupby('Type')['revenue'].mean()
561/218: df_train.groupby('Type')['revenue'].mean()
561/219: df_train.groupby('Type')['revenue'].mean().to_dict()
561/220: df_train.groupby('Type')['revenue'].mean().sort_values()
561/221: df_train.groupby('Type')['revenue'].mean().sort_values().index
561/222:
for i, k in enumerate(['1', '2', '3'], 0):
    print i
561/223:
for i, k in enumerate(['1', '2', '3'], 0):
    print(i, k)
561/224:
for i, k in enumerate(['1', '2', '3'], 10):
    print(i, k)
561/225:
for i, k in enumerate(['1', '2', '3'], 10):
    print(k, i)
561/226:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()

def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
561/227: X_train['revenue'] = y_train
564/1:
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
#from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
564/2:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
564/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
564/4: df_train.head(5)
564/5:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("linear.pdf")
564/6:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(7,7))
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
pdf.close()
564/7:
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
564/8:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
564/9:
df_train.drop(['Id'], axis=1, inplace=True)
df_val.drop(['Id'], axis=1, inplace=True)
564/10:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
564/11:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
564/12: X_train.iloc[:, 3:-2]
564/13:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train.iloc[:, 3:-2], X_train.revenue)
564/14:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
mean_squared_error(X_test.revenue, y_pred, squared=False)
564/15:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=False, annot_kws={"size":8})
#sns.set(font_scale = .7)
564/16:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
564/17:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

X_train.shape, X_test.shape
564/18:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
564/19:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
564/20:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

X_train_selected.shape, X_test_selected.shape
564/21:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
564/22:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
564/23:
for feature in cat_features:
    mappings = find_category_mappings(X_train_selected, feature, 'revenue')
    integer_encode(X_train_selected, X_test_selected, feature, mappings)
564/24:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
564/25:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
564/26: X_test_selected.fillna(-1, inplace=True)
564/27:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
564/28:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[cat_features], y_train)
564/29: regressor.get_params()
564/30:
print('Linear Model Coeff (m)', regressor.coef_)
print('Linear Model Coeff (b)', regressor.intercept_)
564/31: y_predict = regressor.predict(X_test_selected[cat_features])
564/32:
plt.scatter(y_test, y_predict, color = 'r')
plt.ylabel('Model Predictions')
plt.xlabel('True (ground truth)')
564/33:
fig, ax = plt.subplots(figsize = (9, 6))
ax.scatter(y_test, y_predict, color = 'r')

# Set logarithmic scale on the both variables
ax.set_xscale("log")
ax.set_yscale("log");
564/34:
mean_squared_error(y_test, y_predict, squared=False)
### better then before
564/35:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(np.array(X_train), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
564/36:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(np.array(X_train_selected), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
564/37: df_k.sort_values('avg_score', ascending=False)[:5]
564/38:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train.columns[f_indexes])
print(columns_selected)
564/39: X_train_selected
564/40: X_train_selected
564/41: df_k.sort_values('avg_score', ascending=False)
564/42: df_k.sort_values('avg_score', ascending=False)[:5]
564/43: X_train_selected
564/44: X_train
564/45:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train[colmns_selected], y_train)
564/46:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[colmns_selected], y_train)
564/47:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train.columns[f_indexes])
print(columns_selected)
564/48:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[colmns_selected], y_train)
564/49:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train)
564/50:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print(columns_selected)
564/51:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train)
564/52:
y_predict = regressor.predict(X_test[colmns_selected])
mean_squared_error(y_test, y_predict, squared=False)
564/53:
y_predict = regressor.predict(X_test[columns_selected])
mean_squared_error(y_test, y_predict, squared=False)
564/54:
y_predict = regressor.predict(X_test_selected[columns_selected])
mean_squared_error(y_test, y_predict, squared=False)
564/55:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(np.array(X_train_selected), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
564/56: df_k.sort_values('avg_score', ascending=False)[:5]
564/57:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print(columns_selected)
564/58:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train)
564/59:
y_predict = regressor.predict(X_test_selected[columns_selected])
mean_squared_error(y_test, y_predict, squared=False)
564/60: X_test_selected
564/61:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

X_train_selected.shape, X_test_selected.shape
564/62: X_train_selected
564/63:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()

def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
564/64:
selected_features = X_train_selected.columns
print(selected_features)
564/65:
# and now we run a loop over the remaining categorical variables
for variable in selected_features:
    mappings = find_category_mappings(X_train, variable, 'survived')
    integer_encode(X_train, X_test, variable, mappings)
564/66:
# and now we run a loop over the remaining categorical variables
for variable in selected_features:
    mappings = find_category_mappings(X_train, variable, 'survived')
    integer_encode(X_train_selected, X_test_selected, variable, mappings)
564/67:
# and now we run a loop over the remaining categorical variables
for variable in selected_features:
    mappings = find_category_mappings(X_train_selected, variable, 'survived')
    integer_encode(X_train_selected, X_test_selected, variable, mappings)
564/68:
# and now we run a loop over the remaining categorical variables
for variable in selected_features:
    mappings = find_category_mappings(X_train_selected, variable, 'revenue')
    integer_encode(X_train_selected, X_test_selected, variable, mappings)
564/69:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
564/70:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()

def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
564/71:
# and now we run a loop over the remaining categorical variables
for variable in cat_features:
    mappings = find_category_mappings(X_train_selected, variable, 'revenue')
    integer_encode(X_train_selected, X_test_selected, variable, mappings)
564/72:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
564/73: X_train_selected
564/74:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
564/75: X_test
564/76: X_test_selected
564/77:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

X_train_selected.shape, X_test_selected.shape
564/78:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
564/79:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()

def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
564/80:
# and now we run a loop over the remaining categorical variables
for variable in cat_features:
    mappings = find_category_mappings(X_train_selected, variable, 'revenue')
    integer_encode(X_train_selected, X_test_selected, variable, mappings)
564/81: X_test_selected
564/82:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
564/83: X_test_selected
564/84:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
564/85: X_test_selected.fillna(-1, inplace=True)
564/86:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
564/87:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[cat_features], y_train)
564/88: regressor.get_params()
564/89: y_predict = regressor.predict(X_test_selected[cat_features])
564/90:
mean_squared_error(y_test, y_predict, squared=False)
### better than before
564/91: ###
564/92:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(np.array(X_train_selected), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
564/93: df_k.sort_values('avg_score', ascending=False)[:5]
564/94:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print(columns_selected)
564/95:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train)
564/96:
y_predict = regressor.predict(X_test_selected[columns_selected])
mean_squared_error(y_test, y_predict, squared=False)
564/97:
for col in X_train_selected.columns:    
    print(X_train_selected.groupby(col)[col].count() / len(X_train_selected)) # frequency
    print()
564/98:
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
#from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
564/99:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
564/100:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
564/101: df_train.head(5)
564/102:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("linear.pdf")
564/103:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(7,7))
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
pdf.close()
564/104:
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
564/105:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
564/106:
df_train.drop(['Id'], axis=1, inplace=True)
df_val.drop(['Id'], axis=1, inplace=True)
564/107:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
564/108:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
564/109: X_train.iloc[:, 3:-2]
564/110:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train.iloc[:, 3:-2], X_train.revenue)
564/111:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
mean_squared_error(X_test.revenue, y_pred, squared=False)
564/112:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=False, annot_kws={"size":8})
#sns.set(font_scale = .7)
564/113:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
564/114:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
564/115:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

X_train_selected.shape, X_test_selected.shape
564/116:
for col in X_train_selected.columns:    
    print(X_train_selected.groupby(col)[col].count() / len(X_train_selected)) # frequency
    print()
564/117: X_train_selected.groupby(col)[col].count()
564/118: X_train_selected.groupby('P8')['P8'].count()
564/119: X_train_selected.groupby('P8')['P8'].count().sort_values()
564/120: X_train_selected.groupby('P8')['P8'].count()
564/121:
for col in X_train_selected.columns:    
    print(X_train_selected.groupby(col)[col].count().sort_values() / len(X_train_selected)) # frequency
    print()
569/1:
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
#from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
569/2:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
569/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
569/4: df_train.head(5)
569/5:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("linear.pdf")
569/6:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(7,7))
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
pdf.close()
569/7:
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
569/8:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
569/9:
df_train.drop(['Id'], axis=1, inplace=True)
df_val.drop(['Id'], axis=1, inplace=True)
569/10:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
569/11:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
569/12: X_train.iloc[:, 3:-2]
569/13:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train.iloc[:, 3:-2], X_train.revenue)
569/14:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
mean_squared_error(X_test.revenue, y_pred, squared=False)
569/15:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=False, annot_kws={"size":8})
#sns.set(font_scale = .7)
569/16:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
569/17:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
569/18:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

X_train_selected.shape, X_test_selected.shape
569/19:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
569/20: df_val
569/21:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    df_val[variable] = df_val[variable].map(ordinal_mapping)
569/22:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
569/23:
for feature in cat_features:
    mappings = find_category_mappings(X_train_selected, feature, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val, feature, mappings)
569/24:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
569/25:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
569/26:
for feature in cat_features:
    mappings = find_category_mappings(X_train_selected, feature, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val, feature, mappings)
569/27:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
569/28:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
569/29: X_test_selected.fillna(-1, inplace=True)
569/30:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
569/31:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[cat_features], y_train)
569/32: regressor.get_params()
569/33:
print('Linear Model Coeff (m)', regressor.coef_)
print('Linear Model Coeff (b)', regressor.intercept_)
569/34: y_predict = regressor.predict(X_test_selected[cat_features])
569/35:
mean_squared_error(y_test, y_predict, squared=False)
### better than before
569/36: X_test_selected
569/37:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
569/38: X_test_selected.fillna(-1, inplace=True)
569/39:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
570/1:
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
#from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
570/2:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
570/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
570/4: df_train.head(5)
570/5:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("linear.pdf")
570/6:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(7,7))
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
pdf.close()
570/7:
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
570/8:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
570/9:
df_train.drop(['Id'], axis=1, inplace=True)
df_val.drop(['Id'], axis=1, inplace=True)
570/10:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
570/11:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
570/12: X_train.iloc[:, 3:-2]
570/13:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train.iloc[:, 3:-2], X_train.revenue)
570/14:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
mean_squared_error(X_test.revenue, y_pred, squared=False)
570/15:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=False, annot_kws={"size":8})
#sns.set(font_scale = .7)
570/16:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
570/17:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
570/18:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

X_train_selected.shape, X_test_selected.shape
570/19:
for col in X_train_selected.columns:    
    print(X_train_selected.groupby(col)[col].count().sort_values() / len(X_train_selected)) # frequency
    print()
570/20:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
570/21:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
570/22:
for feature in cat_features:
    mappings = find_category_mappings(X_train_selected, feature, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val, feature, mappings)
570/23:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
570/24:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
570/25:
X_test_selected.fillna(-1, inplace=True)
df_val
570/26:
X_test_selected.fillna(-1, inplace=True)
df_val.fillna(-1, inplace=True)
570/27:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
570/28:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train)
570/29: y_predict = regressor.predict(X_test_selected[cat_features])
570/30: y_predict = regressor.predict(X_test_selected)
570/31:
mean_squared_error(y_test, y_predict, squared=False)
### better than before
570/32: df_val
570/33: df_val
571/1:
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
#from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
571/2:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
571/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')

print(df_train.shape)
print(df_val.shape)
571/4: df_val
571/5:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')
df_out = df_val[[√çd]]

print(df_train.shape)
print(df_val.shape)
571/6:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')
df_out = df_val[['Id']]

print(df_train.shape)
print(df_val.shape)
571/7: df_train.head(5)
571/8:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("linear.pdf")
571/9:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(7,7))
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
pdf.close()
571/10:
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
571/11:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
571/12:
df_train.drop(['Id'], axis=1, inplace=True)
df_val.drop(['Id'], axis=1, inplace=True)
571/13:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
571/14:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
571/15: X_train.iloc[:, 3:-2]
571/16:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train.iloc[:, 3:-2], X_train.revenue)
571/17:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
mean_squared_error(X_test.revenue, y_pred, squared=False)
571/18:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=False, annot_kws={"size":8})
#sns.set(font_scale = .7)
571/19:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
571/20:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
571/21:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

X_train_selected.shape, X_test_selected.shape
571/22:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
571/23:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
571/24:
for feature in cat_features:
    mappings = find_category_mappings(X_train_selected, feature, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val, feature, mappings)
571/25:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
571/26:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
571/27:
X_test_selected.fillna(-1, inplace=True)
df_val.fillna(-1, inplace=True)
571/28:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
571/29:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train)
571/30: y_predict = regressor.predict(X_test_selected)
571/31:
mean_squared_error(y_test, y_predict, squared=False)
### better than before
571/32: df_out
571/33:
y_predict = regressor.predict(X_test_selected)
y_val_predict = regressor.predict(df_val)
571/34: df_val
571/35: y_train.head(3)
571/36: X_train.head(3)
571/37: X_test_selected
571/38: X_test_selected.columns
571/39:
y_predict = regressor.predict(X_test_selected)
y_val_predict = regressor.predict(df_val[X_test_selected.columns])
571/40:
mean_squared_error(y_test, y_predict, squared=False)
### better than before
571/41:
df_out.insert(1, "Prediction", y_val_predict, True)
df_t.to_csv("knn.csv", index=False)
571/42:
df_out.insert(1, "Prediction", y_val_predict, True)
df_out.to_csv("lin_regression_no_corr.csv", index=False)
571/43: df_out
572/1:
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
#from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
572/2:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
572/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')
df_out = df_val[['Id']]

print(df_train.shape)
print(df_val.shape)
572/4: df_train.head(5)
572/5:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("linear.pdf")
572/6:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(7,7))
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
pdf.close()
572/7:
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
572/8:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
572/9:
df_train.drop(['Id'], axis=1, inplace=True)
df_val.drop(['Id'], axis=1, inplace=True)
572/10:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
572/11:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
572/12: X_train.iloc[:, 3:-2]
572/13:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train.iloc[:, 3:-2], X_train.revenue)
572/14:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
mean_squared_error(X_test.revenue, y_pred, squared=False)
572/15:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=False, annot_kws={"size":8})
#sns.set(font_scale = .7)
572/16:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
572/17:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
572/18:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

X_train_selected.shape, X_test_selected.shape
572/19:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
572/20:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
572/21:
for feature in cat_features:
    mappings = find_category_mappings(X_train_selected, feature, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val, feature, mappings)
572/22:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
572/23:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
572/24:
X_test_selected.fillna(-1, inplace=True)
df_val.fillna(-1, inplace=True)
572/25:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
572/26:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train)
572/27:
y_predict = regressor.predict(X_test_selected)
y_val_predict = regressor.predict(df_val[X_test_selected.columns])
572/28:
mean_squared_error(y_test, y_predict, squared=False)
### better than before
572/29: df_out
572/30:
df_no_corr = df_out
df_no_corr.insert(1, "Prediction", y_val_predict, True)
df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
572/31: df_no_corr
572/32: df_no_corr.describe()
572/33:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
572/34:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
572/35:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train_scale)
572/36:
y_predict = regressor.predict(X_test_selected)
y_val_predict = regressor.predict(df_val[X_test_selected.columns])
572/37:
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
### better than before
572/38:
df_no_corr = df_out
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
572/39: df_no_corr.describe()
572/40: y_train_scale.hist()
572/41: np.histogram(y_train_scale)
572/42:
np.histogram(y_train_scale)
plt.show()
572/43:
plt.hist(y_train_scale, bins='auto')
plt.show()
572/44:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
572/45:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train_scale)
572/46:
y_predict = regressor.predict(X_test_selected)
y_val_predict = regressor.predict(df_val[X_test_selected.columns])
572/47:
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
### better than before
572/48: y_val_predict
572/49:
df_no_corr = df_out
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
572/50: df_no_corr.describe()
572/51: df_out
572/52:
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
#from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
572/53:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
572/54:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')
df_out = df_val[['Id']]

print(df_train.shape)
print(df_val.shape)
572/55:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
572/56: df_out
572/57: df_no_corr
572/58: df_no_corr[df_no_corr.pre]
572/59: df_no_corr.info()
572/60: df_no_corr.describe()
572/61: df_no_corr[df_no_corr.Prediction < 0]
572/62: df_no_corr[df_no_corr.Prediction < 0]['Prediction'] = 0
572/63: df_no_corr.describe()
572/64: df_no_corr[df_no_corr.Prediction < 0]['Prediction']
572/65: df_no_corr['Prediction'] = np.where(df['Prediction']<0, 0, df['Prediction'])
572/66: df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
572/67: df_no_corr.describe()
572/68:
df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
572/69: ###
572/70:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(np.array(X_train_selected), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
572/71: df_k.sort_values('avg_score', ascending=False)[:5]
572/72:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print(columns_selected)
572/73:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train)
572/74:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], target_scaler.transform(y_train))
572/75:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
572/76:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(np.array(X_train_selected), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
572/77: df_k.sort_values('avg_score', ascending=False)[:5]
572/78:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print(columns_selected)
572/79:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train_scale)
572/80:
y_predict = regressor.predict(X_test_selected[columns_selected])
mean_squared_error(y_test, target_scaler.transform(y_predict), squared=False)
572/81:
y_predict = regressor.predict(X_test_selected[columns_selected])
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
572/82:
y_predict = regressor.predict(X_test_selected[columns_selected])
y_val_predict = regressor.predict(df_val[columns_selected])
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
572/83: df_val
572/84:
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
#from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
572/85:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
572/86:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')
df_out = df_val[['Id']]

print(df_train.shape)
print(df_val.shape)
572/87: df_train.head(5)
572/88:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("linear.pdf")
572/89:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(7,7))
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
pdf.close()
572/90:
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
572/91:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
572/92:
df_train.drop(['Id'], axis=1, inplace=True)
df_val.drop(['Id'], axis=1, inplace=True)
572/93:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
572/94:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
572/95: X_train.iloc[:, 3:-2]
572/96:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train.iloc[:, 3:-2], X_train.revenue)
572/97:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
mean_squared_error(X_test.revenue, y_pred, squared=False)
572/98:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=False, annot_kws={"size":8})
#sns.set(font_scale = .7)
572/99:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
572/100:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
572/101:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

X_train_selected.shape, X_test_selected.shape
572/102:
for col in X_train_selected.columns:    
    print(X_train_selected.groupby(col)[col].count().sort_values() / len(X_train_selected)) # frequency
    print()
572/103:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
572/104:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
572/105:
for feature in cat_features:
    mappings = find_category_mappings(X_train_selected, feature, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val, feature, mappings)
572/106:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
572/107:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
572/108:
X_test_selected.fillna(-1, inplace=True)
df_val.fillna(-1, inplace=True)
572/109:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
572/110:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
572/111:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train_scale)
572/112:
y_predict = regressor.predict(X_test_selected)
y_val_predict = regressor.predict(df_val[X_test_selected.columns])
572/113:
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
### better than before
572/114:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
572/115: df_no_corr.describe()
572/116: df_no_corr[df_no_corr.Prediction < 0]['Prediction'] = 0
572/117:
df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
572/118: ###
572/119:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
572/120:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(np.array(X_train_selected), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
572/121: df_k.sort_values('avg_score', ascending=False)[:5]
572/122:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print(columns_selected)
572/123:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train_scale)
572/124: df_val
572/125:
y_predict = regressor.predict(X_test_selected[columns_selected])
y_val_predict = regressor.predict(df_val[columns_selected])
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
572/126:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
572/127: df_no_corr.describe()
572/128:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

X_train_selected.shape, X_test_selected.shape
572/129:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
572/130:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()

def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
572/131:
# and now we run a loop over the remaining categorical variables
for variable in cat_features:
    mappings = find_category_mappings(X_train_selected, variable, 'revenue')
    integer_encode(X_train_selected, X_test_selected, variable, mappings)
572/132:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
572/133:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
572/134: X_test_selected.fillna(-1, inplace=True)
572/135:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
572/136:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
572/137:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[cat_features], y_train_scale)
572/138: y_predict = regressor.predict(X_test_selected[cat_features])
572/139:
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
### better than before
572/140: X_train_selected
572/141:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train_scale)
572/142: y_predict = regressor.predict(X_test_selected)
572/143:
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
### better than before
572/144: ###
572/145:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(np.array(X_train_selected), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
572/146: df_k.sort_values('avg_score', ascending=False)[:5]
572/147:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print(columns_selected)
572/148:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train_scale)
572/149: X_train_selected[columns_selected]
572/150:
y_predict = regressor.predict(X_test_selected[columns_selected])
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
572/151: X_train_selected
572/152: X_train_selected
572/153:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

X_train_selected.shape, X_test_selected.shape
572/154: X_train_selected
572/155:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(X_train_selected.revenue)
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
572/156:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(X_train_selected.revenue.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
572/157: X_train_selected.revenue.reshape(-1, 1)
572/158: X_train_selected.revenue
572/159: np.array(X_train_selected.revenue)
572/160: np.array(X_train_selected.revenue).revenue.reshape(-1, 1)
572/161: np.array(X_train_selected.revenue).reshape(-1, 1)
572/162:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(np.array(X_train_selected.revenue).reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
572/163: X_train_selected
572/164: target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
572/165: X_train_selected
572/166: target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
572/167: target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
572/168: X_train_selected.revenue = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
572/169: X_train_selected
572/170: X_test_selected.revenue = target_scaler.transform(np.array(X_test_selected.revenue).reshape(-1, 1))
572/171: X_test_selected
572/172:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

X_train_selected.shape, X_test_selected.shape
572/173: X_train_selected
572/174:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(np.array(X_train_selected.revenue).reshape(-1, 1))
#y_train_scale = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
572/175: X_train_selected.revenue = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
572/176: X_train_selected
572/177: X_test_selected
572/178:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
572/179:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()

def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
572/180:
# and now we run a loop over the remaining categorical variables
for variable in cat_features:
    mappings = find_category_mappings(X_train_selected, variable, 'revenue')
    integer_encode(X_train_selected, X_test_selected, variable, mappings)
572/181:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
572/182:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
572/183: X_test_selected.fillna(-1, inplace=True)
572/184:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
572/185:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train_scale)
572/186: y_predict = regressor.predict(X_test_selected)
572/187:
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
### better than before
572/188: y_train
572/189: y_train
572/190:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train)
572/191: y_predict = regressor.predict(X_test_selected)
572/192:
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
### better than before
572/193: y_predict
572/194: target_scaler.inverse_transform(y_predict)
572/195: y_predict
572/196: y_train
572/197: y_predict = regressor.predict(X_test_selected)
572/198: y_predict
572/199: target_scaler.inverse_transform(y_predict)
572/200: target_scaler.inverse_transform(y_predict.reshape(1,-1))
572/201:
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict.reshape(1,-1)), squared=False)
### better than before
572/202: target_scaler.inverse_transform(y_predict.reshape(1,-1))[0]
572/203:
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict.reshape(1,-1))[0, squared=False)
### better than before
572/204:
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict.reshape(1,-1))[0], squared=False)
### better than before
572/205: y_predict = regressor.predict(X_test_selected)
572/206: y_predict
572/207: target_scaler.inverse_transform(y_predict.reshape(1,-1))[0]
572/208:
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict.reshape(1,-1))[0], squared=False)
### better than before
572/209:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

X_train_selected.shape, X_test_selected.shape
572/210: X_train_selected
572/211:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(np.array(X_train_selected.revenue).reshape(-1, 1))
#y_train_scale = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
572/212: X_train_selected.revenue = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
572/213:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
572/214:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()

def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
572/215:
# and now we run a loop over the remaining categorical variables
for variable in cat_features:
    mappings = find_category_mappings(X_train_selected, variable, 'revenue')
    integer_encode(X_train_selected, X_test_selected, variable, mappings)
572/216:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
572/217:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
572/218: X_test_selected.fillna(-1, inplace=True)
572/219:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
572/220: y_train
572/221: X_train_selected
572/222:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
572/223:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train)
572/224: y_predict = regressor.predict(X_test_selected)
572/225: target_scaler.inverse_transform(y_predict.reshape(1,-1))[0]
572/226: target_scaler.inverse_transform(y_predict.reshape(1,-1))
572/227: target_scaler.inverse_transform(y_predict.reshape(1,-1))[0]
572/228: y_predict = target_scaler.inverse_transform(y_predict.reshape(1,-1))[0]
572/229: y_test
572/230:
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict.reshape(1,-1))[0], squared=False)
### better than before
572/231:
mean_squared_error(y_test, y_predict, squared=False)
### better than before
572/232: ###
572/233: y_train
572/234:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(np.array(X_train_selected), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
572/235: df_k.sort_values('avg_score', ascending=False)[:5]
572/236:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print(columns_selected)
572/237:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train_scale)
572/238:
y_predict = regressor.predict(X_test_selected[columns_selected])
y_predict = target_scaler.inverse_transform(y_predict.reshape(1,-1))[0]
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
572/239:
y_predict = regressor.predict(X_test_selected[columns_selected])
y_predict = target_scaler.inverse_transform(y_predict.reshape(1,-1))[0]
mean_squared_error(y_test, y_predict, squared=False)
572/240:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

X_train_selected.shape, X_test_selected.shape
572/241: X_train_selected
572/242:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
572/243:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()

def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
572/244:
# and now we run a loop over the remaining categorical variables
for variable in cat_features:
    mappings = find_category_mappings(X_train_selected, variable, 'revenue')
    integer_encode(X_train_selected, X_test_selected, variable, mappings)
572/245:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
572/246:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
572/247: X_test_selected.fillna(-1, inplace=True)
572/248:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
572/249:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train)
572/250: y_predict = regressor.predict(X_test_selected)
572/251:
mean_squared_error(y_test, y_predict, squared=False)
### better than before
572/252: ###
572/253:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(np.array(X_train_selected), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
572/254: df_k.sort_values('avg_score', ascending=False)[:5]
572/255:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print(columns_selected)
572/256:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train_scale)
572/257:
y_predict = regressor.predict(X_test_selected[columns_selected])
y_predict = target_scaler.inverse_transform(y_predict.reshape(1,-1))[0]
mean_squared_error(y_test, y_predict, squared=False)
572/258:
y_predict = regressor.predict(X_test_selected[columns_selected])
mean_squared_error(y_test, y_predict, squared=False)
572/259: y_predict
572/260:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train)
572/261:
y_predict = regressor.predict(X_test_selected[columns_selected])
mean_squared_error(y_test, y_predict, squared=False)
572/262: y_predict
572/263:
y_predict = regressor.predict(X_test_selected[columns_selected])
y_val_predict = regressor.predict(df_val[columns_selected])
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)

df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)

df_no_corr.describe()
572/264:
y_val_predict = regressor.predict(df_val[columns_selected])
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)

df_no_corr.describe()
572/265:
y_val_predict = regressor.predict(df_val[columns_selected])
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", y_val_predict, True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)

df_no_corr.describe()
572/266: y_val_predict
572/267: df_val[columns_selected]
572/268:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')
df_out = df_val[['Id']]

print(df_train.shape)
print(df_val.shape)
572/269: df_train.head(5)
572/270:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("linear.pdf")
572/271:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(7,7))
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
pdf.close()
572/272:
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
572/273:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
572/274:
df_train.drop(['Id'], axis=1, inplace=True)
df_val.drop(['Id'], axis=1, inplace=True)
572/275:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
572/276:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
572/277: X_train.iloc[:, 3:-2]
572/278:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train.iloc[:, 3:-2], X_train.revenue)
572/279:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
mean_squared_error(X_test.revenue, y_pred, squared=False)
572/280:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=False, annot_kws={"size":8})
#sns.set(font_scale = .7)
572/281:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
572/282:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
572/283:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

X_train_selected.shape, X_test_selected.shape
572/284:
for col in X_train_selected.columns:    
    print(X_train_selected.groupby(col)[col].count().sort_values() / len(X_train_selected)) # frequency
    print()
572/285: df_val_encod = df_val.copy()
572/286:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
572/287:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
572/288:
for feature in cat_features:
    mappings = find_category_mappings(X_train_selected, feature, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, feature, mappings)
572/289:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
572/290:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
572/291:
X_test_selected.fillna(-1, inplace=True)
df_val_encod.fillna(-1, inplace=True)
572/292:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
572/293:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
572/294:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train_scale)
572/295:
y_predict = regressor.predict(X_test_selected)
y_val_predict = regressor.predict(df_val[X_test_selected.columns])
572/296:
y_predict = regressor.predict(X_test_selected)
y_val_predict = regressor.predict(df_val_encod[X_test_selected.columns])
572/297:
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
### better than before
572/298:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
572/299: df_no_corr.describe()
572/300: df_no_corr[df_no_corr.Prediction < 0]['Prediction'] = 0
572/301:
df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
572/302:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
572/303:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(np.array(X_train_selected), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
572/304: df_k.sort_values('avg_score', ascending=False)[:5]
572/305:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print(columns_selected)
572/306:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train_scale)
572/307:
y_predict = regressor.predict(X_test_selected[columns_selected])
y_val_predict = regressor.predict(df_val_encod[columns_selected])
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
572/308:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
572/309: df_no_corr.describe()
572/310: df_no_corr[df_no_corr.Prediction < 0]['Prediction'] = 0
572/311:
df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
572/312:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

X_train_selected.shape, X_test_selected.shape
572/313:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)
df_val_encod = df_val.copy()

X_train_selected.shape, X_test_selected.shape
572/314: X_train_selected
572/315:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(np.array(X_train_selected.revenue).reshape(-1, 1))
#y_train_scale = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
572/316:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
572/317:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()

def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
572/318:
# and now we run a loop over the remaining categorical variables
for variable in cat_features:
    mappings = find_category_mappings(X_train_selected, variable, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encoda, variable, mappings)
572/319:
# and now we run a loop over the remaining categorical variables
for variable in cat_features:
    mappings = find_category_mappings(X_train_selected, variable, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, variable, mappings)
572/320:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
572/321:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
572/322:
X_test_selected.fillna(-1, inplace=True)
df_val_encod.fillna(-1, inplace=True)
572/323:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
572/324:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train)
572/325: y_predict = regressor.predict(X_test_selected)
572/326:
mean_squared_error(y_test, y_predict, squared=False)
### better than before
572/327: ###
572/328:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(np.array(X_train_selected), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
572/329: df_k.sort_values('avg_score', ascending=False)[:5]
572/330:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print(columns_selected)
572/331:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train)
572/332:
y_predict = regressor.predict(X_test_selected[columns_selected])
mean_squared_error(y_test, y_predict, squared=False)
572/333: df_val[columns_selected]
572/334: df_val_encodl[columns_selected]
572/335: df_val_encod[columns_selected]
572/336:
y_val_predict = regressor.predict(df_val_encod[columns_selected])
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", y_val_predict, True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)

df_no_corr.describe()
572/337: df_val[columns_selected]
572/338:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)
df_val_encod = df_val.copy()

X_train_selected.shape, X_test_selected.shape
572/339: X_train_selected
572/340:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(np.array(X_train_selected.revenue).reshape(-1, 1))
#y_train_scale = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
572/341:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
572/342:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()

def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
572/343:
# and now we run a loop over the remaining categorical variables
for variable in cat_features:
    mappings = find_category_mappings(X_train_selected, variable, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, variable, mappings)
572/344:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
572/345:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
572/346:
X_test_selected.fillna(100, inplace=True)
df_val_encod.fillna(100, inplace=True)
572/347:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
572/348:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train)
572/349: y_predict = regressor.predict(X_test_selected)
572/350:
mean_squared_error(y_test, y_predict, squared=False)
### better than before
572/351: ###
572/352:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(np.array(X_train_selected), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
572/353: df_k.sort_values('avg_score', ascending=False)[:5]
572/354:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print(columns_selected)
572/355:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train)
572/356:
y_predict = regressor.predict(X_test_selected[columns_selected])
mean_squared_error(y_test, y_predict, squared=False)
572/357:
y_val_predict = regressor.predict(df_val_encod[columns_selected])
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", y_val_predict, True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)

df_no_corr.describe()
572/358:
df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
572/359:
df_no_corr[df_no_corr.Prediction < 0]['Prediction'] = 0

df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_mean.csv", index=False)
573/1:
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
#from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
573/2:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
573/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')
df_out = df_val[['Id']]

print(df_train.shape)
print(df_val.shape)
573/4: df_train.head(5)
573/5:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("linear.pdf")
573/6:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(7,7))
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
pdf.close()
573/7:
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
573/8:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
573/9:
df_train.drop(['Id'], axis=1, inplace=True)
df_val.drop(['Id'], axis=1, inplace=True)
573/10:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
573/11:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
573/12: X_train.iloc[:, 3:-2]
573/13:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train.iloc[:, 3:-2], X_train.revenue)
573/14:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
mean_squared_error(X_test.revenue, y_pred, squared=False)
573/15:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=False, annot_kws={"size":8})
#sns.set(font_scale = .7)
573/16:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
573/17:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
573/18:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

X_train_selected.shape, X_test_selected.shape
573/19: df_val_encod = df_val.copy()
573/20:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
573/21:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
573/22:
for feature in cat_features:
    mappings = find_category_mappings(X_train_selected, feature, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, feature, mappings)
573/23:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
573/24:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
573/25:
X_test_selected.fillna(100, inplace=True)
df_val_encod.fillna(100, inplace=True)
573/26:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
573/27:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
573/28:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train_scale)
573/29:
y_predict = regressor.predict(X_test_selected)
y_val_predict = regressor.predict(df_val_encod[X_test_selected.columns])
573/30:
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
### better than before
573/31:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
573/32:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(np.array(X_train_selected), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
573/33: df_k.sort_values('avg_score', ascending=False)[:5]
573/34:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print(columns_selected)
573/35:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train_scale)
573/36: df_val_encod
573/37:
y_predict = regressor.predict(X_test_selected[columns_selected])
y_val_predict = regressor.predict(df_val_encod[columns_selected])
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
573/38: y_test
573/39: y_predict
573/40:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
573/41: df_no_corr.describe()
573/42: df_no_corr[df_no_corr.Prediction < 0]['Prediction'] = 0
573/43:
df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
574/1:
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
#from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
574/2:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
574/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')
df_out = df_val[['Id']]

print(df_train.shape)
print(df_val.shape)
574/4: df_train.head(5)
574/5:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("linear.pdf")
574/6:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(7,7))
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
pdf.close()
574/7:
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
574/8:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
574/9:
df_train.drop(['Id'], axis=1, inplace=True)
df_val.drop(['Id'], axis=1, inplace=True)
574/10:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
574/11:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
574/12: X_train.iloc[:, 3:-2]
574/13:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train.iloc[:, 3:-2], X_train.revenue)
574/14:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
mean_squared_error(X_test.revenue, y_pred, squared=False)
574/15:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=False, annot_kws={"size":8})
#sns.set(font_scale = .7)
574/16:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
574/17:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
574/18:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

X_train_selected.shape, X_test_selected.shape
574/19: df_val_encod = df_val.copy()
574/20:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
574/21:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
574/22:
for feature in cat_features:
    mappings = find_category_mappings(X_train_selected, feature, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, feature, mappings)
574/23:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
574/24:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
574/25:
X_test_selected.fillna(100, inplace=True)
df_val_encod.fillna(100, inplace=True)
574/26:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
574/27:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train)
574/28:
y_predict = regressor.predict(X_test_selected)
y_val_predict = regressor.predict(df_val_encod[X_test_selected.columns])
574/29:
mean_squared_error(y_test, y_predict, squared=False)
### better than before
574/30:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
574/31:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train_scale)
574/32:
y_predict = regressor.predict(X_test_selected)
y_val_predict = regressor.predict(df_val_encod[X_test_selected.columns])
574/33:
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
### better than before
574/34: X_test_selected
574/35: X_train_selected_selected
574/36: X_train_selected
575/1:
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
#from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
575/2:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
575/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')
df_out = df_val[['Id']]

print(df_train.shape)
print(df_val.shape)
575/4: df_train.head(5)
575/5:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("linear.pdf")
575/6:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(7,7))
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
pdf.close()
575/7:
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
575/8:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
575/9:
df_train.drop(['Id'], axis=1, inplace=True)
df_val.drop(['Id'], axis=1, inplace=True)
575/10:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
575/11:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
575/12: X_train.iloc[:, 3:-2]
575/13:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train.iloc[:, 3:-2], X_train.revenue)
575/14:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
mean_squared_error(X_test.revenue, y_pred, squared=False)
575/15:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=False, annot_kws={"size":8})
#sns.set(font_scale = .7)
575/16:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
575/17:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
575/18:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

X_train_selected.shape, X_test_selected.shape
575/19: df_val_encod = df_val.copy()
575/20:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
575/21:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
575/22:
for feature in cat_features:
    mappings = find_category_mappings(X_train_selected, feature, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, feature, mappings)
575/23:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
575/24:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
575/25:
X_test_selected.fillna(100, inplace=True)
df_val_encod.fillna(100, inplace=True)
575/26:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
575/27: y_train
575/28: X_train_selected
575/29:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train)
575/30: X_test_selected.columns
575/31:
y_predict = regressor.predict(X_test_selected)
y_val_predict = regressor.predict(df_val_encod[X_test_selected.columns])
575/32:
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
### better than before
575/33:
mean_squared_error(y_test, y_predict, squared=False)
### better than before
575/34:
mean_squared_error(y_test, y_predict, squared=True)
### better than before
575/35:
mean_squared_error(y_test, y_predict, squared=False)
### better than before
575/36:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(np.array(X_train_selected), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
575/37: df_k.sort_values('avg_score', ascending=False)[:5]
575/38:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print(columns_selected)
575/39:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train)
575/40:
y_predict = regressor.predict(X_test_selected[columns_selected])
y_val_predict = regressor.predict(df_val_encod[columns_selected])
mean_squared_error(y_test, y_predict, squared=False)
575/41: y_predict
575/42:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
575/43:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
575/44:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train_scale)
575/45:
y_predict = regressor.predict(X_test_selected)
y_val_predict = regressor.predict(df_val_encod[X_test_selected.columns])
575/46:
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
### better than before
575/47:
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
### better than before
575/48:
X_test_selected.fillna(-1, inplace=True)
df_val_encod.fillna(-1, inplace=True)
575/49: df_val_encod = df_val.copy()
575/50:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
575/51:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
575/52:
for feature in cat_features:
    mappings = find_category_mappings(X_train_selected, feature, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, feature, mappings)
575/53:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

X_train_selected.shape, X_test_selected.shape
575/54: df_val_encod = df_val.copy()
575/55:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
575/56:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
575/57:
for feature in cat_features:
    mappings = find_category_mappings(X_train_selected, feature, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, feature, mappings)
575/58:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
575/59:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
575/60:
X_test_selected.fillna(-1, inplace=True)
df_val_encod.fillna(-1, inplace=True)
575/61:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
575/62:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
575/63:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train_scale)
575/64:
y_predict = regressor.predict(X_test_selected)
y_val_predict = regressor.predict(df_val_encod[X_test_selected.columns])
575/65:
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
### better than before
576/1:
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
#from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
576/2:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
576/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')
df_out = df_val[['Id']]

print(df_train.shape)
print(df_val.shape)
576/4: df_train.head(5)
576/5:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("linear.pdf")
576/6:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(7,7))
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
pdf.close()
576/7:
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
576/8:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
576/9:
df_train.drop(['Id'], axis=1, inplace=True)
df_val.drop(['Id'], axis=1, inplace=True)
576/10:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
576/11:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
576/12: X_train.iloc[:, 3:-2]
576/13:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train.iloc[:, 3:-2], X_train.revenue)
576/14:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
mean_squared_error(X_test.revenue, y_pred, squared=False)
576/15:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=False, annot_kws={"size":8})
#sns.set(font_scale = .7)
576/16:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
576/17:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
576/18:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

X_train_selected.shape, X_test_selected.shape
576/19: df_val_encod = df_val.copy()
576/20:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
576/21:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
576/22:
for feature in cat_features:
    mappings = find_category_mappings(X_train_selected, feature, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, feature, mappings)
576/23:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
576/24:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
576/25:
for col in X_train_selected.columns:    
    print(X_train_selected.groupby(col)[col].count().sort_values() / len(X_train_selected)) # frequency
    print()
576/26: X_train_selected.groupby('P8')['P8'].count()
576/27: df_val_encod = df_val.copy()
576/28:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
576/29:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
576/30:
for feature in cat_features:
    mappings = find_category_mappings(X_train_selected, feature, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, feature, mappings)
576/31:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
576/32:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
576/33:
X_test_selected.fillna(-1, inplace=True)
df_val_encod.fillna(-1, inplace=True)
576/34:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
576/35:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train_scale)
576/36:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train)
576/37:
y_predict = regressor.predict(X_test_selected)
y_val_predict = regressor.predict(df_val_encod[X_test_selected.columns])
576/38:
mean_squared_error(y_test, y_predict, squared=False)
### better than before
576/39:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", y_val_predict, True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
576/40: df_no_corr.describe()
577/1:
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
#from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
577/2:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
577/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')
df_out = df_val[['Id']]

print(df_train.shape)
print(df_val.shape)
577/4: df_train.head(5)
577/5:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("linear.pdf")
577/6:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(7,7))
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
pdf.close()
577/7:
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
577/8:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
577/9:
df_train.drop(['Id'], axis=1, inplace=True)
df_val.drop(['Id'], axis=1, inplace=True)
577/10:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
577/11:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
577/12: X_train.iloc[:, 3:-2]
577/13:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train.iloc[:, 3:-2], X_train.revenue)
577/14:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
mean_squared_error(X_test.revenue, y_pred, squared=False)
577/15:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=False, annot_kws={"size":8})
#sns.set(font_scale = .7)
577/16:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
577/17:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
577/18:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

X_train_selected.shape, X_test_selected.shape
577/19: df_val_encod = df_val.copy()
577/20:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
577/21:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
577/22:
for feature in cat_features:
    mappings = find_category_mappings(X_train_selected, feature, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, feature, mappings)
577/23:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
577/24:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
577/25:
X_test_selected.fillna(-1, inplace=True)
df_val_encod.fillna(-1, inplace=True)
577/26:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
577/27:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
577/28:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train_scale)
577/29:
y_predict = regressor.predict(X_test_selected)
y_val_predict = regressor.predict(df_val_encod[X_test_selected.columns])
577/30:
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
### better than before
577/31:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", y_val_predict, True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
577/32:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
577/33: df_no_corr.describe()
577/34: df_no_corr[df_no_corr.Prediction < 0]['Prediction'] = 0
577/35:
df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
577/36:
df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_label_encode.csv", index=False)
577/37:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(np.array(X_train_selected), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
577/38: df_k.sort_values('avg_score', ascending=False)[:5]
577/39:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print(columns_selected)
577/40:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train_scale)
577/41:
y_predict = regressor.predict(X_test_selected[columns_selected])
y_val_predict = regressor.predict(df_val_encod[columns_selected])
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
577/42:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
577/43: df_no_corr.describe()
577/44: df_no_corr[df_no_corr.Prediction < 0]['Prediction'] = 0
577/45:
df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_label_encode_selected.csv", index=False)
577/46:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)
df_val_encod = df_val.copy()

X_train_selected.shape, X_test_selected.shape
577/47: X_train_selected
577/48:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(np.array(X_train_selected.revenue).reshape(-1, 1))
#y_train_scale = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
577/49: X_train_selected.revenue = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
577/50: X_train_selected
577/51:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
577/52:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()

def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
577/53:
# and now we run a loop over the remaining categorical variables
for variable in cat_features:
    mappings = find_category_mappings(X_train_selected, variable, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, variable, mappings)
577/54:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
577/55:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
577/56:
X_test_selected.fillna(-1, inplace=True)
df_val_encod.fillna(-1, inplace=True)
577/57:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
577/58:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train)
577/59: y_train
577/60: X_train_selected
577/61: y_predict = regressor.predict(X_test_selected)
577/62: y_test
577/63:
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
### better than before
577/64:
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict.reshape(-1,1)), squared=False)
### better than before
577/65:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)
df_val_encod = df_val.copy()

X_train_selected.shape, X_test_selected.shape
577/66:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
577/67:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()

def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
577/68:
# and now we run a loop over the remaining categorical variables
for variable in cat_features:
    mappings = find_category_mappings(X_train_selected, variable, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, variable, mappings)
577/69:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
577/70:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
577/71:
X_test_selected.fillna(1000, inplace=True)
df_val_encod.fillna(1000, inplace=True)
577/72:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
577/73:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)
df_val_encod = df_val.copy()

X_train_selected.shape, X_test_selected.shape
577/74:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
577/75:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()

def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
577/76:
# and now we run a loop over the remaining categorical variables
for variable in cat_features:
    mappings = find_category_mappings(X_train_selected, variable, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, variable, mappings)
577/77:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
577/78:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
577/79:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()]
577/80:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
577/81:
X_test_selected.fillna(100000, inplace=True)
df_val_encod.fillna(100000, inplace=True)
577/82:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
577/83:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train)
577/84: y_predict = regressor.predict(X_test_selected)
577/85:
mean_squared_error(y_test, y_predict, squared=False)
### better than before
577/86:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)
df_val_encod = df_val.copy()

X_train_selected.shape, X_test_selected.shape
577/87:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(np.array(X_train_selected.revenue).reshape(-1, 1))
#y_train_scale = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
577/88: X_train_selected.revenue = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
577/89:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
577/90:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)
df_val_encod = df_val.copy()

X_train_selected.shape, X_test_selected.shape
577/91:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
577/92:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()

def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
577/93:
# and now we run a loop over the remaining categorical variables
for variable in cat_features:
    mappings = find_category_mappings(X_train_selected, variable, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, variable, mappings)
577/94:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
577/95:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
577/96: X_train.min()
577/97: X_train_selected.min()
577/98: X_train_selected.min() * 100000
577/99: X_train_selected.min() / 100000
577/100: X_train_selected.min() / 1000000
577/101:
X_test_selected.fillna(1000000, inplace=True)
df_val_encod.fillna(1000000, inplace=True)
577/102:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
577/103:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train)
577/104: y_predict = regressor.predict(X_test_selected)
577/105:
mean_squared_error(y_test, y_predict, squared=False)
### better than before
577/106:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(np.array(X_train_selected), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
577/107: df_k.sort_values('avg_score', ascending=False)[:5]
577/108:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print(columns_selected)
577/109:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train)
577/110:
y_predict = regressor.predict(X_test_selected[columns_selected])
mean_squared_error(y_test, y_predict, squared=False)
577/111:
y_val_predict = regressor.predict(df_val_encod[columns_selected])
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", y_val_predict, True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)

df_no_corr.describe()
577/112: df_no_corr[df_no_corr.Prediction < 0]['Prediction']
577/113:
df_no_corr[df_no_corr.Prediction < 0]['Prediction'] = 0

df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_mean.csv", index=False)
577/114:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)
df_val_encod = df_val.copy()

X_train_selected.shape, X_test_selected.shape
577/115:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(np.array(X_train_selected.revenue).reshape(-1, 1))
#y_train_scale = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
577/116: X_train_selected.revenue = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
577/117:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
577/118:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()

def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
577/119:
# and now we run a loop over the remaining categorical variables
for variable in cat_features:
    mappings = find_category_mappings(X_train_selected, variable, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, variable, mappings)
577/120:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
577/121:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
577/122:
X_test_selected.fillna(-1, inplace=True)
df_val_encod.fillna(-1, inplace=True)
577/123:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
577/124:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train)
577/125: y_predict = regressor.predict(X_test_selected)
577/126:
mean_squared_error(y_test, y_predict, squared=False)
### better than before
577/127:
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
### better than before
577/128:
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict.reshape(-1,1)), squared=False)
### better than before
577/129: mean_squared_error(y_test, y_predict, squared=False)
577/130: y_train
577/131: y_test
577/132: y_predict
577/133: target_scaler.inverse_transform(y_predict.reshape(-1,1))
577/134:
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict.reshape(-1,1)), squared=False)
### better than before
577/135:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=10)

sfs = sfs.fit(np.array(X_train_selected), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
577/136: df_k.sort_values('avg_score', ascending=False)[:5]
577/137:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print(columns_selected)
577/138:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train)
577/139:
y_predict = regressor.predict(X_test_selected[columns_selected])
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
577/140:
y_predict = regressor.predict(X_test_selected[columns_selected])
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict.reshape(-1,1)), squared=False)
577/141:
y_val_predict = regressor.predict(df_val_encod[columns_selected])
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", y_val_predict, True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)

df_no_corr.describe()
577/142:
y_val_predict = regressor.predict(df_val_encod[columns_selected])
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict.reshape(-1,1)), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)

df_no_corr.describe()
577/143:
df_no_corr[df_no_corr.Prediction < 0]['Prediction'] = 0

df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_mean_scale.csv", index=False)
####
#Score: 2350183.91438
#Private score: 2293530.77368
577/144: df_no_corr.describe()
578/1:
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
#from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
578/2:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
578/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')
df_out = df_val[['Id']]

print(df_train.shape)
print(df_val.shape)
578/4: df_train.head(5)
579/1:
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
#from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
579/2:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
579/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')
df_out = df_val[['Id']]

print(df_train.shape)
print(df_val.shape)
579/4: df_train.head(5)
579/5:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("linear.pdf")
579/6:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(7,7))
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
pdf.close()
579/7:
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
579/8:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
579/9:
df_train.drop(['Id'], axis=1, inplace=True)
df_val.drop(['Id'], axis=1, inplace=True)
579/10:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
579/11:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
579/12: X_train.iloc[:, 3:-2]
579/13:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train.iloc[:, 3:-2], X_train.revenue)
579/14:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
mean_squared_error(X_test.revenue, y_pred, squared=False)
579/15:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
mean_squared_error(X_test.revenue, y_pred, squared=False)
r2_score(X_test.revenue, y_pred)
579/16:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
print(f'RMSE: {mean_squared_error(X_test.revenue, y_pred, squared=False)})
print(f'R2: {r2_score(X_test.revenue, y_pred}))
579/17:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
print(f'RMSE: {mean_squared_error(X_test.revenue, y_pred, squared=False)}')
print(f'R2: {r2_score(X_test.revenue, y_pred)}')
579/18:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
print(f'RMSE: {mean_squared_error(X_test.revenue, y_pred, squared=False):.2f}')
print(f'R2: {r2_score(X_test.revenue, y_pred)}')
579/19:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
print(f'RMSE: {mean_squared_error(X_test.revenue, y_pred, squared=False):.2f}')
print(f'R2: {r2_score(X_test.revenue, y_pred):.2f}')
579/20:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
print(f'RMSE: {mean_squared_error(X_test.revenue, y_pred, squared=False):.2f}')
print(f'R2: {r2_score(X_test.revenue, y_pred):.4f}')
579/21: ### New features
579/22:
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
579/23:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
580/1:
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
#from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
580/2:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
580/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')
df_out = df_val[['Id']]

print(df_train.shape)
print(df_val.shape)
580/4: df_train.head(5)
580/5:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("linear.pdf")
580/6:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(7,7))
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
pdf.close()
580/7:
## A function for days counting
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
580/8:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
580/9:
df_train.drop(['Id'], axis=1, inplace=True)
df_val.drop(['Id'], axis=1, inplace=True)
580/10:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
580/11:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
580/12: X_train.iloc[:, 3:-2]
580/13: X_train.iloc[:, 3:-2].head(5)
580/14:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train.iloc[:, 3:-2], X_train.revenue)
580/15:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
print(f'RMSE: {mean_squared_error(X_test.revenue, y_pred, squared=False):.2f}')
print(f'R2: {r2_score(X_test.revenue, y_pred):.4f}')
580/16:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=False, annot_kws={"size":8})
#sns.set(font_scale = .7)
580/17:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
580/18:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
580/19:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

X_train_selected.shape, X_test_selected.shape
580/20:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

print('New shapes:')
X_train_selected.shape, X_test_selected.shape
580/21: df_val_encod = df_val.copy()
580/22:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
580/23:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
580/24:
for feature in cat_features:
    mappings = find_category_mappings(X_train_selected, feature, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, feature, mappings)
580/25:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
580/26:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
580/27:
X_test_selected.fillna(-1, inplace=True)
df_val_encod.fillna(-1, inplace=True)
580/28:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
580/29:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
580/30:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train_scale)
580/31:
y_predict = regressor.predict(X_test_selected)
y_val_predict = regressor.predict(df_val_encod[X_test_selected.columns])
580/32:
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
### better than before
580/33: sns.histplot(data=y_train)
580/34:
plt.figure(figsize=(6,5))
sns.histplot(data=y_train)
580/35:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
580/36:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train_scale)
580/37:
y_predict = regressor.predict(X_test_selected)
y_val_predict = regressor.predict(df_val_encod[X_test_selected.columns])
580/38:
target_scaler = StandardScaler()
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
580/39:
plt.figure(figsize=(6,5))
sns.histplot(data=y_train_scale)
580/40: from sklearn.preprocessing import Normalizer
580/41:
target_scaler = Normalizer()
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
580/42:
plt.figure(figsize=(6,5))
sns.histplot(data=y_train_scale)
580/43: y_train_scale
580/44:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
580/45:
plt.figure(figsize=(6,5))
sns.histplot(data=y_train_scale)
580/46:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
580/47:
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
### better than before
580/48:
print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
580/49:
#df_no_corr = df_out.copy()
#df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
580/50: df_no_corr.describe()
580/51:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
580/52: df_no_corr.describe()
580/53:
df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_label_encode.csv", index=False)
### 
###Score: 1985413.56674
###Private score: 2083671.58081
580/54:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
580/55:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=20)

sfs = sfs.fit(np.array(X_train_selected), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
580/56: df_k.sort_values('avg_score', ascending=False)[:5]
580/57:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print(columns_selected)
580/58:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train_scale)
580/59:
y_predict = regressor.predict(X_test_selected[columns_selected])
y_val_predict = regressor.predict(df_val_encod[columns_selected])
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
580/60:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print('Selected columns:')
print(columns_selected)
580/61:
y_predict = regressor.predict(X_test_selected[columns_selected])
y_val_predict = regressor.predict(df_val_encod[columns_selected])
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)

print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
580/62:
y_predict = regressor.predict(X_test_selected[columns_selected])
y_val_predict = regressor.predict(df_val_encod[columns_selected])

print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
580/63:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
580/64: df_no_corr.describe()
580/65: df_no_corr[df_no_corr.Prediction < 0]['Prediction'] = 0
580/66:
df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_label_encode_selected.csv", index=False)
#####
### Score: 2017086.38946
### Private score: 2169576.52804
580/67:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)
df_val_encod = df_val.copy()

X_train_selected.shape, X_test_selected.shape
580/68:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(np.array(X_train_selected.revenue).reshape(-1, 1))
#y_train_scale = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
580/69: X_train_selected.revenue = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
580/70:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
580/71:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()

def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
580/72:
# and now we run a loop over the remaining categorical variables
for variable in cat_features:
    mappings = find_category_mappings(X_train_selected, variable, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, variable, mappings)
580/73:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
580/74:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
580/75:
X_test_selected.fillna(-1, inplace=True)
df_val_encod.fillna(-1, inplace=True)
580/76:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
580/77:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train)
580/78: y_predict = regressor.predict(X_test_selected)
580/79:
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict.reshape(-1,1)), squared=False)
### better than before
580/80:
print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
580/81: y_predict = regressor.predict(X_test_selected).reshape(-1,1)
580/82:
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict.reshape(-1,1)), squared=False)
### better than before
580/83:
print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
580/84:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(np.array(X_train_selected.revenue).reshape(-1, 1))
y_train_scale = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
580/85:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)
df_val_encod = df_val.copy()

X_train_selected.shape, X_test_selected.shape
580/86:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(np.array(X_train_selected.revenue).reshape(-1, 1))
y_train_scale = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
580/87: X_train_selected.revenue = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
580/88:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
580/89:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()

def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
580/90:
# and now we run a loop over the remaining categorical variables
for variable in cat_features:
    mappings = find_category_mappings(X_train_selected, variable, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, variable, mappings)
580/91:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
580/92:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
580/93:
X_test_selected.fillna(-1, inplace=True)
df_val_encod.fillna(-1, inplace=True)
580/94:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
580/95:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train_scale)
580/96: y_predict = regressor.predict(X_test_selected).reshape(-1,1)
580/97:
print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
580/98: y_predict
580/99: target_scaler.inverse_transform(y_predict)
580/100:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)
df_val_encod = df_val.copy()

X_train_selected.shape, X_test_selected.shape
580/101:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(np.array(X_train_selected.revenue).reshape(-1, 1))
y_train_scale = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
580/102: X_train_selected.revenue = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
580/103:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
580/104:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()

def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
580/105: X_train_selected
580/106:
# and now we run a loop over the remaining categorical variables
for variable in cat_features:
    mappings = find_category_mappings(X_train_selected, variable, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, variable, mappings)
580/107: X_train_selected
580/108:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
580/109:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
580/110:
X_test_selected.fillna(-0.1, inplace=True)
df_val_encod.fillna(-0.1, inplace=True)
580/111:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
580/112:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train)
580/113: y_predict = regressor.predict(X_test_selected).reshape(-1,1)
580/114: target_scaler.inverse_transform(y_predict)
580/115:
print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
580/116:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)
df_val_encod = df_val.copy()

X_train_selected.shape, X_test_selected.shape
580/117:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(np.array(X_train_selected.revenue).reshape(-1, 1))
y_train_scale = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
580/118: X_train_selected.revenue = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
580/119:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
580/120:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()

def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
580/121: X_train_selected
580/122:
# and now we run a loop over the remaining categorical variables
for variable in cat_features:
    mappings = find_category_mappings(X_train_selected, variable, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, variable, mappings)
580/123:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
580/124:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
580/125:
X_test_selected.fillna(-0.01, inplace=True)
df_val_encod.fillna(-0.01, inplace=True)
580/126:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
580/127:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train)
580/128: y_predict = regressor.predict(X_test_selected).reshape(-1,1)
580/129:
print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
580/130: X_test_selected.describe()
580/131: X_train_selected
580/132: y_train
580/133:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=20)

sfs = sfs.fit(np.array(X_train_selected), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
580/134: df_k.sort_values('avg_score', ascending=False)[:5]
580/135:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print(columns_selected)
580/136:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train)
580/137:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print('Selected columns:')
print(columns_selected)
580/138:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train)
580/139:
y_predict = regressor.predict(X_test_selected[columns_selected]).reshape(-1,1)
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
580/140:
y_predict = regressor.predict(X_test_selected[columns_selected]).reshape(-1,1)
mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)

print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
580/141: mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False)
580/142:
y_predict = regressor.predict(X_test_selected[columns_selected]).reshape(-1,1)

print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
580/143:
y_val_predict = regressor.predict(df_val_encod[columns_selected])
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict.reshape(-1,1)), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)

df_no_corr.describe()
580/144:
df_no_corr[df_no_corr.Prediction < 0]['Prediction'] = 0

df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_mean.csv", index=False)
####
#Score: 2350183.91438
#Private score: 2293530.77368
580/145: df_no_corr.describe()
580/146:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

X_train_selected.shape, X_test_selected.shape
580/147:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
580/148:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()

def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
580/149:
# and now we run a loop over the remaining categorical variables
for variable in cat_features:
    mappings = find_category_mappings(X_train_selected, variable, 'revenue')
    integer_encode(X_train_selected, X_test_selected, variable, mappings)
580/150:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
580/151:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
580/152: X_test_selected.fillna(-1, inplace=True)
580/153:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
580/154:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[cat_features], y_train)
580/155: y_predict = regressor.predict(X_test_selected[cat_features])
580/156:
mean_squared_error(y_test, y_predict, squared=False)
### better than before
580/157: ###
581/1:
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
#from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
581/2:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
581/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')
df_out = df_val[['Id']]

print(df_train.shape)
print(df_val.shape)
581/4: df_train.head(5)
581/5:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("linear.pdf")
581/6:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(7,7))
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
pdf.close()
581/7:
## A function for days counting
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
581/8:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
581/9:
df_train.drop(['Id'], axis=1, inplace=True)
df_val.drop(['Id'], axis=1, inplace=True)
581/10:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
581/11:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
581/12:
### choose only P features
X_train.iloc[:, 3:-2].head(5)
581/13:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train.iloc[:, 3:-2], X_train.revenue)
581/14:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
print(f'RMSE: {mean_squared_error(X_test.revenue, y_pred, squared=False):.2f}')
print(f'R2: {r2_score(X_test.revenue, y_pred):.4f}')
581/15:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=False, annot_kws={"size":8})
#sns.set(font_scale = .7)
581/16:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
581/17:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
581/18:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

print('New shapes:')
X_train_selected.shape, X_test_selected.shape
581/19: df_val_encod = df_val.copy()
581/20:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
581/21:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
581/22:
for feature in cat_features:
    mappings = find_category_mappings(X_train_selected, feature, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, feature, mappings)
581/23:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
581/24:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
581/25:
X_test_selected.fillna(-1, inplace=True)
df_val_encod.fillna(-1, inplace=True)
581/26:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
581/27:
plt.figure(figsize=(6,5))
sns.histplot(data=y_train)
581/28:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
581/29:
plt.figure(figsize=(6,5))
sns.histplot(data=y_train_scale)
581/30:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train_scale)
581/31:
y_predict = regressor.predict(X_test_selected)
y_val_predict = regressor.predict(df_val_encod[X_test_selected.columns])
581/32:
print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
581/33:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
581/34: df_no_corr.describe()
581/35:
df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_label_encode.csv", index=False)
### 
###Score: 1985413.56674
###Private score: 2083671.58081
581/36:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
581/37:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=20)

sfs = sfs.fit(np.array(X_train_selected), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
581/38: df_k.sort_values('avg_score', ascending=False)[:5]
581/39:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print('Selected columns:')
print(columns_selected)
581/40:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train_scale)
581/41:
y_predict = regressor.predict(X_test_selected[columns_selected])
y_val_predict = regressor.predict(df_val_encod[columns_selected])

print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
581/42:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
581/43: df_no_corr.describe()
581/44: df_no_corr[df_no_corr.Prediction < 0]['Prediction'] = 0
581/45:
df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_label_encode_selected.csv", index=False)
#####
### Score: 2017086.38946
### Private score: 2169576.52804
581/46:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)
df_val_encod = df_val.copy()

X_train_selected.shape, X_test_selected.shape
581/47:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(np.array(X_train_selected.revenue).reshape(-1, 1))
y_train_scale = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
581/48: X_train_selected.revenue = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
581/49:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
581/50:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()

def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
581/51: X_train_selected
581/52:
# and now we run a loop over the remaining categorical variables
for variable in cat_features:
    mappings = find_category_mappings(X_train_selected, variable, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, variable, mappings)
581/53:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
581/54:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
581/55:
X_test_selected.fillna(-0.01, inplace=True)
df_val_encod.fillna(-0.01, inplace=True)
581/56:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
581/57:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train)
581/58: y_predict = regressor.predict(X_test_selected).reshape(-1,1)
581/59:
print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
581/60: ###
581/61:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=20)

sfs = sfs.fit(np.array(X_train_selected), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
581/62: df_k.sort_values('avg_score', ascending=False)[:5]
581/63:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print('Selected columns:')
print(columns_selected)
581/64:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train)
581/65:
y_predict = regressor.predict(X_test_selected[columns_selected]).reshape(-1,1)

print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
581/66:
y_val_predict = regressor.predict(df_val_encod[columns_selected])
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict.reshape(-1,1)), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)

df_no_corr.describe()
581/67:
df_no_corr[df_no_corr.Prediction < 0]['Prediction'] = 0

df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_mean.csv", index=False)
####
##Score: 2265715.45540
##Private score: 2271094.18920
582/1:
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
#from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
582/2:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
582/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')
df_out = df_val[['Id']]

print(df_train.shape)
print(df_val.shape)
582/4:
#data frame for summarizing
metrics = pd.DataFrame(columns=['model', 'encoder', 'features', 'test rmse', 'test r2', 'private score', 'public score'])
582/5: df_train.head(5)
582/6:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("linear.pdf")
582/7:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(7,7))
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
pdf.close()
582/8:
## A function for days counting
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
582/9:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
582/10:
df_train.drop(['Id'], axis=1, inplace=True)
df_val.drop(['Id'], axis=1, inplace=True)
582/11:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
582/12:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
582/13:
### choose only P features
X_train.iloc[:, 3:-2].head(5)
582/14:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train.iloc[:, 3:-2], X_train.revenue)
582/15:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
print(f'RMSE: {mean_squared_error(X_test.revenue, y_pred, squared=False):.2f}')
print(f'R2: {r2_score(X_test.revenue, y_pred):.4f}')
582/16:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=False, annot_kws={"size":8})
#sns.set(font_scale = .7)
582/17:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
582/18:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
582/19:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

print('New shapes:')
X_train_selected.shape, X_test_selected.shape
582/20:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
582/21:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", y_pred), True)
582/22:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", y_pred, True)
582/23: df_out
582/24: df_val
582/25: df_val.iloc[:, 3:-2]
582/26:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", regressor.predict(df_val.iloc[:, 3:-2]), True)
582/27:  regressor.predict(df_val.iloc[:, 3:-2])
582/28: df_val
582/29: df_val.iloc[:, 3:-1]
582/30:  regressor.predict(df_val.iloc[:, 3:-1])
582/31: y_val = regressor.predict(df_val.iloc[:, 3:-1])
582/32:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", y_val, True)
582/33: y_val = regressor.predict(df_val.iloc[:, 3:-1])
582/34:
df_val = df_out.copy()
df_val.insert(1, "Prediction", y_val, True)
585/1:
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
#from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
585/2:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
585/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')
df_out = df_val[['Id']]

print(df_train.shape)
print(df_val.shape)
585/4:
#data frame for summarizing
metrics = pd.DataFrame(columns=['model', 'encoder', 'features', 'test rmse', 'test r2', 'private score', 'public score'])
585/5: df_train.head(5)
585/6:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("linear.pdf")
585/7:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(7,7))
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
pdf.close()
585/8:
## A function for days counting
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
585/9:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
585/10:
df_train.drop(['Id'], axis=1, inplace=True)
df_val.drop(['Id'], axis=1, inplace=True)
585/11:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
585/12:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
585/13:
### choose only P features
X_train.iloc[:, 3:-2].head(5)
585/14:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train.iloc[:, 3:-2], X_train.revenue)
585/15:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
print(f'RMSE: {mean_squared_error(X_test.revenue, y_pred, squared=False):.2f}')
print(f'R2: {r2_score(X_test.revenue, y_pred):.4f}')
585/16: y_val = regressor.predict(df_val.iloc[:, 3:-1])
585/17:
df_val_out = df_out.copy()
df_val_out.insert(1, "Prediction", y_val, True)
585/18: df_val
585/19: df_val_out
585/20: df_val_out.describe()
585/21:
df_val_out['Prediction'] = np.where(df_val_out['Prediction']<0, 0, df_val_out['Prediction'])
df_val_out.to_csv("lin_regression_P.csv", index=False)
585/22: df_val_out.describe()
585/23: metrics = pd.DataFrame(columns=['model', 'encoder', 'features', 'test rmse', 'test r2', 'private score', 'public score'])
585/24:
metrics = metrics.append({
                              'model': LinearRegression,
                              'encoder': no preproc, 
                              'features': 'P1 - P37',
                              'test rmse': mean_squared_error(X_test.revenue, y_pred, squared=False),
                              'test r2': r2_score(X_test.revenue, y_pred), 
                              'private score': 2890660.29258,
                              'public score': 2938337.32737,
                             },
                               ignore_index=True
                            )
585/25:
metrics = metrics.append({
                              'model': LinearRegression,
                              'encoder': 'no preproc', 
                              'features': 'P1 - P37',
                              'test rmse': mean_squared_error(X_test.revenue, y_pred, squared=False),
                              'test r2': r2_score(X_test.revenue, y_pred), 
                              'private score': 2890660.29258,
                              'public score': 2938337.32737,
                             },
                               ignore_index=True
                            )
585/26: metrics
585/27:
metrics = metrics.append({
                              'model': 'LinearRegression',
                              'encoder': 'no preproc', 
                              'features': 'P1 - P37',
                              'test rmse': mean_squared_error(X_test.revenue, y_pred, squared=False),
                              'test r2': r2_score(X_test.revenue, y_pred), 
                              'private score': 2890660.29258,
                              'public score': 2938337.32737,
                             },
                               ignore_index=True
                            )
585/28: metrics
585/29:
df = pd.DataFrame.from_records(['LinearRegression',
                                'no preproc',
                                'P1 - P37',
                                mean_squared_error(X_test.revenue, y_pred, squared=False),
                                r2_score(X_test.revenue, y_pred),
                                2890660.29258,
                                2938337.32737
                               ])
586/1:
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
#from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
586/2:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
586/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')
df_out = df_val[['Id']]

print(df_train.shape)
print(df_val.shape)
586/4:
#data frame for summarizing
metrics = pd.DataFrame(columns=['model', 'encoder', 'features', 'test rmse', 'test r2', 'private score', 'public score'])
586/5:
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
#from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
586/6:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
586/7:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')
df_out = df_val[['Id']]

print(df_train.shape)
print(df_val.shape)
586/8:
#data frame for summarizing
metrics = pd.DataFrame(columns=['model', 'encoder', 'features', 'test rmse', 'test r2', 'private score', 'public score'])
586/9: df_train.head(5)
586/10:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("linear.pdf")
586/11:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(7,7))
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
pdf.close()
586/12:
## A function for days counting
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
586/13:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
586/14:
df_train.drop(['Id'], axis=1, inplace=True)
df_val.drop(['Id'], axis=1, inplace=True)
586/15:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
586/16:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
586/17:
### choose only P features
X_train.iloc[:, 3:-2].head(5)
586/18:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train.iloc[:, 3:-2], X_train.revenue)
586/19:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
print(f'RMSE: {mean_squared_error(X_test.revenue, y_pred, squared=False):.2f}')
print(f'R2: {r2_score(X_test.revenue, y_pred):.4f}')
586/20: y_val = regressor.predict(df_val.iloc[:, 3:-1])
586/21:
df_val_out = df_out.copy()
df_val_out.insert(1, "Prediction", y_val, True)
586/22:
df_val_out['Prediction'] = np.where(df_val_out['Prediction']<0, 0, df_val_out['Prediction'])
df_val_out.to_csv("lin_regression_P.csv", index=False)
## Score: 2938337.32737
## Private score: 2890660.29258
586/23: metrics = pd.DataFrame(columns=['model', 'encoder', 'features', 'test rmse', 'test r2', 'private score', 'public score'])
586/24:
metrics = metrics.append({
                              'model': 'LinearRegression',
                              'encoder': 'no preproc', 
                              'features': 'P1 - P37',
                              'test rmse': mean_squared_error(X_test.revenue, y_pred, squared=False),
                              'test r2': r2_score(X_test.revenue, y_pred), 
                              'private score': 2890660.29258,
                              'public score': 2938337.32737,
                             },
                               ignore_index=True
                            )
586/25: metrics
586/26:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=False, annot_kws={"size":8})
#sns.set(font_scale = .7)
586/27:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
586/28:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
586/29:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

print('New shapes:')
X_train_selected.shape, X_test_selected.shape
586/30: df_val_encod = df_val.copy()
586/31:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
586/32:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
586/33:
for feature in cat_features:
    mappings = find_category_mappings(X_train_selected, feature, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, feature, mappings)
586/34:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
586/35:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
586/36:
X_test_selected.fillna(-1, inplace=True)
df_val_encod.fillna(-1, inplace=True)
586/37:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
586/38:
plt.figure(figsize=(6,5))
sns.histplot(data=y_train)
586/39:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
586/40:
plt.figure(figsize=(6,5))
sns.histplot(data=y_train_scale)
586/41:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train_scale)
586/42:
y_predict = regressor.predict(X_test_selected)
y_val_predict = regressor.predict(df_val_encod[X_test_selected.columns])
586/43:
print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
586/44:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
586/45: df_no_corr.describe()
586/46:
df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_label_encode.csv", index=False)
### 
###Score: 1985413.56674
###Private score: 2083671.58081
586/47:
metrics = metrics.append({
                              'model': 'LinearRegression',
                              'encoder': 'ordered integer encoding', 
                              'features': cat_features,
                              'test rmse': mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False),
                              'test r2': rr2_score(y_test, target_scaler.inverse_transform(y_predict), 
                              'private score': 2083671.58081,
                              'public score': 1985413.56674
                             },
                               ignore_index=True
                            )
586/48:
metrics = metrics.append({
                              'model': 'LinearRegression',
                              'encoder': 'ordered integer encoding', 
                              'features': cat_features,
                              'test rmse': mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False),
                              'test r2': r2_score(y_test, target_scaler.inverse_transform(y_predict), 
                              'private score': 2083671.58081,
                              'public score': 1985413.56674,
                             },
                               ignore_index=True
                            )
586/49:
metrics = metrics.append({
                              'model': 'LinearRegression',
                              'encoder': 'ordered integer encoding', 
                              'features': cat_features,
                              'test rmse': mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False),
                              'test r2': r2_score(y_test, target_scaler.inverse_transform(y_predict), 
                              'private score': 2083671.58081,
                              'public score': 1985413.56674,
                             },
                               ignore_index=True
                            )
586/50: metrics
586/51:
metrics = metrics.append({
                              'model': 'LinearRegression',
                              'encoder': 'ordered integer encoding', 
                              'features': cat_features,
                              'test rmse': mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False),
                              'test r2': r2_score(y_test, target_scaler.inverse_transform(y_predict), 
                              'private score': 2083671.58081,
                              'public score': 1985413.56674,
                             },
                               ignore_index=True
                            )
586/52:
metrics = metrics.append({
                            'model': 'LinearRegression',
                            'encoder': 'ordered integer encoding', 
                            'features': cat_features,
                            'test rmse': mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False),
                            'test r2': r2_score(y_test, target_scaler.inverse_transform(y_predict)),
                            'private score': 2083671.58081,
                            'public score': 1985413.56674,
                         },
                        ignore_index=True
                        )
586/53:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
586/54:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=20)

sfs = sfs.fit(np.array(X_train_selected), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
586/55: df_k.sort_values('avg_score', ascending=False)[:5]
586/56:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print('Selected columns:')
print(columns_selected)
586/57:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train_scale)
586/58:
y_predict = regressor.predict(X_test_selected[columns_selected])
y_val_predict = regressor.predict(df_val_encod[columns_selected])

print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
586/59:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
586/60: df_no_corr.describe()
586/61: df_no_corr[df_no_corr.Prediction < 0]['Prediction'] = 0
586/62:
df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_label_encode_selected.csv", index=False)
#####
### Score: 2017086.38946
### Private score: 2169576.52804
586/63:
df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_label_encode_selected.csv", index=False)
#####
### Score: 2131788.74924
### Private score: 2183281.28971
586/64:
metrics = metrics.append({
                            'model': 'LinearRegression',
                            'encoder': 'ordered integer encoding + fs', 
                            'features': columns_selected,
                            'test rmse': mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False),
                            'test r2': r2_score(y_test, target_scaler.inverse_transform(y_predict)),
                            'private score': 2183281.28971,
                            'public score': 2131788.74924,
                         },
                        ignore_index=True
                        )
586/65:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)
df_val_encod = df_val.copy()

X_train_selected.shape, X_test_selected.shape
586/66:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(np.array(X_train_selected.revenue).reshape(-1, 1))
y_train_scale = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
586/67: X_train_selected.revenue = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
586/68:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
586/69:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()

def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
586/70:
# and now we run a loop over the remaining categorical variables
for variable in cat_features:
    mappings = find_category_mappings(X_train_selected, variable, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, variable, mappings)
586/71:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
586/72:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
586/73:
X_test_selected.fillna(-0.01, inplace=True)
df_val_encod.fillna(-0.01, inplace=True)
586/74:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
586/75:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train)
586/76: y_predict = regressor.predict(X_test_selected).reshape(-1,1)
586/77:
print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
586/78: X_test_selected
586/79: df_val_encod
586/80:
y_predict = regressor.predict(X_test_selected).reshape(-1,1)
y_val_predict = regressor.predict(df_val_encod).reshape(-1,1)
586/81: X_test_selected
586/82: df_val_encod
586/83: df_val_encod.columns
586/84: X_test_selected.columns
586/85:
y_predict = regressor.predict(X_test_selected).reshape(-1,1)
y_val_predict = regressor.predict(df_val_encod[cat_features]).reshape(-1,1)
586/86: X_test_selected.columns
586/87: cat_features
586/88:
y_predict = regressor.predict(X_test_selected).reshape(-1,1)
y_val_predict = regressor.predict(df_val_encod[X_test_selected.columns]).reshape(-1,1)
587/1:
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
#from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
587/2:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
587/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')
df_out = df_val[['Id']]

print(df_train.shape)
print(df_val.shape)
587/4:
#data frame for summarizing
metrics = pd.DataFrame(columns=['model', 'encoder', 'features', 'test rmse', 'test r2', 'private score', 'public score'])
587/5: df_train.head(5)
587/6:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("linear.pdf")
587/7:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(7,7))
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
pdf.close()
587/8:
## A function for days counting
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
587/9:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
587/10:
df_train.drop(['Id'], axis=1, inplace=True)
df_val.drop(['Id'], axis=1, inplace=True)
587/11:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
587/12:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
587/13:
### choose only P features
X_train.iloc[:, 3:-2].head(5)
587/14:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train.iloc[:, 3:-2], X_train.revenue)
587/15:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
print(f'RMSE: {mean_squared_error(X_test.revenue, y_pred, squared=False):.2f}')
print(f'R2: {r2_score(X_test.revenue, y_pred):.4f}')
587/16: y_val = regressor.predict(df_val.iloc[:, 3:-1])
587/17:
df_val_out = df_out.copy()
df_val_out.insert(1, "Prediction", y_val, True)
587/18:
df_val_out['Prediction'] = np.where(df_val_out['Prediction']<0, 0, df_val_out['Prediction'])
df_val_out.to_csv("lin_regression_P.csv", index=False)
## Score: 2938337.32737
## Private score: 2890660.29258
587/19: metrics = pd.DataFrame(columns=['model', 'encoder', 'features', 'test rmse', 'test r2', 'private score', 'public score'])
587/20:
metrics = metrics.append({
                            'model': 'LinearRegression',
                            'encoder': 'no preproc', 
                            'features': 'P1 - P37',
                            'test rmse': mean_squared_error(X_test.revenue, y_pred, squared=False),
                            'test r2': r2_score(X_test.revenue, y_pred), 
                            'private score': 2890660.29258,
                            'public score': 2938337.32737,
                         },
                               ignore_index=True
                        )
587/21: metrics
587/22:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=False, annot_kws={"size":8})
#sns.set(font_scale = .7)
587/23:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
587/24:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
587/25:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

print('New shapes:')
X_train_selected.shape, X_test_selected.shape
587/26: df_val_encod = df_val.copy()
587/27:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
587/28:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
587/29:
for feature in cat_features:
    mappings = find_category_mappings(X_train_selected, feature, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, feature, mappings)
587/30:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
587/31:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
587/32:
X_test_selected.fillna(-1, inplace=True)
df_val_encod.fillna(-1, inplace=True)
587/33:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
587/34:
plt.figure(figsize=(6,5))
sns.histplot(data=y_train)
587/35:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
587/36:
plt.figure(figsize=(6,5))
sns.histplot(data=y_train_scale)
587/37:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train_scale)
587/38:
y_predict = regressor.predict(X_test_selected)
y_val_predict = regressor.predict(df_val_encod[X_test_selected.columns])
587/39:
print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
587/40:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
587/41: df_no_corr.describe()
587/42:
df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_label_encode.csv", index=False)
### 
###Score: 1985413.56674
###Private score: 2083671.58081
587/43: X_test_selected.columns
587/44:
metrics = metrics.append({
                            'model': 'LinearRegression',
                            'encoder': 'ordered integer encoding', 
                            'features': X_test_selected.columns,
                            'test rmse': mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False),
                            'test r2': r2_score(y_test, target_scaler.inverse_transform(y_predict)),
                            'private score': 2083671.58081,
                            'public score': 1985413.56674,
                         },
                        ignore_index=True
                        )
587/45:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
587/46:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=20)

sfs = sfs.fit(np.array(X_train_selected), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
587/47: df_k.sort_values('avg_score', ascending=False)[:5]
587/48:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print('Selected columns:')
print(columns_selected)
587/49:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train_scale)
587/50:
y_predict = regressor.predict(X_test_selected[columns_selected])
y_val_predict = regressor.predict(df_val_encod[columns_selected])

print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
587/51:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
587/52: df_no_corr.describe()
587/53: df_no_corr[df_no_corr.Prediction < 0]['Prediction'] = 0
587/54:
df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_label_encode_selected.csv", index=False)
#####
### Score: 2131788.74924
### Private score: 2183281.28971
587/55:
metrics = metrics.append({
                            'model': 'LinearRegression',
                            'encoder': 'ordered integer encoding + fs', 
                            'features': X_test_selected.columns,
                            'test rmse': mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False),
                            'test r2': r2_score(y_test, target_scaler.inverse_transform(y_predict)),
                            'private score': 2183281.28971,
                            'public score': 2131788.74924,
                         },
                        ignore_index=True
                        )
587/56: metrics
587/57: metrics.encoder
587/58: metrics.encoder[2]
587/59: metrics.encoder[3]
587/60: metrics.encoder[1]
587/61: metrics.columns
587/62: metrics.features
587/63: metrics.features[2]
587/64:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)
df_val_encod = df_val.copy()

X_train_selected.shape, X_test_selected.shape
587/65:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(np.array(X_train_selected.revenue).reshape(-1, 1))
y_train_scale = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
587/66: X_train_selected.revenue = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
587/67:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
587/68:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()

def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
587/69:
# and now we run a loop over the remaining categorical variables
for variable in cat_features:
    mappings = find_category_mappings(X_train_selected, variable, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, variable, mappings)
587/70:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
587/71:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
587/72:
X_test_selected.fillna(-0.01, inplace=True)
df_val_encod.fillna(-0.01, inplace=True)
587/73:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
587/74:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train)
587/75:
y_predict = regressor.predict(X_test_selected).reshape(-1,1)
y_val_predict = regressor.predict(df_val_encod[X_test_selected.columns]).reshape(-1,1)
587/76:
print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
587/77:
df_mean_encode_out = df_out.copy()
df_mean_encode_out.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
587/78: df_no_corr.describe()
587/79:
df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_label_encode_selected.csv", index=False)
#####
### Score: 2131788.74924
### Private score: 2183281.28971
587/80: X_test_selected.columns
587/81:
metrics = metrics.append({
                            'model': 'LinearRegression',
                            'encoder': 'mean encoding', 
                            'features': X_test_selected.columns,
                            'test rmse': mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False),
                            'test r2': r2_score(y_test, target_scaler.inverse_transform(y_predict)),
                            'private score': 2183281.28971,
                            'public score': 2131788.74924,
                         },
                        ignore_index=True
                        )
587/82: ###
587/83:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=20)

sfs = sfs.fit(np.array(X_train_selected), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
587/84: df_k.sort_values('avg_score', ascending=False)[:5]
587/85:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print('Selected columns:')
print(columns_selected)
587/86:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train)
587/87:
y_predict = regressor.predict(X_test_selected[columns_selected]).reshape(-1,1)

print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
587/88:
y_val_predict = regressor.predict(df_val_encod[columns_selected])
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict.reshape(-1,1)), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)

df_no_corr.describe()
587/89:
#df_no_corr[df_no_corr.Prediction < 0]['Prediction'] = 0

df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_mean.csv", index=False)
####
##Score: 2265715.45540
##Private score: 2271094.18920
587/90:
metrics = metrics.append({
                            'model': 'LinearRegression',
                            'encoder': 'mean encoding + fs', 
                            'features': columns_selected,
                            'test rmse': mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False),
                            'test r2': r2_score(y_test, target_scaler.inverse_transform(y_predict)),
                            'private score': 2183281.28971,
                            'public score': 2131788.74924,
                         },
                        ignore_index=True
                        )
587/91: metrics
587/92: X_train_selected
587/93: X_train_selected.iloc[:, 3:]
588/1:
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
#from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
588/2:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
588/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')
df_out = df_val[['Id']]

print(df_train.shape)
print(df_val.shape)
588/4:
#data frame for summarizing
metrics = pd.DataFrame(columns=['model', 'encoder', 'features', 'test rmse', 'test r2', 'private score', 'public score'])
588/5: df_train.head(5)
588/6:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("linear.pdf")
588/7:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(7,7))
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
pdf.close()
588/8:
## A function for days counting
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
588/9:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
588/10:
df_train.drop(['Id'], axis=1, inplace=True)
df_val.drop(['Id'], axis=1, inplace=True)
588/11:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
588/12:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
588/13:
### choose only P features
X_train.iloc[:, 3:-2].head(5)
588/14:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train.iloc[:, 3:-2], X_train.revenue)
588/15:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
print(f'RMSE: {mean_squared_error(X_test.revenue, y_pred, squared=False):.2f}')
print(f'R2: {r2_score(X_test.revenue, y_pred):.4f}')
588/16: y_val = regressor.predict(df_val.iloc[:, 3:-1])
588/17:
df_val_out = df_out.copy()
df_val_out.insert(1, "Prediction", y_val, True)
588/18:
df_val_out['Prediction'] = np.where(df_val_out['Prediction']<0, 0, df_val_out['Prediction'])
df_val_out.to_csv("lin_regression_P.csv", index=False)
## Score: 2938337.32737
## Private score: 2890660.29258
588/19: metrics = pd.DataFrame(columns=['model', 'encoder', 'features', 'test rmse', 'test r2', 'private score', 'public score'])
588/20:
metrics = metrics.append({
                            'model': 'LinearRegression',
                            'encoder': 'no preproc', 
                            'features': 'P1 - P37',
                            'test rmse': mean_squared_error(X_test.revenue, y_pred, squared=False),
                            'test r2': r2_score(X_test.revenue, y_pred), 
                            'private score': 2890660.29258,
                            'public score': 2938337.32737,
                         },
                               ignore_index=True
                        )
588/21: metrics
588/22:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=False, annot_kws={"size":8})
#sns.set(font_scale = .7)
588/23:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
588/24:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
588/25:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

print('New shapes:')
X_train_selected.shape, X_test_selected.shape
588/26: X_train_selected.iloc[:, 3:]
588/27:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train.iloc[:, 3:-2], X_train.revenue)
588/28:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
588/29: X_train_selected.iloc[:, 3:]
588/30:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected.iloc[:, 3:], y_train)
588/31:
y_pred = regressor.predict(X_test.iloc[:, 3:])
print(f'RMSE: {mean_squared_error(y_test, y_pred, squared=False):.2f}')
print(f'R2: {r2_score(y_test, y_pred):.4f}')
588/32:
y_pred = regressor.predict(X_test_selected.iloc[:, 3:])
print(f'RMSE: {mean_squared_error(y_test, y_pred, squared=False):.2f}')
print(f'R2: {r2_score(y_test, y_pred):.4f}')
588/33: df_val
588/34: df_val[X_test_selected.columns]
588/35: selected_columns = X_train_selected.columns[3:]
588/36: selected_columns
588/37:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[selected_columns], y_train)
588/38:
y_pred = regressor.predict(X_test_selected[selected_columns])
print(f'RMSE: {mean_squared_error(y_test, y_pred, squared=False):.2f}')
print(f'R2: {r2_score(y_test, y_pred):.4f}')
588/39: y_val = regressor.predict(df_val[selected_columns])
588/40:
df_val_out = df_out.copy()
df_val_out.insert(1, "Prediction", y_val, True)
588/41:
df_val_out['Prediction'] = np.where(df_val_out['Prediction']<0, 0, df_val_out['Prediction'])
df_val_out.to_csv("lin_reg_no_corr.csv", index=False)
## Score: 2938337.32737
## Private score: 2890660.29258
588/42: df_val_out.describe()
588/43:
metrics = metrics.append({
                            'model': 'LinearRegression',
                            'encoder': 'no corr features', 
                            'features': selected_columns,
                            'test rmse': mean_squared_error(X_test.revenue, y_pred, squared=False),
                            'test r2': r2_score(X_test.revenue, y_pred), 
                            'private score': 2460886.50276,
                            'public score': 2206471.72576,
                         },
                               ignore_index=True
                        )
588/44: metrics
588/45: selected_columns
588/46: selected_columns.to_list()
588/47: df_val_encod = df_val.copy()
588/48:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
588/49:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
588/50:
for feature in cat_features:
    mappings = find_category_mappings(X_train_selected, feature, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, feature, mappings)
588/51:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)
df_val_encod = df_val.copy()

X_train_selected.shape, X_test_selected.shape
588/52:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
588/53:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
588/54:
for feature in cat_features:
    mappings = find_category_mappings(X_train_selected, feature, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, feature, mappings)
588/55:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
588/56:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
588/57:
X_test_selected.fillna(-1, inplace=True)
df_val_encod.fillna(-1, inplace=True)
588/58:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
588/59:
plt.figure(figsize=(6,5))
sns.histplot(data=y_train)
588/60:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
588/61:
plt.figure(figsize=(6,5))
sns.histplot(data=y_train_scale)
588/62:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train_scale)
588/63:
y_predict = regressor.predict(X_test_selected)
y_val_predict = regressor.predict(df_val_encod[X_test_selected.columns])
588/64:
print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
588/65:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
588/66: df_no_corr.describe()
588/67:
df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_label_encode.csv", index=False)
### 
###Score: 1985413.56674
###Private score: 2083671.58081
588/68:
metrics = metrics.append({
                            'model': 'LinearRegression',
                            'encoder': 'ordered integer encoding', 
                            'features': X_test_selected.columns,
                            'test rmse': mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False),
                            'test r2': r2_score(y_test, target_scaler.inverse_transform(y_predict)),
                            'private score': 2083671.58081,
                            'public score': 1985413.56674,
                         },
                        ignore_index=True
                        )
588/69: metrics
588/70: X_test_selected.columns
588/71: X_test_selected
588/72:
metrics = metrics.append({
                            'model': 'LinearRegression',
                            'encoder': 'ordered integer encoding', 
                            'features': selected_columns,
                            'test rmse': mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False),
                            'test r2': r2_score(y_test, target_scaler.inverse_transform(y_predict)),
                            'private score': 2083671.58081,
                            'public score': 1985413.56674,
                         },
                        ignore_index=True
                        )
588/73: selected_columns
588/74: selected_columns = X_train_selected.columns.to_list()
588/75: X_test_selected
588/76: selected_columns
588/77: len(selected_columns)
588/78: X_test_selected.shape
588/79: metrics
588/80:
metrics = metrics.append({
                            'model': 'LinearRegression',
                            'encoder': 'ordered integer encoding', 
                            'features': selected_columns,
                            'test rmse': mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False),
                            'test r2': r2_score(y_test, target_scaler.inverse_transform(y_predict)),
                            'private score': 2083671.58081,
                            'public score': 1985413.56674,
                         },
                        ignore_index=True
                        )
588/81: metrics
588/82:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
588/83:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=20)

sfs = sfs.fit(np.array(X_train_selected), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
588/84: df_k.sort_values('avg_score', ascending=False)[:5]
588/85:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print('Selected columns:')
print(columns_selected)
588/86:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train_scale)
588/87:
y_predict = regressor.predict(X_test_selected[columns_selected])
y_val_predict = regressor.predict(df_val_encod[columns_selected])

print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
588/88:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
588/89: df_no_corr.describe()
588/90: df_no_corr[df_no_corr.Prediction < 0]['Prediction'] = 0
588/91:
df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_label_encode_selected.csv", index=False)
#####
### Score: 2131788.74924
### Private score: 2183281.28971
588/92: columns_selected
588/93:
metrics = metrics.append({
                            'model': 'LinearRegression',
                            'encoder': 'ordered integer encoding + fs', 
                            'features': columns_selected,
                            'test rmse': mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False),
                            'test r2': r2_score(y_test, target_scaler.inverse_transform(y_predict)),
                            'private score': 2183281.28971,
                            'public score': 2131788.74924,
                         },
                        ignore_index=True
                        )
588/94: metrics
588/95:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)
df_val_encod = df_val.copy()

X_train_selected.shape, X_test_selected.shape
588/96:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(np.array(X_train_selected.revenue).reshape(-1, 1))
y_train_scale = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
588/97: X_train_selected.revenue = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
588/98:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
588/99:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()

def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
588/100:
# and now we run a loop over the remaining categorical variables
for variable in cat_features:
    mappings = find_category_mappings(X_train_selected, variable, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, variable, mappings)
588/101:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
588/102:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
588/103:
X_test_selected.fillna(-0.01, inplace=True)
df_val_encod.fillna(-0.01, inplace=True)
588/104:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
588/105:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train)
588/106:
y_predict = regressor.predict(X_test_selected).reshape(-1,1)
y_val_predict = regressor.predict(df_val_encod[X_test_selected.columns]).reshape(-1,1)
588/107:
print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
588/108:
df_mean_encode_out = df_out.copy()
df_mean_encode_out.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
588/109: df_no_corr.describe()
588/110:
df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_label_encode_selected.csv", index=False)
#####
### Score: 2131788.74924
### Private score: 2183281.28971
588/111:
metrics = metrics.append({
                            'model': 'LinearRegression',
                            'encoder': 'mean encoding', 
                            'features': X_test_selected.columns.to_list(,
                            'test rmse': mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False),
                            'test r2': r2_score(y_test, target_scaler.inverse_transform(y_predict)),
                            'private score': 2183281.28971,
                            'public score': 2131788.74924,
                         },
                        ignore_index=True
                        )
588/112:
metrics = metrics.append({
                            'model': 'LinearRegression',
                            'encoder': 'mean encoding', 
                            'features': X_test_selected.columns.to_list(),
                            'test rmse': mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False),
                            'test r2': r2_score(y_test, target_scaler.inverse_transform(y_predict)),
                            'private score': 2183281.28971,
                            'public score': 2131788.74924,
                         },
                        ignore_index=True
                        )
588/113: metrics
588/114: ###
588/115:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=20)

sfs = sfs.fit(np.array(X_train_selected), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
588/116: df_k.sort_values('avg_score', ascending=False)[:5]
588/117:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print('Selected columns:')
print(columns_selected)
588/118:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train)
588/119:
y_predict = regressor.predict(X_test_selected[columns_selected]).reshape(-1,1)

print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
588/120:
y_val_predict = regressor.predict(df_val_encod[columns_selected])
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict.reshape(-1,1)), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)

df_no_corr.describe()
588/121:
#df_no_corr[df_no_corr.Prediction < 0]['Prediction'] = 0

df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_mean.csv", index=False)
####
##Score: 2265715.45540
##Private score: 2271094.18920
588/122:
metrics = metrics.append({
                            'model': 'LinearRegression',
                            'encoder': 'mean encoding + fs', 
                            'features': columns_selected,
                            'test rmse': mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False),
                            'test r2': r2_score(y_test, target_scaler.inverse_transform(y_predict)),
                            'private score': 2183281.28971,
                            'public score': 2131788.74924,
                         },
                        ignore_index=True
                        )
588/123: metrics
589/1:
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
#from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
589/2:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
589/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')
df_out = df_val[['Id']]

print(df_train.shape)
print(df_val.shape)
589/4:
#data frame for summarizing
metrics = pd.DataFrame(columns=['model', 'encoder', 'features', 'test rmse', 'test r2', 'private score', 'public score'])
589/5: df_train.head(5)
589/6:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("linear.pdf")
589/7:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(7,7))
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
pdf.close()
589/8:
## A function for days counting
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
589/9:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
589/10:
df_train.drop(['Id'], axis=1, inplace=True)
df_val.drop(['Id'], axis=1, inplace=True)
589/11:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
589/12:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
589/13:
### choose only P features
X_train.iloc[:, 3:-2].head(5)
589/14:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train.iloc[:, 3:-2], X_train.revenue)
589/15:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
print(f'RMSE: {mean_squared_error(X_test.revenue, y_pred, squared=False):.2f}')
print(f'R2: {r2_score(X_test.revenue, y_pred):.4f}')
589/16: y_val = regressor.predict(df_val.iloc[:, 3:-1])
589/17:
df_val_out = df_out.copy()
df_val_out.insert(1, "Prediction", y_val, True)
589/18:
df_val_out['Prediction'] = np.where(df_val_out['Prediction']<0, 0, df_val_out['Prediction'])
df_val_out.to_csv("lin_regression_P.csv", index=False)
## Score: 2938337.32737
## Private score: 2890660.29258
589/19: metrics = pd.DataFrame(columns=['model', 'encoder', 'features', 'test rmse', 'test r2', 'private score', 'public score'])
589/20:
metrics = metrics.append({
                            'model': 'LinearRegression',
                            'encoder': 'no preproc', 
                            'features': 'P1 - P37',
                            'test rmse': mean_squared_error(X_test.revenue, y_pred, squared=False),
                            'test r2': r2_score(X_test.revenue, y_pred), 
                            'private score': 2890660.29258,
                            'public score': 2938337.32737,
                         },
                               ignore_index=True
                        )
589/21: metrics
589/22:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=False, annot_kws={"size":8})
#sns.set(font_scale = .7)
589/23:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
589/24:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
589/25:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

print('New shapes:')
X_train_selected.shape, X_test_selected.shape
589/26:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
589/27: X_train_selected.columns[3:]
589/28: columns_selected = X_train_selected.columns[3:].to_list()
589/29:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[selected_columns], y_train)
589/30:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train)
589/31:
y_pred = regressor.predict(X_test_selected[columns_selected])
print(f'RMSE: {mean_squared_error(y_test, y_pred, squared=False):.2f}')
print(f'R2: {r2_score(y_test, y_pred):.4f}')
589/32: y_val = regressor.predict(df_val[columns_selected])
589/33:
df_val_out = df_out.copy()
df_val_out.insert(1, "Prediction", y_val, True)
589/34:
df_val_out['Prediction'] = np.where(df_val_out['Prediction']<0, 0, df_val_out['Prediction'])
df_val_out.to_csv("lin_reg_no_corr.csv", index=False)
### Score: 2206471.72576
### Private score: 2460886.50276
589/35:
metrics = metrics.append({
                            'model': 'LinearRegression',
                            'encoder': 'no corr features', 
                            'features': columns_selected,
                            'test rmse': mean_squared_error(X_test.revenue, y_pred, squared=False),
                            'test r2': r2_score(X_test.revenue, y_pred), 
                            'private score': 2460886.50276,
                            'public score': 2206471.72576,
                         },
                               ignore_index=True
                        )
589/36:
X_test_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)
df_val_encod = df_val.copy()

X_train_selected.shape, X_test_selected.shape
589/37: columns_selected = X_train_selected.columns.to_list()
589/38:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
589/39:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
589/40:
for feature in cat_features:
    mappings = find_category_mappings(X_train_selected, feature, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, feature, mappings)
589/41: X_train
589/42: X_test_selected
589/43:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)
df_val_encod = df_val.copy()

X_train_selected.shape, X_test_selected.shape
589/44: columns_selected = X_train_selected.columns.to_list()
589/45:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
589/46:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
589/47:
for feature in cat_features:
    mappings = find_category_mappings(X_train_selected, feature, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, feature, mappings)
589/48:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
589/49:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
589/50:
X_test_selected.fillna(-1, inplace=True)
df_val_encod.fillna(-1, inplace=True)
589/51:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
589/52:
plt.figure(figsize=(6,5))
sns.histplot(data=y_train)
589/53:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
589/54:
plt.figure(figsize=(6,5))
sns.histplot(data=y_train_scale)
589/55:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train_scale)
589/56:
y_predict = regressor.predict(X_test_selected)
y_val_predict = regressor.predict(df_val_encod[X_test_selected.columns])
589/57:
print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
589/58:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
589/59: df_no_corr.describe()
589/60:
df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_label_encode.csv", index=False)
### 
###Score: 1985413.56674
###Private score: 2083671.58081
589/61:
metrics = metrics.append({
                            'model': 'LinearRegression',
                            'encoder': 'ordered integer encoding', 
                            'features': columns_selected,
                            'test rmse': mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False),
                            'test r2': r2_score(y_test, target_scaler.inverse_transform(y_predict)),
                            'private score': 2083671.58081,
                            'public score': 1985413.56674,
                         },
                        ignore_index=True
                        )
589/62: columns_selected
589/63:
metrics = metrics.append({
                            'model': 'LinearRegression',
                            'encoder': 'ordered integer encoding', 
                            'features': X_train_selected.columns.to_list(),
                            'test rmse': mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False),
                            'test r2': r2_score(y_test, target_scaler.inverse_transform(y_predict)),
                            'private score': 2083671.58081,
                            'public score': 1985413.56674,
                         },
                        ignore_index=True
                        )
589/64: columns_selected
589/65:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
589/66:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=20)

sfs = sfs.fit(np.array(X_train_selected), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
589/67: df_k.sort_values('avg_score', ascending=False)[:5]
589/68:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print('Selected columns:')
print(columns_selected)
589/69:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train_scale)
589/70:
y_predict = regressor.predict(X_test_selected[columns_selected])
y_val_predict = regressor.predict(df_val_encod[columns_selected])

print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
589/71:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
589/72: df_no_corr.describe()
589/73: df_no_corr[df_no_corr.Prediction < 0]['Prediction'] = 0
589/74:
df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_label_encode_selected.csv", index=False)
#####
### Score: 2131788.74924
### Private score: 2183281.28971
589/75:
metrics = metrics.append({
                            'model': 'LinearRegression',
                            'encoder': 'ordered integer encoding + fs', 
                            'features': columns_selected,
                            'test rmse': mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False),
                            'test r2': r2_score(y_test, target_scaler.inverse_transform(y_predict)),
                            'private score': 2183281.28971,
                            'public score': 2131788.74924,
                         },
                        ignore_index=True
                        )
589/76: metrics
589/77:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)
df_val_encod = df_val.copy()

X_train_selected.shape, X_test_selected.shape
589/78:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(np.array(X_train_selected.revenue).reshape(-1, 1))
y_train_scale = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
589/79: X_train_selected.revenue = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
589/80:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
589/81:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()

def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
589/82:
# and now we run a loop over the remaining categorical variables
for variable in cat_features:
    mappings = find_category_mappings(X_train_selected, variable, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, variable, mappings)
589/83:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
589/84:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
589/85:
X_test_selected.fillna(-0.01, inplace=True)
df_val_encod.fillna(-0.01, inplace=True)
589/86:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
589/87:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train)
589/88:
y_predict = regressor.predict(X_test_selected).reshape(-1,1)
y_val_predict = regressor.predict(df_val_encod[X_test_selected.columns]).reshape(-1,1)
589/89:
print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
589/90:
df_mean_encode_out = df_out.copy()
df_mean_encode_out.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
589/91: df_no_corr.describe()
589/92:
df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_label_encode_selected.csv", index=False)
#####
### Score: 2131788.74924
### Private score: 2183281.28971
589/93:
metrics = metrics.append({
                            'model': 'LinearRegression',
                            'encoder': 'mean encoding', 
                            'features': X_test_selected.columns.to_list(),
                            'test rmse': mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False),
                            'test r2': r2_score(y_test, target_scaler.inverse_transform(y_predict)),
                            'private score': 2183281.28971,
                            'public score': 2131788.74924,
                         },
                        ignore_index=True
                        )
589/94: ###
589/95:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=20)

sfs = sfs.fit(np.array(X_train_selected), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
589/96: df_k.sort_values('avg_score', ascending=False)[:5]
589/97:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print('Selected columns:')
print(columns_selected)
589/98:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train)
589/99:
y_predict = regressor.predict(X_test_selected[columns_selected]).reshape(-1,1)

print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
589/100:
y_val_predict = regressor.predict(df_val_encod[columns_selected])
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict.reshape(-1,1)), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)

df_no_corr.describe()
589/101:
#df_no_corr[df_no_corr.Prediction < 0]['Prediction'] = 0

df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_mean.csv", index=False)
####
##Score: 2265715.45540
##Private score: 2271094.18920
589/102:
metrics = metrics.append({
                            'model': 'LinearRegression',
                            'encoder': 'mean encoding + fs', 
                            'features': columns_selected,
                            'test rmse': mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False),
                            'test r2': r2_score(y_test, target_scaler.inverse_transform(y_predict)),
                            'private score': 2183281.28971,
                            'public score': 2131788.74924,
                         },
                        ignore_index=True
                        )
589/103: metrics
590/1:
import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'

from datetime import date
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
#from collections import defaultdict

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
590/2:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
590/3:
df_train = pd.read_csv('data/train.csv')
df_val = pd.read_csv('data/test.csv')
df_out = df_val[['Id']]

print(df_train.shape)
print(df_val.shape)
590/4:
#data frame for summarizing
metrics = pd.DataFrame(columns=['model', 'encoder', 'features', 'test rmse', 'test r2', 'private score', 'public score'])
590/5: df_train.head(5)
590/6:
### save results in .pdf file 
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("linear.pdf")
590/7:
for col in df_train.columns[2:-1]:    
    fig = plt.figure(figsize=(7,7))
    fig = df_train.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
pdf.close()
590/8:
## A function for days counting
own_date = date(2015, 1, 1) ### This competition ended in 2015 
def transform_date(df, new_date):
    df[['Open Date']] = df[['Open Date']].apply(pd.to_datetime)
    df['open_days'] = (pd.Timestamp(new_date) - df['Open Date']).dt.days
    df.drop(['Open Date'], axis=1, inplace=True)
    return df
590/9:
df_train = transform_date(df_train, own_date)
df_val = transform_date(df_val, own_date)
590/10:
df_train.drop(['Id'], axis=1, inplace=True)
df_val.drop(['Id'], axis=1, inplace=True)
590/11:
percentage = 15
bounder = int(np.floor(len(df_train)*percentage/100))
590/12:
X_train = df_train.iloc[bounder:, :]
X_test = df_train.iloc[:bounder, :]

X_train.shape, X_test.shape
590/13:
### choose only P features
X_train.iloc[:, 3:-2].head(5)
590/14:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train.iloc[:, 3:-2], X_train.revenue)
590/15:
y_pred = regressor.predict(X_test.iloc[:, 3:-2])
print(f'RMSE: {mean_squared_error(X_test.revenue, y_pred, squared=False):.2f}')
print(f'R2: {r2_score(X_test.revenue, y_pred):.4f}')
590/16: y_val = regressor.predict(df_val.iloc[:, 3:-1])
590/17:
df_val_out = df_out.copy()
df_val_out.insert(1, "Prediction", y_val, True)
590/18:
df_val_out['Prediction'] = np.where(df_val_out['Prediction']<0, 0, df_val_out['Prediction'])
df_val_out.to_csv("lin_regression_P.csv", index=False)
## Score: 2938337.32737
## Private score: 2890660.29258
590/19: metrics = pd.DataFrame(columns=['model', 'encoder', 'features', 'test rmse', 'test r2', 'private score', 'public score'])
590/20:
metrics = metrics.append({
                            'model': 'LinearRegression',
                            'encoder': 'no preproc', 
                            'features': 'P1 - P37',
                            'test rmse': mean_squared_error(X_test.revenue, y_pred, squared=False),
                            'test r2': r2_score(X_test.revenue, y_pred), 
                            'private score': 2890660.29258,
                            'public score': 2938337.32737,
                         },
                               ignore_index=True
                        )
590/21:
plt.figure(figsize=(10,6))
sns.heatmap(X_train.corr(),annot=False, annot_kws={"size":8})
#sns.set(font_scale = .7)
590/22:
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns    
    corr_matrix = dataset.corr()    
    for i in range(len(corr_matrix.columns)):    
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
590/23:
corr_features = correlation(X_train, 0.9)
print('correlated features: ', len(set(corr_features)) )
590/24:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)

print('New shapes:')
X_train_selected.shape, X_test_selected.shape
590/25:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
590/26: columns_selected = X_train_selected.columns[3:].to_list()
590/27:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train)
590/28:
y_pred = regressor.predict(X_test_selected[columns_selected])
print(f'RMSE: {mean_squared_error(y_test, y_pred, squared=False):.2f}')
print(f'R2: {r2_score(y_test, y_pred):.4f}')
590/29: y_val = regressor.predict(df_val[columns_selected])
590/30:
df_val_out = df_out.copy()
df_val_out.insert(1, "Prediction", y_val, True)
590/31:
df_val_out['Prediction'] = np.where(df_val_out['Prediction']<0, 0, df_val_out['Prediction'])
df_val_out.to_csv("lin_reg_no_corr.csv", index=False)
### Score: 2206471.72576
### Private score: 2460886.50276
590/32:
metrics = metrics.append({
                            'model': 'LinearRegression',
                            'encoder': 'no corr features', 
                            'features': columns_selected,
                            'test rmse': mean_squared_error(X_test.revenue, y_pred, squared=False),
                            'test r2': r2_score(X_test.revenue, y_pred), 
                            'private score': 2460886.50276,
                            'public score': 2206471.72576,
                         },
                               ignore_index=True
                        )
590/33:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)
df_val_encod = df_val.copy()

X_train_selected.shape, X_test_selected.shape
590/34: columns_selected = X_train_selected.columns.to_list()
590/35:
def find_category_mappings(df, variable, target):
    # first  we generate an ordered list with the labels
    ordered_labels = df.groupby([variable])[target].mean().sort_values().index
    # return the dictionary with mappings
    return {k: i for i, k in enumerate(ordered_labels, 0)}


def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
590/36:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
590/37:
for feature in cat_features:
    mappings = find_category_mappings(X_train_selected, feature, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, feature, mappings)
590/38:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
590/39:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
590/40:
X_test_selected.fillna(-1, inplace=True)
df_val_encod.fillna(-1, inplace=True)
590/41:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
590/42:
plt.figure(figsize=(6,5))
sns.histplot(data=y_train)
590/43:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
590/44:
plt.figure(figsize=(6,5))
sns.histplot(data=y_train_scale)
590/45:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train_scale)
590/46:
y_predict = regressor.predict(X_test_selected)
y_val_predict = regressor.predict(df_val_encod[X_test_selected.columns])
590/47:
print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
590/48:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
590/49: df_no_corr.describe()
590/50:
df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_label_encode.csv", index=False)
### 
###Score: 1985413.56674
###Private score: 2083671.58081
590/51:
metrics = metrics.append({
                            'model': 'LinearRegression',
                            'encoder': 'ordered integer encoding', 
                            'features': X_train_selected.columns.to_list(),
                            'test rmse': mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False),
                            'test r2': r2_score(y_test, target_scaler.inverse_transform(y_predict)),
                            'private score': 2083671.58081,
                            'public score': 1985413.56674,
                         },
                        ignore_index=True
                        )
590/52:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(y_train.values.reshape(-1, 1))
y_train_scale = target_scaler.transform(y_train.values.reshape(-1, 1))
590/53:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=20)

sfs = sfs.fit(np.array(X_train_selected), y_train_scale)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
590/54: df_k.sort_values('avg_score', ascending=False)[:5]
590/55:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print('Selected columns:')
print(columns_selected)
590/56:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train_scale)
590/57:
y_predict = regressor.predict(X_test_selected[columns_selected])
y_val_predict = regressor.predict(df_val_encod[columns_selected])

print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
590/58:
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
590/59: df_no_corr.describe()
590/60: df_no_corr[df_no_corr.Prediction < 0]['Prediction'] = 0
590/61:
df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_label_encode_selected.csv", index=False)
#####
### Score: 2131788.74924
### Private score: 2183281.28971
590/62:
metrics = metrics.append({
                            'model': 'LinearRegression',
                            'encoder': 'ordered integer encoding + fs', 
                            'features': columns_selected,
                            'test rmse': mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False),
                            'test r2': r2_score(y_test, target_scaler.inverse_transform(y_predict)),
                            'private score': 2183281.28971,
                            'public score': 2131788.74924,
                         },
                        ignore_index=True
                        )
590/63: metrics
590/64:
X_train_selected = X_train.drop(labels=corr_features, axis=1)
X_test_selected = X_test.drop(labels=corr_features, axis=1)
df_val_encod = df_val.copy()

X_train_selected.shape, X_test_selected.shape
590/65:
target_scaler = MinMaxScaler(feature_range = (0,1))
target_scaler.fit(np.array(X_train_selected.revenue).reshape(-1, 1))
y_train_scale = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
590/66: X_train_selected.revenue = target_scaler.transform(np.array(X_train_selected.revenue).reshape(-1, 1))
590/67:
cat_features = ['City', 'City Group', 'Type', 
                'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P11', 'P14', 'P17', 'P19', 
                'P20', 'P21', 'P22', 'P23', 'P24', 'P27', 'P28', 'P29', 'P31', 'P33', 'P37']
590/68:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()

def integer_encode(train, test, val, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
    val[variable] = val[variable].map(ordinal_mapping)
590/69:
# and now we run a loop over the remaining categorical variables
for variable in cat_features:
    mappings = find_category_mappings(X_train_selected, variable, 'revenue')
    integer_encode(X_train_selected, X_test_selected, df_val_encod, variable, mappings)
590/70:
for col in cat_features:    
    fig = plt.figure(figsize=(7,7))
    fig = X_train_selected.groupby([col])['revenue'].mean().plot()
    fig.set_title(f'Monotonic relationship between {col} and revenue')
    fig.set_ylabel('Mean revenue')
    plt.show()
590/71:
# Check X_test after features encoding
X_test_selected.loc[:, X_test_selected.isna().any()].head(3)
590/72:
X_test_selected.fillna(-0.01, inplace=True)
df_val_encod.fillna(-0.01, inplace=True)
590/73:
y_train = X_train_selected.pop('revenue')
y_test = X_test_selected.pop('revenue')
590/74:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected, y_train)
590/75:
y_predict = regressor.predict(X_test_selected).reshape(-1,1)
y_val_predict = regressor.predict(df_val_encod[X_test_selected.columns]).reshape(-1,1)
590/76:
print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
590/77:
df_mean_encode_out = df_out.copy()
df_mean_encode_out.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)
590/78: df_no_corr.describe()
590/79:
df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_label_encode_selected.csv", index=False)
#####
### Score: 2131788.74924
### Private score: 2183281.28971
590/80:
metrics = metrics.append({
                            'model': 'LinearRegression',
                            'encoder': 'mean encoding', 
                            'features': X_test_selected.columns.to_list(),
                            'test rmse': mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False),
                            'test r2': r2_score(y_test, target_scaler.inverse_transform(y_predict)),
                            'private score': 2183281.28971,
                            'public score': 2131788.74924,
                         },
                        ignore_index=True
                        )
590/81: ###
590/82:
sfs = SFS(LinearRegression(),
          k_features=2,
          forward=False,
          verbose=0,
          scoring='neg_mean_squared_error',
          cv=20)

sfs = sfs.fit(np.array(X_train_selected), y_train)
df_k = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
590/83: df_k.sort_values('avg_score', ascending=False)[:5]
590/84:
f_indexes = list(df_k.sort_values('avg_score', ascending=False).iloc[0, 0])
columns_selected = list(X_train_selected.columns[f_indexes])
print('Selected columns:')
print(columns_selected)
590/85:
regressor = LinearRegression(fit_intercept = True)
regressor.fit(X_train_selected[columns_selected], y_train)
590/86:
y_predict = regressor.predict(X_test_selected[columns_selected]).reshape(-1,1)

print(f'RMSE: {mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False):.2f}')
print(f'R2: {r2_score(y_test, target_scaler.inverse_transform(y_predict)):.4f}')
590/87:
y_val_predict = regressor.predict(df_val_encod[columns_selected])
df_no_corr = df_out.copy()
df_no_corr.insert(1, "Prediction", target_scaler.inverse_transform(y_val_predict.reshape(-1,1)), True)
#df_no_corr.to_csv("lin_regression_no_corr.csv", index=False)

df_no_corr.describe()
590/88:
#df_no_corr[df_no_corr.Prediction < 0]['Prediction'] = 0

df_no_corr['Prediction'] = np.where(df_no_corr['Prediction']<0, 0, df_no_corr['Prediction'])
df_no_corr.to_csv("lin_regression_mean.csv", index=False)
####
##Score: 2265715.45540
##Private score: 2271094.18920
590/89:
metrics = metrics.append({
                            'model': 'LinearRegression',
                            'encoder': 'mean encoding + fs', 
                            'features': columns_selected,
                            'test rmse': mean_squared_error(y_test, target_scaler.inverse_transform(y_predict), squared=False),
                            'test r2': r2_score(y_test, target_scaler.inverse_transform(y_predict)),
                            'private score': 2183281.28971,
                            'public score': 2131788.74924,
                         },
                        ignore_index=True
                        )
590/90: metrics
591/1: #### Target scaling, MinMax(0,1)
596/1:
import pandas as pd
import numpy as np
596/2:
# load dataset

df = pd.read_csv('data/mushrooms.csv')
df.shape
596/3: df
596/4:  * In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfitting *
596/5:
# separate dataset into train and test

X_train, X_test, y_train, y_test = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
596/6:
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
596/7:
# load dataset

df = pd.read_csv('data/mushrooms.csv')
df.shape
596/8:
# separate dataset into train and test

X_train, X_test, y_train, y_test = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
596/9: X_train.head(5)
596/10: df.class
596/11: df
596/12: df.class.value_counts()
596/13: df['class'].value_counts()
596/14: df['class'].value_counts()/len(df)
596/15:
constant_features = [feat for feat in X_train.columns if X_train[feat].nunique() == 1]

len(constant_features)
596/16: constant_features
596/17: X_test['veil-type'].value_counts()
596/18:
X_train.drop(labels=constant_features, axis=1, inplace=True)
X_test.drop(labels=constant_features, axis=1, inplace=True)

X_train.shape, X_test.shape
596/19:
constant_features = [
    feat for feat in X_train.columns if X_train[feat].std() == 0
]
596/20:
# check for duplicated features in the training set:

# create an empty dictionary, where we will store 
# the groups of duplicates
duplicated_feat_pairs = {}

# create an empty list to collect features
# that were found to be duplicated
_duplicated_feat = []


# iterate over every feature in our dataset:
for i in range(0, len(X_train.columns)):
    
    # this bit helps me understand where the loop is at:
    if i % 10 == 0:  
        print(i)
    
    # choose 1 feature:
    feat_1 = X_train.columns[i]
    
    # check if this feature has already been identified
    # as a duplicate of another one. If it was, it should be stored in
    # our _duplicated_feat list.
    
    # If this feature was already identified as a duplicate, we skip it, if
    # it has not yet been identified as a duplicate, then we proceed:
    if feat_1 not in _duplicated_feat:
    
        # create an empty list as an entry for this feature in the dictionary:
        duplicated_feat_pairs[feat_1] = []

        # now, iterate over the remaining features of the dataset:
        for feat_2 in X_train.columns[i + 1:]:

            # check if this second feature is identical to the first one
            if X_train[feat_1].equals(X_train[feat_2]):

                # if it is identical, append it to the list in the dictionary
                duplicated_feat_pairs[feat_1].append(feat_2)
                
                # and append it to our monitor list for duplicated variables
                _duplicated_feat.append(feat_2)
                
                # done!
596/21: _duplicated_feat
596/22: X_train[X_train.columns].astype(str).apply(lambda x: ', '.join(x), axis = 1)
596/23: X_train[X_train.columns].astype(str).apply(lambda x: ''.join(x), axis = 1)
596/24: X_train['combination'] = X_train[X_train.columns].astype(str).apply(lambda x: ''.join(x), axis = 1)
596/25: X_train['combination']
596/26: X_train
596/27: X_train.duplicated(subset=['combination'])
596/28: X_train[X_train.duplicated(subset=['combination'])]
596/29: X_train[X_train.duplicated(subset=['combination'])].shape
596/30: X_train.duplicated
596/31: X_train.duplicated()
596/32: X_train.loc[:,~X_train.columns.duplicated()].copy()
596/33: X_train.loc[:,X_train.columns.duplicated()].copy()
596/34: X_train
596/35: X_train['v'] = X_train.odor
596/36: X_train.loc[:,X_train.columns.duplicated()].copy()
596/37: X_train
596/38: X_train.loc[:,~X_train.columns.duplicated()].copy()
596/39:
# check for duplicated features in the training set:

# create an empty dictionary, where we will store the groups of duplicates
duplicated_feat_pairs = {}

# create an empty list to collect features that were found to be duplicated
columns_dupl = []

# iterate over every feature in our dataset:
for i in range(0, len(X_train.columns)):  
    # this bit helps me understand where the loop is at:
    if i % 10 == 0:  
        print(i)
    
    # choose 1 feature:
    feat_1 = X_train.columns[i]
    
    # check if this feature has already been identified
    # as a duplicate of another one. If it was, it should be stored in
    # our _duplicated_feat list.
    
    # If this feature was already identified as a duplicate, we skip it, if
    # it has not yet been identified as a duplicate, then we proceed:
    if feat_1 not in _duplicated_feat:
    
        # create an empty list as an entry for this feature in the dictionary:
        duplicated_feat_pairs[feat_1] = []

        # now, iterate over the remaining features of the dataset:
        for feat_2 in X_train.columns[i + 1:]:

            # check if this second feature is identical to the first one
            if X_train[feat_1].equals(X_train[feat_2]):

                # if it is identical, append it to the list in the dictionary
                duplicated_feat_pairs[feat_1].append(feat_2)
                
                # and append it to our monitor list for duplicated variables
                _duplicated_feat.append(feat_2)
                
                # done!
596/40: _duplicated_feat
596/41: X_train.T
596/42:
# check for duplicated features in the training set:

# create an empty dictionary, where we will store the groups of duplicates
duplicated_feat_pairs = {}

# create an empty list to collect features that were found to be duplicated
columns_dupl = []

# iterate over every feature in our dataset:
for i in range(0, len(X_train.columns)):    
    # choose 1 feature:
    feat_1 = X_train.columns[i]
    
    # check if this feature has already been identified as a duplicate of another one.
    # If it was, it should be stored in our columns_dupl list.
    # If this feature was already identified as a duplicate, we skip it, if
    # it has not yet been identified as a duplicate, then we proceed:
    if feat_1 not in columns_dupl:
        duplicated_feat_pairs[feat_1] = []
        # now, iterate over the remaining features of the dataset:
        for feat_2 in X_train.columns[i + 1:]:
            # check if this second feature is identical to the first one
            if X_train[feat_1].equals(X_train[feat_2]):
                # if it is identical, append it to the list in the dictionary
                duplicated_feat_pairs[feat_1].append(feat_2)               
                # and append it to our monitor list for duplicated variables
                columns_dupl.append(feat_2)
596/43: columns_dupl
596/44: duplicated_feat_pairs
596/45: len(columns_dupl)
600/1:
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
600/2:
# load dataset

df = pd.read_csv('data/mushrooms.csv')
df.shape
600/3: df['class'].value_counts()/len(df)
600/4:
# separate dataset into train and test

X_train, X_test, y_train, y_test = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
600/5: X_train.head(5)
600/6: ### 1. Constant features
600/7:
constant_features = [feat for feat in X_train.columns if X_train[feat].nunique() == 1]

len(constant_features)
600/8:
X_train.drop(labels=constant_features, axis=1, inplace=True)
X_test.drop(labels=constant_features, axis=1, inplace=True)

X_train.shape, X_test.shape
600/9: X_train[X_train.duplicated(subset=['combination'])].shape
600/10:
# check for duplicated features in the training set:
# create an empty dictionary, where we will store the groups of duplicates
duplicated_feat_pairs = {}

# create an empty list to collect features that were found to be duplicated
columns_dupl = []

# iterate over every feature in our dataset:
for i in range(0, len(X_train.columns)):    
    # choose 1 feature:
    feat_1 = X_train.columns[i]
    
    # check if this feature has already been identified as a duplicate of another one.
    # If it was, it should be stored in our columns_dupl list.
    # If this feature was already identified as a duplicate, we skip it, if
    # it has not yet been identified as a duplicate, then we proceed:
    if feat_1 not in columns_dupl:
        duplicated_feat_pairs[feat_1] = []
        # now, iterate over the remaining features of the dataset:
        for feat_2 in X_train.columns[i + 1:]:
            # check if this second feature is identical to the first one
            if X_train[feat_1].equals(X_train[feat_2]):
                # if it is identical, append it to the list in the dictionary
                duplicated_feat_pairs[feat_1].append(feat_2)               
                # and append it to our monitor list for duplicated variables
                columns_dupl.append(feat_2)
600/11: len(columns_dupl)
600/12: duplicated_feat_pairs
600/13: duplicated_feat_pairs.keys()
600/14: #### 3.1 Chi-Squared
600/15:
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split

from sklearn.feature_selection import chi2
600/16:
f_score = chi2(X_train.fillna(0), y_train)

# the 2 arrays of values
f_score
600/17: X_train
600/18:
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split

from sklearn.preprocessing import OrdinalEncoder
from sklearn.feature_selection import chi2
600/19: ord_enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.NaN)
600/20:
X_train_transform = ord_e.fit_transform(X_train)
X_train_transform = pd.DataFrame(X_train_transform)
600/21:
X_train_transform = ord_enc.fit_transform(X_train)
X_train_transform = pd.DataFrame(X_train_transform)
600/22: X_train_transform
600/23: ord_enc
600/24: ord_enc
600/25: ord_enc.categories
600/26: ord_enc.categories_
600/27: ord_enc.n_features_in_
600/28: ord_enc.feature_names_in_
600/29: ord_enc.categories_
600/30: X_train_transform
600/31: X_train_transform.describe()
600/32: X_train_transform.isna()
600/33: X_train_transform.isna().sum()
600/34: X_train
600/35: X_train.isnull()
600/36: X_train.isnull().sum()
600/37: X_train
600/38:
for cols in X_train.columns:
    X_train[cols].value_counts()
600/39:
for cols in X_train.columns:
    print(X_train[cols].value_counts())
600/40:
f_score = chi2(X_train, y_train)

# the 2 arrays of values
f_score
600/41:
f_score = chi2(X_train_transform, y_train)

# the 2 arrays of values
f_score
600/42:
f_score = chi2(X_train_transform, y_train)

# the 2 arrays of values
f_score[1]
600/43:
f_score = chi2(X_train_transform, y_train)

# the 2 arrays of values
f_score
600/44:
f_score = chi2(X_train_transform, y_train)

# the 2 arrays of values : Chi2 statistics for each feature and P-values for each feature.
f_score
600/45:
pvalues = pd.Series(f_score[1])
pvalues.index = X_train.columns
pvalues.sort_values(ascending=True)
600/46: ring-type
600/47:

X_train['ring-type']
600/48: y_train
600/49:

X_train['ring-type'].value_countsu()
600/50:

X_train['ring-type'].value_counts()
600/51:

X_train['gill-color'].value_counts()
600/52:
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split

from sklearn.preprocessing import OrdinalEncoder
from sklearn.feature_selection import chi2
from sklearn.feature_selection import mutual_info_classif
600/53: mi = mutual_info_classif(X_train, y_train)
600/54: mi = mutual_info_classif(X_train_transform, y_train)
600/55:
mi = pd.Series(mi)
mi.index = X_train.columns
mi.sort_values(ascending=False).plot.bar(figsize=(20, 6))
plt.ylabel('Mutual Information')
600/56:
mi = pd.Series(mi)
mi.index = X_train_transform.columns
mi.sort_values(ascending=False).plot.bar(figsize=(20, 6))
plt.ylabel('Mutual Information')
600/57: import matplotlib.pyplot as plt
600/58:
mi = pd.Series(mi)
mi.index = X_train_transform.columns
mi.sort_values(ascending=False).plot.bar(figsize=(20, 6))
plt.ylabel('Mutual Information')
600/59: X_train_transform
600/60:
mi = pd.Series(mi)
mi.index = X_train.columns
mi.sort_values(ascending=False).plot.bar(figsize=(20, 6))
plt.ylabel('Mutual Information')
600/61:
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split

from sklearn.preprocessing import OrdinalEncoder
from sklearn.feature_selection import chi2
from sklearn.feature_selection import mutual_info_classif

from sklearn.feature_selection import SelectKBest, SelectPercentile
600/62:
# select features
sel_ = SelectKBest(mutual_info_classif, k=10).fit(X_train_transform, y_train)

# display features
X_train.columns[sel_.get_support()]
600/63: ### SelectKBest example
600/64:
# to remove the rest of the features:
X_train_sel = sel_.transform(X_train)
X_test_sel = sel_.transform(X_test)
print(X_train_sel.shape, X_test_sel.shape)
600/65:
X_train_transform = ord_enc.fit_transform(X_train)
X_train_transform = pd.DataFrame(X_train_transform)

X_test_transform = ord_enc.transform(X_train)
X_test_transform = pd.DataFrame(X_train_transform)
600/66:
X_train_transform.columns = X_train.columns
X_test_transform.columns = X_test.columns
600/67:
f_score = chi2(X_train_transform, y_train)

# the 2 arrays of values : Chi2 statistics for each feature and P-values for each feature.
f_score
600/68:
pvalues = pd.Series(f_score[1])
pvalues.index = X_train.columns
pvalues.sort_values(ascending=True)
600/69: mi = mutual_info_classif(X_train_transform, y_train)
600/70:
mi = pd.Series(mi)
mi.index = X_train.columns
mi.sort_values(ascending=False).plot.bar(figsize=(20, 6))
plt.ylabel('Mutual Information')
600/71:
mi = pd.Series(mi)
mi.index = X_train_transform.columns
mi.sort_values(ascending=False).plot.bar(figsize=(20, 6))
plt.ylabel('Mutual Information')
600/72:
pvalues = pd.Series(f_score[1])
pvalues.index = X_train_transform.columns
pvalues.sort_values(ascending=True)
600/73:
# select features
sel_ = SelectKBest(mutual_info_classif, k=10).fit(X_train_transform, y_train)

# display features
X_train.columns[sel_.get_support()]
600/74:
# to remove the rest of the features:
X_train_sel = sel_.transform(X_train)
X_test_sel = sel_.transform(X_test)
print(X_train_sel.shape, X_test_sel.shape)
600/75: ### Select top 10th percentile features
600/76:
# Select the features in the top percentile
sel_ = SelectPercentile(mutual_info_regression, percentile=10).fit(X_train_transform, y_train)

# display the features
X_train_transform.columns[sel_.get_support()]
600/77:
# Select the features in the top percentile
sel_ = SelectPercentile(mutual_info_classif, percentile=10).fit(X_train_transform, y_train)

# display the features
X_train_transform.columns[sel_.get_support()]
600/78:
X_train_sel = sel_.transform(X_train)
X_test_sel = sel_.transform(X_test)
print(X_train_sel.shape, X_test_sel.shape)
602/1:
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split

from sklearn.preprocessing import OrdinalEncoder
from sklearn.feature_selection import chi2
from sklearn.feature_selection import mutual_info_classif

from sklearn.feature_selection import SelectKBest, SelectPercentile
602/2: import matplotlib.pyplot as plt
602/3:
# load dataset

df = pd.read_csv('data/mushrooms.csv')
df.shape
602/4:
### check target
df['class'].value_counts()/len(df)
602/5:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
602/6: X_train.head(5)
602/7:
constant_features = [feat for feat in X_train.columns if X_train[feat].nunique() == 1]

len(constant_features)
602/8: constant_features
602/9:
constant_features = [feat for feat in X_train.columns if X_train[feat].nunique() == 1]

len(constant_features)
602/10:
X_train.drop(labels=constant_features, axis=1, inplace=True)
X_test.drop(labels=constant_features, axis=1, inplace=True)

X_train.shape, X_test.shape
602/11: constant_features
602/12:
# check for duplicated features in the training set:
# create an empty dictionary, where we will store the groups of duplicates
duplicated_feat_pairs = {}

# create an empty list to collect features that were found to be duplicated
columns_dupl = []

# iterate over every feature in our dataset:
for i in range(0, len(X_train.columns)):    
    # choose 1 feature:
    feat_1 = X_train.columns[i]
    
    # check if this feature has already been identified as a duplicate of another one.
    # If it was, it should be stored in our columns_dupl list.
    # If this feature was already identified as a duplicate, we skip it, if
    # it has not yet been identified as a duplicate, then we proceed:
    if feat_1 not in columns_dupl:
        duplicated_feat_pairs[feat_1] = []
        # now, iterate over the remaining features of the dataset:
        for feat_2 in X_train.columns[i + 1:]:
            # check if this second feature is identical to the first one
            if X_train[feat_1].equals(X_train[feat_2]):
                # if it is identical, append it to the list in the dictionary
                duplicated_feat_pairs[feat_1].append(feat_2)               
                # and append it to our monitor list for duplicated variables
                columns_dupl.append(feat_2)
602/13:
len(columns_dupl)
### There is no duplicated features
602/14:
# check for duplicated features in the training set:
# create an empty dictionary, where we will store the groups of duplicates
duplicated_feat_pairs = {}

# create an empty list to collect features that were found to be duplicated
columns_dupl = []

# iterate over every feature in our dataset:
for i in range(0, len(X_train.columns)):    
    # choose 1 feature:
    feat_1 = X_train.columns[i]
    
    # check if this feature has already been identified as a duplicate of another one.
    # If it was, it should be stored in our columns_dupl list.
    # If this feature was already identified as a duplicate, we skip it, if
    # it has not yet been identified as a duplicate, then we proceed:
    if feat_1 not in columns_dupl:
        duplicated_feat_pairs[feat_1] = []
        # now, iterate over the remaining features of the dataset:
        for feat_2 in X_train.columns[i + 1:]:
            # check if this second feature is identical to the first one
            if X_train[feat_1].equals(X_train[feat_2]):
                # if it is identical, append it to the list in the dictionary
                duplicated_feat_pairs[feat_1].append(feat_2)               
                # and append it to our monitor list for duplicated variables
                columns_dupl.append(feat_2)
602/15:
len(columns_dupl)
### There is no duplicated features
602/16: ord_enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.NaN)
602/17:
X_train_transform = ord_enc.fit_transform(X_train)
X_train_transform = pd.DataFrame(X_train_transform)

X_test_transform = ord_enc.transform(X_train)
X_test_transform = pd.DataFrame(X_train_transform)
602/18:
X_train_transform.columns = X_train.columns
X_test_transform.columns = X_test.columns
602/19:
f_score = chi2(X_train_transform, y_train)

# the 2 arrays of values : Chi2 statistics for each feature and P-values for each feature.
f_score
602/20:
pvalues = pd.Series(f_score[1])
pvalues.index = X_train_transform.columns
pvalues.sort_values(ascending=True)
602/21: mi = mutual_info_classif(X_train_transform, y_train)
602/22:
mi = pd.Series(mi)
mi.index = X_train_transform.columns
mi.sort_values(ascending=False).plot.bar(figsize=(20, 6))
plt.ylabel('Mutual Information')
602/23:
mi = pd.Series(mi)
mi.index = X_train_transform.columns
mi.sort_values(ascending=False).plot.bar(figsize=(20, 6))
plt.ylabel('Mutual Information')
602/24: ### 4.2.1 SelectKBest example
602/25: mi
602/26: mi.dtype
602/27: sorted(mi)
602/28: mi.sort_values()
602/29: mi.sort_values(ascending=False)
602/30:
# select features
sel_ = SelectKBest(mutual_info_classif, k=10).fit(X_train_transform, y_train)

# display features
X_train.columns[sel_.get_support()]
602/31:
# to remove the rest of the features:
X_train_sel = sel_.transform(X_train)
X_test_sel = sel_.transform(X_test)
print(X_train_sel.shape, X_test_sel.shape)
602/32: ### Select top 10th percentile features
602/33:
# Select the features in the top percentile
sel_ = SelectPercentile(mutual_info_classif, percentile=10).fit(X_train_transform, y_train)

# display the features
X_train_transform.columns[sel_.get_support()]
602/34:
X_train_sel = sel_.transform(X_train)
X_test_sel = sel_.transform(X_test)
print(X_train_sel.shape, X_test_sel.shape)
603/1:
import numpy as np
import pandas as pd

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf
603/2:
# load dataset
df = pd.read_csv('data/mushrooms.csv')
df.shape
603/3:
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf
603/4:
### check target
df['class'].value_counts()/len(df)
603/5:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
603/6: X_train.head(5)
603/7: X_train['class'] = y_train
603/8: X_train.head(5)
603/9:
plt.figure(figsize=(15, 8))
splot = sns.countplot(data=df, x='odor',
                      hue='class',
                      order=data['odor'].value_counts().index,
                      palette=['red', 'forestgreen'],)
                      
splot.set_xticklabels(['Tapering', 'Enlarging'])

for p in splot.patches:
    splot.annotate(format(p.get_height(), '.1f'),
                   (p.get_x() + p.get_width() / 2., p.get_height()),
                   ha='center', va='center',
                   xytext=(0, 9),
                   textcoords='offset points')

plt.legend(['Poisonous', 'Edible'], loc='upper right')
plt.ylabel('Number of the Mushrooms', fontsize=14)
plt.xlabel('Types of the Stalk Shapes', fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.title('Distribution of the Mushrooms by their Classes vs Stalk Shapes', fontsize=20)
603/10:
plt.figure(figsize=(15, 8))
splot = sns.countplot(data=df, x='odor',
                      hue='class',
                      order=df['odor'].value_counts().index,
                      palette=['red', 'forestgreen'],)
                      
splot.set_xticklabels(['Tapering', 'Enlarging'])

for p in splot.patches:
    splot.annotate(format(p.get_height(), '.1f'),
                   (p.get_x() + p.get_width() / 2., p.get_height()),
                   ha='center', va='center',
                   xytext=(0, 9),
                   textcoords='offset points')

plt.legend(['Poisonous', 'Edible'], loc='upper right')
plt.ylabel('Number of the Mushrooms', fontsize=14)
plt.xlabel('Types of the Stalk Shapes', fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.title('Distribution of the Mushrooms by their Classes vs Stalk Shapes', fontsize=20)
603/11:
plt.figure(figsize=(15, 8))
splot = sns.countplot(data=df, x='odor',
                      hue='class',
                      order=df['odor'].value_counts().index,
                      palette=['red', 'forestgreen'],)
                      
#splot.set_xticklabels(['Tapering', 'Enlarging'])

for p in splot.patches:
    splot.annotate(format(p.get_height(), '.1f'),
                   (p.get_x() + p.get_width() / 2., p.get_height()),
                   ha='center', va='center',
                   xytext=(0, 9),
                   textcoords='offset points')

plt.legend(['Poisonous', 'Edible'], loc='upper right')
plt.ylabel('Number of the Mushrooms', fontsize=14)
plt.xlabel('Types of the Stalk Shapes', fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.title('Distribution of the Mushrooms by their Classes vs Stalk Shapes', fontsize=20)
603/12:
plt.figure(figsize=(15, 8))
splot = sns.countplot(data=df, x='odor',
                      hue='class',
                      order=df['odor'].value_counts().index,
                      palette=['red', 'forestgreen'],)
                      
#splot.set_xticklabels(['Tapering', 'Enlarging'])



plt.legend(['Poisonous', 'Edible'], loc='upper right')
plt.ylabel('Number of the Mushrooms', fontsize=14)
plt.xlabel('Types of the Stalk Shapes', fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.title('Distribution of the Mushrooms by their Classes vs Stalk Shapes', fontsize=20)
603/13:
plt.figure(figsize=(15, 8))
fg = sns.countplot(data=df, x='odor',
                      hue='class',
                      order=df['odor'].value_counts().index,
                      palette=['red', 'forestgreen'],)
                      
#splot.set_xticklabels(['Tapering', 'Enlarging'])

for p in fg.patches:
    fg.annotate(format(p.get_height(), '.1f'),
                   (p.get_x() + p.get_width() / 2., p.get_height()),
                   ha='center', va='center',
                   xytext=(0, 9),
                   textcoords='offset points')

plt.legend(['Poisonous', 'Edible'], loc='upper right')
plt.ylabel('Number of the Mushrooms', fontsize=14)
plt.xlabel('Types of the Stalk Shapes', fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.title('Distribution of the Mushrooms by their Classes vs Stalk Shapes', fontsize=20)
603/14:
for col in cols:
 #   plt.figure()
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'olivedrab']
                      )





    fig, axes = plt.subplots(1, 4, figsize=(18,5))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], binwidth = 0.5, ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.05, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_estimator=np.mean, ax=axes[2])
    sns.histplot(df_val[p_col], ax=axes[3], binwidth = 0.5, color='red')
    

    axes[0].set_title(f'Train {p_col}')
    axes[1].set_title(f'{p_col} and revenue')
    axes[2].set_title(f'{p_col} and revenue (mean)')
    axes[3].set_title(f'Test {p_col}' )
    pdf.savefig( fig )
    
### Open days and revenue
fig, axes = plt.subplots(1, 3, figsize=(18,5))
sns.set_style("whitegrid")
sns.histplot(df_train['open_days'], kde=True, ax=axes[0])
sns.regplot(x=df_train['open_days'], y=df_train.revenue, ax=axes[1])
sns.histplot(df_val['open_days'], kde=True, ax=axes[2], color='red')

axes[0].set_title(f'Train. Hist of open days')
axes[1].set_title(f'Open days and revenue')
axes[2].set_title(f'Test. Hist of open days')
pdf.savefig( fig )

### close file
pdf.close()
603/15: cols = X_test.columns
603/16:
for col in cols:
 #   plt.figure()
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'olivedrab']
                      )





    fig, axes = plt.subplots(1, 4, figsize=(18,5))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], binwidth = 0.5, ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.05, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_estimator=np.mean, ax=axes[2])
    sns.histplot(df_val[p_col], ax=axes[3], binwidth = 0.5, color='red')
    

    axes[0].set_title(f'Train {p_col}')
    axes[1].set_title(f'{p_col} and revenue')
    axes[2].set_title(f'{p_col} and revenue (mean)')
    axes[3].set_title(f'Test {p_col}' )
    pdf.savefig( fig )
    
### Open days and revenue
fig, axes = plt.subplots(1, 3, figsize=(18,5))
sns.set_style("whitegrid")
sns.histplot(df_train['open_days'], kde=True, ax=axes[0])
sns.regplot(x=df_train['open_days'], y=df_train.revenue, ax=axes[1])
sns.histplot(df_val['open_days'], kde=True, ax=axes[2], color='red')

axes[0].set_title(f'Train. Hist of open days')
axes[1].set_title(f'Open days and revenue')
axes[2].set_title(f'Test. Hist of open days')
pdf.savefig( fig )

### close file
pdf.close()
603/17:
for col in cols:
 #   plt.figure()
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'forestfreen']
                      )





    fig, axes = plt.subplots(1, 4, figsize=(18,5))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], binwidth = 0.5, ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.05, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_estimator=np.mean, ax=axes[2])
    sns.histplot(df_val[p_col], ax=axes[3], binwidth = 0.5, color='red')
    

    axes[0].set_title(f'Train {p_col}')
    axes[1].set_title(f'{p_col} and revenue')
    axes[2].set_title(f'{p_col} and revenue (mean)')
    axes[3].set_title(f'Test {p_col}' )
    pdf.savefig( fig )
    
### Open days and revenue
fig, axes = plt.subplots(1, 3, figsize=(18,5))
sns.set_style("whitegrid")
sns.histplot(df_train['open_days'], kde=True, ax=axes[0])
sns.regplot(x=df_train['open_days'], y=df_train.revenue, ax=axes[1])
sns.histplot(df_val['open_days'], kde=True, ax=axes[2], color='red')

axes[0].set_title(f'Train. Hist of open days')
axes[1].set_title(f'Open days and revenue')
axes[2].set_title(f'Test. Hist of open days')
pdf.savefig( fig )

### close file
pdf.close()
603/18:
for col in cols:
 #   plt.figure()
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'forestgreen']
                      )





    fig, axes = plt.subplots(1, 4, figsize=(18,5))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], binwidth = 0.5, ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.05, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_estimator=np.mean, ax=axes[2])
    sns.histplot(df_val[p_col], ax=axes[3], binwidth = 0.5, color='red')
    

    axes[0].set_title(f'Train {p_col}')
    axes[1].set_title(f'{p_col} and revenue')
    axes[2].set_title(f'{p_col} and revenue (mean)')
    axes[3].set_title(f'Test {p_col}' )
    pdf.savefig( fig )
    
### Open days and revenue
fig, axes = plt.subplots(1, 3, figsize=(18,5))
sns.set_style("whitegrid")
sns.histplot(df_train['open_days'], kde=True, ax=axes[0])
sns.regplot(x=df_train['open_days'], y=df_train.revenue, ax=axes[1])
sns.histplot(df_val['open_days'], kde=True, ax=axes[2], color='red')

axes[0].set_title(f'Train. Hist of open days')
axes[1].set_title(f'Open days and revenue')
axes[2].set_title(f'Test. Hist of open days')
pdf.savefig( fig )

### close file
pdf.close()
603/19:
for col in cols:
 #   plt.figure()
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )





    fig, axes = plt.subplots(1, 4, figsize=(18,5))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], binwidth = 0.5, ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.05, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_estimator=np.mean, ax=axes[2])
    sns.histplot(df_val[p_col], ax=axes[3], binwidth = 0.5, color='red')
    

    axes[0].set_title(f'Train {p_col}')
    axes[1].set_title(f'{p_col} and revenue')
    axes[2].set_title(f'{p_col} and revenue (mean)')
    axes[3].set_title(f'Test {p_col}' )
    pdf.savefig( fig )
    
### Open days and revenue
fig, axes = plt.subplots(1, 3, figsize=(18,5))
sns.set_style("whitegrid")
sns.histplot(df_train['open_days'], kde=True, ax=axes[0])
sns.regplot(x=df_train['open_days'], y=df_train.revenue, ax=axes[1])
sns.histplot(df_val['open_days'], kde=True, ax=axes[2], color='red')

axes[0].set_title(f'Train. Hist of open days')
axes[1].set_title(f'Open days and revenue')
axes[2].set_title(f'Test. Hist of open days')
pdf.savefig( fig )

### close file
pdf.close()
603/20:
for col in cols:
 #   plt.figure()
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )

    for p in fg.patches:
    fg.annotate(format(p.get_height(), '.1f'),
                   (p.get_x() + p.get_width() / 2., p.get_height()),
                   ha='center', va='center',
                   xytext=(0, 9),
                   textcoords='offset points')




    fig, axes = plt.subplots(1, 4, figsize=(18,5))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], binwidth = 0.5, ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.05, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_estimator=np.mean, ax=axes[2])
    sns.histplot(df_val[p_col], ax=axes[3], binwidth = 0.5, color='red')
    

    axes[0].set_title(f'Train {p_col}')
    axes[1].set_title(f'{p_col} and revenue')
    axes[2].set_title(f'{p_col} and revenue (mean)')
    axes[3].set_title(f'Test {p_col}' )
    pdf.savefig( fig )
    
### Open days and revenue
fig, axes = plt.subplots(1, 3, figsize=(18,5))
sns.set_style("whitegrid")
sns.histplot(df_train['open_days'], kde=True, ax=axes[0])
sns.regplot(x=df_train['open_days'], y=df_train.revenue, ax=axes[1])
sns.histplot(df_val['open_days'], kde=True, ax=axes[2], color='red')

axes[0].set_title(f'Train. Hist of open days')
axes[1].set_title(f'Open days and revenue')
axes[2].set_title(f'Test. Hist of open days')
pdf.savefig( fig )

### close file
pdf.close()
603/21:
for col in cols:
 #   plt.figure()
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.1f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    textcoords='offset points'
                   )




    fig, axes = plt.subplots(1, 4, figsize=(18,5))
    sns.set_style("whitegrid")
    sns.histplot(df_train[p_col], binwidth = 0.5, ax=axes[0])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_jitter=.05, ax=axes[1])
    sns.regplot(x=df_train[p_col], y=df_train.revenue, x_estimator=np.mean, ax=axes[2])
    sns.histplot(df_val[p_col], ax=axes[3], binwidth = 0.5, color='red')
    

    axes[0].set_title(f'Train {p_col}')
    axes[1].set_title(f'{p_col} and revenue')
    axes[2].set_title(f'{p_col} and revenue (mean)')
    axes[3].set_title(f'Test {p_col}' )
    pdf.savefig( fig )
    
### Open days and revenue
fig, axes = plt.subplots(1, 3, figsize=(18,5))
sns.set_style("whitegrid")
sns.histplot(df_train['open_days'], kde=True, ax=axes[0])
sns.regplot(x=df_train['open_days'], y=df_train.revenue, ax=axes[1])
sns.histplot(df_val['open_days'], kde=True, ax=axes[2], color='red')

axes[0].set_title(f'Train. Hist of open days')
axes[1].set_title(f'Open days and revenue')
axes[2].set_title(f'Test. Hist of open days')
pdf.savefig( fig )

### close file
pdf.close()
603/22:
for col in cols:
 #   plt.figure()
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.1f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    textcoords='offset points'
                   )
603/23:
for col in cols:
 #   plt.figure()
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.1f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right')
    plt.ylabel('Number of the Mushrooms', fontsize=12)
  #  plt.xlabel('Types of the Stalk Shapes', fontsize=14)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=20)
603/24:
for col in cols:
 #   plt.figure()
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.1f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right')
    plt.ylabel('Number of the Mushrooms', fontsize=12)
  #  plt.xlabel('Types of the Stalk Shapes', fontsize=14)
    plt.xticks(fontsize=22)
    plt.yticks(fontsize=12)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=20)
603/25:
for col in cols:
 #   plt.figure()
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.1f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    #xytext=(0, 9),
                    size=15,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right')
    plt.ylabel('Number of the Mushrooms', fontsize=12)
  #  plt.xlabel('Types of the Stalk Shapes', fontsize=14)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=12)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
603/26:
for col in cols:
 #   plt.figure()
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.1f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=15,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right')
    plt.ylabel('Number of the Mushrooms', fontsize=12)
  #  plt.xlabel('Types of the Stalk Shapes', fontsize=14)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=12)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
603/27:
for col in cols:
 #   plt.figure()
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=13,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right')
    plt.ylabel('Number of the Mushrooms', fontsize=12)
  #  plt.xlabel('Types of the Stalk Shapes', fontsize=14)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=12)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
603/28:
plt.figure(figsize=(15, 8))
fg = sns.countplot(data=df, x='odor',
                      hue='class',
                      order=df['odor'].value_counts().index,
                      palette=['red', 'forestgreen'],)
                      
#splot.set_xticklabels(['Tapering', 'Enlarging'])

for p in fg.patches:
    fg.annotate(format(p.get_height()),
                   (p.get_x() + p.get_width() / 2., p.get_height()),
                   ha='center', va='center',
                   xytext=(0, 9),
                   textcoords='offset points')

plt.legend(['Poisonous', 'Edible'], loc='upper right')
plt.ylabel('Number of the Mushrooms', fontsize=14)
plt.xlabel('Types of the Stalk Shapes', fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.title('Distribution of the Mushrooms by their Classes vs Stalk Shapes', fontsize=20)
603/29:
for col in cols:
 #   plt.figure()
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=11,
          #          textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right')
    plt.ylabel('Number of the Mushrooms', fontsize=12)
  #  plt.xlabel('Types of the Stalk Shapes', fontsize=14)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=12)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
603/30:
for col in cols:
 #   plt.figure()
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=11,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right')
    plt.ylabel('Number of the Mushrooms', fontsize=12)
  #  plt.xlabel('Types of the Stalk Shapes', fontsize=14)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=12)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
603/31:
for col in cols:
 #   plt.figure()
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', title_fontsize='40')
    plt.ylabel('Number of the Mushrooms', fontsize=12)
  #  plt.xlabel('Types of the Stalk Shapes', fontsize=14)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=12)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
603/32:
for col in cols:
 #   plt.figure()
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right')
    plt.setp(g._legend.get_texts(), fontsize=16)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
  #  plt.xlabel('Types of the Stalk Shapes', fontsize=14)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=12)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
603/33:
for col in cols:
 #   plt.figure()
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right')
    plt.setp(fg._legend.get_texts(), fontsize=16)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
  #  plt.xlabel('Types of the Stalk Shapes', fontsize=14)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=12)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
603/34:
for col in cols:
 #   plt.figure()
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', prop={'size': 6})

    plt.ylabel('Number of the Mushrooms', fontsize=12)
  #  plt.xlabel('Types of the Stalk Shapes', fontsize=14)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=12)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
603/35:
for col in cols:
 #   plt.figure()
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', prop={'size': 16})

    plt.ylabel('Number of the Mushrooms', fontsize=12)
  #  plt.xlabel('Types of the Stalk Shapes', fontsize=14)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=12)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
603/36:
for col in cols:
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', prop={'size': 14})

    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=12)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=14)
603/37:
for col in cols:
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)

    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=12)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
603/38:
for col in cols:
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the {col} feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
603/39:
for col in cols:
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
603/40:
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf
603/41:
# load dataset
df = pd.read_csv('data/mushrooms.csv')
df.shape
603/42:
### check target
df['class'].value_counts()/len(df)
603/43:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
603/44: cols = X_train.columns
603/45: X_train['class'] = y_train
603/46: X_train.head(5)
603/47:
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("countplots.pdf")
603/48:
for col in cols:
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
    
pdf.close()
603/49: pdf.close()
603/50: X_train
603/51: pdf.close()
604/1:
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf
604/2:
# load dataset
df = pd.read_csv('data/mushrooms.csv')
df.shape
604/3:
### check target
df['class'].value_counts()/len(df)
604/4:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
604/5:
cols = X_train.columns
X_train['class'] = y_train
X_train.head(5)
604/6:
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("countplots.pdf")
604/7:
for col in cols:
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
    
pdf.close()
605/1:
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf
605/2:
# load dataset
df = pd.read_csv('data/mushrooms.csv')
df.shape
605/3:
### check target
df['class'].value_counts()/len(df)
605/4:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
605/5:
cols = X_train.columns
X_train['class'] = y_train
X_train.head(5)
605/6:
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("countplots.pdf")
605/7:
for col in cols:
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
605/8: pdf.close()
606/1:
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf
606/2:
# load dataset
df = pd.read_csv('data/mushrooms.csv')
df.shape
606/3:
### check target
df['class'].value_counts()/len(df)
606/4:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
606/5:
cols = X_train.columns
X_train['class'] = y_train
X_train.head(5)
606/6:
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("countplots.pdf")
606/7:
for col in cols:
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
    pdf.savefig( fig )
pdf.close()
606/8:
for col in cols:
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
    pdf.savefig(fg)
pdf.close()
606/9:
for col in cols:
    plt.figure(figsize=(15, 8))
    fig, axes = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )

    for p in fig.patches:
        fig.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
    pdf.savefig(fig)
pdf.close()
606/10:
for col in cols:
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
    pdf.savefig(fg)
pdf.close()
606/11:
for col in cols:
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
    fg.dtype
    pdf.savefig(fg)
pdf.close()
606/12:
for col in cols:
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
606/13:
for col in cols:
    plt.figure(figsize=(15, 8))
    fg = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen']
                      )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
    pdf.savefig(plt)
606/14:
for col in cols:
    fig, ax = plt.subplots(figsize=(15, 8))
    sns.countplot(data=X_train, 
                  x=col,
                  hue='class',
                  order=df[col].value_counts().index,
                  palette=['tomato', 'seagreen'],
                  ax=ax
                 )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
    pdf.savefig(plt)
606/15:
for col in cols:
    fig, ax = plt.subplots(figsize=(15, 8))
    sns.countplot(data=X_train, 
                  x=col,
                  hue='class',
                  order=df[col].value_counts().index,
                  palette=['tomato', 'seagreen'],
                  ax=ax
                 )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
    pdf.savefig(fig)
606/16:
for col in cols:
    fig, ax = plt.subplots(figsize=(15, 8))
    fig = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen'],
                       ax=ax
                      )

    for p in fg.patches:
        fg.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
    pdf.savefig(plt)
606/17:
for col in cols:
    fig, ax = plt.subplots(figsize=(15, 8))
    fig = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen'],
                       ax=ax
                      )

    for p in fig.patches:
        fg.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
    pdf.savefig(plt)
606/18:
for col in cols:
    fig, ax = plt.subplots(figsize=(15, 8))
    fig = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen'],
                       ax=ax
                      )

    for p in fig.patches:
        fg.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
    pdf.savefig(fig)
606/19:
for col in cols:
    fig, ax = plt.subplots(figsize=(15, 8))
    ax = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen'],
                       ax=ax
                      )

    for p in fig.patches:
        fg.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
    pdf.savefig(fig)
606/20:
for col in cols:
    fig, ax = plt.subplots(figsize=(15, 8))
    ax = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen'],
                       ax=ax
                      )

    for p in ax.patches:
        fg.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
#    pdf.savefig(fig)
606/21:
for col in cols:
    fig, ax = plt.subplots(figsize=(15, 8))
    ax = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen'],
                       ax=ax
                      )

    for p in ax.patches:
        ax.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
#    pdf.savefig(fig)
606/22:
for col in cols:
    fig, ax = plt.subplots(figsize=(15, 8))
    ax = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen'],
                       ax=ax
                      )

    for p in ax.patches:
        ax.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
    pdf.savefig(fig)
607/1:
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf
607/2:
# load dataset
df = pd.read_csv('data/mushrooms.csv')
df.shape
607/3:
### check target
df['class'].value_counts()/len(df)
607/4:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
607/5:
cols = X_train.columns
X_train['class'] = y_train
X_train.head(5)
607/6:
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("countplots.pdf")
607/7:
for col in cols:
    fig, ax = plt.subplots(figsize=(15, 8))
    ax = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen'],
                       ax=ax
                      )

    for p in ax.patches:
        ax.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
    pdf.savefig(fig)
    
### close file
pdf.close()
607/8: X_train
607/9: y_train
607/10: X_train.drop('class', axis=1, inplace=True)
607/11: X_train
607/12: pd.get_dummies(X_train)
607/13:
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf


from sklearn.preprocessing import OneHotEncoder
607/14: ohe=OneHotEncoder()
607/15: ohe.fit_transform(X_train)
607/16: ohe.fit_transform(X_train).toarray()
607/17: ohe.categories_
607/18:
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder
607/19:
transformer = make_column_transformer(
    (OneHotEncoder(), X_train.columns),
    remainder='passthrough')
607/20:
transformed = transformer.fit_transform(X_train)
transformed_df = pd.DataFrame(transformed, columns=transformer.get_feature_names())
print(transformed_df.head())
608/1:
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder
608/2:
# load dataset
df = pd.read_csv('data/mushrooms.csv')
df.shape
608/3:
### check target
df['class'].value_counts()/len(df)
608/4:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
608/5:
cols = X_train.columns
X_train['class'] = y_train
X_train.head(5)
608/6:
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("countplots.pdf")
608/7:
for col in cols:
    fig, ax = plt.subplots(figsize=(15, 8))
    ax = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen'],
                       ax=ax
                      )

    for p in ax.patches:
        ax.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
    pdf.savefig(fig)
    
### close file
pdf.close()
608/8: ###
608/9: X_train.drop('class', axis=1, inplace=True)
608/10:  X_train.columns
608/11:  X_train.columns.to_list()
608/12:
transformer = make_column_transformer((OneHotEncoder(),  
                                       X_train.columns.to_list()), 
                                       remainder='passthrough')
608/13:
transformed = transformer.fit_transform(X_train)
transformed_df = pd.DataFrame(transformed, columns=transformer.get_feature_names())
print(transformed_df.head())
608/14: transformed
608/15: transformer.get_feature_names()
608/16: X_train
608/17:
transformer = make_column_transformer((OneHotEncoder(), ['island', 'sex']),
                                       remainder='passthrough')

transformed = transformer.fit_transform(X_train)
transformed_df = pd.DataFrame(transformed, columns=transformer.get_feature_names())
print(transformed_df.head())
608/18: X_train.columns
608/19:
transformer = make_column_transformer((OneHotEncoder(), ['cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor',
       'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',
       'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',
       'stalk-surface-below-ring', 'stalk-color-above-ring',
       'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number',
       'ring-type', 'spore-print-color', 'population', 'habitat']),
                                       remainder='passthrough')

transformed = transformer.fit_transform(X_train)
transformed_df = pd.DataFrame(transformed, columns=transformer.get_feature_names())
print(transformed_df.head())
608/20: transformer.get_feature_names()
608/21:
encoder = OneHotEncoder(categories='auto',
                        drop='first', # to return k-1, use drop=false to return k dummies
                        sparse=False,
                        handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train)
608/22: encoder.categories_
608/23: X_train_trans = encoder.transform(X_train)
608/24:
X_test_trans = encoder.transform(X_test)
X_test_trans = pd.DataFrame(X_test_trans)
X_test_trans.columns = encoder.get_feature_names()

X_test_trans.head()
608/25:
X_train_trans = encoder.transform(X_train)
X_train_trans = pd.DataFrame(X_train_trans)
X_train_trans.columns = encoder.get_feature_names()
608/26:
X_train_trans = encoder.transform(X_train)
X_train_trans = pd.DataFrame(X_train_trans)
X_train_trans.columns = encoder.get_feature_names()

X_train_trans.head(5)
608/27:
X_train_trans = encoder.transform(X_train)
X_train_trans = pd.DataFrame(X_train_trans)
X_train_trans.columns = encoder.get_feature_names_out()

X_train_trans.head(5)
608/28:
X_test_trans = encoder.transform(X_test)
X_test_trans = pd.DataFrame(X_test_trans)
X_test_trans.columns = encoder.get_feature_names_out()

X_test_trans.head()
608/29:
X_train_trans = encoder.transform(X_train)
X_train_trans = pd.DataFrame(X_train_trans)
X_train_trans.columns = encoder.get_feature_names_out()

X_train_trans.shape
608/30:
X_test_trans = encoder.transform(X_test)
X_test_trans = pd.DataFrame(X_test_trans)
X_test_trans.columns = encoder.get_feature_names_out()

X_test_trans.shape
608/31: X_train_trans
608/32:
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
608/33:
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from collections import defaultdict
608/34: d = defaultdict(LabelEncoder)
608/35:
# Encoding the variable
X_train_label_encode = X_train.apply(lambda x: d[x.name].fit_transform(X_train))

# # Using the dictionary to encode future data
X_test_label_encode = X_test.apply(lambda x: d[x.name].transform(X_test))
608/36: d = defaultdict(LabelEncoder)
608/37:
# Encoding the variable
X_train_label_encode = X_train.apply(lambda x: d[x.name].fit_transform(X_train))

# # Using the dictionary to encode future data
X_test_label_encode = X_test.apply(lambda x: d[x.name].transform(X_test))
608/38:
# Encoding the variable
X_train_label_encode = X_train.apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
X_test_label_encode = X_test.apply(lambda x: d[x.name].transform(x))
608/39: X_train_label_encode
608/40:
encoder = OneHotEncoder(categories='auto',
                        drop='first', # to return k-1, use drop=false to return k dummies
                        sparse=False,
                        handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train)
608/41:
X_train_trans = encoder.transform(X_train)
X_train_trans = pd.DataFrame(X_train_trans)
X_train_trans.columns = encoder.get_feature_names_out()

X_train_trans.shape
608/42:
# separate dataset into train and validation data set
X, X_val, y, y_val = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X.shape, X_val.shape
608/43:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    X,  # drop the target
    y,  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
608/44:
cols = X_train.columns
X_train['class'] = y_train
X_train.head(5)
608/45:
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("countplots.pdf")
608/46:
for col in cols:
    fig, ax = plt.subplots(figsize=(15, 8))
    ax = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen'],
                       ax=ax
                      )

    for p in ax.patches:
        ax.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
    pdf.savefig(fig)
    
### close file
pdf.close()
608/47: ###
608/48: X_train.drop('class', axis=1, inplace=True)
608/49:
encoder = OneHotEncoder(categories='auto',
                        drop='first', # to return k-1, use drop=false to return k dummies
                        sparse=False,
                        handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train)
608/50:
X_train_trans = encoder.transform(X_train)
X_train_trans = pd.DataFrame(X_train_trans)
X_train_trans.columns = encoder.get_feature_names_out()

X_train_trans.shape
608/51:
X_test_trans = encoder.transform(X_test)
X_test_trans = pd.DataFrame(X_test_trans)
X_test_trans.columns = encoder.get_feature_names_out()

X_test_trans.shape
608/52: X_train_trans.shape(5)
608/53: X_train_trans.head(5)
608/54: from sklearn.naive_bayes import GaussianNB
608/55:
gnb = GaussianNB()
gnb.fit(X_train, y_train)
608/56:
gnb = GaussianNB()
gnb.fit(X_train_trans, y_train)
608/57:
gnb = GaussianNB()
gnb.fit(X_train_trans, y_train)
# making predictions on the testing set
y_pred = gnb.predict(X_test)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print("Gaussian Naive Bayes model accuracy(in %):", metrics.accuracy_score(y_test, y_pred)*100)
608/58:
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from collections import defaultdict
608/59:
gnb = GaussianNB()
gnb.fit(X_train_trans, y_train)
# making predictions on the testing set
y_pred = gnb.predict(X_test_trans)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print("Gaussian Naive Bayes model accuracy(in %):", accuracy_score(y_test, y_pred)*100)
608/60:
gnb = GaussianNB()
gnb.fit(X_train_trans, y_train)
# making predictions on the testing set
y_pred = gnb.predict(X_test_trans)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test, y_pred)*100}:.2f')
608/61:
gnb = GaussianNB()
gnb.fit(X_train_trans, y_train)
# making predictions on the testing set
y_pred = gnb.predict(X_test_trans)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test, y_pred)*100:.2f}')
608/62:
gnb = GaussianNB()
gnb.fit(X_train_trans, y_train)
# making predictions on the testing set
y_pred = gnb.predict(X_test_trans)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test, y_pred)*100:.2f}')
print(confusion_matrix(y_test, y_pred))
608/63: d = defaultdict(LabelEncoder)
608/64:
# Encoding the variable
X_train_label_encode = X_train.apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
X_test_label_encode = X_test.apply(lambda x: d[x.name].transform(x))
608/65: X_train_label_encode
608/66: X_train_label_encode.shape
608/67: X_test_label_encode.shape
608/68:
gnb = GaussianNB()
gnb.fit(X_train_trans, y_train)
# making predictions on the testing set
y_pred = gnb.predict(X_test_trans)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test, y_pred)*100:.2f}')
print(confusion_matrix(y_test, y_pred))
print(f1_score(y_test, y_pred))
608/69: f1_score(y_test, y_pred)
608/70: f1_score(y_test, y_pred)
608/71:
gnb = GaussianNB()
gnb.fit(X_train_trans, y_train)
# making predictions on the testing set
y_pred = gnb.predict(X_test_trans)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test, y_pred)*100:.2f}')
print(confusion_matrix(y_test, y_pred))
608/72: f1_score(y_test, y_pred)
608/73: y_test
608/74: y_pred
612/1:
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from collections import defaultdict
612/2: from sklearn.naive_bayes import GaussianNB
612/3:
# load dataset
df = pd.read_csv('data/mushrooms.csv')
df.shape
612/4:
### check target
df['class'].value_counts()/len(df)
612/5:
# separate dataset into train and validation data set
X, X_val, y, y_val = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X.shape, X_val.shape
612/6:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    X,  # drop the target
    y,  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
612/7:
cols = X_train.columns
X_train['class'] = y_train
X_train.head(5)
612/8:
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("countplots.pdf")
612/9:
for col in cols:
    fig, ax = plt.subplots(figsize=(15, 8))
    ax = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen'],
                       ax=ax
                      )

    for p in ax.patches:
        ax.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
    pdf.savefig(fig)
    
### close file
pdf.close()
612/10: X_train.drop('class', axis=1, inplace=True)
612/11: target_dic = {'e':0, 'p': 1}
612/12: target_dic
612/13: target_dic[0]
612/14: target_dic['e']
612/15: y_train
612/16: y_train.map(target_dic)
612/17:
y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)
y_val_encode = y_val.map(target_dic)
612/18: y_train_encode
612/19: y_test_encode
612/20: y_val_encode
612/21:
encoder = OneHotEncoder(categories='auto',
                        drop='first', # to return k-1, use drop=false to return k dummies
                        sparse=False,
                        handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train)
612/22:
X_train_trans = encoder.transform(X_train)
X_train_trans = pd.DataFrame(X_train_trans)
X_train_trans.columns = encoder.get_feature_names_out()

X_train_trans.shape
612/23:
X_test_trans = encoder.transform(X_test)
X_test_trans = pd.DataFrame(X_test_trans)
X_test_trans.columns = encoder.get_feature_names_out()

X_test_trans.shape
612/24: X_train_trans.head(5)
612/25:
gnb = GaussianNB()
gnb.fit(X_train_trans, y_train)
# making predictions on the testing set
y_pred = gnb.predict(X_test_trans)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test, y_pred)*100:.2f}')
print(confusion_matrix(y_test, y_pred))
612/26: f1_score(y_test, y_pred)
612/27:
gnb = GaussianNB()
gnb.fit(X_train_trans, y_train_encode)
# making predictions on the testing set
y_pred = gnb.predict(X_test_trans)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test, y_pred)*100:.2f}')
print(confusion_matrix(y_test_encode, y_pred))
612/28:
gnb = GaussianNB()
gnb.fit(X_train_trans, y_train_encode)
# making predictions on the testing set
y_pred = gnb.predict(X_test_trans)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(confusion_matrix(y_test_encode, y_pred))
612/29: f1_score(y_test_encode, y_pred)
612/30:
gnb = GaussianNB()
gnb.fit(X_train_trans, y_train_encode)
# making predictions on the testing set
y_pred = gnb.predict(X_test_trans)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
613/1:
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from collections import defaultdict
613/2: from sklearn.naive_bayes import GaussianNB
613/3:
# load dataset
df = pd.read_csv('data/mushrooms.csv')
df.shape
613/4:
### check target
df['class'].value_counts()/len(df)
613/5:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
613/6:
cols = X_train.columns
X_train['class'] = y_train
X_train.head(5)
613/7:
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("countplots.pdf")
613/8:
for col in cols:
    fig, ax = plt.subplots(figsize=(15, 8))
    ax = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen'],
                       ax=ax
                      )

    for p in ax.patches:
        ax.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
    pdf.savefig(fig)
    
### close file
pdf.close()
613/9: X_train.drop('class', axis=1, inplace=True)
613/10: target_dic = {'e':0, 'p': 1}
613/11: ###
613/12:
encoder = OneHotEncoder(categories='auto',
                        drop='first', # to return k-1, use drop=false to return k dummies
                        sparse=False,
                        handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train)
613/13:
X_train_trans = encoder.transform(X_train)
X_train_trans = pd.DataFrame(X_train_trans)
X_train_trans.columns = encoder.get_feature_names_out()

X_train_trans.shape
613/14:
X_test_trans = encoder.transform(X_test)
X_test_trans = pd.DataFrame(X_test_trans)
X_test_trans.columns = encoder.get_feature_names_out()

X_test_trans.shape
613/15:
X_val_trans = encoder.transform(X_val)
X_val_trans = pd.DataFrame(X_test_trans)
X_val_trans.columns = encoder.get_feature_names_out()

X_test_trans.shape
613/16: X_train_trans.head(5)
613/17:
gnb = GaussianNB()
gnb.fit(X_train_trans, y_train_encode)
# making predictions on the testing set
y_pred = gnb.predict(X_test_trans)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
613/18: X_train.drop('class', axis=1, inplace=True)
614/1:
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from collections import defaultdict
614/2: from sklearn.naive_bayes import GaussianNB
614/3:
# load dataset
df = pd.read_csv('data/mushrooms.csv')
df.shape
614/4:
### check target
df['class'].value_counts()/len(df)
614/5:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
614/6:
cols = X_train.columns
X_train['class'] = y_train
X_train.head(5)
614/7:
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("countplots.pdf")
614/8:
for col in cols:
    fig, ax = plt.subplots(figsize=(15, 8))
    ax = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen'],
                       ax=ax
                      )

    for p in ax.patches:
        ax.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
    pdf.savefig(fig)
    
### close file
pdf.close()
614/9: X_train.drop('class', axis=1, inplace=True)
614/10: target_dic = {'e':0, 'p': 1}
614/11:
y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)
y_val_encode = y_val.map(target_dic)
614/12:
y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)
614/13: ###
614/14:
encoder = OneHotEncoder(categories='auto',
                        drop='first', # to return k-1, use drop=false to return k dummies
                        sparse=False,
                        handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train)
614/15:
X_train_trans = encoder.transform(X_train)
X_train_trans = pd.DataFrame(X_train_trans)
X_train_trans.columns = encoder.get_feature_names_out()

X_train_trans.shape
614/16:
X_test_trans = encoder.transform(X_test)
X_test_trans = pd.DataFrame(X_test_trans)
X_test_trans.columns = encoder.get_feature_names_out()

X_test_trans.shape
614/17:
X_val_trans = encoder.transform(X_val)
X_val_trans = pd.DataFrame(X_test_trans)
X_val_trans.columns = encoder.get_feature_names_out()

X_test_trans.shape
614/18: X_train_trans.head(5)
614/19:
gnb = GaussianNB()
gnb.fit(X_train_trans, y_train_encode)
# making predictions on the testing set
y_pred = gnb.predict(X_test_trans)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
614/20: d = defaultdict(LabelEncoder)
614/21:
# Encoding the variable
X_train_label_encode = X_train.apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
X_test_label_encode = X_test.apply(lambda x: d[x.name].transform(x))
614/22: X_train_label_encode.shape
614/23: X_test_label_encode.shape
614/24:
gnb = GaussianNB()
gnb.fit(X_train_label_encode, y_train_encode)
# making predictions on the testing set
y_pred = gnb.predict(X_test_label_encode)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
614/25: ### 3. Ordinal Encoding
614/26:
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder
from collections import defaultdict
614/27: ord_e = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.NaN)
614/28:
X_train_transform = ord_enc.fit_transform(X_train)
X_train_transform = pd.DataFrame(X_train_transform)
614/29: ord_enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.NaN)
614/30:
X_train_transform = ord_enc.fit_transform(X_train)
X_train_transform = pd.DataFrame(X_train_transform)
614/31: X_train_transform
614/32: X_test_transform
614/33:
X_test_transform = pd.DataFrame.from_records(ord_enc.transform(X_test))
#X_test_transform = X_test_transform.apply(lambda column: column.replace(np.NaN, max(column)+1), axis=0)
614/34: X_test_transform
614/35:
X_train_transform.columns = X_train.columns
X_test_transform.columns = X_test.columns
614/36: X_train_transform
614/37:
gnb = GaussianNB()
gnb.fit(X_train_transform, y_train_encode)
# making predictions on the testing set
y_pred = gnb.predict(X_test_transform)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
614/38: X_train_transform
614/39: X_train
614/40: frequency_map = (X_train['odor'].value_counts() / len(X_train) ).to_dict()
614/41: frequency_map
614/42: X_train
614/43: X_train_transform = X_train.copy()
614/44: X_train_transform
614/45: X_train_transform = X_train.copy()
614/46:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()
614/47:
for col in X_train_transform.columns:
    frequency_map = (X_train_transform[col].value_counts() / len(X_train_transform) ).to_dict()
    X_train_transform[col] = X_train_transform[col].map(frequency_map)
    X_test_transform[col] = X_test_transform[col].map(frequency_map)
614/48: X_train_transform
614/49: X_train_transform.describe()
614/50:
gnb = GaussianNB()
gnb.fit(X_train_transform, y_train_encode)
# making predictions on the testing set
y_pred = gnb.predict(X_test_transform)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
614/51:
for col in X_train_transform.columns:
    frequency_map = (X_train_transform[col].value_counts()).to_dict()
    X_train_transform[col] = X_train_transform[col].map(frequency_map)
    X_test_transform[col] = X_test_transform[col].map(frequency_map)
614/52:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()
614/53:
for col in X_train_transform.columns:
    frequency_map = (X_train_transform[col].value_counts()).to_dict()
    X_train_transform[col] = X_train_transform[col].map(frequency_map)
    X_test_transform[col] = X_test_transform[col].map(frequency_map)
614/54: X_train_transform
614/55:
gnb = GaussianNB()
gnb.fit(X_train_transform, y_train_encode)
# making predictions on the testing set
y_pred = gnb.predict(X_test_transform)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
614/56:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()
614/57:
for col in X_train_transform.columns:
    frequency_map = (X_train_transform[col].value_counts() / len(X_train_transform) ).to_dict()
    X_train_transform[col] = X_train_transform[col].map(frequency_map)
    X_test_transform[col] = X_test_transform[col].map(frequency_map)
614/58:
gnb = GaussianNB()
gnb.fit(X_train_transform, y_train_encode)
# making predictions on the testing set
y_pred = gnb.predict(X_test_transform)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
614/59: X_train_transform
614/60:
X_train['class'] = y_train
for col in X_train.columns:  
    fig = plt.figure()
    fig = X_train.groupby([var])['class'].mean().plot()
    fig.set_title(f'Relationship between {col} and Survival')
    fig.set_ylabel('Mean Survival')
    plt.show()
614/61:
X_train['class'] = y_train
for col in X_train.columns:  
    fig = plt.figure()
    fig = X_train.groupby([col])['class'].mean().plot()
    fig.set_title(f'Relationship between {col} and Survival')
    fig.set_ylabel('Mean Survival')
    plt.show()
614/62:
X_train['class'] = y_train_encode
for col in X_train.columns:  
    fig = plt.figure()
    fig = X_train.groupby([col])['class'].mean().plot()
    fig.set_title(f'Relationship between {col} and Survival')
    fig.set_ylabel('Mean Survival')
    plt.show()
614/63:
def find_category_mappings(df, variable, target):
    tmp = pd.DataFrame(df.groupby([variable])[target].mean())  
    tmp['non-target'] = 1 - tmp[target]  
    tmp['ratio'] = tmp[target] / tmp['non-target']
    return tmp['ratio'].to_dict()

def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
614/64: X_train.columns.to_list()
614/65: X_train.columns.to_list()[:-1]
614/66:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()
614/67:
def find_category_mappings(df, variable, target):
    tmp = pd.DataFrame(df.groupby([variable])[target].mean())  
    tmp['non-target'] = 1 - tmp[target]  
    tmp['ratio'] = tmp[target] / tmp['non-target']
    return tmp['ratio'].to_dict()

def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
614/68:
for col in X_train_transform.columns.to_list()[:-1]:    
    mappings = find_category_mappings(X_train_transform, variable, 'class')  
    integer_encode(X_train_transform, X_test_transform, variable, mappings)
614/69:
for col in X_train_transform.columns.to_list()[:-1]:    
    mappings = find_category_mappings(X_train_transform, col, 'class')  
    integer_encode(X_train_transform, X_test_transform, col, mappings)
614/70: X_train_transform
614/71: X_train.odor.value_counts()
614/72: X_train.groupby('obor')['class'].mean()
614/73: X_train.groupby(['obor'])['class'].mean()
614/74: X_train.groupby(['obor'])
614/75: X_train.groupby('obor')['class'].mean()
614/76: X_train
614/77: X_train.groupby('odor')['class'].mean()
614/78: X_train[X_train.odor == 'a']
614/79: X_train[X_train.cap-shape == 'a']
614/80: X_train[X_train['cap-shape'] == 'a']
614/81: X_train[X_train['cap-shape'] == 's']
614/82:
X_train['class'] = y_train_encode
for col in X_train.columns:  
    fig = plt.figure()
    fig = X_train.groupby([col])['class'].mean().plot()
    fig.set_title(f'Relationship between {col} and Poisonous')
    fig.set_ylabel('Mean poisonous')
    plt.show()
614/83: X_train[X_train.index == 7466]
614/84: X_train_transform
614/85:
X_train_transform.replace([np.inf, -np.inf], 1, inplace=True)
X_test_transform.replace([np.inf, -np.inf], 1, inplace=True)
614/86: X_train_transform
614/87: X_test_transform
614/88:
X_train_transform.replace([np.inf, -np.inf], 1, inplace=True)
X_train.drop('class', axis=1, inplace=True)
X_test_transform.replace([np.inf, -np.inf], 1, inplace=True)
614/89:
gnb = GaussianNB()
gnb.fit(X_train_transform, y_train_encode)
# making predictions on the testing set
y_pred = gnb.predict(X_test_transform)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
614/90:
X_train_transform.replace([np.inf, -np.inf], 1, inplace=True)
X_train_transform.drop('class', axis=1, inplace=True)
X_test_transform.replace([np.inf, -np.inf], 1, inplace=True)
614/91:
gnb = GaussianNB()
gnb.fit(X_train_transform, y_train_encode)
# making predictions on the testing set
y_pred = gnb.predict(X_test_transform)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
614/92:
X_train_transform['class'] = y_train_encode
for col in X_train_transform.columns:  
    fig = plt.figure()
    fig = X_train_transform.groupby([col])['class'].mean().plot()
    fig.set_title(f'Relationship between {col} and Poisonous')
    fig.set_ylabel('Mean poisonous')
    plt.show()
614/93:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()
614/94:
def find_category_mappings(df, variable, target):
    tmp = pd.DataFrame(df.groupby([variable])[target].mean())  
    tmp['non-target'] = 1 - tmp[target]  
    tmp['ratio'] = tmp[target] / tmp['non-target']
    return tmp['ratio'].to_dict()

def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
614/95:
for col in X_train_transform.columns.to_list()[:-1]:    
    mappings = find_category_mappings(X_train_transform, col, 'class')  
    integer_encode(X_train_transform, X_test_transform, col, mappings)
614/96:
X_train['class'] = y_train_encode
for col in X_train.columns:  
    fig = plt.figure()
    fig = X_train.groupby([col])['class'].mean().plot()
    fig.set_title(f'Relationship between {col} and Poisonous')
    fig.set_ylabel('Mean poisonous')
    plt.show()
614/97:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()
614/98:
def find_category_mappings(df, variable, target):
    tmp = pd.DataFrame(df.groupby([variable])[target].mean())  
    tmp['non-target'] = 1 - tmp[target]  
    tmp['ratio'] = tmp[target] / tmp['non-target']
    return tmp['ratio'].to_dict()

def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
614/99:
for col in X_train_transform.columns.to_list()[:-1]:    
    mappings = find_category_mappings(X_train_transform, col, 'class')  
    integer_encode(X_train_transform, X_test_transform, col, mappings)
614/100:
X_train_transform.replace([np.inf, -np.inf], 0, inplace=True)
X_train_transform.drop('class', axis=1, inplace=True)
X_test_transform.replace([np.inf, -np.inf], 0, inplace=True)
614/101:
gnb = GaussianNB()
gnb.fit(X_train_transform, y_train_encode)
# making predictions on the testing set
y_pred = gnb.predict(X_test_transform)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
614/102:
X_train_transform['class'] = y_train_encode
for col in X_train_transform.columns:  
    fig = plt.figure()
    fig = X_train_transform.groupby([col])['class'].mean().plot()
    fig.set_title(f'Relationship between {col} and Poisonous')
    fig.set_ylabel('Mean poisonous')
    plt.show()
614/103: X_train_transform
614/104: X_train_transform['gill-color']
614/105: X_train_transform['gill-color'].value_counts()
614/106: X_train_transform[X_train_transform['gill-color' == 0]]
614/107: X_train_transform[X_train_transform['gill-color'] == 0]
614/108: X_train
614/109: X_train.groupby('gill-color')
614/110: X_train.groupby('gill-color')['class'].mean()
614/111:
X_train['class'] = y_train_encode
for col in X_train.columns:  
    fig = plt.figure()
    fig = X_train.groupby([col])['class'].mean().plot()
    fig.set_title(f'Relationship between {col} and Poisonous')
    fig.set_ylabel('Mean poisonous')
    plt.show()
614/112:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()
614/113:
def find_category_mappings(df, variable, target):
    tmp = pd.DataFrame(df.groupby([variable])[target].mean())  
    tmp['non-target'] = 1 - tmp[target]  
    tmp['ratio'] = tmp[target] / tmp['non-target']
    return tmp['ratio'].to_dict()

def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
614/114:
for col in X_train_transform.columns.to_list()[:-1]:    
    mappings = find_category_mappings(X_train_transform, col, 'class')  
    integer_encode(X_train_transform, X_test_transform, col, mappings)
614/115: X_train_transform
614/116: X_train_transform.isna()
614/117: X_train_transform[X_train_transform.isna()]
614/118: X_train_transform.isna().sum()
614/119: X_train_transform[X_train_transform == np.inf]
614/120: X_train_transform[X_train_transform == np.inf]
614/121: X_train_transform[X_train_transform == np.inf].sum()
614/122: X_train_transform[X_train_transform == -np.inf].sum()
614/123:
X_train_transform.replace(np.inf, 1, inplace=True)
X_train_transform.drop('class', axis=1, inplace=True)
X_test_transform.replace(np.inf, 1, inplace=True)
614/124:
gnb = GaussianNB()
gnb.fit(X_train_transform, y_train_encode)
# making predictions on the testing set
y_pred = gnb.predict(X_test_transform)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
614/125:
X_train_transform['class'] = y_train_encode
for col in X_train_transform.columns:  
    fig = plt.figure()
    fig = X_train_transform.groupby([col])['class'].mean().plot()
    fig.set_title(f'Relationship between {col} and Poisonous')
    fig.set_ylabel('Mean poisonous')
    plt.show()
614/126: X_train.groupby('stalk-color-above-ring')['class'].mean()
614/127:
X_train['class'] = y_train_encode
for col in X_train.columns:  
    fig = plt.figure()
    fig = X_train.groupby([col])['class'].mean().plot()
    fig.set_title(f'Relationship between {col} and Poisonous')
    fig.set_ylabel('Mean poisonous')
    plt.show()
614/128:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()
614/129:
def find_category_mappings(df, variable, target):
    tmp = pd.DataFrame(df.groupby([variable])[target].mean())  
    tmp['non-target'] = 1 - tmp[target]  
    tmp['ratio'] = tmp[target] / tmp['non-target']
    return tmp['ratio'].to_dict()

def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
614/130:
for col in X_train_transform.columns.to_list()[:-1]:    
    mappings = find_category_mappings(X_train_transform, col, 'class')  
    integer_encode(X_train_transform, X_test_transform, col, mappings)
614/131:
X_train_transform.replace(np.inf, 1, inplace=True)
X_train_transform.drop('class', axis=1, inplace=True)
X_test_transform.replace(np.inf, 1, inplace=True)
614/132:
gnb = GaussianNB()
gnb.fit(X_train_transform, y_train_encode)
# making predictions on the testing set
y_pred = gnb.predict(X_test_transform)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
614/133:
def find_category_mappings(df, variable, target):
    df_copy = df.copy()
    
    # total positive class
    total_pos = df_copy[target].sum()
    
    # total negative class
    total_neg = len(df_copy) - df_copy[target].sum()

    # non target
    df_copy['non-target'] = 1 - df_copy[target]

    # % of positive class per category, respect to total positive class
    pos_perc = df_copy.groupby([variable])[target].sum() / total_pos

    # % of negative class per category, respect to total negative class
    neg_perc = df_copy.groupby([variable])['non-target'].sum() / total_neg

    # let's concatenate
    prob_tmp = pd.concat([pos_perc, neg_perc], axis=1)
    
    # let's calculate the Weight of Evidence
    prob_tmp['woe'] = np.log(prob_tmp[target]/prob_tmp['non-target'])
    return prob_tmp['woe'].to_dict()


def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
614/134: X_train['class'] = y_train_encode
614/135:
for col in X_test.columns.to_list():
    mappings = find_category_mappings(X_train, variable, 'class')
    integer_encode(X_train, X_test, variable, mappings)
614/136:
for col in X_test.columns.to_list():
    mappings = find_category_mappings(X_train, col, 'class')
    integer_encode(X_train, X_test, variable, mappings)
614/137:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()

X_train_transform['class'] = y_train_encode
614/138:
def find_category_mappings(df, variable, target):
    df_copy = df.copy()
    
    # total positive class
    total_pos = df_copy[target].sum()
    
    # total negative class
    total_neg = len(df_copy) - df_copy[target].sum()

    # non target
    df_copy['non-target'] = 1 - df_copy[target]

    # % of positive class per category, respect to total positive class
    pos_perc = df_copy.groupby([variable])[target].sum() / total_pos

    # % of negative class per category, respect to total negative class
    neg_perc = df_copy.groupby([variable])['non-target'].sum() / total_neg

    # let's concatenate
    prob_tmp = pd.concat([pos_perc, neg_perc], axis=1)
    
    # let's calculate the Weight of Evidence
    prob_tmp['woe'] = np.log(prob_tmp[target]/prob_tmp['non-target'])
    return prob_tmp['woe'].to_dict()


def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
614/139:
for col in X_test.columns.to_list():
    mappings = find_category_mappings(X_train_transform, col, 'class')
    integer_encode(X_train_transform, X_test_transform, col, mappings)
614/140: X_train_transform
614/141: X_test_transform
614/142:
X_train_transform.replace([np.inf, -np.inf], 0, inplace=True)
X_train_transform.drop('class', axis=1, inplace=True)
X_test_transform.replace([np.inf, -np.inf], 0, inplace=True)
614/143: X_train_transform
614/144: X_train_transform.shape
614/145:
gnb = GaussianNB()
gnb.fit(X_train_transform, y_train_encode)
# making predictions on the testing set
y_pred = gnb.predict(X_test_transform)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
614/146:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()


def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
614/147:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()

X_train_transform['class'] = y_train_encode
614/148:
for col in X_test.columns.to_list():
    mappings = X_train_transform(X_train, col, 'class')
    integer_encode(X_train_transform, X_test_transform, variable, mappings)
614/149:
for col in X_test.columns.to_list():
    mappings = find_category_mappings(X_train, col, 'class')
    integer_encode(X_train_transform, X_test_transform, variable, mappings)
614/150:
for col in X_test.columns.to_list():
    mappings = find_category_mappings(X_train, col, 'class')
    integer_encode(X_train_transform, X_test_transform, col, mappings)
614/151: X_train_transform
614/152: X_train_transform.drop('class', axis=1, inplace=True)
614/153:
gnb = GaussianNB()
gnb.fit(X_train_transform, y_train_encode)
# making predictions on the testing set
y_pred = gnb.predict(X_test_transform)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
615/1:
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder
from collections import defaultdict
615/2: from sklearn.naive_bayes import GaussianNB
615/3:
# load dataset
df = pd.read_csv('data/mushrooms.csv')
df.shape
615/4:
### check target
df['class'].value_counts()/len(df)
615/5:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
615/6:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
615/7:
cat_columns = X_train.columns
X_train['class'] = y_train
X_train.head(5)
615/8:
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("countplots.pdf")
615/9:
for col in cat_columns:
    fig, ax = plt.subplots(figsize=(15, 8))
    ax = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen'],
                       ax=ax
                      )

    for p in ax.patches:
        ax.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
    pdf.savefig(fig)
    
### close file
pdf.close()
615/10:
### Encode target
target_dic = {'e':0, 'p': 1}

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)
615/11:
encoder = OneHotEncoder(categories='auto',
                        drop='first', # to return k-1, use drop=false to return k dummies
                        sparse=False,
                        handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train)
615/12:
X_train_trans = encoder.transform(X_train)
X_train_trans = pd.DataFrame(X_train_trans)
X_train_trans.columns = encoder.get_feature_names_out()

X_train_trans.shape
615/13:
X_test_trans = encoder.transform(X_test)
X_test_trans = pd.DataFrame(X_test_trans)
X_test_trans.columns = encoder.get_feature_names_out()

X_test_trans.shape
615/14: X_train
615/15: metrics = pd.DataFrame(columns=['encoder' ,'Test acc', 'F1 score'])
615/16:
encoder = OneHotEncoder(categories='auto',
                        drop='first', # to return k-1, use drop=false to return k dummies
                        sparse=False,
                        handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train)
615/17: X_train
615/18: X_train.drop('class', axis=1, inplace=True)
615/19:
encoder = OneHotEncoder(categories='auto',
                        drop='first', # to return k-1, use drop=false to return k dummies
                        sparse=False,
                        handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train)
615/20:
X_train_trans = encoder.transform(X_train)
X_train_trans = pd.DataFrame(X_train_trans)
X_train_trans.columns = encoder.get_feature_names_out()

X_train_trans.shape
615/21:
X_test_trans = encoder.transform(X_test)
X_test_trans = pd.DataFrame(X_test_trans)
X_test_trans.columns = encoder.get_feature_names_out()

X_test_trans.shape
615/22: X_train_trans.head(5)
615/23: X_train.shape
615/24: X_train.shape[1]
615/25: metrics
615/26:
gaussian_model(encoder_name, X_train, X_test, y_train, y_test):
    global metrics
    # check shape
    if X_train.shape[1] == X_test.shape[1]:
        gnb = GaussianNB()
        gnb.fit(X_train, X_test)
        y_pred = gnb.predict(X_train)
        
        print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test, y_pred)*100:.2f}')
        print(f'F1 score: {f1_score(y_test, y_pred):.2f}')
        print(confusion_matrix(y_test, y_pred))
        
        metrics = metrics.append({
                              'encoder': encoder_name,
                              'Test acc': accuracy_score(y_test, y_pred)*100, 
                              'F1 score': f1_score(y_test, y_pred),
                             },
                               ignore_index=True
                            )
    else:
        print('The shape of train and test data set is not similar')
615/27:
def gaussian_model(encoder_name, X_train, X_test, y_train, y_test):
    global metrics
    # check shape
    if X_train.shape[1] == X_test.shape[1]:
        gnb = GaussianNB()
        gnb.fit(X_train, X_test)
        y_pred = gnb.predict(X_train)
        
        print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test, y_pred)*100:.2f}')
        print(f'F1 score: {f1_score(y_test, y_pred):.2f}')
        print(confusion_matrix(y_test, y_pred))
        
        metrics = metrics.append({
                              'encoder': encoder_name,
                              'Test acc': accuracy_score(y_test, y_pred)*100, 
                              'F1 score': f1_score(y_test, y_pred),
                             },
                               ignore_index=True
                            )
    else:
        print('The shape of train and test data set is not similar')
615/28: gaussian_model('One hot', X_train_trans, X_test_trans, y_train_encode, y_test_encode)
615/29:
def gaussian_model(encoder_name, X_train, X_test, y_train, y_test):
    global metrics
    # check shape
    if X_train.shape[1] == X_test.shape[1]:
        gnb = GaussianNB()
        gnb.fit(X_train, y_train)
        y_pred = gnb.predict(X_train)
        
        print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test, y_pred)*100:.2f}')
        print(f'F1 score: {f1_score(y_test, y_pred):.2f}')
        print(confusion_matrix(y_test, y_pred))
        
        metrics = metrics.append({
                              'encoder': encoder_name,
                              'Test acc': accuracy_score(y_test, y_pred)*100, 
                              'F1 score': f1_score(y_test, y_pred),
                             },
                               ignore_index=True
                            )
    else:
        print('The shape of train and test data set is not similar')
615/30: gaussian_model('One hot', X_train_trans, X_test_trans, y_train_encode, y_test_encode)
615/31:
def gaussian_model(encoder_name, X_train, X_test, y_train, y_test):
    global metrics
    # check shape
    if X_train.shape[1] == X_test.shape[1]:
        gnb = GaussianNB()
        gnb.fit(X_train, y_train)
        y_pred = gnb.predict(X_test)
        
        print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test, y_pred)*100:.2f}')
        print(f'F1 score: {f1_score(y_test, y_pred):.2f}')
        print(confusion_matrix(y_test, y_pred))
        
        metrics = metrics.append({
                              'encoder': encoder_name,
                              'Test acc': accuracy_score(y_test, y_pred)*100, 
                              'F1 score': f1_score(y_test, y_pred),
                             },
                               ignore_index=True
                            )
    else:
        print('The shape of train and test data set is not similar')
615/32: gaussian_model('One hot', X_train_trans, X_test_trans, y_train_encode, y_test_encode)
615/33:
gnb = GaussianNB()
gnb.fit(X_train_trans, y_train_encode)
# making predictions on the testing set
y_pred = gnb.predict(X_test_trans)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
615/34: metrics
615/35: metrics
615/36: df_result = pd.DataFrame(columns=['encoder' ,'Test acc', 'F1 score'])
615/37: X_train.drop('class', axis=1, inplace=True)
615/38:
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder
from collections import defaultdict
615/39: from sklearn.naive_bayes import GaussianNB
615/40:
# load dataset
df = pd.read_csv('data/mushrooms.csv')
df.shape
615/41:
### check target
df['class'].value_counts()/len(df)
### it's a balanced data set
615/42:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
615/43:
cat_columns = X_train.columns
X_train['class'] = y_train
X_train.head(5)
615/44:
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("countplots.pdf")
615/45:
for col in cat_columns:
    fig, ax = plt.subplots(figsize=(15, 8))
    ax = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen'],
                       ax=ax
                      )

    for p in ax.patches:
        ax.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
    pdf.savefig(fig)
    
### close file
pdf.close()
615/46:
### Encode target
target_dic = {'e':0, 'p': 1}

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)
615/47: df_result = pd.DataFrame(columns=['encoder' ,'Test acc', 'F1 score'])
615/48: X_train.drop('class', axis=1, inplace=True)
615/49:
encoder = OneHotEncoder(categories='auto',
                        drop='first', # to return k-1, use drop=false to return k dummies
                        sparse=False,
                        handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train)
615/50:
X_train_trans = encoder.transform(X_train)
X_train_trans = pd.DataFrame(X_train_trans)
X_train_trans.columns = encoder.get_feature_names_out()

X_train_trans.shape
615/51:
X_test_trans = encoder.transform(X_test)
X_test_trans = pd.DataFrame(X_test_trans)
X_test_trans.columns = encoder.get_feature_names_out()

X_test_trans.shape
615/52: df_result
615/53:
def gaussian_model(encoder_name, X_train, X_test, y_train, y_test):
    global metrics
    # check shape
    if X_train.shape[1] == X_test.shape[1]:
        gnb = GaussianNB()
        gnb.fit(X_train, y_train)
        y_pred = gnb.predict(X_test)
        
        print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test, y_pred)*100:.2f}')
        print(f'F1 score: {f1_score(y_test, y_pred):.2f}')
        print(confusion_matrix(y_test, y_pred))
        
        df_result = df_result.append({
                              'encoder': encoder_name,
                              'Test acc': accuracy_score(y_test, y_pred)*100, 
                              'F1 score': f1_score(y_test, y_pred),
                             },
                               ignore_index=True
                            )
    else:
        print('The shape of train and test data set is not similar')
615/54: gaussian_model('One hot', X_train_trans, X_test_trans, y_train_encode, y_test_encode)
615/55:
def gaussian_model(encoder_name, X_train, X_test, y_train, y_test):
    global df_result
    # check shape
    if X_train.shape[1] == X_test.shape[1]:
        gnb = GaussianNB()
        gnb.fit(X_train, y_train)
        y_pred = gnb.predict(X_test)
        
        print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test, y_pred)*100:.2f}')
        print(f'F1 score: {f1_score(y_test, y_pred):.2f}')
        print(confusion_matrix(y_test, y_pred))
        
        df_result = df_result.append({
                              'encoder': encoder_name,
                              'Test acc': accuracy_score(y_test, y_pred)*100, 
                              'F1 score': f1_score(y_test, y_pred),
                             },
                               ignore_index=True
                            )
    else:
        print('The shape of train and test data set is not similar')
615/56: gaussian_model('One hot', X_train_trans, X_test_trans, y_train_encode, y_test_encode)
615/57: df_result
615/58:
import numpy as np
import pandas as pd
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder
from collections import defaultdict
615/59: from sklearn.naive_bayes import GaussianNB
615/60:
# load dataset
df = pd.read_csv('data/mushrooms.csv')
df.shape
615/61:
### check target
df['class'].value_counts()/len(df)
### it's a balanced data set
615/62:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
615/63:
cat_columns = X_train.columns
X_train['class'] = y_train
X_train.head(5)
615/64:
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("countplots.pdf")
615/65:
for col in cat_columns:
    fig, ax = plt.subplots(figsize=(15, 8))
    ax = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen'],
                       ax=ax
                      )

    for p in ax.patches:
        ax.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
    pdf.savefig(fig)
    
### close file
pdf.close()
615/66:
### Encode target
target_dic = {'e':0, 'p': 1}

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)
615/67: df_result = pd.DataFrame(columns=['encoder' ,'Test acc', 'F1 score'])
615/68: X_train.drop('class', axis=1, inplace=True)
615/69:
encoder = OneHotEncoder(categories='auto',
                        drop='first', # to return k-1, use drop=false to return k dummies
                        sparse=False,
                        handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train)
615/70:
X_train_trans = encoder.transform(X_train)
X_train_trans = pd.DataFrame(X_train_trans)
X_train_trans.columns = encoder.get_feature_names_out()

X_train_trans.shape
615/71:
X_test_trans = encoder.transform(X_test)
X_test_trans = pd.DataFrame(X_test_trans)
X_test_trans.columns = encoder.get_feature_names_out()

X_test_trans.shape
615/72: df_result
615/73:
def gaussian_model(encoder_name, X_train, X_test, y_train, y_test):
    global df_result
    # check shape
    if X_train.shape[1] == X_test.shape[1]:
        gnb = GaussianNB()
        gnb.fit(X_train, y_train)
        y_pred = gnb.predict(X_test)
        
        print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test, y_pred)*100:.2f}')
        print(f'F1 score: {f1_score(y_test, y_pred):.2f}')
        print(confusion_matrix(y_test, y_pred))
        
        df_result = df_result.append({
                              'encoder': encoder_name,
                              'Test acc': accuracy_score(y_test, y_pred)*100, 
                              'F1 score': f1_score(y_test, y_pred),
                             },
                               ignore_index=True
                            )
    else:
        print('The shape of train and test data set is not similar')
615/74: gaussian_model('One hot', X_train_trans, X_test_trans, y_train_encode, y_test_encode)
615/75: df_result
615/76: d = defaultdict(LabelEncoder)
615/77:
# Encoding the variable
X_train_label_encode = X_train.apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
X_test_label_encode = X_test.apply(lambda x: d[x.name].transform(x))
615/78: gaussian_model('Label Encoding', X_train_label_encode, X_test_label_encode, y_train_encode, y_test_encode)
615/79:
gnb = GaussianNB()
gnb.fit(X_train_label_encode, y_train_encode)
# making predictions on the testing set
y_pred = gnb.predict(X_test_label_encode)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
615/80: ord_enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.NaN)
615/81:
X_train_transform = ord_enc.fit_transform(X_train)
X_train_transform = pd.DataFrame(X_train_transform)
X_test_transform = pd.DataFrame.from_records(ord_enc.transform(X_test))
615/82:
X_train_transform.columns = X_train.columns
X_test_transform.columns = X_test.columns
615/83: gaussian_model('Ordinal Encoding', X_train_label_encode, X_test_label_encode, y_train_encode, y_test_encode)
615/84:
gnb = GaussianNB()
gnb.fit(X_train_transform, y_train_encode)
# making predictions on the testing set
y_pred = gnb.predict(X_test_transform)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
615/85:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()
615/86:
for col in X_train_transform.columns:
    frequency_map = (X_train_transform[col].value_counts() / len(X_train_transform) ).to_dict()
    X_train_transform[col] = X_train_transform[col].map(frequency_map)
    X_test_transform[col] = X_test_transform[col].map(frequency_map)
615/87: gaussian_model('Count and Frequency', X_train_transform, X_test_transform, y_train_encode, y_test_encode)
615/88:
gnb = GaussianNB()
gnb.fit(X_train_transform, y_train_encode)
# making predictions on the testing set
y_pred = gnb.predict(X_test_transform)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
615/89:
X_train['class'] = y_train_encode
for col in X_train.columns:  
    fig = plt.figure()
    fig = X_train.groupby([col])['class'].mean().plot()
    fig.set_title(f'Relationship between {col} and Poisonous')
    fig.set_ylabel('Mean poisonous')
    plt.show()
615/90:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()
615/91:
def find_category_mappings(df, variable, target):
    tmp = pd.DataFrame(df.groupby([variable])[target].mean())  
    tmp['non-target'] = 1 - tmp[target]  
    tmp['ratio'] = tmp[target] / tmp['non-target']
    return tmp['ratio'].to_dict()

def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
615/92:
for col in X_train_transform.columns.to_list()[:-1]:    
    mappings = find_category_mappings(X_train_transform, col, 'class')  
    integer_encode(X_train_transform, X_test_transform, col, mappings)
615/93:
X_train_transform.replace(np.inf, 1, inplace=True)
X_train_transform.drop('class', axis=1, inplace=True)
X_test_transform.replace(np.inf, 1, inplace=True)
615/94: gaussian_model('Count and Frequency', X_train_transform, X_test_transform, y_train_encode, y_test_encode)
615/95: gaussian_model('Probability Ratio Encoding', X_train_transform, X_test_transform, y_train_encode, y_test_encode)
615/96:
gnb = GaussianNB()
gnb.fit(X_train_transform, y_train_encode)
# making predictions on the testing set
y_pred = gnb.predict(X_test_transform)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
615/97:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()

X_train_transform['class'] = y_train_encode
615/98:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()


def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
615/99:
for col in X_train.columns.to_list():
    mappings = find_category_mappings(X_train, col, 'class')
    integer_encode(X_train_transform, X_test_transform, col, mappings)
615/100:
for col in cat_columns:
    mappings = find_category_mappings(X_train_transform, col, 'class')
    integer_encode(X_train_transform, X_test_transform, col, mappings)
615/101: X_train_transform.drop('class', axis=1, inplace=True)
615/102: gaussian_model('Mean Encoding', X_train_transform, X_test_transform, y_train_encode, y_test_encode)
615/103:
gnb = GaussianNB()
gnb.fit(X_train_transform, y_train_encode)
# making predictions on the testing set
y_pred = gnb.predict(X_test_transform)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
615/104:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()

X_train_transform['class'] = y_train_encode
615/105:
def find_category_mappings(df, variable, target):
    df_copy = df.copy()
    
    # total positive class
    total_pos = df_copy[target].sum()
    
    # total negative class
    total_neg = len(df_copy) - df_copy[target].sum()

    # non target
    df_copy['non-target'] = 1 - df_copy[target]

    # % of positive class per category, respect to total positive class
    pos_perc = df_copy.groupby([variable])[target].sum() / total_pos

    # % of negative class per category, respect to total negative class
    neg_perc = df_copy.groupby([variable])['non-target'].sum() / total_neg

    # let's concatenate
    prob_tmp = pd.concat([pos_perc, neg_perc], axis=1)
    
    # let's calculate the Weight of Evidence
    prob_tmp['woe'] = np.log(prob_tmp[target]/prob_tmp['non-target'])
    return prob_tmp['woe'].to_dict()


def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
615/106:
for col in X_test.columns.to_list():
    mappings = find_category_mappings(X_train_transform, col, 'class')
    integer_encode(X_train_transform, X_test_transform, col, mappings)
615/107:
X_train_transform.replace([np.inf, -np.inf], 0, inplace=True)
X_train_transform.drop('class', axis=1, inplace=True)
X_test_transform.replace([np.inf, -np.inf], 0, inplace=True)
615/108: X_train_transform.shape
615/109: gaussian_model('Weight of evidence', X_train_transform, X_test_transform, y_train_encode, y_test_encode)
615/110:
gnb = GaussianNB()
gnb.fit(X_train_transform, y_train_encode)
# making predictions on the testing set
y_pred = gnb.predict(X_test_transform)
  
# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
615/111: df_result
616/1:
import numpy as np
import pandas as pd
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder
from collections import defaultdict
616/2: from sklearn.naive_bayes import GaussianNB
616/3:
# load dataset
df = pd.read_csv('data/mushrooms.csv')
df.shape
616/4:
### check target
df['class'].value_counts()/len(df)
### it's a balanced data set
616/5:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
616/6:
cat_columns = X_train.columns
X_train['class'] = y_train
X_train.head(5)
616/7:
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("countplots.pdf")
616/8:
for col in cat_columns:
    fig, ax = plt.subplots(figsize=(15, 8))
    ax = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen'],
                       ax=ax
                      )

    for p in ax.patches:
        ax.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
    pdf.savefig(fig)
    
### close file
pdf.close()
616/9:
### Encode target
target_dic = {'e':0, 'p': 1}

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)
616/10: df_result = pd.DataFrame(columns=['encoder' ,'Test acc', 'F1 score'])
616/11: X_train.drop('class', axis=1, inplace=True)
616/12:
encoder = OneHotEncoder(categories='auto',
                        drop='first', # to return k-1, use drop=false to return k dummies
                        sparse=False,
                        handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train)
616/13:
X_train_trans = encoder.transform(X_train)
X_train_trans = pd.DataFrame(X_train_trans)
X_train_trans.columns = encoder.get_feature_names_out()

X_train_trans.shape
616/14:
X_test_trans = encoder.transform(X_test)
X_test_trans = pd.DataFrame(X_test_trans)
X_test_trans.columns = encoder.get_feature_names_out()

X_test_trans.shape
616/15:
X_train_trans = pd.DataFrame(encoder.transform(X_train))
X_train_trans.columns = encoder.get_feature_names_out()

X_train_trans.shape
616/16:
X_train_transform = pd.DataFrame(encoder.transform(X_train))
X_test_transform = pd.DataFrame(encoder.transform(X_test))
X_train_trans.columns, X_test_transform.columns = encoder.get_feature_names_out()

X_train_trans.shape
616/17:
X_train_transform = pd.DataFrame(encoder.transform(X_train))
X_test_transform = pd.DataFrame(encoder.transform(X_test))
X_train_trans.columns, X_test_transform.columns = encoder.get_feature_names_out(), encoder.get_feature_names_out()

X_train_trans.shape
616/18: X_train_trans
616/19: X_train_transform
616/20:
X_train_transform = pd.DataFrame(encoder.transform(X_train))
X_test_transform = pd.DataFrame(encoder.transform(X_test))
X_train_transform.columns = encoder.get_feature_names_out()
X_test_transform.columns = encoder.get_feature_names_out()

X_train_transform.shape, X_test_transform.shape
616/21:
def gaussian_model(encoder_name, X_train, X_test, y_train, y_test):
    global df_result
    # check shape
    if X_train.shape[1] == X_test.shape[1]:
        gnb = GaussianNB()
        gnb.fit(X_train, y_train)
        y_pred = gnb.predict(X_test)
        
        print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test, y_pred)*100:.2f}')
        print(f'F1 score: {f1_score(y_test, y_pred):.2f}')
        print(confusion_matrix(y_test, y_pred))
        
        df_result = df_result.append({
                              'encoder': encoder_name,
                              'Test acc': accuracy_score(y_test, y_pred)*100, 
                              'F1 score': f1_score(y_test, y_pred),
                             },
                               ignore_index=True
                            )
    else:
        print('The shape of train and test data set is not similar')
616/22: gaussian_model('One hot', X_train_trans, X_test_trans, y_train_encode, y_test_encode)
616/23: d = defaultdict(LabelEncoder)
616/24:
# Encoding the variable
X_train_label_encode = X_train.apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
X_test_label_encode = X_test.apply(lambda x: d[x.name].transform(x))
616/25: gaussian_model('Label Encoding', X_train_label_encode, X_test_label_encode, y_train_encode, y_test_encode)
617/1:
import numpy as np
import pandas as pd
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder
from collections import defaultdict
617/2: from sklearn.naive_bayes import GaussianNB
617/3:
# load dataset
df = pd.read_csv('data/mushrooms.csv')
df.shape
617/4:
### check target
df['class'].value_counts()/len(df)
### it's a balanced data set
617/5:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
617/6:
cat_columns = X_train.columns
X_train['class'] = y_train
X_train.head(5)
617/7:
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("countplots.pdf")
617/8:
for col in cat_columns:
    fig, ax = plt.subplots(figsize=(15, 8))
    ax = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen'],
                       ax=ax
                      )

    for p in ax.patches:
        ax.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
    pdf.savefig(fig)
    
### close file
pdf.close()
617/9:
### Encode target
target_dic = {'e':0, 'p': 1}

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)
617/10: df_result = pd.DataFrame(columns=['encoder' ,'Test acc', 'F1 score'])
617/11: X_train.drop('class', axis=1, inplace=True)
617/12:
encoder = OneHotEncoder(categories='auto',
                        drop='first', # to return k-1, use drop=false to return k dummies
                        sparse=False,
                        handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train)
617/13:
X_train_transform = pd.DataFrame(encoder.transform(X_train))
X_test_transform = pd.DataFrame(encoder.transform(X_test))
X_train_transform.columns = encoder.get_feature_names_out()
X_test_transform.columns = encoder.get_feature_names_out()

X_train_transform.shape, X_test_transform.shape
617/14: X_train_transform.head(5)
617/15:
def gaussian_model(encoder_name, X_train, X_test, y_train, y_test):
    global df_result
    # check shape
    if X_train.shape[1] == X_test.shape[1]:
        gnb = GaussianNB()
        gnb.fit(X_train, y_train)
        y_pred = gnb.predict(X_test)
        
        print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test, y_pred)*100:.2f}')
        print(f'F1 score: {f1_score(y_test, y_pred):.2f}')
        print(confusion_matrix(y_test, y_pred))
        
        df_result = df_result.append({
                              'encoder': encoder_name,
                              'Test acc': accuracy_score(y_test, y_pred)*100, 
                              'F1 score': f1_score(y_test, y_pred),
                             },
                               ignore_index=True
                            )
    else:
        print('The shape of train and test data set is not similar')
617/16: gaussian_model('One hot', X_train_trans, X_test_trans, y_train_encode, y_test_encode)
617/17: gaussian_model('One hot', X_train_transform, X_test_transform, y_train_encode, y_test_encode)
617/18: d = defaultdict(LabelEncoder)
617/19:
# Encoding the variable
X_train_label_encode = X_train.apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
X_test_label_encode = X_test.apply(lambda x: d[x.name].transform(x))
617/20: X_train_label_encode.head(5)
617/21: gaussian_model('Label Encoding', X_train_label_encode, X_test_label_encode, y_train_encode, y_test_encode)
617/22: ord_enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.NaN)
617/23:
X_train_transform = ord_enc.fit_transform(X_train)
X_train_transform = pd.DataFrame(X_train_transform)
X_test_transform = pd.DataFrame.from_records(ord_enc.transform(X_test))
617/24:
X_train_transform.columns = X_train.columns
X_test_transform.columns = X_test.columns
617/25: gaussian_model('Ordinal Encoding', X_train_label_encode, X_test_label_encode, y_train_encode, y_test_encode)
617/26: X_train_transform.head(5)
617/27: X_train_transform.describe()
617/28: X_train_label_encode.describe()
617/29:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()
617/30:
for col in X_train_transform.columns:
    frequency_map = (X_train_transform[col].value_counts() / len(X_train_transform) ).to_dict()
    X_train_transform[col] = X_train_transform[col].map(frequency_map)
    X_test_transform[col] = X_test_transform[col].map(frequency_map)
617/31: gaussian_model('Count and Frequency', X_train_transform, X_test_transform, y_train_encode, y_test_encode)
617/32: X_train_transform.describe()
617/33:
X_train['class'] = y_train_encode
for col in X_train.columns:  
    fig = plt.figure()
    fig = X_train.groupby([col])['class'].mean().plot()
    fig.set_title(f'Relationship between {col} and Poisonous')
    fig.set_ylabel('Mean poisonous')
    plt.show()
617/34:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()
617/35:
def find_category_mappings(df, variable, target):
    tmp = pd.DataFrame(df.groupby([variable])[target].mean())  
    tmp['non-target'] = 1 - tmp[target]  
    tmp['ratio'] = tmp[target] / tmp['non-target']
    return tmp['ratio'].to_dict()

def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
617/36:
for col in X_train_transform.columns.to_list()[:-1]:    
    mappings = find_category_mappings(X_train_transform, col, 'class')  
    integer_encode(X_train_transform, X_test_transform, col, mappings)
617/37:
X_train_transform.replace(np.inf, 1, inplace=True)
X_train_transform.drop('class', axis=1, inplace=True)
X_test_transform.replace(np.inf, 1, inplace=True)
617/38: X_train_transform.describe()
617/39: gaussian_model('Probability Ratio Encoding', X_train_transform, X_test_transform, y_train_encode, y_test_encode)
617/40:
X_train_transform['class'] = y_train_encode
for col in X_train_transform.columns:  
    fig = plt.figure()
    fig = X_train_transform.groupby([col])['class'].mean().plot()
    fig.set_title(f'Relationship between {col} and Poisonous')
    fig.set_ylabel('Mean poisonous')
    plt.show()
617/41: X_train
617/42: X_train.groupby('cap-shape')['class'].mean()
617/43: X_train[X_train['cap-shape'] == 1]
617/44: X_train[X_train['cap-shape'] == 'c']
617/45: ### The function has a break at 1 when one of the feature classes contains only poisonous mushrooms
617/46:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()

X_train_transform['class'] = y_train_encode
617/47:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()


def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
617/48:
for col in cat_columns:
    mappings = find_category_mappings(X_train_transform, col, 'class')
    integer_encode(X_train_transform, X_test_transform, col, mappings)
617/49: X_train_transform.drop('class', axis=1, inplace=True)
617/50: gaussian_model('Mean Encoding', X_train_transform, X_test_transform, y_train_encode, y_test_encode)
617/51:
X_train_transform['class'] = y_train_encode
for col in X_train_transform.columns:  
    fig = plt.figure()
    fig = X_train_transform.groupby([col])['class'].mean().plot()
    fig.set_title(f'Relationship between {col} and Poisonous')
    fig.set_ylabel('Mean poisonous')
    plt.show()
617/52:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()

X_train_transform['class'] = y_train_encode
617/53:
def find_category_mappings(df, variable, target):
    df_copy = df.copy()
    
    # total positive class
    total_pos = df_copy[target].sum()
    
    # total negative class
    total_neg = len(df_copy) - df_copy[target].sum()

    # non target
    df_copy['non-target'] = 1 - df_copy[target]

    # % of positive class per category, respect to total positive class
    pos_perc = df_copy.groupby([variable])[target].sum() / total_pos

    # % of negative class per category, respect to total negative class
    neg_perc = df_copy.groupby([variable])['non-target'].sum() / total_neg

    # let's concatenate
    prob_tmp = pd.concat([pos_perc, neg_perc], axis=1)
    
    # let's calculate the Weight of Evidence
    prob_tmp['woe'] = np.log(prob_tmp[target]/prob_tmp['non-target'])
    return prob_tmp['woe'].to_dict()


def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
617/54:
for col in cat_columns:
    mappings = find_category_mappings(X_train_transform, col, 'class')
    integer_encode(X_train_transform, X_test_transform, col, mappings)
617/55:
X_train_transform.replace([np.inf, -np.inf], 0, inplace=True)
X_train_transform.drop('class', axis=1, inplace=True)
X_test_transform.replace([np.inf, -np.inf], 0, inplace=True)
617/56: X_train_transform.shape
617/57: X_train_transform.describe()
617/58: gaussian_model('Weight of evidence', X_train_transform, X_test_transform, y_train_encode, y_test_encode)
617/59:
X_train_transform['class'] = y_train_encode
for col in X_train_transform.columns:  
    fig = plt.figure()
    fig = X_train_transform.groupby([col])['class'].mean().plot()
    fig.set_title(f'Relationship between {col} and Poisonous')
    fig.set_ylabel('Mean poisonous')
    plt.show()
617/60: df_result
617/61: df_result.sort_values('F1 score')
617/62: df_result.sort_values('F1 score', ascending=cFalse)
617/63: df_result.sort_values('F1 score', ascending=false)
617/64: df_result.sort_values('F1 score', ascending=False)
617/65: import category_encoders as ce
617/66: !conda install category_encoders
617/67: !conda install -c conda-forge category_encoders
617/68: import category_encoders as ce
617/69: df_result.sort_values('F1 score', ascending=False)
617/70:

encoder = ce.LeaveOneOutEncoder(return_df=True)
X_train_loo = encoder.fit_transform(X_train, y_train)
X_train_loo.dtypes
617/71: X_train
617/72:

encoder = ce.LeaveOneOutEncoder(return_df=True)
X_train_loo = encoder.fit_transform(X_train, y_train_encode)
X_train_loo.dtypes
617/73: X_train_loo
617/74:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()

X_train_transform['class'] = y_train_encode
617/75:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()


def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
617/76:
for col in cat_columns:
    mappings = find_category_mappings(X_train_transform, col, 'class')
    integer_encode(X_train_transform, X_test_transform, col, mappings)
617/77: X_train_transform.drop('class', axis=1, inplace=True)
617/78: X_train_transform
617/79:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()

X_train_transform['class'] = y_train_encode
617/80: X_train_loo
617/81: encoder
617/82: encoder.get_params()
617/83: encoder.transform_leave_one_out
617/84: encoder.fit_column_map
617/85: encoder.get_params
617/86: encoder.get_params[0]
617/87: encoder.get_params()
617/88: encoder.transform(X_test)
617/89: encoder.transform(X_test_transform)
617/90: X_test
617/91: X_train
617/92: X_train.drop('class', axis=1, inplace=True)
617/93:

encoder = ce.LeaveOneOutEncoder(return_df=True)
X_train_loo = encoder.fit_transform(X_train, y_train_encode)
X_train_loo.dtypes
617/94: encoder.transform(X_test)
617/95: X_test
617/96: X_train.groupby('cap-shape')
617/97:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()

X_train_transform['class'] = y_train_encode
617/98: X_train_transform.groupby('cap-shape')['class'].mean()
617/99: X_train_transform
617/100: X_train_transform.head(2)
617/101: X_train_transform.head(3)
617/102: X_train_transform[X_train_transform['cap-shape'] != 'x'].groupby('cap-shape')['class'].mean()
617/103: X_train_transform[X_train_transform['cap-shape'] == 'x'].groupby('cap-shape')['class'].mean()
617/104: X_train_loo
617/105: X_train_loo.groupby('cap-shape')['cap-shape'].count()
617/106: X_train_transform.groupby('cap-shape')['cap-shape'].count()
617/107: import category_encoders as ce
617/108:

encoder = ce.LeaveOneOutEncoder(return_df=True)
X_train_loo = encoder.fit_transform(X_train, y_train_encode)
X_train_loo.dtypes
617/109: encoder.transform(X_test)
617/110: X_test_loo = encoder.transform(X_test)
617/111: X_test_loo
617/112: X_test_loo['cap-shape'].value_counts()
617/113: X_train_loo['cap-shape'].value_counts()
617/114: X_train
617/115: X_train_transform
617/116: X_test
617/117: X_test[X_test['cap-shape' == 'k']]
617/118: X_test[X_test['cap-shape'] == 'k']
617/119: idx = X_test[X_test['cap-shape'] == 'k'].index
617/120: X_test[X_test['cap-shape'] == 'k']
617/121: X_test[X_test.index == idx]
617/122: X_test_loo[X_test_loo.index == idx]
617/123: X_test_loo.index == idx
617/124: X_test_loo.index
617/125: X_test_loo.index.isin(idx)
617/126: X_test_loo[X_test_loo.index.isin(idx)]
617/127: X_train.groupby['cap-shape']['class'].mean()
617/128:
X_train['class'] = y_test_encode
X_train.groupby['cap-shape']['class'].mean()
617/129:
X_train['class'] = y_test_encode
X_train.groupby('cap-shape')['class'].mean()
617/130: X_train
617/131: y_test_encode
617/132:
X_train['class'] = y_test_encode
X_train.groupby('cap-shape')['class'].mean()
617/133: X_train['class'] = y_test_encode
617/134:
X_train['class'] = y_test_encode
X_train
617/135:
X_train['class'] = y_test_encode
y_test_encode
618/1:
import numpy as np
import pandas as pd
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder
from collections import defaultdict
618/2: from sklearn.naive_bayes import GaussianNB
618/3:
# load dataset
df = pd.read_csv('data/mushrooms.csv')
df.shape
618/4:
### check target
df['class'].value_counts()/len(df)
### it's a balanced data set
618/5:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
618/6:
cat_columns = X_train.columns
X_train['class'] = y_train
X_train.head(5)
618/7:
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("countplots.pdf")
618/8:
for col in cat_columns:
    fig, ax = plt.subplots(figsize=(15, 8))
    ax = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen'],
                       ax=ax
                      )

    for p in ax.patches:
        ax.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
    pdf.savefig(fig)
    
### close file
pdf.close()
618/9:
### Encode target
target_dic = {'e':0, 'p': 1}

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)
618/10: df_result = pd.DataFrame(columns=['encoder' ,'Test acc', 'F1 score'])
618/11: X_train.drop('class', axis=1, inplace=True)
618/12:
encoder = OneHotEncoder(categories='auto',
                        drop='first', # to return k-1, use drop=false to return k dummies
                        sparse=False,
                        handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train)
618/13:
X_train_transform = pd.DataFrame(encoder.transform(X_train))
X_test_transform = pd.DataFrame(encoder.transform(X_test))
X_train_transform.columns = encoder.get_feature_names_out()
X_test_transform.columns = encoder.get_feature_names_out()

X_train_transform.shape, X_test_transform.shape
618/14: X_train_transform.head(5)
618/15:
def gaussian_model(encoder_name, X_train, X_test, y_train, y_test):
    global df_result
    # check shape
    if X_train.shape[1] == X_test.shape[1]:
        gnb = GaussianNB()
        gnb.fit(X_train, y_train)
        y_pred = gnb.predict(X_test)
        
        print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test, y_pred)*100:.2f}')
        print(f'F1 score: {f1_score(y_test, y_pred):.2f}')
        print(confusion_matrix(y_test, y_pred))
        
        df_result = df_result.append({
                              'encoder': encoder_name,
                              'Test acc': accuracy_score(y_test, y_pred)*100, 
                              'F1 score': f1_score(y_test, y_pred),
                             },
                               ignore_index=True
                            )
    else:
        print('The shape of train and test data set is not similar')
618/16: gaussian_model('One hot', X_train_transform, X_test_transform, y_train_encode, y_test_encode)
618/17: d = defaultdict(LabelEncoder)
618/18:
# Encoding the variable
X_train_label_encode = X_train.apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
X_test_label_encode = X_test.apply(lambda x: d[x.name].transform(x))
618/19: X_train_label_encode.describe()
618/20: gaussian_model('Label Encoding', X_train_label_encode, X_test_label_encode, y_train_encode, y_test_encode)
618/21: ord_enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.NaN)
618/22:
X_train_transform = ord_enc.fit_transform(X_train)
X_train_transform = pd.DataFrame(X_train_transform)
X_test_transform = pd.DataFrame.from_records(ord_enc.transform(X_test))
618/23:
X_train_transform.columns = X_train.columns
X_test_transform.columns = X_test.columns
618/24: X_train_transform.describe()
618/25: gaussian_model('Ordinal Encoding', X_train_label_encode, X_test_label_encode, y_train_encode, y_test_encode)
618/26:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()
618/27:
for col in X_train_transform.columns:
    frequency_map = (X_train_transform[col].value_counts() / len(X_train_transform) ).to_dict()
    X_train_transform[col] = X_train_transform[col].map(frequency_map)
    X_test_transform[col] = X_test_transform[col].map(frequency_map)
618/28: X_train_transform.describe()
618/29: gaussian_model('Count and Frequency', X_train_transform, X_test_transform, y_train_encode, y_test_encode)
618/30:
X_train['class'] = y_train_encode
for col in X_train.columns:  
    fig = plt.figure()
    fig = X_train.groupby([col])['class'].mean().plot()
    fig.set_title(f'Relationship between {col} and Poisonous')
    fig.set_ylabel('Mean poisonous')
    plt.show()
618/31:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()
618/32:
def find_category_mappings(df, variable, target):
    tmp = pd.DataFrame(df.groupby([variable])[target].mean())  
    tmp['non-target'] = 1 - tmp[target]  
    tmp['ratio'] = tmp[target] / tmp['non-target']
    return tmp['ratio'].to_dict()

def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
618/33:
for col in X_train_transform.columns.to_list()[:-1]:    
    mappings = find_category_mappings(X_train_transform, col, 'class')  
    integer_encode(X_train_transform, X_test_transform, col, mappings)
618/34:
X_train_transform.replace(np.inf, 1, inplace=True)
X_train_transform.drop('class', axis=1, inplace=True)
X_test_transform.replace(np.inf, 1, inplace=True)
618/35: X_train_transform.describe()
618/36: gaussian_model('Probability Ratio Encoding', X_train_transform, X_test_transform, y_train_encode, y_test_encode)
618/37:
X_train_transform['class'] = y_train_encode
for col in X_train_transform.columns:  
    fig = plt.figure()
    fig = X_train_transform.groupby([col])['class'].mean().plot()
    fig.set_title(f'Relationship between {col} and Poisonous')
    fig.set_ylabel('Mean poisonous')
    plt.show()
618/38: ### The function has a break at 1 when one of the feature classes contains only poisonous mushrooms
618/39:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()

X_train_transform['class'] = y_train_encode
618/40:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()


def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
618/41:
for col in cat_columns:
    mappings = find_category_mappings(X_train_transform, col, 'class')
    integer_encode(X_train_transform, X_test_transform, col, mappings)
618/42: X_train_transform.drop('class', axis=1, inplace=True)
618/43: gaussian_model('Mean Encoding', X_train_transform, X_test_transform, y_train_encode, y_test_encode)
618/44:
X_train_transform['class'] = y_train_encode
for col in X_train_transform.columns:  
    fig = plt.figure()
    fig = X_train_transform.groupby([col])['class'].mean().plot()
    fig.set_title(f'Relationship between {col} and Poisonous')
    fig.set_ylabel('Mean poisonous')
    plt.show()
618/45:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()

X_train_transform['class'] = y_train_encode
618/46:
def find_category_mappings(df, variable, target):
    df_copy = df.copy()
    
    # total positive class
    total_pos = df_copy[target].sum()
    
    # total negative class
    total_neg = len(df_copy) - df_copy[target].sum()

    # non target
    df_copy['non-target'] = 1 - df_copy[target]

    # % of positive class per category, respect to total positive class
    pos_perc = df_copy.groupby([variable])[target].sum() / total_pos

    # % of negative class per category, respect to total negative class
    neg_perc = df_copy.groupby([variable])['non-target'].sum() / total_neg

    # let's concatenate
    prob_tmp = pd.concat([pos_perc, neg_perc], axis=1)
    
    # let's calculate the Weight of Evidence
    prob_tmp['woe'] = np.log(prob_tmp[target]/prob_tmp['non-target'])
    return prob_tmp['woe'].to_dict()


def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
618/47:
for col in cat_columns:
    mappings = find_category_mappings(X_train_transform, col, 'class')
    integer_encode(X_train_transform, X_test_transform, col, mappings)
618/48:
X_train_transform.replace([np.inf, -np.inf], 0, inplace=True)
X_train_transform.drop('class', axis=1, inplace=True)
X_test_transform.replace([np.inf, -np.inf], 0, inplace=True)
618/49: X_train_transform.describe()
618/50: gaussian_model('Weight of evidence', X_train_transform, X_test_transform, y_train_encode, y_test_encode)
618/51:
X_train_transform['class'] = y_train_encode
for col in X_train_transform.columns:  
    fig = plt.figure()
    fig = X_train_transform.groupby([col])['class'].mean().plot()
    fig.set_title(f'Relationship between {col} and Poisonous')
    fig.set_ylabel('Mean poisonous')
    plt.show()
618/52: #The function has a break at 1 or 0 when one of the feature classes contains only poisonous mushrooms or edible
618/53: df_result.sort_values('F1 score', ascending=False)
618/54: import category_encoders as ce
618/55:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()

X_train_transform['class'] = y_train_encode
618/56:

encoder = ce.LeaveOneOutEncoder(return_df=True)
X_train_loo = encoder.fit_transform(X_train, y_train_encode)
X_train_loo.dtypes
618/57: X_test_loo = encoder.transform(X_test)
618/58: X_train
618/59:

encoder = ce.LeaveOneOutEncoder(return_df=True)
X_train_loo = encoder.fit_transform(X_train_transform, y_train_encode)
X_train_loo.dtypes
618/60: X_test_loo = encoder.transform(X_test)
618/61:

encoder = ce.LeaveOneOutEncoder(return_df=True)
X_train_loo = encoder.fit_transform(X_train_transform, y_train_encode)
X_train_loo.dtypes
618/62: X_test_loo = encoder.transform(X_test)
618/63: X_test_loo = encoder.transform(X_test_transform)
618/64: X_train_transform.drop('class', axis=1, inplace=True)
618/65:

encoder = ce.LeaveOneOutEncoder(return_df=True)
X_train_loo = encoder.fit_transform(X_train_transform, y_train_encode)
X_train_loo.dtypes
618/66: X_test_loo = encoder.transform(X_test_transform)
618/67: X_train_loo['cap-shape'].value_counts()
618/68: X_test_loo['cap-shape'].value_counts()
618/69: 0.720544*478 + 0.722054*185
618/70: (0.720544*478 + 0.722054*185) / (478+185)
618/71: (0.109244*319 + 0.106443*39) / (319+39)
618/72: X_train_transform
618/73:
for index, row in X_train_transform.iterrows():
    print(row['cap-shape'], row['bruises'])
618/74:
for index, row in X_train.iterrows():
    print(row['cap-shape'], row['bruises'])
618/75: X_train
618/76:
for index, row in X_train.iterrows():
    class_name = row['cap-shape']
    X_train[X_train['cap-shape'] == class_name]['class'].sum()
618/77:
for index, row in X_train.iterrows():
    print(row)
618/78: X_train.groupby('cap-shape')['class'].mean()
618/79: len(X_train)
618/80: 6499 * 0.493900
618/81: (6499 * 0.493900 )/ 6498
618/82: X_train.groupby('cap-shape')['class'].size()
618/83: (2541 * 0.493900 )/ 2540
618/84: X_train
618/85: X_train.groupby('cap-shape')
618/86:
X_train.groupby('cap-shape').agg({
                '_weighted_target_': 'sum',
                '_weight_': 'sum'
            })
618/87: X_train.groupby('cap-shape').agg({'w': 'sum', 'q': 'sum'})
618/88: X_train.groupby('cap-shape').agg({'class': 'sum', 'q': 'sum'})
618/89: X_train.groupby('cap-shape').agg({'class': 'sum', 'class': 'sum'})
618/90: X_train.groupby('cap-shape').agg({'class': 'sum', 'class': 'mean'})
618/91: X_train.groupby('cap-shape').agg({'class': ['sum', 'mean'], 'class': 'mean'})
618/92: X_train.groupby('cap-shape').agg({'class': ['sum', 'mean']})
618/93: X_train_loo
618/94: X_train_loo.groupby('cap-shape').size()
618/95: 0.108939*39/38
618/96: (0.108939*39)/38
618/97: (0.108939*39-1)/38
618/98: X_train
618/99: X_train.groupby('cap-shape').agg({'class': ['sum', 'mean']})
618/100: X_train.groupby('cap-shape').size()
618/101: X_train.groupby('cap-shape').agg({'class': ['sum', 'mean']})
618/102: X_train.groupby('cap-shape').agg({'class': ['count', 'sum', 'mean']})
618/103: (0.108939*358-1)/357
618/104: X_train.groupby('cap-shape').agg({'class': ['count', 'sum', 'mean']}).reset_index()
618/105: X_train.groupby('cap-shape').agg({'class': ['count', 'sum', 'mean']})
618/106: X_train.groupby('cap-shape').agg({'class': ['count', 'sum', 'mean']}).flat()
618/107: X_train.groupby('cap-shape').agg({'class': ['count', 'sum', 'mean']}).unstack(level=0)
618/108: X_train.groupby('cap-shape').agg({'class': ['count', 'sum', 'mean']}).unstack(level=-1)
618/109: X_train.groupby('cap-shape').agg({'class': ['count', 'sum', 'mean']}).unstack(level=1)
618/110: X_train.groupby('cap-shape').agg({'class': ['count', 'sum', 'mean']})
618/111: X_train.groupby('cap-shape').agg({'class': ['count', 'sum', 'mean']}).reset_index(axis=1)
618/112: X_train.groupby('cap-shape').agg({'class': ['count', 'sum', 'mean']}).reset_index(level=[0,1])
618/113: X_train.groupby('cap-shape').agg({'class': ['count', 'sum', 'mean']}).reset_index(level=[0])
618/114: X_train.groupby('cap-shape').agg({'class': ['count', 'sum', 'mean']}).reset_index(level=[1])
618/115: X_train.groupby('cap-shape').agg({'class': ['count', 'sum', 'mean']}).reset_index(level=[0])
618/116: X_train.groupby('cap-shape').agg({'class': ['count', 'sum', 'mean']}).reset_index(level=[0, 0])
618/117: X_train.groupby('cap-shape').agg({'class': ['count', 'sum', 'mean']}).unstack().reset_index()
618/118: X_train.groupby('cap-shape').agg({'class': ['count', 'sum', 'mean']}).droplevel()
618/119: X_train.groupby('cap-shape').agg({'class': ['count', 'sum', 'mean']}).droplevel(level=0)
618/120: X_train.groupby('cap-shape').agg({'class': ['count', 'sum', 'mean']}).reset_index()
618/121: d = X_train.groupby('cap-shape').agg({'class': ['count', 'sum', 'mean']}).reset_index()
618/122: d
618/123: d.columns
618/124: d = X_train.groupby('cap-shape').agg({'class': ['count', 'sum', 'mean']}).droplevel(axis=1, level=0).reset_index()
618/125: d
618/126: X_train.groupby('cap-shape').agg({'class': ['count', 'sum', 'mean']}).droplevel(axis=1, level=0)
618/127: d = X_train.groupby('cap-shape').agg({'class': ['count', 'sum', 'mean']}).droplevel(axis=1, level=0)
618/128: d
618/129: d['c']
618/130: d.iloc[['b']]
618/131: d.iloc['b']
618/132: d.loc['b']
618/133: d.loc['b']['sum']
618/134: d.loc['b']
618/135: X_train
618/136:
for i, row in X_train.iterrows():
    print(row)
618/137:
for i in X_train.index:
    print(i)
618/138:
for i in X_train.index:
    val = X_train.loc[i, 'cap-shape']
    print(val)
 #  if <something>:
 #      df.at[i, 'ifor'] = x
 #  else:
 #      df.at[i, 'ifor'] = y
618/139:
for i in X_train.index:
    val = X_train.loc[i, 'cap-shape']
    print(d.loc[val])
    print(val)
 #  if <something>:
 #      df.at[i, 'ifor'] = x
 #  else:
 #      df.at[i, 'ifor'] = y
618/140:
for i in X_train.index:
    val = X_train.loc[i, 'cap-shape']
    new_val = (d.loc[val]['mean']*d.loc[val]['count'] - X_train.loc[i, 'class']) / (d.loc[val]['count']-1)
    print(d.loc[val])
    print(new_val)
 #  if <something>:
 #      df.at[i, 'ifor'] = x
 #  else:
 #      df.at[i, 'ifor'] = y
618/141:
for i in X_train.index:
    val = X_train.loc[i, 'cap-shape']
    new_val = (d.loc[val]['mean']*d.loc[val]['count'] - X_train.loc[i, 'class']) / (d.loc[val]['count']-1)
    print(new_val)
 #  if <something>:
 #      df.at[i, 'ifor'] = x
 #  else:
 #      df.at[i, 'ifor'] = y
618/142: d = defaultdict(LabelEncoder)
618/143: d
618/144: d['cap-shape']
618/145:
# Encoding the variable
X_train_label_encode = X_train.apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
#X_test_label_encode = X_test.apply(lambda x: d[x.name].transform(x))
618/146: d['cap-shape']
618/147: d['cap-shape'].get_params)
618/148: d['cap-shape'].get_params()
618/149: d = defaultdict(LabelEncoder)
618/150:
# Encoding the variable
X_train_label_encode = X_train.apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
#X_test_label_encode = X_test.apply(lambda x: d[x.name].transform(x))
618/151: d['cap-shape'].get_params()
618/152: X_test.apply(lambda x: d[x.name].transform(x))
618/153: d['cap-shape'].get_params()
618/154: d['cap-shape']
618/155: d
618/156: new_dic = {}
618/157:
for i in X_train.index:
    val = X_train.loc[i, 'cap-shape']
    new_val = (d.loc[val]['mean']*d.loc[val]['count'] - X_train.loc[i, 'class']) / (d.loc[val]['count']-1)
    print(new_val)
    new_dic[val] += new_val
 #  if <something>:
 #      df.at[i, 'ifor'] = x
 #  else:
 #      df.at[i, 'ifor'] = y
618/158: d = X_train.groupby('cap-shape').agg({'class': ['count', 'sum', 'mean']}).droplevel(axis=1, level=0)
618/159:
for i in X_train.index:
    val = X_train.loc[i, 'cap-shape']
    new_val = (d.loc[val]['mean']*d.loc[val]['count'] - X_train.loc[i, 'class']) / (d.loc[val]['count']-1)
    print(new_val)
    new_dic[val] += new_val
 #  if <something>:
 #      df.at[i, 'ifor'] = x
 #  else:
 #      df.at[i, 'ifor'] = y
618/160:
for i in X_train.index:
    val = X_train.loc[i, 'cap-shape']
    new_val = (d.loc[val]['mean']*d.loc[val]['count'] - X_train.loc[i, 'class']) / (d.loc[val]['count']-1)
    print(new_val)
    new_dic[0] += new_val
 #  if <something>:
 #      df.at[i, 'ifor'] = x
 #  else:
 #      df.at[i, 'ifor'] = y
618/161:
for i in X_train.index:
    val = X_train.loc[i, 'cap-shape']
    new_val = (d.loc[val]['mean']*d.loc[val]['count'] - X_train.loc[i, 'class']) / (d.loc[val]['count']-1)
    print(new_val)
    new_dic[str(val)] += new_val
 #  if <something>:
 #      df.at[i, 'ifor'] = x
 #  else:
 #      df.at[i, 'ifor'] = y
618/162: new_dict=dict()
618/163:
for i in X_train.index:
    val = X_train.loc[i, 'cap-shape']
    new_val = (d.loc[val]['mean']*d.loc[val]['count'] - X_train.loc[i, 'class']) / (d.loc[val]['count']-1)
    print(new_val)
    new_dic[str(val)] += new_val
 #  if <something>:
 #      df.at[i, 'ifor'] = x
 #  else:
 #      df.at[i, 'ifor'] = y
618/164:
for i in X_train.index:
    val = X_train.loc[i, 'cap-shape']
    new_val = (d.loc[val]['mean']*d.loc[val]['count'] - X_train.loc[i, 'class']) / (d.loc[val]['count']-1)
    print(new_val)
    new_dic[str(val)] = new_val
 #  if <something>:
 #      df.at[i, 'ifor'] = x
 #  else:
 #      df.at[i, 'ifor'] = y
618/165:
for i in X_train.index:
    val = X_train.loc[i, 'cap-shape']
    new_val = (d.loc[val]['mean']*d.loc[val]['count'] - X_train.loc[i, 'class']) / (d.loc[val]['count']-1)
    print(new_val)
    new_dic[str(val)] = new_dic[str(val)] + new_val
 #  if <something>:
 #      df.at[i, 'ifor'] = x
 #  else:
 #      df.at[i, 'ifor'] = y
618/166: new_dict
618/167: new_dict=dict()
618/168:
for i in X_train.index:
    val = X_train.loc[i, 'cap-shape']
    new_val = (d.loc[val]['mean']*d.loc[val]['count'] - X_train.loc[i, 'class']) / (d.loc[val]['count']-1)
    print(new_val)
    new_dic[str(val)] = new_dic[str(val)] + new_val
 #  if <something>:
 #      df.at[i, 'ifor'] = x
 #  else:
 #      df.at[i, 'ifor'] = y
618/169: new_dict
618/170:
for i in X_train.index:
    val = X_train.loc[i, 'cap-shape']
    new_val = (d.loc[val]['mean']*d.loc[val]['count'] - X_train.loc[i, 'class']) / (d.loc[val]['count']-1)
    print(new_val)
    new_dic[str(val)] = new_val
 #  if <something>:
 #      df.at[i, 'ifor'] = x
 #  else:
 #      df.at[i, 'ifor'] = y
618/171: new_dict
618/172: new_dict = dict()
618/173:
for i in X_train.index:
    val = X_train.loc[i, 'cap-shape']
    new_val = (d.loc[val]['mean']*d.loc[val]['count'] - X_train.loc[i, 'class']) / (d.loc[val]['count']-1)
    print(new_val)
    new_dic[str(val)] = new_val
 #  if <something>:
 #      df.at[i, 'ifor'] = x
 #  else:
 #      df.at[i, 'ifor'] = y
618/174: new_dict
618/175:
for i in X_train.index:
    val = X_train.loc[i, 'cap-shape']
    new_val = (d.loc[val]['mean']*d.loc[val]['count'] - X_train.loc[i, 'class']) / (d.loc[val]['count']-1)
    print(new_val)
    new_dict[str(val)] = new_val
 #  if <something>:
 #      df.at[i, 'ifor'] = x
 #  else:
 #      df.at[i, 'ifor'] = y
618/176: new_dict
618/177: new_dict = dict()
618/178:
for i in X_train.index:
    val = X_train.loc[i, 'cap-shape']
    new_val = (d.loc[val]['mean']*d.loc[val]['count'] - X_train.loc[i, 'class']) / (d.loc[val]['count']-1)
    print(new_val)
    new_dict[str(val)] = new_dict[str(val)] + new_val
 #  if <something>:
 #      df.at[i, 'ifor'] = x
 #  else:
 #      df.at[i, 'ifor'] = y
618/179:
for i in X_train.index:
    val = X_train.loc[i, 'cap-shape']
    new_val = (d.loc[val]['mean']*d.loc[val]['count'] - X_train.loc[i, 'class']) / (d.loc[val]['count']-1)
    print(new_val)
    if new_dict[str(val)]:
        print('1')
    else:
        print('2')
    new_dict[str(val)] = new_dict[str(val)] + new_val
 #  if <something>:
 #      df.at[i, 'ifor'] = x
 #  else:
 #      df.at[i, 'ifor'] = y
618/180: new_dict = dict()
618/181:
for i in X_train.index:
    val = X_train.loc[i, 'cap-shape']
    new_val = (d.loc[val]['mean']*d.loc[val]['count'] - X_train.loc[i, 'class']) / (d.loc[val]['count']-1)
    print(new_val)
    if str(val) in new_dict:
        new_dict[str(val)] += new_val
    else:
        new_dict[str(val)] = new_val
    
 #  if <something>:
 #      df.at[i, 'ifor'] = x
 #  else:
 #      df.at[i, 'ifor'] = y
618/182: new_dict
618/183: new_dict/39
618/184: new_dict
618/185:
for i in X_train.index:
    val = X_train.loc[i, 'cap-shape']
    new_val = (d.loc[val]['mean']*d.loc[val]['count'] - X_train.loc[i, 'class']) / (d.loc[val]['count']-1)
    print(new_val)
    if str(val) in new_dict:
        new_dict[str(val)] += (new_val/d.loc[val]['count'])
    else:
        new_dict[str(val)] = new_val/d.loc[val]['count']
    
 #  if <something>:
 #      df.at[i, 'ifor'] = x
 #  else:
 #      df.at[i, 'ifor'] = y
618/186: new_dict = dict()
618/187:
for i in X_train.index:
    val = X_train.loc[i, 'cap-shape']
    new_val = (d.loc[val]['mean']*d.loc[val]['count'] - X_train.loc[i, 'class']) / (d.loc[val]['count']-1)
    print(new_val)
    if str(val) in new_dict:
        new_dict[str(val)] += (new_val/d.loc[val]['count'])
    else:
        new_dict[str(val)] = new_val/d.loc[val]['count']
    
 #  if <something>:
 #      df.at[i, 'ifor'] = x
 #  else:
 #      df.at[i, 'ifor'] = y
618/188: new_dict
591/2:
# step backward feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
618/189: X_train.groupby('cap-shape').agg({'class': ['count', 'sum', 'mean']}).droplevel(axis=1, level=0)
618/190: new_dict = dict()
618/191:
for i in X_train.index:
    val = X_train.loc[i, 'cap-shape']
    new_val = (d.loc[val]['mean']*d.loc[val]['count'] - X_train.loc[i, 'class']) / (d.loc[val]['count']-1)
    print(new_val)
    if str(val) in new_dict:
        new_dict[str(val)] += (new_val/d.loc[val]['count'])
    else:
        new_dict[str(val)] = new_val/d.loc[val]['count']
    
 #  if <something>:
 #      df.at[i, 'ifor'] = x
 #  else:
 #      df.at[i, 'ifor'] = y
618/192: new_dict
618/193:

encoder = ce.LeaveOneOutEncoder(return_df=True)
X_train_loo = encoder.fit_transform(X_train_transform, y_train_encode)
X_train_loo.dtypes
618/194: X_test_loo = encoder.transform(X_test_transform)
618/195:
def find_category_mappings(df, variable, target, return_df=True):
    dic_group = df.groupby(variable).agg({target: ['count', 'mean']}).droplevel(axis=1, level=0)    
    new_dict = dict()
    
    for i in df.index:
        val = df.loc[i, variable]
        new_val = (dic_group.loc[val]['mean']*dic_group.loc[val]['count'] - df.loc[i, 'class']) \
                    / (dic_group.loc[val]['count']-1)
        print(new_val)
    if str(val) in new_dict:
        new_dict[str(val)] += (new_val/d.loc[val]['count'])
    else:
        new_dict[str(val)] = new_val/d.loc[val]['count']
    
    return df.groupby([variable])[target].mean().to_dict()
618/196:
def find_category_mappings(df, variable, target, return_df=True):
    dic_group = df.groupby(variable).agg({target: ['count', 'mean']}).droplevel(axis=1, level=0)    
    new_dict = dict()
    
    for i in df.index:
        val = df.loc[i, variable]
        new_val = (dic_group.loc[val]['mean']*dic_group.loc[val]['count'] - df.loc[i, 'class']) \
                    / (dic_group.loc[val]['count']-1)
        print(new_val)
        if str(val) in new_dict:
            new_dict[str(val)] += (new_val/d.loc[val]['count'])
        else:
            new_dict[str(val)] = new_val/d.loc[val]['count']
    
    return new_dict
618/197: find_category_mappings(X_train, 'cap-shape', ' class')
618/198: X_train
618/199: find_category_mappings(X_train, 'cap-shape', 'class')
618/200:
def find_category_mappings(df, variable, target, return_df=False):
    dic_group = df.groupby(variable).agg({target: ['count', 'mean']}).droplevel(axis=1, level=0)    
    new_dict = dict()
    new_var_feature = []
    for i in df.index:
        val = df.loc[i, variable]
        new_val = (dic_group.loc[val]['mean']*dic_group.loc[val]['count'] - df.loc[i, 'class']) \
                    / (dic_group.loc[val]['count']-1)
        new_var_feature.append(new_val)
        if str(val) in new_dict:
            new_dict[str(val)] += (new_val/d.loc[val]['count'])
        else:
            new_dict[str(val)] = new_val/d.loc[val]['count']
    if return_df == True:
        df[variable] = new_var_feature
    return new_dict
618/201: find_category_mappings(X_train, 'cap-shape', 'class')
618/202: X_train
618/203: X_train.head(3)
618/204: find_category_mappings(X_train, 'cap-shape', 'class', True)
618/205: X_train.head(3)
618/206: X_train_loo
618/207: X_train.head(-3:)
618/208: X_train.head(-3)
618/209: X_train.head(-10:)
618/210: X_train.head(-10)
618/211: X_train.head()
618/212: X_train.iloc[:, -10:]
618/213: X_train.iloc[-10:, :]
618/214: X_train_loo.iloc[-10:, :]
618/215: X_train
618/216:
def leave_one_out_encode(train, test, variable, loo_mapping, return_df=True):
    if return_df==False:
        train[variable] = train[variable].map(loo_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
618/217:
for col in ['cap-surface', 'cap-color']:
    mappings = find_category_mappings(X_train, col, 'class', True)
    integer_encode(X_train, X_test, col, mappings)
618/218:
def find_category_mappings(df, variable, target, return_df=False):
    dic_group = df.groupby(variable).agg({target: ['count', 'mean']}).droplevel(axis=1, level=0)    
    new_dict = dict()
    new_var_feature = []
    for i in df.index:
        val = df.loc[i, variable]
        new_val = (dic_group.loc[val]['mean']*dic_group.loc[val]['count'] - df.loc[i, 'class']) \
                    / (dic_group.loc[val]['count']-1)
        new_var_feature.append(new_val)
        if str(val) in new_dict:
            new_dict[str(val)] += (new_val/dic_group.loc[val]['count'])
        else:
            new_dict[str(val)] = new_val/dic_group.loc[val]['count']
    if return_df == True:
        df[variable] = new_var_feature
    return new_dict
618/219:
for col in ['cap-surface', 'cap-color']:
    mappings = find_category_mappings(X_train, col, 'class', True)
    integer_encode(X_train, X_test, col, mappings)
618/220: X_train
618/221: X_test
618/222:
import numpy as np
import pandas as pd
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder
from collections import defaultdict
618/223: from sklearn.naive_bayes import GaussianNB
618/224:
# load dataset
df = pd.read_csv('data/mushrooms.csv')
df.shape
618/225:
### check target
df['class'].value_counts()/len(df)
### it's a balanced data set
618/226:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
618/227:
cat_columns = X_train.columns
X_train['class'] = y_train
X_train.head(5)
618/228:
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("countplots.pdf")
618/229:
for col in cat_columns:
    fig, ax = plt.subplots(figsize=(15, 8))
    ax = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen'],
                       ax=ax
                      )

    for p in ax.patches:
        ax.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
    pdf.savefig(fig)
    
### close file
pdf.close()
618/230:
### Encode target
target_dic = {'e':0, 'p': 1}

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)
618/231: df_result = pd.DataFrame(columns=['encoder' ,'Test acc', 'F1 score'])
618/232: X_train.drop('class', axis=1, inplace=True)
618/233:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()

X_train_transform['class'] = y_train_encode
618/234: X_train_transform
618/235:
def find_category_mappings(df, variable, target, return_df=False):
    dic_group = df.groupby(variable).agg({target: ['count', 'mean']}).droplevel(axis=1, level=0)    
    new_dict = dict()
    new_var_feature = []
    for i in df.index:
        val = df.loc[i, variable]
        new_val = (dic_group.loc[val]['mean']*dic_group.loc[val]['count'] - df.loc[i, 'class']) \
                    / (dic_group.loc[val]['count']-1)
        new_var_feature.append(new_val)
        if str(val) in new_dict:
            new_dict[str(val)] += (new_val/dic_group.loc[val]['count'])
        else:
            new_dict[str(val)] = new_val/dic_group.loc[val]['count']
    if return_df == True:
        df[variable] = new_var_feature
    return new_dict
618/236:
def leave_one_out_encode(train, test, variable, loo_mapping, return_df=True):
    if return_df==False:
        train[variable] = train[variable].map(loo_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
618/237:
for col in ['cap-surface', 'cap-color']:
    mappings = find_category_mappings(X_train_transform, col, 'class', True)
    integer_encode(X_train_transform, X_test_transform, col, mappings)
618/238: X_train_transform
618/239:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()

X_train_transform['class'] = y_train_encode
618/240:
def find_category_mappings(df, variable, target, return_df=False):
    global df
    dic_group = df.groupby(variable).agg({target: ['count', 'mean']}).droplevel(axis=1, level=0)    
    new_dict = dict()
    new_var_feature = []
    for i in df.index:
        val = df.loc[i, variable]
        new_val = (dic_group.loc[val]['mean']*dic_group.loc[val]['count'] - df.loc[i, 'class']) \
                    / (dic_group.loc[val]['count']-1)
        new_var_feature.append(new_val)
        if str(val) in new_dict:
            new_dict[str(val)] += (new_val/dic_group.loc[val]['count'])
        else:
            new_dict[str(val)] = new_val/dic_group.loc[val]['count']
    if return_df == True:
        df[variable] = new_var_feature
    return new_dict
618/241:
def find_category_mappings(df, variable, target, return_df=False):
    dic_group = df.groupby(variable).agg({target: ['count', 'mean']}).droplevel(axis=1, level=0)    
    new_dict = dict()
    new_var_feature = []
    for i in df.index:
        val = df.loc[i, variable]
        new_val = (dic_group.loc[val]['mean']*dic_group.loc[val]['count'] - df.loc[i, 'class']) \
                    / (dic_group.loc[val]['count']-1)
        new_var_feature.append(new_val)
        if str(val) in new_dict:
            new_dict[str(val)] += (new_val/dic_group.loc[val]['count'])
        else:
            new_dict[str(val)] = new_val/dic_group.loc[val]['count']
    if return_df == True:
        df[variable] = new_var_feature
    return new_dict, new_var_feature
618/242: find_category_mappings(X_train_transform, 'cap-surface', 'class')
618/243: find_category_mappings(X_train_transform, 'cap-surface', 'class')[0]
618/244: find_category_mappings(X_train_transform, 'cap-surface', 'class')[1]
618/245: X_train_transform
618/246:
def leave_one_out_encode(train, test, variable, loo_mapping, return_df=True):
    if return_df==False:
        train[variable] = train[variable].map(loo_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
618/247:
for col in ['cap-surface', 'cap-color']:
    mappings = find_category_mappings(X_train_transform, col, 'class', True)
    leave_one_out_encode(X_train_transform, X_test_transform, col, mappings)
618/248:
def find_category_mappings(df, variable, target, return_df=False):
    dic_group = df.groupby(variable).agg({target: ['count', 'mean']}).droplevel(axis=1, level=0)    
    new_dict = dict()
    new_var_feature = []
    for i in df.index:
        val = df.loc[i, variable]
        new_val = (dic_group.loc[val]['mean']*dic_group.loc[val]['count'] - df.loc[i, 'class']) \
                    / (dic_group.loc[val]['count']-1)
        new_var_feature.append(new_val)
        if str(val) in new_dict:
            new_dict[str(val)] += (new_val/dic_group.loc[val]['count'])
        else:
            new_dict[str(val)] = new_val/dic_group.loc[val]['count']
    if return_df == True:
        df[variable] = new_var_feature
    return new_dict
618/249: find_category_mappings(X_train_transform, 'cap-surface', 'class')[1]
618/250:
def leave_one_out_encode(train, test, variable, loo_mapping, return_df=True):
    if return_df==False:
        train[variable] = train[variable].map(loo_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
618/251:
for col in ['cap-surface', 'cap-color']:
    mappings = find_category_mappings(X_train_transform, col, 'class', True)
    leave_one_out_encode(X_train_transform, X_test_transform, col, mappings)
618/252:
def leave_one_out_encode(train, test, variable, loo_mapping, return_df=True):
    if return_df==False:
        train[variable] = train[variable].map(loo_mapping)
    test[variable] = test[variable].map(loo_mapping)
618/253:
for col in ['cap-surface', 'cap-color']:
    mappings = find_category_mappings(X_train_transform, col, 'class', True)
    leave_one_out_encode(X_train_transform, X_test_transform, col, mappings)
618/254: X_train_transform
618/255:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()

X_train_transform['class'] = y_train_encode
618/256:
def find_category_mappings(df, variable, target, return_df=False):
    dic_group = df.groupby(variable).agg({target: ['count', 'mean']}).droplevel(axis=1, level=0)    
    new_dict = dict()
    new_var_feature = []
    for i in df.index:
        val = df.loc[i, variable]
        new_val = (dic_group.loc[val]['mean']*dic_group.loc[val]['count'] - df.loc[i, 'class']) \
                    / (dic_group.loc[val]['count']-1)
        new_var_feature.append(new_val)
        if str(val) in new_dict:
            new_dict[str(val)] += (new_val/dic_group.loc[val]['count'])
        else:
            new_dict[str(val)] = new_val/dic_group.loc[val]['count']
    if return_df == True:
        df[variable] = new_var_feature
    return new_dict
618/257:
def leave_one_out_encode(train, test, variable, loo_mapping, return_df=True):
    if return_df==False:
        train[variable] = train[variable].map(loo_mapping)
    test[variable] = test[variable].map(loo_mapping)
618/258:
for col in ['cap-surface', 'cap-color']:
    mappings = find_category_mappings(X_train_transform, col, 'class', True)
    leave_one_out_encode(X_train_transform, X_test_transform, col, mappings)
618/259: X_train_transform
618/260: X_train_loo
618/261: X_test_transform
618/262: X_test_loo
618/263:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()

X_train_transform['class'] = y_train_encode
618/264:
def find_category_mappings(df, variable, target, return_df=False):
    dic_group = df.groupby(variable).agg({target: ['count', 'mean']}).droplevel(axis=1, level=0)    
    new_dict = dict()
    new_var_feature = []
    for i in df.index:
        val = df.loc[i, variable]
        new_val = (dic_group.loc[val]['mean']*dic_group.loc[val]['count'] - df.loc[i, 'class']) \
                    / (dic_group.loc[val]['count']-1)
        new_var_feature.append(new_val)
        if str(val) in new_dict:
            new_dict[str(val)] += (new_val/dic_group.loc[val]['count'])
        else:
            new_dict[str(val)] = new_val/dic_group.loc[val]['count']
    if return_df == True:
        df[variable] = new_var_feature
    return new_dict
618/265:
def leave_one_out_encode(train, test, variable, loo_mapping, return_df=True):
    if return_df==False:
        train[variable] = train[variable].map(loo_mapping)
    test[variable] = test[variable].map(loo_mapping)
618/266:
for col in cat_columns:
    mappings = find_category_mappings(X_train_transform, col, 'class', True)
    leave_one_out_encode(X_train_transform, X_test_transform, col, mappings)
618/267: X_train_transform
618/268: X_train_transform.drop('class', axis=1, inplace=True)
618/269: gaussian_model('Leave one out', X_train_transform, X_test_transform, y_train_encode, y_test_encode)
618/270: df_result.sort_values('F1 score', ascending=False)
620/1:
import numpy as np
import pandas as pd
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder
from collections import defaultdict
620/2: from sklearn.naive_bayes import GaussianNB
620/3:
# load dataset
df = pd.read_csv('data/mushrooms.csv')
df.shape
620/4:
### check target
df['class'].value_counts()/len(df)
### it's a balanced data set
620/5:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
620/6:
cat_columns = X_train.columns
X_train['class'] = y_train
X_train.head(5)
620/7:
plt.style.use('seaborn')
pdf = matplotlib.backends.backend_pdf.PdfPages("countplots.pdf")
620/8:
for col in cat_columns:
    fig, ax = plt.subplots(figsize=(15, 8))
    ax = sns.countplot(data=X_train, 
                       x=col,
                       hue='class',
                       order=df[col].value_counts().index,
                       palette=['tomato', 'seagreen'],
                       ax=ax
                      )

    for p in ax.patches:
        ax.annotate(format(p.get_height(), '.0f'),
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    xytext=(0, 9),
                    size=12,
                    textcoords='offset points'
                   )
        
    plt.legend(['Poisonous', 'Edible'], loc='upper right', fontsize=16)
    plt.xlabel(f'Types of the "{col}" feature', fontsize=14)
    plt.ylabel('Number of the Mushrooms', fontsize=12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.title(f'Distribution of the Mushrooms by their Classes vs {col}', fontsize=16)
    pdf.savefig(fig)
    
### close file
pdf.close()
620/9:
### Encode target
target_dic = {'e':0, 'p': 1}

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)
620/10: df_result = pd.DataFrame(columns=['encoder' ,'Test acc', 'F1 score'])
620/11: X_train.drop('class', axis=1, inplace=True)
620/12:
encoder = OneHotEncoder(categories='auto',
                        drop='first', # to return k-1, use drop=false to return k dummies
                        sparse=False,
                        handle_unknown='error') # helps deal with rare labels

encoder.fit(X_train)
620/13:
X_train_transform = pd.DataFrame(encoder.transform(X_train))
X_test_transform = pd.DataFrame(encoder.transform(X_test))
X_train_transform.columns = encoder.get_feature_names_out()
X_test_transform.columns = encoder.get_feature_names_out()

X_train_transform.shape, X_test_transform.shape
620/14: X_train_transform.head(5)
620/15:
def gaussian_model(encoder_name, X_train, X_test, y_train, y_test):
    global df_result
    # check shape
    if X_train.shape[1] == X_test.shape[1]:
        gnb = GaussianNB()
        gnb.fit(X_train, y_train)
        y_pred = gnb.predict(X_test)
        
        print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test, y_pred)*100:.2f}')
        print(f'F1 score: {f1_score(y_test, y_pred):.2f}')
        print(confusion_matrix(y_test, y_pred))
        
        df_result = df_result.append({
                              'encoder': encoder_name,
                              'Test acc': accuracy_score(y_test, y_pred)*100, 
                              'F1 score': f1_score(y_test, y_pred),
                             },
                               ignore_index=True
                            )
    else:
        print('The shape of train and test data set is not similar')
620/16: gaussian_model('One hot', X_train_transform, X_test_transform, y_train_encode, y_test_encode)
620/17: d = defaultdict(LabelEncoder)
620/18:
# Encoding the variable
X_train_label_encode = X_train.apply(lambda x: d[x.name].fit_transform(x))

# # Using the dictionary to encode future data
X_test_label_encode = X_test.apply(lambda x: d[x.name].transform(x))
620/19: X_train_label_encode.describe()
620/20: gaussian_model('Label Encoding', X_train_label_encode, X_test_label_encode, y_train_encode, y_test_encode)
620/21: ord_enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.NaN)
620/22:
X_train_transform = ord_enc.fit_transform(X_train)
X_train_transform = pd.DataFrame(X_train_transform)
X_test_transform = pd.DataFrame.from_records(ord_enc.transform(X_test))
620/23:
X_train_transform.columns = X_train.columns
X_test_transform.columns = X_test.columns
620/24: X_train_transform.describe()
620/25: gaussian_model('Ordinal Encoding', X_train_label_encode, X_test_label_encode, y_train_encode, y_test_encode)
620/26:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()
620/27:
for col in X_train_transform.columns:
    frequency_map = (X_train_transform[col].value_counts() / len(X_train_transform) ).to_dict()
    X_train_transform[col] = X_train_transform[col].map(frequency_map)
    X_test_transform[col] = X_test_transform[col].map(frequency_map)
620/28: X_train_transform.describe()
620/29: gaussian_model('Count and Frequency', X_train_transform, X_test_transform, y_train_encode, y_test_encode)
620/30:
X_train['class'] = y_train_encode
for col in X_train.columns:  
    fig = plt.figure()
    fig = X_train.groupby([col])['class'].mean().plot()
    fig.set_title(f'Relationship between {col} and Poisonous')
    fig.set_ylabel('Mean poisonous')
    plt.show()
620/31:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()
620/32:
def find_category_mappings(df, variable, target):
    tmp = pd.DataFrame(df.groupby([variable])[target].mean())  
    tmp['non-target'] = 1 - tmp[target]  
    tmp['ratio'] = tmp[target] / tmp['non-target']
    return tmp['ratio'].to_dict()

def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
620/33:
for col in X_train_transform.columns.to_list()[:-1]:    
    mappings = find_category_mappings(X_train_transform, col, 'class')  
    integer_encode(X_train_transform, X_test_transform, col, mappings)
620/34:
X_train_transform.replace(np.inf, 1, inplace=True)
X_train_transform.drop('class', axis=1, inplace=True)
X_test_transform.replace(np.inf, 1, inplace=True)
620/35: X_train_transform.describe()
620/36: gaussian_model('Probability Ratio Encoding', X_train_transform, X_test_transform, y_train_encode, y_test_encode)
620/37:
X_train_transform['class'] = y_train_encode
for col in X_train_transform.columns:  
    fig = plt.figure()
    fig = X_train_transform.groupby([col])['class'].mean().plot()
    fig.set_title(f'Relationship between {col} and Poisonous')
    fig.set_ylabel('Mean poisonous')
    plt.show()
620/38: ### The function has a break to 1 when one of the feature classes contains only poisonous mushrooms
620/39:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()

X_train_transform['class'] = y_train_encode
620/40:
def find_category_mappings(df, variable, target):
    return df.groupby([variable])[target].mean().to_dict()


def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
620/41:
for col in cat_columns:
    mappings = find_category_mappings(X_train_transform, col, 'class')
    integer_encode(X_train_transform, X_test_transform, col, mappings)
620/42: X_train_transform.drop('class', axis=1, inplace=True)
620/43: gaussian_model('Mean Encoding', X_train_transform, X_test_transform, y_train_encode, y_test_encode)
620/44:
X_train_transform['class'] = y_train_encode
for col in X_train_transform.columns:  
    fig = plt.figure()
    fig = X_train_transform.groupby([col])['class'].mean().plot()
    fig.set_title(f'Relationship between {col} and Poisonous')
    fig.set_ylabel('Mean poisonous')
    plt.show()
620/45:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()

X_train_transform['class'] = y_train_encode
620/46:
def find_category_mappings(df, variable, target):
    df_copy = df.copy()
    
    # total positive class
    total_pos = df_copy[target].sum()
    
    # total negative class
    total_neg = len(df_copy) - df_copy[target].sum()

    # non target
    df_copy['non-target'] = 1 - df_copy[target]

    # % of positive class per category, respect to total positive class
    pos_perc = df_copy.groupby([variable])[target].sum() / total_pos

    # % of negative class per category, respect to total negative class
    neg_perc = df_copy.groupby([variable])['non-target'].sum() / total_neg

    # let's concatenate
    prob_tmp = pd.concat([pos_perc, neg_perc], axis=1)
    
    # let's calculate the Weight of Evidence
    prob_tmp['woe'] = np.log(prob_tmp[target]/prob_tmp['non-target'])
    return prob_tmp['woe'].to_dict()


def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
620/47:
for col in cat_columns:
    mappings = find_category_mappings(X_train_transform, col, 'class')
    integer_encode(X_train_transform, X_test_transform, col, mappings)
620/48:
X_train_transform.replace([np.inf, -np.inf], 0, inplace=True)
X_train_transform.drop('class', axis=1, inplace=True)
X_test_transform.replace([np.inf, -np.inf], 0, inplace=True)
620/49: X_train_transform.describe()
620/50: gaussian_model('Weight of evidence', X_train_transform, X_test_transform, y_train_encode, y_test_encode)
620/51:
X_train_transform['class'] = y_train_encode
for col in X_train_transform.columns:  
    fig = plt.figure()
    fig = X_train_transform.groupby([col])['class'].mean().plot()
    fig.set_title(f'Relationship between {col} and Poisonous')
    fig.set_ylabel('Mean poisonous')
    plt.show()
620/52: #The function has a break to 1 or 0 when one of the feature classes contains only poisonous mushrooms or edible
620/53:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()

X_train_transform['class'] = y_train_encode
620/54:
def find_category_mappings(df, variable, target, return_df=False):
    dic_group = df.groupby(variable).agg({target: ['count', 'mean']}).droplevel(axis=1, level=0)    
    new_dict = dict()
    new_var_feature = []
    for i in df.index:
        val = df.loc[i, variable]
        new_val = (dic_group.loc[val]['mean']*dic_group.loc[val]['count'] - df.loc[i, 'class']) \
                    / (dic_group.loc[val]['count']-1)
        new_var_feature.append(new_val)
        if str(val) in new_dict:
            new_dict[str(val)] += (new_val/dic_group.loc[val]['count'])
        else:
            new_dict[str(val)] = new_val/dic_group.loc[val]['count']
    if return_df == True:
        df[variable] = new_var_feature
    return new_dict
620/55:
def leave_one_out_encode(train, test, variable, loo_mapping, return_df=True):
    if return_df==False:
        train[variable] = train[variable].map(loo_mapping)
    test[variable] = test[variable].map(loo_mapping)
620/56:
for col in cat_columns:
    mappings = find_category_mappings(X_train_transform, col, 'class', True)
    leave_one_out_encode(X_train_transform, X_test_transform, col, mappings)
620/57: X_train_transform.drop('class', axis=1, inplace=True)
620/58: gaussian_model('Leave one out', X_train_transform, X_test_transform, y_train_encode, y_test_encode)
620/59:
X_train_transform['class'] = y_train_encode
for col in X_train_transform.columns:  
    fig = plt.figure()
    fig = X_train_transform.groupby([col])['class'].mean().plot()
    fig.set_title(f'Relationship between {col} and Poisonous')
    fig.set_ylabel('Mean poisonous')
    plt.show()
620/60: X_train_transform
620/61:
X_train_transform['class'] = y_train_encode
for col in X_train_transform.columns:  
    fig = plt.figure()
    fig = X_train_transform.groupby([col])['class'].mean().plot()
    fig.set_title(f'Relationship between {col} and Poisonous')
    fig.set_ylabel('Mean poisonous')
    plt.show()
620/62: X_train_transform
620/63: X_train_transform.groupby(['cap-shape'])['class'].mean()
620/64:
X_test_transform['class'] = y_test_encode
for col in X_test_transform.columns:  
    fig = plt.figure()
    fig = X_test_transform.groupby([col])['class'].mean().plot()
    fig.set_title(f'Relationship between {col} and Poisonous')
    fig.set_ylabel('Mean poisonous')
    plt.show()
620/65: df_result.sort_values('F1 score', ascending=False)
620/66: df_result.sort_values('F1 score', ascending=False)
624/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import (
    cross_val_score,
    train_test_split,
    GridSearchCV,
)

from skopt import gp_minimize
from skopt.plots import plot_convergence, plot_gaussian_process
from skopt.space import Integer
from skopt.utils import use_named_args
626/1:
import numpy as np
import pandas as pd
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
626/2:
# load dataset
df = pd.read_csv('data/mushrooms.csv')
df.shape
626/3:
### check target
df['class'].value_counts()/len(df)
### it's a balanced data set
626/4:
import numpy as np
import pandas as pd
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

from sklearn.model_selection import train_test_split
626/5:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
626/6:
cat_columns = X_train.columns
X_train['class'] = y_train
X_train.head(5)
626/7:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()
626/8:
def find_category_mappings(df, variable, target):
    tmp = pd.DataFrame(df.groupby([variable])[target].mean())  
    tmp['non-target'] = 1 - tmp[target]  
    tmp['ratio'] = tmp[target] / tmp['non-target']
    return tmp['ratio'].to_dict()

def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
626/9:
for col in X_train_transform.columns.to_list()[:-1]:    
    mappings = find_category_mappings(X_train_transform, col, 'class')  
    integer_encode(X_train_transform, X_test_transform, col, mappings)
626/10: X_train_transform
626/11:
### Encode target
target_dic = {'e':0, 'p': 1}

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)
626/12:
cat_columns = X_train.columns
X_train['class'] = y_train
X_train.head(5)
627/1:
import numpy as np
import pandas as pd
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

from sklearn.model_selection import train_test_split
627/2:
# load dataset
df = pd.read_csv('data/mushrooms.csv')
df.shape
627/3:
### check target
df['class'].value_counts()/len(df)
### it's a balanced data set
627/4:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
627/5:
### Encode target
target_dic = {'e':0, 'p': 1}

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)
627/6:
cat_columns = X_train.columns
X_train['class'] = y_train_encode
X_train.head(5)
627/7:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()
627/8:
def find_category_mappings(df, variable, target):
    tmp = pd.DataFrame(df.groupby([variable])[target].mean())  
    tmp['non-target'] = 1 - tmp[target]  
    tmp['ratio'] = tmp[target] / tmp['non-target']
    return tmp['ratio'].to_dict()

def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
627/9:
for col in X_train_transform.columns.to_list()[:-1]:    
    mappings = find_category_mappings(X_train_transform, col, 'class')  
    integer_encode(X_train_transform, X_test_transform, col, mappings)
627/10:
X_train_transform.replace(np.inf, 1, inplace=True)
X_train_transform.drop('class', axis=1, inplace=True)
X_test_transform.replace(np.inf, 1, inplace=True)
627/11: X_train_transform
627/12: from sklearn.naive_bayes import GaussianNB
627/13:
gnb = GaussianNB()
y_pred = gnb.predict(X_test)
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
627/14:
gnb = GaussianNB()
gnb.fit(X_train, y_train_encode)
y_pred = gnb.predict(X_test)
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
627/15:
gnb = GaussianNB()
gnb.fit(X_train_transform, y_train_encode)
y_pred = gnb.predict(X_test_transform)
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
627/16:
import numpy as np
import pandas as pd
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score
627/17:
gnb = GaussianNB()
gnb.fit(X_train_transform, y_train_encode)
y_pred = gnb.predict(X_test_transform)
print(f'Gaussian Naive Bayes model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
627/18:
# determine the hyperparameter space

param_grid = [
    Integer(10, 120, name="n_estimators"),
    Real(0, 0.999, name="min_samples_split"),
    Integer(1, 5, name="max_depth"),
    Categorical(['deviance', 'exponential'], name="loss"),
]

# Scikit-optimize parameter grid is a list
type(param_grid)
627/19:
from skopt import gp_minimize
from skopt.plots import plot_convergence
from skopt.space import Real, Integer, Categorical
from skopt.utils import use_named_args
627/20:
# determine the hyperparameter space

param_grid = [
    Integer(10, 120, name="n_estimators"),
    Real(0, 0.999, name="min_samples_split"),
    Integer(1, 5, name="max_depth"),
    Categorical(['deviance', 'exponential'], name="loss"),
]

# Scikit-optimize parameter grid is a list
type(param_grid)
627/21:
# set up the gradient boosting classifier

gbm = GradientBoostingClassifier(random_state=0)
627/22:
from sklearn.ensemble import GradientBoostingClassifier

from skopt import gp_minimize
from skopt.plots import plot_convergence
from skopt.space import Real, Integer, Categorical
from skopt.utils import use_named_args
627/23:
# set up the gradient boosting classifier

gbm = GradientBoostingClassifier(random_state=0)
627/24:
# We design a function to maximize the accuracy, of a GBM,
# with cross-validation

# the decorator allows our objective function to receive the parameters as
# keyword arguments. This is a requirement of Scikit-Optimize.
@use_named_args(param_grid)
def objective(**params):
    
    # model with new parameters
    gbm.set_params(**params)

    # optimization function (hyperparam response function)
    value = np.mean(
        cross_val_score(
            gbm, 
            X_train,
            y_train,
            cv=3,
            n_jobs=-4,
            scoring='accuracy')
    )

    # negate because we need to minimize
    return -value
627/25:
# gp_minimize performs by default GP Optimization 
# using a Marten Kernel

gp_ = gp_minimize(
    objective, # the objective function to minimize
    param_grid, # the hyperparameter space
    n_initial_points=10, # the number of points to evaluate f(x) to start of
    acq_func='EI', # the acquisition function
    n_calls=50, # the number of subsequent evaluations of f(x)
    random_state=0, 
)
627/26:
import numpy as np
import pandas as pd
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score
627/27:
# gp_minimize performs by default GP Optimization 
# using a Marten Kernel

gp_ = gp_minimize(
    objective, # the objective function to minimize
    param_grid, # the hyperparameter space
    n_initial_points=10, # the number of points to evaluate f(x) to start of
    acq_func='EI', # the acquisition function
    n_calls=50, # the number of subsequent evaluations of f(x)
    random_state=0, 
)
627/28: X_train_transform
627/29:
# We design a function to maximize the accuracy, of a GBM,
# with cross-validation

# the decorator allows our objective function to receive the parameters as
# keyword arguments. This is a requirement of Scikit-Optimize.
@use_named_args(param_grid)
def objective(**params):
    
    # model with new parameters
    gbm.set_params(**params)

    # optimization function (hyperparam response function)
    value = np.mean(
        cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=3,
            n_jobs=-4,
            scoring='accuracy')
    )

    # negate because we need to minimize
    return -value
627/30:
# gp_minimize performs by default GP Optimization 
# using a Marten Kernel

gp_ = gp_minimize(
    objective, # the objective function to minimize
    param_grid, # the hyperparameter space
    n_initial_points=10, # the number of points to evaluate f(x) to start of
    acq_func='EI', # the acquisition function
    n_calls=50, # the number of subsequent evaluations of f(x)
    random_state=0, 
)
627/31:
# function value at the minimum.
# note that it is the negative of the accuracy

"Best score=%.4f" % gp_.fun
627/32:
print("""Best parameters:
=========================
- n_estimators=%d
- min_samples_split=%.6f
- max_depth=%d
- loss = %s""" % (gp_.x[0], 
                gp_.x[1],
                gp_.x[2],
                gp_.x[3]))
627/33:
# determine the hyperparameter space

param_grid = [
    Integer(10, 50, name="n_estimators"),
    Real(0, 0.999, name="min_samples_split"),
    Integer(1, 5, name="max_depth"),
    Categorical(['deviance', 'exponential'], name="loss"),
]

# Scikit-optimize parameter grid is a list
type(param_grid)
627/34:
# set up the gradient boosting classifier

gbm = GradientBoostingClassifier(random_state=0)
627/35:
# We design a function to maximize the accuracy, of a GBM,
# with cross-validation

# the decorator allows our objective function to receive the parameters as
# keyword arguments. This is a requirement of Scikit-Optimize.
@use_named_args(param_grid)
def objective(**params):
    
    # model with new parameters
    gbm.set_params(**params)

    # optimization function (hyperparam response function)
    value = np.mean(
        cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=3,
            n_jobs=-4,
            scoring='accuracy')
    )

    # negate because we need to minimize
    return -value
627/36:
# gp_minimize performs by default GP Optimization 
# using a Marten Kernel

gp_ = gp_minimize(
    objective, # the objective function to minimize
    param_grid, # the hyperparameter space
    n_initial_points=10, # the number of points to evaluate f(x) to start of
    acq_func='EI', # the acquisition function
    n_calls=50, # the number of subsequent evaluations of f(x)
    random_state=0, 
)
627/37:
# function value at the minimum.
# note that it is the negative of the accuracy

"Best score=%.4f" % gp_.fun
627/38:
print("""Best parameters:
=========================
- n_estimators=%d
- min_samples_split=%.6f
- max_depth=%d
- loss = %s""" % (gp_.x[0], 
                gp_.x[1],
                gp_.x[2],
                gp_.x[3]))
627/39: plot_convergence(gp_)
625/1: gp_.models
625/2:
# gp_minimize performs by default GP Optimization 
# using a Marten Kernel

gp_ = gp_minimize(
    objective, # the objective function to minimize
    param_grid, # the hyperparameter space
    n_initial_points=10, # the number of points to evaluate f(x) to start of
    acq_func='EI', # the acquisition function
    n_calls=50, # the number of subsequent evaluations of f(x)
    random_state=0, 
)
628/1:
import numpy as np
import pandas as pd

from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import cross_val_score, train_test_split

from skopt import gp_minimize
from skopt.plots import plot_convergence
from skopt.space import Real, Integer, Categorical
from skopt.utils import use_named_args
628/2:
# load dataset
breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y=True)
X = pd.DataFrame(breast_cancer_X)
y = pd.Series(breast_cancer_y).map({0:1, 1:0})

X.head()
628/3:
# the target:
# percentage of benign (0) and malign tumors (1)

y.value_counts() / len(y)
628/4:
# split dataset into a train and test set

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=0)

X_train.shape, X_test.shape
628/5:
# With Integer, we create a space of integers, sampled uniformly
# between the minimum and maximum indicated values

Integer(10, 120, name="n_estimators")
628/6:
# With Real, we create a space of real values, sampled uniformly
# between the minimum and maximum indicated values

Real(0, 0.999, name="min_samples_split")
628/7:
# With Categorical, we create a space of categories

Categorical(['deviance', 'exponential'], name="loss")
628/8:
# determine the hyperparameter space

param_grid = [
    Integer(10, 120, name="n_estimators"),
    Real(0, 0.999, name="min_samples_split"),
    Integer(1, 5, name="max_depth"),
    Categorical(['deviance', 'exponential'], name="loss"),
]

# Scikit-optimize parameter grid is a list
type(param_grid)
628/9:
# set up the gradient boosting classifier

gbm = GradientBoostingClassifier(random_state=0)
628/10:
# We design a function to maximize the accuracy, of a GBM,
# with cross-validation

# the decorator allows our objective function to receive the parameters as
# keyword arguments. This is a requirement of Scikit-Optimize.
@use_named_args(param_grid)
def objective(**params):
    
    # model with new parameters
    gbm.set_params(**params)

    # optimization function (hyperparam response function)
    value = np.mean(
        cross_val_score(
            gbm, 
            X_train,
            y_train,
            cv=3,
            n_jobs=-4,
            scoring='accuracy')
    )

    # negate because we need to minimize
    return -value
628/11:
# gp_minimize performs by default GP Optimization 
# using a Marten Kernel

gp_ = gp_minimize(
    objective, # the objective function to minimize
    param_grid, # the hyperparameter space
    n_initial_points=10, # the number of points to evaluate f(x) to start of
    acq_func='EI', # the acquisition function
    n_calls=50, # the number of subsequent evaluations of f(x)
    random_state=0, 
)
627/40: gp_.models
628/12: gp_
627/41: gp_
627/42:
plot_evaluations(result=gp_, plot_dims=dim_names)
plt.show()
627/43: gbm
627/44:
from sklearn.naive_bayes import GaussianNB
import pickle
627/45:
clf = GradientBoostingClassifier()
clf.fit(X_train_transform, y_train_encode)

y_pred = clf.predict(X_test)
confusion_matrix(y_test_encode, y_pred)
627/46: X_train_transform
627/47:
clf = GradientBoostingClassifier()
clf.fit(X_train_transform, y_train_encode)

y_pred = clf.predict(X_test_transform)
confusion_matrix(y_test_encode, y_pred)
627/48:
# determine the hyperparameter space
param_grid = [
    Integer(10, 50, name="n_estimators"),
    Real(0, 0.999, name="min_samples_split"),
    Integer(1, 5, name="max_depth"),
    Categorical(['deviance', 'exponential'], name="loss"),
]
627/49:
clf = GradientBoostingClassifier()
clf.fit(X_train_transform, y_train_encode)

y_pred = clf.predict(X_test_transform)
confusion_matrix(y_test_encode, y_pred)
627/50:
clf = GradientBoostingClassifier(n_estimators=10, max_depth=3)
clf.fit(X_train_transform, y_train_encode)

y_pred = clf.predict(X_test_transform)
confusion_matrix(y_test_encode, y_pred)
627/51:
clf = GradientBoostingClassifier(n_estimators=5, max_depth=3)
clf.fit(X_train_transform, y_train_encode)

y_pred = clf.predict(X_test_transform)
confusion_matrix(y_test_encode, y_pred)
627/52:
clf = GradientBoostingClassifier(n_estimators=5, max_depth=2)
clf.fit(X_train_transform, y_train_encode)

y_pred = clf.predict(X_test_transform)
confusion_matrix(y_test_encode, y_pred)
627/53:
clf = GradientBoostingClassifier(n_estimators=5, max_depth=2)
clf.fit(X_train_transform, y_train_encode)

print(f'Gradient Boosting Classifier model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
627/54:
with open('dump.pkl', 'wb') as f:
        pickle.dump(clf, f)
627/55: best_accuracy = 0
627/56:
# We design a function to maximize the accuracy, of a GBM,
# with cross-validation

# the decorator allows our objective function to receive the parameters as
# keyword arguments. This is a requirement of Scikit-Optimize.
@use_named_args(param_grid)
def objective(**params):
    
    # model with new parameters
    gbm.set_params(**params)

    # optimization function (hyperparam response function)
    value = np.mean(
        cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=10,
            scoring='accuracy')
    )

    global best_accuracy
    if value > best_accuracy:
        with open('dump.pkl', 'wb') as f:
            pickle.dump(gbm, f)
        
        # Update the classification accuracy.
        best_accuracy = value

    # Delete the Keras model with these hyper-parameters from memory.
 #   del gbm
    # negate because we need to minimize
    return -value
627/57:
# gp_minimize performs by default GP Optimization 
# using a Marten Kernel

gp_ = gp_minimize(
    objective, # the objective function to minimize
    param_grid, # the hyperparameter space
    n_initial_points=10, # the number of points to evaluate f(x) to start of
    acq_func='EI', # the acquisition function
    n_calls=50, # the number of subsequent evaluations of f(x)
    random_state=0, 
)
627/58: best_accuracy
627/59:
# function value at the minimum.
# note that it is the negative of the accuracy

"Best score=%.4f" % gp_.fun
627/60:
print("""Best parameters:
=========================
- n_estimators=%d
- min_samples_split=%.6f
- max_depth=%d
- loss = %s""" % (gp_.x[0], 
                gp_.x[1],
                gp_.x[2],
                gp_.x[3]))
627/61:
# determine the hyperparameter space
param_grid = [
    Integer(5, 20 name="n_estimators"),
    Real(0, 0.999, name="min_samples_split"),
    Integer(1, 5, name="max_depth"),
    Categorical(['deviance', 'exponential'], name="loss"),
]
627/62:
# determine the hyperparameter space
param_grid = [
    Integer(5, 20, name="n_estimators"),
    Real(0, 0.999, name="min_samples_split"),
    Integer(1, 5, name="max_depth"),
    Categorical(['deviance', 'exponential'], name="loss"),
]
627/63:
# set up the gradient boosting classifier
gbm = GradientBoostingClassifier(random_state=0)
627/64: best_accuracy = 0
627/65:
# We design a function to maximize the accuracy, of a GBM,
# with cross-validation

# the decorator allows our objective function to receive the parameters as
# keyword arguments. This is a requirement of Scikit-Optimize.
@use_named_args(param_grid)
def objective(**params):
    
    # model with new parameters
    gbm.set_params(**params)

    # optimization function (hyperparam response function)
    value = np.mean(
        cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=10,
            scoring='accuracy')
    )

    global best_accuracy
    if value > best_accuracy:
        with open('dump.pkl', 'wb') as f:
            pickle.dump(gbm, f)
        
        # Update the classification accuracy.
        best_accuracy = value

    # Delete the Keras model with these hyper-parameters from memory.
 #   del gbm
    # negate because we need to minimize
    return -value
627/66:
# gp_minimize performs by default GP Optimization 
# using a Marten Kernel

gp_ = gp_minimize(
    objective, # the objective function to minimize
    param_grid, # the hyperparameter space
    n_initial_points=10, # the number of points to evaluate f(x) to start of
    acq_func='EI', # the acquisition function
    n_calls=50, # the number of subsequent evaluations of f(x)
    random_state=0, 
)
627/67: best_accuracy_accuracy
627/68: best_accuracy
627/69:
# function value at the minimum.
# note that it is the negative of the accuracy

"Best score=%.4f" % gp_.fun
627/70:
# determine the hyperparameter space
param_grid = [
    Integer(5, 40, name="n_estimators"),
    Real(0, 0.999, name="min_samples_split"),
    Integer(1, 5, name="max_depth"),
    Categorical(['deviance', 'exponential'], name="loss"),
]
627/71:
# set up the gradient boosting classifier
gbm = GradientBoostingClassifier(random_state=0)
627/72: best_accuracy = 0
627/73:
# We design a function to maximize the accuracy, of a GBM,
# with cross-validation

# the decorator allows our objective function to receive the parameters as
# keyword arguments. This is a requirement of Scikit-Optimize.
@use_named_args(param_grid)
def objective(**params):
    
    # model with new parameters
    gbm.set_params(**params)

    # optimization function (hyperparam response function)
    value = np.mean(
        cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=10,
            scoring='accuracy')
    )

    global best_accuracy
    if value > best_accuracy:
        with open('dump.pkl', 'wb') as f:
            pickle.dump(gbm, f)
        
        # Update the classification accuracy.
        best_accuracy = value

    # Delete the Keras model with these hyper-parameters from memory.
 #   del gbm
    # negate because we need to minimize
    return -value
627/74:
# gp_minimize performs by default GP Optimization 
# using a Marten Kernel

gp_ = gp_minimize(
    objective, # the objective function to minimize
    param_grid, # the hyperparameter space
    n_initial_points=10, # the number of points to evaluate f(x) to start of
    acq_func='EI', # the acquisition function
    n_calls=50, # the number of subsequent evaluations of f(x)
    random_state=0, 
)
627/75: best_accuracy
627/76:
# function value at the minimum.
# note that it is the negative of the accuracy

"Best score=%.4f" % gp_.fun
627/77:
clf = GradientBoostingClassifier(n_estimators=5, max_depth=2)
clf.fit(X_train_transform, y_train_encode)
y_pred = clf.predict(X_test_transform)

print(f'Gradient Boosting Classifier model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
627/78:
print("""Best parameters:
=========================
- n_estimators=%d
- min_samples_split=%.6f
- max_depth=%d
- loss = %s""" % (gp_.x[0], 
                gp_.x[1],
                gp_.x[2],
                gp_.x[3]))
627/79: plot_convergence(gp_)
627/80: loaded_model = pickle.load(open('dump.pkl', 'rb'))
627/81: loaded_model
627/82: loaded_model.predict(X_train_transform)
627/83: loaded_model.score(X_train_transform, y_test)
627/84:
# We design a function to maximize the accuracy, of a GBM,
# with cross-validation

# the decorator allows our objective function to receive the parameters as
# keyword arguments. This is a requirement of Scikit-Optimize.
@use_named_args(param_grid)
def objective(**params):
    
    # model with new parameters
    gbm.set_params(**params)

    # optimization function (hyperparam response function)
    value = np.mean(
        cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=10,
            scoring='accuracy')
    )

    global best_accuracy
    if value > best_accuracy:
        pickle.dump(model, open('dump.pkl', 'wb'))
        
        # Update the classification accuracy.
        best_accuracy = value

    # Delete the Keras model with these hyper-parameters from memory.
 #   del gbm
    # negate because we need to minimize
    return -value
627/85:
# gp_minimize performs by default GP Optimization 
# using a Marten Kernel

gp_ = gp_minimize(
    objective, # the objective function to minimize
    param_grid, # the hyperparameter space
    n_initial_points=10, # the number of points to evaluate f(x) to start of
    acq_func='EI', # the acquisition function
    n_calls=50, # the number of subsequent evaluations of f(x)
    random_state=0, 
)
627/86: best_accuracy
627/87:
# function value at the minimum.
# note that it is the negative of the accuracy

"Best score=%.4f" % gp_.fun
627/88:
print("""Best parameters:
=========================
- n_estimators=%d
- min_samples_split=%.6f
- max_depth=%d
- loss = %s""" % (gp_.x[0], 
                gp_.x[1],
                gp_.x[2],
                gp_.x[3]))
627/89: plot_convergence(gp_)
627/90:
# We design a function to maximize the accuracy, of a GBM,
# with cross-validation

# the decorator allows our objective function to receive the parameters as
# keyword arguments. This is a requirement of Scikit-Optimize.
@use_named_args(param_grid)
def objective(**params):
    
    # model with new parameters
    gbm.set_params(**params)

    # optimization function (hyperparam response function)
    value = np.mean(
        cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=10,
            scoring='accuracy')
    )

    global best_accuracy
    if value > best_accuracy:
        gbm.fit(X_train_transform, y_train_encode)
        pickle.dump(model, open('dump.pkl', 'wb'))
        
        # Update the classification accuracy.
        best_accuracy = value

    # Delete the Keras model with these hyper-parameters from memory.
 #   del gbm
    # negate because we need to minimize
    return -value
627/91:
# gp_minimize performs by default GP Optimization 
# using a Marten Kernel

gp_ = gp_minimize(
    objective, # the objective function to minimize
    param_grid, # the hyperparameter space
    n_initial_points=10, # the number of points to evaluate f(x) to start of
    acq_func='EI', # the acquisition function
    n_calls=50, # the number of subsequent evaluations of f(x)
    random_state=0, 
)
627/92: best_accuracy
627/93:
# function value at the minimum.
# note that it is the negative of the accuracy

"Best score=%.4f" % gp_.fun
627/94:
print("""Best parameters:
=========================
- n_estimators=%d
- min_samples_split=%.6f
- max_depth=%d
- loss = %s""" % (gp_.x[0], 
                gp_.x[1],
                gp_.x[2],
                gp_.x[3]))
627/95: plot_convergence(gp_)
627/96:
loaded_model = pickle.load(open('dump.pkl', 'rb'))
y_pred = loaded_model.predict(X_test_transform)

print(f'Gradient Boosting Classifier model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
627/97:
loaded_model = pickle.load(open('dump.pkl', 'rb'))
y_pred = loaded_model.predict(X_test_transform)

print(f'Gradient Boosting Classifier model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
627/98:
# We design a function to maximize the accuracy, of a GBM,
# with cross-validation

# the decorator allows our objective function to receive the parameters as
# keyword arguments. This is a requirement of Scikit-Optimize.
@use_named_args(param_grid)
def objective(**params):
    
    # model with new parameters
    gbm.set_params(**params)

    # optimization function (hyperparam response function)
    value = np.mean(
        cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=10,
            scoring='accuracy')
    )

    global best_accuracy
    if value > best_accuracy:
        print('!')
        gbm.fit(X_train_transform, y_train_encode)
        pickle.dump(model, open('dump.pkl', 'wb'))
        
        # Update the classification accuracy.
        best_accuracy = value

    # Delete the Keras model with these hyper-parameters from memory.
 #   del gbm
    # negate because we need to minimize
    return -value
627/99:
# gp_minimize performs by default GP Optimization 
# using a Marten Kernel

gp_ = gp_minimize(
    objective, # the objective function to minimize
    param_grid, # the hyperparameter space
    n_initial_points=10, # the number of points to evaluate f(x) to start of
    acq_func='EI', # the acquisition function
    n_calls=50, # the number of subsequent evaluations of f(x)
    random_state=0, 
)
627/100: best_accuracy = 0
627/101:
# We design a function to maximize the accuracy, of a GBM,
# with cross-validation

# the decorator allows our objective function to receive the parameters as
# keyword arguments. This is a requirement of Scikit-Optimize.
@use_named_args(param_grid)
def objective(**params):
    
    # model with new parameters
    gbm.set_params(**params)

    # optimization function (hyperparam response function)
    value = np.mean(
        cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=10,
            scoring='accuracy')
    )

    global best_accuracy
    if value > best_accuracy:
        print('!')
        gbm.fit(X_train_transform, y_train_encode)
        pickle.dump(model, open('dump.pkl', 'wb'))
        
        # Update the classification accuracy.
        best_accuracy = value

    # Delete the Keras model with these hyper-parameters from memory.
 #   del gbm
    # negate because we need to minimize
    return -value
627/102:
# gp_minimize performs by default GP Optimization 
# using a Marten Kernel

gp_ = gp_minimize(
    objective, # the objective function to minimize
    param_grid, # the hyperparameter space
    n_initial_points=10, # the number of points to evaluate f(x) to start of
    acq_func='EI', # the acquisition function
    n_calls=50, # the number of subsequent evaluations of f(x)
    random_state=0, 
)
627/103: best_accuracy = 0
627/104:
# We design a function to maximize the accuracy, of a GBM,
# with cross-validation

# the decorator allows our objective function to receive the parameters as
# keyword arguments. This is a requirement of Scikit-Optimize.
@use_named_args(param_grid)
def objective(**params):
    
    # model with new parameters
    gbm.set_params(**params)

    # optimization function (hyperparam response function)
    value = np.mean(
        cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=10,
            scoring='accuracy')
    )

    global best_accuracy
    if value > best_accuracy:
        print('!')
        gbm.fit(X_train_transform, y_train_encode)
        pickle.dump(gbm, open('dump.pkl', 'wb'))
        
        # Update the classification accuracy.
        best_accuracy = value

    # Delete the Keras model with these hyper-parameters from memory.
 #   del gbm
    # negate because we need to minimize
    return -value
627/105:
# gp_minimize performs by default GP Optimization 
# using a Marten Kernel

gp_ = gp_minimize(
    objective, # the objective function to minimize
    param_grid, # the hyperparameter space
    n_initial_points=10, # the number of points to evaluate f(x) to start of
    acq_func='EI', # the acquisition function
    n_calls=50, # the number of subsequent evaluations of f(x)
    random_state=0, 
)
627/106: best_accuracy
627/107:
# function value at the minimum.
# note that it is the negative of the accuracy

"Best score=%.4f" % gp_.fun
627/108:
print("""Best parameters:
=========================
- n_estimators=%d
- min_samples_split=%.6f
- max_depth=%d
- loss = %s""" % (gp_.x[0], 
                gp_.x[1],
                gp_.x[2],
                gp_.x[3]))
627/109: plot_convergence(gp_)
627/110:
loaded_model = pickle.load(open('dump.pkl', 'rb'))
y_pred = loaded_model.predict(X_test_transform)

print(f'Gradient Boosting Classifier model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
630/1:
import numpy as np
import pandas as pd
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score
630/2:
from sklearn.ensemble import GradientBoostingClassifier
import pickle

from skopt import gp_minimize
from skopt.plots import plot_convergence
from skopt.space import Real, Integer, Categorical
from skopt.utils import use_named_args
630/3:
# load dataset
df = pd.read_csv('data/mushrooms.csv')
df.shape
630/4:
### check target
df['class'].value_counts()/len(df)
### it's a balanced data set
630/5:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
630/6:
### Encode target
target_dic = {'e':0, 'p': 1}

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)
630/7:
cat_columns = X_train.columns
X_train['class'] = y_train_encode
X_train.head(5)
630/8:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()
630/9:
def find_category_mappings(df, variable, target):
    tmp = pd.DataFrame(df.groupby([variable])[target].mean())  
    tmp['non-target'] = 1 - tmp[target]  
    tmp['ratio'] = tmp[target] / tmp['non-target']
    return tmp['ratio'].to_dict()

def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
630/10:
for col in X_train_transform.columns.to_list()[:-1]:    
    mappings = find_category_mappings(X_train_transform, col, 'class')  
    integer_encode(X_train_transform, X_test_transform, col, mappings)
630/11:
X_train_transform.replace(np.inf, 1, inplace=True)
X_train_transform.drop('class', axis=1, inplace=True)
X_test_transform.replace(np.inf, 1, inplace=True)
630/12: X_train_transform/head(5)
630/13: X_train_transform.head(5)
630/14:
clf = GradientBoostingClassifier(n_estimators=5, max_depth=2)
clf.fit(X_train_transform, y_train_encode)
y_pred = clf.predict(X_test_transform)

print(f'Gradient Boosting Classifier model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
630/15:
# determine the hyperparameter space
param_grid = [
    Integer(5, 40, name="n_estimators"),
    Real(0, 0.999, name="min_samples_split"),
    Integer(1, 5, name="max_depth"),
    Categorical(['deviance', 'exponential'], name="loss"),
]
630/16:
best_accuracy = 0
# Design a function to maximize the accuracy, of a GBM, with cross-validation
# The decorator allows the objective function to receive the parameters as
# keyword arguments. This is a requirement of Scikit-Optimize.
@use_named_args(param_grid)
def objective(**params):
    
    # model with new parameters
    gbm.set_params(**params)

    # optimization function (hyperparam response function)
    value = np.mean(
        cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=10,
            scoring='accuracy')
    )

    # save the best model
    global best_accuracy
    if value > best_accuracy:
        gbm.fit(X_train_transform, y_train_encode)
        pickle.dump(gbm, open('dump.pkl', 'wb'))
        
        # Update the classification accuracy.
        best_accuracy = value

    # negate because we need to minimize
    return -value
630/17:
# gp_minimize performs by default GP Optimization 
# using a Marten Kernel

gp_ = gp_minimize(
    objective, # the objective function to minimize
    param_grid, # the hyperparameter space
    n_initial_points=10, # the number of points to evaluate f(x) to start of
    acq_func='EI', # the acquisition function
    n_calls=50, # the number of subsequent evaluations of f(x)
    random_state=0, 
)
630/18:
# set up the gradient boosting classifier
gbm = GradientBoostingClassifier(random_state=0)
630/19:
# gp_minimize performs by default GP Optimization 
# using a Marten Kernel

gp_ = gp_minimize(
    objective, # the objective function to minimize
    param_grid, # the hyperparameter space
    n_initial_points=10, # the number of points to evaluate f(x) to start of
    acq_func='EI', # the acquisition function
    n_calls=50, # the number of subsequent evaluations of f(x)
    random_state=0, 
)
630/20:
# function value at the minimum.
print(f'Best score={gp_.fun:.2f}')
630/21:
# function value at the minimum.
print(f'Best score = {-gp_.fun:.2f}')
630/22:
print("""Best parameters:
=========================
- n_estimators=%d
- min_samples_split=%.6f
- max_depth=%d
- loss = %s""" % (gp_.x[0], 
                gp_.x[1],
                gp_.x[2],
                gp_.x[3]))
630/23:
print(f'Best parameters:\n
        n_estimators = {gp_.x[0]}\n
        min_samples_split = {gp_.x[1]}\n
        max_depth = {gp_.x[2]}\n
        loss = {gp_.x[3]}')
630/24:
print(f'Best parameters:\n
        n_estimators = {gp_.x[0]} \n
        min_samples_split = {gp_.x[1]}\n
        max_depth = {gp_.x[2]}\n
        loss = {gp_.x[3]}')
630/25:
print(f'Best parameters:/n
        n_estimators = {gp_.x[0]} \n
        min_samples_split = {gp_.x[1]}\n
        max_depth = {gp_.x[2]}\n
        loss = {gp_.x[3]}')
630/26:
print(f'Best parameters:\n
        n_estimators = {gp_.x[0]} \n
        min_samples_split = {gp_.x[1]}\n
        max_depth = {gp_.x[2]}\n
        loss = {gp_.x[3]}')
630/27:
print(f'Best parameters: \n
        n_estimators = {gp_.x[0]} \n
        min_samples_split = {gp_.x[1]}\n
        max_depth = {gp_.x[2]}\n
        loss = {gp_.x[3]}')
630/28:
print(f'Best parameters: \n/
        n_estimators = {gp_.x[0]} \n
        min_samples_split = {gp_.x[1]}\n
        max_depth = {gp_.x[2]}\n
        loss = {gp_.x[3]}')
630/29:
print(f'Best parameters: \n\
        n_estimators = {gp_.x[0]} \n
        min_samples_split = {gp_.x[1]}\n
        max_depth = {gp_.x[2]}\n
        loss = {gp_.x[3]}')
630/30:
print(f'Best parameters: \n\
        \n_estimators = {gp_.x[0]} \n
        min_samples_split = {gp_.x[1]}\n
        max_depth = {gp_.x[2]}\n
        loss = {gp_.x[3]}')
630/31:
print(f'Best parameters: \n\
        n_estimators = {gp_.x[0]} \n
        min_samples_split = {gp_.x[1]}\n
        max_depth = {gp_.x[2]}\n
        loss = {gp_.x[3]}')
630/32:
print(f'Best parameters: \n\
n_estimators = {gp_.x[0]} \n
        min_samples_split = {gp_.x[1]}\n
        max_depth = {gp_.x[2]}\n
        loss = {gp_.x[3]}')
630/33:
print(f'Best parameters: \
        n_estimators = {gp_.x[0]} \n
        min_samples_split = {gp_.x[1]}\n
        max_depth = {gp_.x[2]}\n
        loss = {gp_.x[3]}')
630/34:
print(f'Best parameters: \
n_estimators = {gp_.x[0]} \n
        min_samples_split = {gp_.x[1]}\n
        max_depth = {gp_.x[2]}\n
        loss = {gp_.x[3]}')
630/35:
print(f'Best parameters: \n n_estimators = {gp_.x[0]} \n
        min_samples_split = {gp_.x[1]}\n
        max_depth = {gp_.x[2]}\n
        loss = {gp_.x[3]}')
630/36:
print(f'Best parameters: \n n_estimators = {gp_.x[0]} \n min_samples_split = {gp_.x[1]}\n max_depth = {gp_.x[2]}\n
        loss = {gp_.x[3]}')
630/37:
print(f'Best parameters: \n n_estimators = {gp_.x[0]} \n \
min_samples_split = {gp_.x[1]}\n max_depth = {gp_.x[2]}\n
        loss = {gp_.x[3]}')
630/38:
print(f'Best parameters: \n n_estimators = {gp_.x[0]} \n \
\min_samples_split = {gp_.x[1]}\n max_depth = {gp_.x[2]}\n
        loss = {gp_.x[3]}')
630/39:
print(f'Best parameters: \n n_estimators = {gp_.x[0]} \n'\
'min_samples_split = {gp_.x[1]}\n max_depth = {gp_.x[2]}\n
        loss = {gp_.x[3]}')
630/40:
print(f'Best parameters: \n n_estimators = {gp_.x[0]} \n'\
      'min_samples_split = {gp_.x[1]}\n max_depth = {gp_.x[2]}\n
        loss = {gp_.x[3]}')
630/41:
print(f'Best parameters: \n n_estimators = {gp_.x[0]} \n'/
      'min_samples_split = {gp_.x[1]}\n max_depth = {gp_.x[2]}\n
        loss = {gp_.x[3]}')
630/42:
print(f'Best parameters: \n n_estimators = {gp_.x[0]} \n'/
      'min_samples_split = {gp_.x[1]}\n max_depth = {gp_.x[2]}\n loss = {gp_.x[3]}')
630/43:
print(f'Best parameters: \n')
print(f'n_estimators = {gp_.x[0]}\n')


n_estimators = {gp_.x[0]} \n'/
      'min_samples_split = {gp_.x[1]}\n max_depth = {gp_.x[2]}\n loss = {gp_.x[3]}')
630/44:
print(f'Best parameters:')
print(f'n_estimators = {gp_.x[0]}')
print(f'min_samples_split = {gp_.x[1]}')
print(f'max_depth = {gp_.x[2]}')
print(f'loss = {gp_.x[3]}')
630/45:
print(f'Best parameters:')
print(f'n_estimators = {gp_.x[0]}')
print(f'min_samples_split = {gp_.x[1]}')
print(f'max_depth = {gp_.x[2]:.4f}')
print(f'loss = {gp_.x[3]}')
630/46:
print(f'Best parameters:')
print(f'n_estimators = {gp_.x[0]}')
print(f'min_samples_split = {gp_.x[1]:.5f}')
print(f'max_depth = {gp_.x[2]:.1f}')
print(f'loss = {gp_.x[3]}')
630/47:
print(f'Best parameters:')
print(f'n_estimators = {gp_.x[0]}')
print(f'min_samples_split = {gp_.x[1]:.4f}')
print(f'max_depth = {gp_.x[2]:.1f}')
print(f'loss = {gp_.x[3]}')
630/48:
print(f'Best parameters:')
print(f'\tn_estimators = {gp_.x[0]}')
print(f'min_samples_split = {gp_.x[1]:.4f}')
print(f'max_depth = {gp_.x[2]:.1f}')
print(f'loss = {gp_.x[3]}')
630/49:
print(f'Best parameters:')
print(f'\t n_estimators = {gp_.x[0]}')
print(f'\t min_samples_split = {gp_.x[1]:.4f}')
print(f'\t max_depth = {gp_.x[2]:.1f}')
print(f'\t loss = {gp_.x[3]}')
630/50: plot_convergence(gp_)
630/51:
loaded_model = pickle.load(open('dump.pkl', 'rb'))
y_pred = loaded_model.predict(X_test_transform)

print(f'Gradient Boosting Classifier model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
630/52:
loaded_model = pickle.load(open('dump.pkl', 'rb'))
y_pred = loaded_model.predict(X_test_transform)

print(f'Gradient Boosting Classifier model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}%')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
630/53:
clf = GradientBoostingClassifier(n_estimators=5, max_depth=2)
clf.fit(X_train_transform, y_train_encode)
y_pred = clf.predict(X_test_transform)

print(f'Gradient Boosting Classifier model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}%')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
   1:
import numpy as np
import pandas as pd
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score
   2:
from sklearn.ensemble import GradientBoostingClassifier
import pickle

from skopt import gp_minimize
from skopt.plots import plot_convergence
from skopt.space import Real, Integer, Categorical
from skopt.utils import use_named_args
   3:
# load dataset
df = pd.read_csv('data/mushrooms.csv')
df.shape
   4:
### check target
df['class'].value_counts()/len(df)
### it's a balanced data set
   5:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
   6:
### Encode target
target_dic = {'e':0, 'p': 1}

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)
   7:
cat_columns = X_train.columns
X_train['class'] = y_train_encode
X_train.head(5)
   8:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()
   9:
def find_category_mappings(df, variable, target):
    tmp = pd.DataFrame(df.groupby([variable])[target].mean())  
    tmp['non-target'] = 1 - tmp[target]  
    tmp['ratio'] = tmp[target] / tmp['non-target']
    return tmp['ratio'].to_dict()

def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
  10:
for col in X_train_transform.columns.to_list()[:-1]:    
    mappings = find_category_mappings(X_train_transform, col, 'class')  
    integer_encode(X_train_transform, X_test_transform, col, mappings)
  11:
X_train_transform.replace(np.inf, 1, inplace=True)
X_train_transform.drop('class', axis=1, inplace=True)
X_test_transform.replace(np.inf, 1, inplace=True)
  12: X_train_transform.head(5)
  13:
clf = GradientBoostingClassifier(n_estimators=5, max_depth=2)
clf.fit(X_train_transform, y_train_encode)
y_pred = clf.predict(X_test_transform)

print(f'Gradient Boosting Classifier model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}%')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
  14:
# set up the gradient boosting classifier
gbm = GradientBoostingClassifier(random_state=0)
  15:
# determine the hyperparameter space
param_grid = [
    Integer(5, 40, name="n_estimators"),
    Real(0, 0.999, name="min_samples_split"),
    Integer(1, 5, name="max_depth"),
    Categorical(['deviance', 'exponential'], name="loss"),
]
  16:
best_accuracy = 0
# Design a function to maximize the accuracy, of a GBM, with cross-validation
# The decorator allows the objective function to receive the parameters as
# keyword arguments. This is a requirement of Scikit-Optimize.
@use_named_args(param_grid)
def objective(**params):
    
    # model with new parameters
    gbm.set_params(**params)

    # optimization function (hyperparam response function)
    value = np.mean(
        cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=10,
            scoring='accuracy')
    )

    # save the best model
    global best_accuracy
    if value > best_accuracy:
        gbm.fit(X_train_transform, y_train_encode)
        pickle.dump(gbm, open('dump.pkl', 'wb'))
        
        # Update the classification accuracy.
        best_accuracy = value

    # negate because we need to minimize
    return -value
  17:
# gp_minimize performs by default GP Optimization 
# using a Marten Kernel

gp_ = gp_minimize(
    objective, # the objective function to minimize
    param_grid, # the hyperparameter space
    n_initial_points=10, # the number of points to evaluate f(x) to start of
    acq_func='EI', # the acquisition function
    n_calls=50, # the number of subsequent evaluations of f(x)
    random_state=0, 
)
  18:
# function value at the minimum.
print(f'Best score = {-gp_.fun:.2f}')
  19:
print(f'Best parameters:')
print(f'\t n_estimators = {gp_.x[0]}')
print(f'\t min_samples_split = {gp_.x[1]:.4f}')
print(f'\t max_depth = {gp_.x[2]:.1f}')
print(f'\t loss = {gp_.x[3]}')
  20: plot_convergence(gp_)
  21:
loaded_model = pickle.load(open('dump.pkl', 'rb'))
y_pred = loaded_model.predict(X_test_transform)

print(f'Gradient Boosting Classifier model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}%')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
  22: print(loaded_model.feature_importances_)
  23: from xgboost import plot_importance
  24:
from xgboost import plot_importance
from matplotlib import pyplot
  25:
plot_importance(model)
pyplot.show()
  26:
plot_importance(loaded_model)
pyplot.show()
  27:
feat_imp = pd.Series(loaded_model.feature_importances_, predictors).sort_values(ascending=False)
feat_imp.plot(kind='bar', title='Feature Importances')
plt.ylabel('Feature Importance Score')
  28:
feat_imp = pd.Series(loaded_model.feature_importances_, X_train_transform.columns).sort_values(ascending=False)
feat_imp.plot(kind='bar', title='Feature Importances')
plt.ylabel('Feature Importance Score')
  29:
feat_imp = pd.Series(loaded_model.feature_importances_, X_train_transform.columns).sort_values(ascending=False)
feat_imp.plot(kind='bar', title='Feature Importances')
  30: df
  31: df.value_counts()
  32:
for col in df.columns:
    print(df[col].value_counts())
  33:
import numpy as np
import pandas as pd
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score
  34:
from sklearn.ensemble import GradientBoostingClassifier
import pickle

from skopt import gp_minimize
from skopt.plots import plot_convergence
from skopt.space import Real, Integer, Categorical
from skopt.utils import use_named_args
  35:
# load dataset
df = pd.read_csv('data/mushrooms.csv')
df.shape
  36: df.drop(['veil-type', 'odor'], axis=1, inplace=True)
  37:
### check target
df['class'].value_counts()/len(df)
### it's a balanced data set
  38:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
  39:
### Encode target
target_dic = {'e':0, 'p': 1}

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)
  40:
cat_columns = X_train.columns
X_train['class'] = y_train_encode
X_train.head(5)
  41:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()
  42:
def find_category_mappings(df, variable, target):
    tmp = pd.DataFrame(df.groupby([variable])[target].mean())  
    tmp['non-target'] = 1 - tmp[target]  
    tmp['ratio'] = tmp[target] / tmp['non-target']
    return tmp['ratio'].to_dict()

def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
  43:
for col in X_train_transform.columns.to_list()[:-1]:    
    mappings = find_category_mappings(X_train_transform, col, 'class')  
    integer_encode(X_train_transform, X_test_transform, col, mappings)
  44:
X_train_transform.replace(np.inf, 1, inplace=True)
X_train_transform.drop('class', axis=1, inplace=True)
X_test_transform.replace(np.inf, 1, inplace=True)
  45: X_train_transform.head(5)
  46:
clf = GradientBoostingClassifier(n_estimators=5, max_depth=2)
clf.fit(X_train_transform, y_train_encode)
y_pred = clf.predict(X_test_transform)

print(f'Gradient Boosting Classifier model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}%')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
  47:
# set up the gradient boosting classifier
gbm = GradientBoostingClassifier(random_state=0)
  48:
# determine the hyperparameter space
param_grid = [
    Integer(5, 50, name="n_estimators"),
    Real(0, 0.999, name="min_samples_split"),
    Integer(1, 5, name="max_depth"),
    Categorical(['deviance', 'exponential'], name="loss"),
]
  49:
best_accuracy = 0
# Design a function to maximize the accuracy, of a GBM, with cross-validation
# The decorator allows the objective function to receive the parameters as
# keyword arguments. This is a requirement of Scikit-Optimize.
@use_named_args(param_grid)
def objective(**params):
    
    # model with new parameters
    gbm.set_params(**params)

    # optimization function (hyperparam response function)
    value = np.mean(
        cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=10,
            scoring='accuracy')
    )

    # save the best model
    global best_accuracy
    if value > best_accuracy:
        gbm.fit(X_train_transform, y_train_encode)
        pickle.dump(gbm, open('dump.pkl', 'wb'))
        
        # Update the classification accuracy.
        best_accuracy = value

    # negate because we need to minimize
    return -value
  50:
# gp_minimize performs by default GP Optimization 
# using a Marten Kernel

gp_ = gp_minimize(
    objective, # the objective function to minimize
    param_grid, # the hyperparameter space
    n_initial_points=10, # the number of points to evaluate f(x) to start of
    acq_func='EI', # the acquisition function
    n_calls=50, # the number of subsequent evaluations of f(x)
    random_state=0, 
)
  51: X_train_transform
  52: X_train_transform.isna()
  53: X_train_transform.isna().sum()
  54: X_train_transform
  55: y_train_encode
  56: y_train_encode
  57:
# gp_minimize performs by default GP Optimization 
# using a Marten Kernel

gp_ = gp_minimize(
    objective, # the objective function to minimize
    param_grid, # the hyperparameter space
    n_initial_points=10, # the number of points to evaluate f(x) to start of
    acq_func='EI', # the acquisition function
    n_calls=50, # the number of subsequent evaluations of f(x)
    random_state=0, 
)
  58: X_train_transform
  59:
best_accuracy = 0
# Design a function to maximize the accuracy, of a GBM, with cross-validation
# The decorator allows the objective function to receive the parameters as
# keyword arguments. This is a requirement of Scikit-Optimize.
@use_named_args(param_grid)
def objective(**params):
    
    # model with new parameters
    gbm.set_params(**params)

    # optimization function (hyperparam response function)
    value = np.mean(
        cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=4,
            scoring='accuracy')
    )

    # save the best model
    
    # negate because we need to minimize
    return -value
  60:
# gp_minimize performs by default GP Optimization 
# using a Marten Kernel

gp_ = gp_minimize(
    objective, # the objective function to minimize
    param_grid, # the hyperparameter space
    n_initial_points=10, # the number of points to evaluate f(x) to start of
    acq_func='EI', # the acquisition function
    n_calls=50, # the number of subsequent evaluations of f(x)
    random_state=0, 
)
  61:
X_train_transform.replace(np.inf, 1, inplace=True)
X_train_transform.drop('class', axis=1, inplace=True)
X_test_transform.replace(np.inf, 1, inplace=True)
  62:
clf = GradientBoostingClassifier(n_estimators=5, max_depth=2)
clf.fit(X_train_transform, y_train_encode)
y_pred = clf.predict(X_test_transform)

print(f'Gradient Boosting Classifier model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}%')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
  63:
# set up the gradient boosting classifier
gbm = GradientBoostingClassifier(random_state=0)
  64:
# determine the hyperparameter space
param_grid = [
    Integer(5, 50, name="n_estimators"),
    Real(0, 0.999, name="min_samples_split"),
    Integer(1, 5, name="max_depth"),
    Categorical(['deviance', 'exponential'], name="loss"),
]
  65:
best_accuracy = 0
# Design a function to maximize the accuracy, of a GBM, with cross-validation
# The decorator allows the objective function to receive the parameters as
# keyword arguments. This is a requirement of Scikit-Optimize.
@use_named_args(param_grid)
def objective(**params):
    
    # model with new parameters
    gbm.set_params(**params)

    # optimization function (hyperparam response function)
    value = np.mean(
        cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=4,
            scoring='accuracy')
    )

    # save the best model
    global best_accuracy
    if value > best_accuracy:
        gbm.fit(X_train_transform, y_train_encode)
        pickle.dump(gbm, open('dump.pkl', 'wb'))
        
        # Update the classification accuracy.
        best_accuracy = value

    # negate because we need to minimize
    return -value
  66:
# gp_minimize performs by default GP Optimization 
# using a Marten Kernel

gp_ = gp_minimize(
    objective, # the objective function to minimize
    param_grid, # the hyperparameter space
    n_initial_points=10, # the number of points to evaluate f(x) to start of
    acq_func='EI', # the acquisition function
    n_calls=50, # the number of subsequent evaluations of f(x)
    random_state=0, 
)
  67:
cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=4,
            scoring='accuracy')
    )
  68:
cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=4,
            scoring='accuracy')
  69: X_train_transform
  70:
import numpy as np
import pandas as pd
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score
  71:
from sklearn.ensemble import GradientBoostingClassifier
import pickle

from skopt import gp_minimize
from skopt.plots import plot_convergence
from skopt.space import Real, Integer, Categorical
from skopt.utils import use_named_args
  72:
# load dataset
df = pd.read_csv('data/mushrooms.csv')
df.shape
  73:
### I dropped the veil-type feature because it's constant and odor feature 
### because it had the highest feature importance in the previous iteration
#df.drop(['veil-type', 'odor'], axis=1, inplace=True)
  74:
### check target
df['class'].value_counts()/len(df)
### it's a balanced data set
  75:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
  76:
### Encode target
target_dic = {'e':0, 'p': 1}

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)
  77:
cat_columns = X_train.columns
X_train['class'] = y_train_encode
X_train.head(5)
  78:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()
  79:
def find_category_mappings(df, variable, target):
    tmp = pd.DataFrame(df.groupby([variable])[target].mean())  
    tmp['non-target'] = 1 - tmp[target]  
    tmp['ratio'] = tmp[target] / tmp['non-target']
    return tmp['ratio'].to_dict()

def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
  80:
for col in X_train_transform.columns.to_list()[:-1]:    
    mappings = find_category_mappings(X_train_transform, col, 'class')  
    integer_encode(X_train_transform, X_test_transform, col, mappings)
  81:
X_train_transform.replace(np.inf, 1, inplace=True)
X_train_transform.drop('class', axis=1, inplace=True)
X_test_transform.replace(np.inf, 1, inplace=True)
  82: X_train_transform.head(5)
  83:
clf = GradientBoostingClassifier(n_estimators=5, max_depth=2)
clf.fit(X_train_transform, y_train_encode)
y_pred = clf.predict(X_test_transform)

print(f'Gradient Boosting Classifier model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}%')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
  84:
# set up the gradient boosting classifier
gbm = GradientBoostingClassifier(random_state=0)
  85:
# determine the hyperparameter space
param_grid = [
    Integer(5, 50, name="n_estimators"),
    Real(0, 0.999, name="min_samples_split"),
    Integer(1, 5, name="max_depth"),
    Categorical(['deviance', 'exponential'], name="loss"),
]
  86: X_train_transform
  87:
cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=4,
            scoring='accuracy')
  88:
# load dataset
df = pd.read_csv('data/mushrooms.csv')
df.shape
  89:
### I dropped the veil-type feature because it's constant and odor feature 
### because it had the highest feature importance in the previous iteration
df.drop(['veil-type', 'odor'], axis=1, inplace=True)
  90:
### check target
df['class'].value_counts()/len(df)
### it's a balanced data set
  91:
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(
    df.drop(labels=['class'], axis=1),  # drop the target
    df['class'],  # just the target
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape
  92:
### Encode target
target_dic = {'e':0, 'p': 1}

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)

y_train_encode = y_train.map(target_dic)
y_test_encode = y_test.map(target_dic)
  93:
cat_columns = X_train.columns
X_train['class'] = y_train_encode
X_train.head(5)
  94:
X_train_transform = X_train.copy()
X_test_transform = X_test.copy()
  95:
def find_category_mappings(df, variable, target):
    tmp = pd.DataFrame(df.groupby([variable])[target].mean())  
    tmp['non-target'] = 1 - tmp[target]  
    tmp['ratio'] = tmp[target] / tmp['non-target']
    return tmp['ratio'].to_dict()

def integer_encode(train, test, variable, ordinal_mapping):
    train[variable] = train[variable].map(ordinal_mapping)
    test[variable] = test[variable].map(ordinal_mapping)
  96:
for col in X_train_transform.columns.to_list()[:-1]:    
    mappings = find_category_mappings(X_train_transform, col, 'class')  
    integer_encode(X_train_transform, X_test_transform, col, mappings)
  97:
X_train_transform.replace(np.inf, 1, inplace=True)
X_train_transform.drop('class', axis=1, inplace=True)
X_test_transform.replace(np.inf, 1, inplace=True)
  98: X_train_transform.head(5)
  99:
clf = GradientBoostingClassifier(n_estimators=5, max_depth=2)
clf.fit(X_train_transform, y_train_encode)
y_pred = clf.predict(X_test_transform)

print(f'Gradient Boosting Classifier model accuracy: {accuracy_score(y_test_encode, y_pred)*100:.2f}%')
print(f'F1 score: {f1_score(y_test_encode, y_pred):.2f}')
print(confusion_matrix(y_test_encode, y_pred))
 100:
# set up the gradient boosting classifier
gbm = GradientBoostingClassifier(random_state=0)
 101:
# determine the hyperparameter space
param_grid = [
    Integer(5, 50, name="n_estimators"),
    Real(0, 0.999, name="min_samples_split"),
    Integer(1, 5, name="max_depth"),
    Categorical(['deviance', 'exponential'], name="loss"),
]
 102: X_train_transform
 103:
cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=4,
            scoring='accuracy')
 104:
best_accuracy = 0
# Design a function to maximize the accuracy, of a GBM, with cross-validation
# The decorator allows the objective function to receive the parameters as
# keyword arguments. This is a requirement of Scikit-Optimize.
@use_named_args(param_grid)
def objective(**params):
    
    # model with new parameters
    gbm.set_params(**params)

    # optimization function (hyperparam response function)
    value = np.mean(
        cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=4,
            scoring='accuracy')
    )

    # save the best model
    global best_accuracy
    if value > best_accuracy:
        gbm.fit(X_train_transform, y_train_encode)
        pickle.dump(gbm, open('dump.pkl', 'wb'))
        
        # Update the classification accuracy.
        best_accuracy = value

    # negate because we need to minimize
    return -value
 105:
# gp_minimize performs by default GP Optimization 
# using a Marten Kernel

gp_ = gp_minimize(
    objective, # the objective function to minimize
    param_grid, # the hyperparameter space
    n_initial_points=10, # the number of points to evaluate f(x) to start of
    acq_func='EI', # the acquisition function
    n_calls=50, # the number of subsequent evaluations of f(x)
    random_state=0, 
)
 106:
cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=4,
            scoring='accuracy')
 107: best_accuracy = 0
 108:
# Design a function to maximize the accuracy, of a GBM, with cross-validation
# The decorator allows the objective function to receive the parameters as
# keyword arguments. This is a requirement of Scikit-Optimize.
@use_named_args(param_grid)
def objective(**params):
    
    # model with new parameters
    gbm.set_params(**params)

    # optimization function (hyperparam response function)
    value = np.mean(
        cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=4,
            scoring='accuracy')
    )

    # save the best model
    global best_accuracy
    if value > best_accuracy:
        gbm.fit(X_train_transform, y_train_encode)
        pickle.dump(gbm, open('dump.pkl', 'wb'))
        
        # Update the classification accuracy.
        best_accuracy = value

    # negate because we need to minimize
    return -value
 109:
# gp_minimize performs by default GP Optimization 
# using a Marten Kernel

gp_ = gp_minimize(
    objective, # the objective function to minimize
    param_grid, # the hyperparameter space
    n_initial_points=10, # the number of points to evaluate f(x) to start of
    acq_func='EI', # the acquisition function
    n_calls=50, # the number of subsequent evaluations of f(x)
    random_state=0, 
)
 110:
cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=4,
            scoring='accuracy')
 111:
# set up the gradient boosting classifier
gbm = GradientBoostingClassifier(random_state=0)
 112:
cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=4,
            scoring='accuracy')
 113: best_accuracy = 0
 114:
# Design a function to maximize the accuracy, of a GBM, with cross-validation
# The decorator allows the objective function to receive the parameters as
# keyword arguments. This is a requirement of Scikit-Optimize.
@use_named_args(param_grid)
def objective(**params):
    
    # model with new parameters
    gbm.set_params(**params)

    # optimization function (hyperparam response function)
    value = np.mean(
        cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=4,
            scoring='accuracy')
    )

    # save the best model
    global best_accuracy
    if value > best_accuracy:
        gbm.fit(X_train_transform, y_train_encode)
        pickle.dump(gbm, open('dump.pkl', 'wb'))
        
        # Update the classification accuracy.
        best_accuracy = value

    # negate because we need to minimize
    return -value
 115:
# gp_minimize performs by default GP Optimization 
# using a Marten Kernel

gp_ = gp_minimize(
    objective, # the objective function to minimize
    param_grid, # the hyperparameter space
    n_initial_points=10, # the number of points to evaluate f(x) to start of
    acq_func='EI', # the acquisition function
    n_calls=50, # the number of subsequent evaluations of f(x)
    random_state=0, 
)
 116:
# set up the gradient boosting classifier
gbm = GradientBoostingClassifier(random_state=0)
 117:
# determine the hyperparameter space
param_grid = [
    Integer(5, 50, name="n_estimators"),
    Real(0, 0.999, name="min_samples_split"),
    Integer(1, 5, name="max_depth"),
    Categorical(['deviance', 'exponential'], name="loss"),
]
 118: X_train_transform
 119:
cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=4,
            scoring='accuracy')
 120: best_accuracy = 0
 121:
# Design a function to maximize the accuracy, of a GBM, with cross-validation
# The decorator allows the objective function to receive the parameters as
# keyword arguments. This is a requirement of Scikit-Optimize.
@use_named_args(param_grid)
def objective(**params):
    
    # model with new parameters
    gbm.set_params(**params)

    # optimization function (hyperparam response function)
    value = np.mean(
        cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=4,
            scoring='accuracy')
    )

    # save the best model
  

    # negate because we need to minimize
    return -value
 122:
# gp_minimize performs by default GP Optimization 
# using a Marten Kernel

gp_ = gp_minimize(
    objective, # the objective function to minimize
    param_grid, # the hyperparameter space
    n_initial_points=10, # the number of points to evaluate f(x) to start of
    acq_func='EI', # the acquisition function
    n_calls=50, # the number of subsequent evaluations of f(x)
    random_state=0, 
)
 123:
cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=4,
            scoring='accuracy')
 124:
# determine the hyperparameter space
param_grid = [
    Integer(5, 50, name="n_estimators"),
    Real(0, 0.999, name="min_samples_split"),
    Integer(1, 5, name="max_depth"),
    Categorical(['deviance', 'exponential'], name="loss"),
]
 125: X_train_transform
 126:
cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=4,
            scoring='accuracy')
 127:
# set up the gradient boosting classifier
gbm = GradientBoostingClassifier(random_state=0)
 128:
# determine the hyperparameter space
param_grid = [
    Integer(5, 50, name="n_estimators"),
    Real(0, 0.999, name="min_samples_split"),
    Integer(1, 5, name="max_depth"),
    Categorical(['deviance', 'exponential'], name="loss"),
]
 129:
cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=4,
            scoring='accuracy')
 130:
cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=4,
            scoring='accuracy')
 131:
cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=4,
            scoring='accuracy')
 132: best_accuracy = 1
 133:
# Design a function to maximize the accuracy, of a GBM, with cross-validation
# The decorator allows the objective function to receive the parameters as
# keyword arguments. This is a requirement of Scikit-Optimize.
@use_named_args(param_grid)
def objective(**params):
    
    # model with new parameters
    gbm.set_params(**params)

    # optimization function (hyperparam response function)
    value = np.mean(
        cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=4,
            scoring='accuracy')
    )

    # save the best model
    global best_accuracy
    if value > best_accuracy:
        gbm.fit(X_train_transform, y_train_encode)
        pickle.dump(gbm, open('dump.pkl', 'wb'))
        
        # Update the classification accuracy.
        best_accuracy = value

    # negate because we need to minimize
    return -value
 134:
# gp_minimize performs by default GP Optimization 
# using a Marten Kernel

gp_ = gp_minimize(
    objective, # the objective function to minimize
    param_grid, # the hyperparameter space
    n_initial_points=10, # the number of points to evaluate f(x) to start of
    acq_func='EI', # the acquisition function
    n_calls=50, # the number of subsequent evaluations of f(x)
    random_state=0, 
)
 135:
cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=4,
            scoring='accuracy')
 136:
# determine the hyperparameter space

param_grid = [
    Integer(5, 120, name="n_estimators"),
    Real(0.0,nnn9, name="min_sampvvv_split"),
    Integer(1, 5, name="max_deprrr),vars
    Categorical(['deviance', 'exddddntial'], name="loss"),
]

# Scikit-optimize parameter grid is a list
type(param_grid)
 137:
# determine the hyperparameter space

param_grid = [
    Integer(5, 50, name="n_estimators"),
    Real(0.0, 0.999, name="min_samples_split"),
    Integer(1, 5, name="max_depth"),
    Categorical(['deviance', 'exponential'], name="loss"),
]

# Scikit-optimize parameter grid is a list
type(param_grid)
 138:
cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=4,
            scoring='accuracy')
 139:
# set up the gradient boosting classifier
gbm = GradientBoostingClassifier(random_state=0)
 140:
# determine the hyperparameter space

param_grid = [
    Integer(5, 50, name="n_estimators"),
    Real(0.0, 0.999, name="min_samples_split"),
    Integer(1, 5, name="max_depth"),
    Categorical(['deviance', 'exponential'], name="loss"),
]

# Scikit-optimize parameter grid is a list
type(param_grid)
 141:
cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=4,
            scoring='accuracy')
 142: best_accuracy = 1
 143:
# Design a function to maximize the accuracy, of a GBM, with cross-validation
# The decorator allows the objective function to receive the parameters as
# keyword arguments. This is a requirement of Scikit-Optimize.
@use_named_args(param_grid)
def objective(**params):
    
    # model with new parameters
    gbm.set_params(**params)

    # optimization function (hyperparam response function)
    value = np.mean(
        cross_val_score(
            gbm, 
            X_train_transform,
            y_train_encode,
            cv=4,
            scoring='accuracy')
    )

    # save the best model
    global best_accuracy
    if value > best_accuracy:
        gbm.fit(X_train_transform, y_train_encode)
        pickle.dump(gbm, open('dump.pkl', 'wb'))
        
        # Update the classification accuracy.
        best_accuracy = value

    # negate because we need to minimize
    return -value
 144:
# gp_minimize performs by default GP Optimization 
# using a Marten Kernel

gp_ = gp_minimize(
    objective, # the objective function to minimize
    param_grid, # the hyperparameter space
    n_initial_points=10, # the number of points to evaluate f(x) to start of
    acq_func='EI', # the acquisition function
    n_calls=50, # the number of subsequent evaluations of f(x)
    random_state=0, 
)
 145: %history -g -f filename
